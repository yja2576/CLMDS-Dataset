[
    {
        "title": "FiLM: Visual Reasoning with a General Conditioning Layer",
        "abstract": "We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers inﬂuence neural network computation via a simple, feature-wise afﬁne transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning — answering image-related questions which require a multi-step, high-level process — a task which has proven difﬁcult for standard deep learning methods that do not explicitly model reasoning. Speciﬁcally, we show on visual reasoning tasks that FiLM layers 1) halve state-of-theart error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modiﬁcations, and 4) generalize well to challenging, new data from few examples or even zero-shot.",
        "introduction": "The ability to reason about everyday visual input is a fundamental building block of human intelligence. Some have\nargued that for artiﬁcial agents to learn this complex, structured process, it is necessary to build in aspects of reasoning, such as compositionality (Hu et al. 2017; Johnson et\nal. 2017b) or relational computation (Santoro et al. 2017).\nHowever, if a model made from general-purpose components could learn to visually reason, such an architecture\nwould likely be more widely applicable across domains.\nTo understand if such a general-purpose architecture exists, we take advantage of the recently proposed CLEVR\ndataset (Johnson et al. 2017a) that tests visual reasoning via\nquestion answering. Examples from CLEVR are shown in\nFigure 1. Visual question answering, the general task of asking questions about images, has its own line of datasets (Malinowski and Fritz 2014; Geman et al. 2015; Antol et al.\n2015) which generally focus on asking a diverse set of\nsimpler questions on images, often answerable in a single\nglance. From these datasets, a number of effective, generalpurpose deep learning models have emerged for visual question answering (Malinowski, Rohrbach, and Fritz 2015;\nYang et al. 2016; Lu et al. 2016; Anderson et al. 2017). However, tests on CLEVR show that these general deep learning\napproaches struggle to learn structured, multi-step reasoning (Johnson et al. 2017a). In particular, these methods tend\n(a) Q: What number of\ncylinders\nare\nsmall\npurple things or yellow rubber\nthings? A: 2\n(b) Q: What color is the\nother object that is the same\nshape as the large brown\nmatte thing? A: Brown\nFigure 1: CLEVR examples and FiLM model answers.\nto exploit biases in the data rather than capture complex underlying structure behind reasoning (Goyal et al. 2017).\nIn this work, we show that a general model architecture\ncan achieve strong visual reasoning with a method we introduce as FiLM: Feature-wise Linear Modulation. A FiLM\nlayer carries out a simple, feature-wise afﬁne transformation\non a neural network’s intermediate features, conditioned on\nan arbitrary input. In the case of visual reasoning, FiLM layers enable a Recurrent Neural Network (RNN) over an input\nquestion to inﬂuence Convolutional Neural Network (CNN)\ncomputation over an image. This process adaptively and radically alters the CNN’s behavior as a function of the input\nquestion, allowing the overall model to carry out a variety\nof reasoning tasks, ranging from counting to comparing, for\nexample. FiLM can be thought of as a generalization of Conditional Normalization, which has proven highly successful\nfor image stylization (Dumoulin, Shlens, and Kudlur 2017;\nGhiasi et al. 2017; Huang and Belongie 2017), speech recognition (Kim, Song, and Bengio 2017), and visual question\nanswering (de Vries et al. 2017), demonstrating FiLM’s\nbroad applicability.\nIn this paper, which expands upon a shorter report (Perez\net al. 2017), our key contribution is that we show FiLM is\na strong conditioning method by showing the following on\nvisual reasoning tasks:\n1. FiLM models achieve state-of-the-art across a variety of\nvisual reasoning tasks, often by signiﬁcant margins.\n2. FiLM operates in a coherent manner. It learns a complex,\nunderlying structure and manipulates the conditioned network’s features in a selective manner. It also enables the\ny\non Artificial Intelligence (AAAI-18)\nCNN to properly localize question-referenced objects.\n3. FiLM is robust; many FiLM model ablations still outperform prior state-of-the-art. Notably, we ﬁnd there is no\nclose link between normalization and the success of a conditioned afﬁne transformation, a previously untouched assumption. Thus, we relax the conditions under which this\nmethod can be applied.\n4. FiLM models learn from little data to generalize to more\ncomplex and/or substantially different data than seen during training. We also introduce a novel FiLM-based zeroshot generalization method that further improves and validates FiLM’s generalization capabilities.",
        "method": "Our model processes the question-image input using FiLM,\nillustrated in Figure 2. We start by explaining FiLM and then\ndescribe our particular model for visual reasoning.\n2.1\nFeature-wise Linear Modulation\nFiLM learns to adaptively inﬂuence the output of a neural\nnetwork by applying an afﬁne transformation, or FiLM, to\nthe network’s intermediate features, based on some input.\nMore formally, FiLM learns functions f and h which output\nγi,c and βi,c as a function of input xi:\nγi,c = fc(xi)\nβi,c = hc(xi),\n(1)\nwhere γi,c and βi,c modulate a neural network’s activations\nFi,c, whose subscripts refer to the ith input’s cth feature or\nfeature map, via a feature-wise afﬁne transformation:\nFiLM(Fi,c|γi,c, βi,c) = γi,cFi,c + βi,c.\n(2)\nf and h can be arbitrary functions such as neural networks.\nModulation of a target neural network’s processing can be\nbased on the same input to that neural network or some other\ninput, as in the case of multi-modal or conditional tasks. For\nCNNs, f and h thus modulate the per-feature-map distribution of activations based on xi, agnostic to spatial location.\nIn practice, it is easier to refer to f and h as a single function that outputs one (γ, β) vector, since, for example, it\nis often beneﬁcial to share parameters across f and h for\nmore efﬁcient learning. We refer to this single function as\nthe FiLM generator. We also refer to the network to which\nFiLM layers are applied as the Feature-wise Linearly Modulated network, the FiLM-ed network.\nFiLM layers empower the FiLM generator to manipulate\nfeature maps of a target, FiLM-ed network by scaling them\nup or down, negating them, shutting them off, selectively\nthresholding them (when followed by a ReLU), and more.\nEach feature map is conditioned independently, giving the\nFiLM generator moderately ﬁne-grained control over activations at each FiLM layer.\nAs FiLM only requires two parameters per modulated feature map, it is a scalable and computationally efﬁcient conditioning method. In particular, FiLM has a computational\ncost that does not scale with the image resolution.\nFigure 2: A single FiLM layer for a CNN. The dot signiﬁes\na Hadamard product. Various combinations of γ and β can\nmodulate individual feature maps in a variety of ways.\n2.2\nModel\nOur FiLM model consists of a FiLM-generating linguistic pipeline and a FiLM-ed visual pipeline as depicted in\nFigure 3. The FiLM generator processes a question xi using a Gated Recurrent Unit (GRU) network (Chung et al.\n2014) with 4096 hidden units that takes in learned, 200dimensional word embeddings. The ﬁnal GRU hidden state\nis a question embedding, from which the model predicts\n(γn\ni,·, βn\ni,·) for each nth residual block via afﬁne projection.\nThe visual pipeline extracts 128 14 × 14 image feature\nmaps from a resized, 224 × 224 image input using either\na CNN trained from scratch or a ﬁxed, pre-trained feature\nextractor with a learned layer of 3 × 3 convolutions. The\nCNN trained from scratch consists of 4 layers with 128 4 ×\n4 kernels each, ReLU activations, and batch normalization,\nsimilar to prior work on CLEVR (Santoro et al. 2017). The\nﬁxed feature extractor outputs the conv4 layer of a ResNet101 (He et al. 2016) pre-trained on ImageNet (Russakovsky\net al. 2015) to match prior work on CLEVR (Johnson et al.\n2017a; 2017b). Image features are processed by several —\n4 for our model — FiLM-ed residual blocks (ResBlocks)\nwith 128 feature maps and a ﬁnal classiﬁer. The classiﬁer\nconsists of a 1 × 1 convolution to 512 feature maps, global\nmax-pooling, and a two-layer MLP with 1024 hidden units\nthat outputs a softmax distribution over ﬁnal answers.\nEach FiLM-ed ResBlock starts with a 1 × 1 convolution followed by one 3 × 3 convolution with an architecture as depicted in Figure 3. We turn the parameters of batch\nnormalization layers that immediately precede FiLM layers\noff. Drawing from prior work on CLEVR (Hu et al. 2017;\nSantoro et al. 2017) and visual reasoning (Watters et al.\n2017), we concatenate two coordinate feature maps indicating relative x and y spatial position (scaled from −1 to\n1) with the image features, each ResBlock’s input, and the\nclassiﬁer’s input to facilitate spatial reasoning.\nWe train our model end-to-end from scratch with\nFigure 3: The FiLM generator (left), FiLM-ed network (middle), and residual block architecture (right) of our model.\nAdam (Kingma and Ba 2015) (learning rate 3e−4), weight\ndecay (1e−5), batch size 64, and batch normalization and\nReLU throughout FiLM-ed network. Our model uses only\nimage-question-answer triplets from the training set without data augmentation. We employ early stopping based on\nvalidation accuracy, training for 80 epochs maximum. Empirically, we found FiLM had a large capacity, so many architectural and hyperparameter choices were for added regularization.\nWe stress that our model relies solely on feature-wise\nafﬁne conditioning to use question information inﬂuence the\nvisual pipeline behavior to answer questions. This approach\ndiffers from classical visual question answering pipelines\nwhich fuse image and language information into a single\nembedding via element-wise product, concatenation, attention, and/or more advanced methods (Yang et al. 2016;\nLu et al. 2016; Anderson et al. 2017).",
        "related work": "FiLM can be viewed as a generalization of Conditional Normalization (CN) methods. CN replaces the parameters of the\nfeature-wise afﬁne transformation typical in normalization\nlayers, as introduced originally (Ioffe and Szegedy 2015),\nwith a learned function of some conditioning information.\nVarious forms of CN have proven highly effective across a\nnumber of domains: Conditional Instance Norm (Dumoulin,\nShlens, and Kudlur 2017; Ghiasi et al. 2017) and Adaptive\nInstance Norm (Huang and Belongie 2017) for image stylization, Dynamic Layer Norm for speech recognition (Kim,\nSong, and Bengio 2017), and Conditional Batch Norm for\ngeneral visual question answering on complex scenes such\nas VQA and GuessWhat?! (de Vries et al. 2017). This work\ncomplements our own, as we seek to show that feature-wise\nafﬁne conditioning is effective for multi-step reasoning and\nunderstand the underlying mechanism behind its success.\nNotably, prior work in CN has not examined whether\nthe afﬁne transformation must be placed directly after normalization. Rather, prior work includes normalization in the\nmethod name for instructive purposes or due to implementation details. We investigate the connection between FiLM\nand normalization, ﬁnding it not strictly necessary for the\nafﬁne transformation to occur directly after normalization.\nThus, we provide a uniﬁed framework for all of these methods through FiLM, as well as a normalization-free relaxation\nof this approach which can be more broadly applied.\nBeyond CN, there are many connections between FiLM\nand other conditioning methods. A common approach, used\nfor example in Conditional DCGANs (Radford, Metz, and\nChintala 2016), is to concatenate constant feature maps\nof conditioning information with convolutional layer input.\nThough not as parameter efﬁcient, this method simply results in a feature-wise conditional bias. Likewise, concatenating conditioning information with fully-connected layer\ninput amounts to a feature-wise conditional bias. Other approaches such as WaveNet (van den Oord et al. 2016a) and\nConditional PixelCNN (van den Oord et al. 2016b) directly\nadd a conditional feature-wise bias. These approaches are\nequivalent to FiLM with γ = 1, which we compare FiLM\nto in the Experiments section. In reinforcement learning,\nan alternate formulation of FiLM has been used to train\none game-conditioned deep Q-network to play ten Atari\ngames (Kirkpatrick et al. 2017), though FiLM was neither\nthe focus of this work nor analyzed as a major component.\nOther methods gate an input’s features as a function of\nthat same input, rather than a separate conditioning input. These methods include LSTMs for sequence modeling (Hochreiter and Schmidhuber 1997), Convolutional Sequence to Sequence for machine translation (Gehring et al.\n2017), and even the ImageNet 2017 winning model, Squeeze\nand Excitation Networks (Hu, Shen, and Sun 2017). This\napproach amounts to a feature-wise, conditional scaling, restricted to between 0 and 1, while FiLM consists of both\nscaling and shifting, each unrestricted. In the Experiments\nsection, we show the effect of restricting FiLM’s scaling to\nbetween 0 and 1 for visual reasoning. We ﬁnd it noteworthy\nthat this general approach of feature modulation is effective\nacross a variety of settings and architectures.\nThere are even broader links between FiLM and other\nmethods. For example, FiLM can be viewed as using one\nnetwork to generate parameters of another network, making it a form of hypernetwork (Ha, Dai, and Le 2016). Also,\nFiLM has potential ties with conditional computation and\nmixture of experts methods, where specialized network subparts are active on a per-example basis (Jordan and Jacobs\n1994; Eigen, Ranzato, and Sutskever 2014; Shazeer et al.\n2017); we later provide evidence that FiLM learns to selectively highlight or suppress feature maps based on conditioning information. Those methods select at a sub-network\nlevel while FiLM selects at a feature map level.\nIn the domain of visual reasoning, one leading method is\nthe Program Generator + Execution Engine model (Johnson et al. 2017b). This approach consists of a sequenceto-sequence Program Generator, which takes in a question\nModel\nOverall\nCount\nExist\nCompare\nNumbers\nQuery\nAttribute\nCompare\nAttribute\nHuman (Johnson et al. 2017b)\n92.6\n86.7\n96.6\n86.5\n95.0\n96.0\nQ-type baseline (Johnson et al. 2017b)\n41.8\n34.6\n50.2\n51.0\n36.0\n51.3\nLSTM (Johnson et al. 2017b)\n46.8\n41.7\n61.1\n69.8\n36.8\n51.8\nCNN+LSTM (Johnson et al. 2017b)\n52.3\n43.7\n65.2\n67.1\n49.3\n53.0\nCNN+LSTM+SA (Santoro et al. 2017)\n76.6\n64.4\n82.7\n77.4\n82.6\n75.4\nN2NMN* (Hu et al. 2017)\n83.7\n68.5\n85.7\n84.9\n90.0\n88.7\nPG+EE (9K prog.)* (Johnson et al. 2017b)\n88.6\n79.7\n89.7\n79.1\n92.6\n96.0\nPG+EE (700K prog.)* (Johnson et al. 2017b)\n96.9\n92.7\n97.1\n98.7\n98.1\n98.9\nCNN+LSTM+RN†‡ (Santoro et al. 2017)\n95.5\n90.1\n97.8\n93.6\n97.9\n97.1\nCNN+GRU+FiLM\n97.7\n94.3\n99.1\n96.8\n99.1\n99.1\nCNN+GRU+FiLM‡\n97.6\n94.3\n99.3\n93.4\n99.3\n99.3\nTable 1: CLEVR accuracy (overall and per-question-type) by baselines, competing methods, and FiLM. (*) denotes use of\nextra supervision via program labels. (†) denotes use of data augmentation. (‡) denotes training from raw pixels.\nand outputs a sequence corresponding to a tree of composable neural modules, each of which is a two or three layer\nresidual block. This tree of neural modules is assembled to\nform the Execution Engine that then predicts an answer from\nthe image. This modular approach is part of a line of neural module network methods (Andreas et al. 2016a; 2016b;\nHu et al. 2017), of which End-to-End Module Networks (Hu\net al. 2017) have also been tested on visual reasoning. These\nmodels use strong priors by explicitly modeling the compositional nature of reasoning and by training with additional\nprogram labels, i.e. ground-truth step-by-step instructions\non how to correctly answer a question. End-to-End Module Networks further build in model biases via per-module,\nhand-crafted neural architectures for speciﬁc functions. Our\napproach learns directly from visual and textual input without additional cues or a specialized architecture.\nRelation Networks (RNs) are another leading approach\nfor visual reasoning (Santoro et al. 2017). RNs succeed by\nexplicitly building in a comparison-based prior. RNs use an\nMLP to carry out pairwise comparisons over each location\nof extracted convolutional features over an image, including LSTM-extracted question features as input to this MLP.\nRNs then element-wise sum over the resulting comparison\nvectors to form another vector from which a ﬁnal classiﬁer predicts the answer. We note that RNs have a computational cost that scales quadratically in spatial resolution,\nwhile FiLM’s cost is independent of spatial resolution. Notably, since RNs concatenate question features with MLP input, a form of feature-wise conditional biasing as explained\nearlier, their conditioning approach is related to FiLM.",
        "experiments": "First, we test our model on visual reasoning with the CLEVR\ntask and use trained FiLM models to analyze what FiLM\nlearns. Second, we explore how well our model generalizes\nto more challenging questions with the CLEVR-Humans\ntask. Finally, we examine how FiLM performs in fewshot and zero-shot generalization settings using the CLEVR\nCompositional Generalization Test. Our code is available\nat https://github.com/ethanjperez/ﬁlm.\n4.1\nCLEVR Task\nCLEVR is a synthetic dataset of 700K (image, question, answer, program) tuples (Johnson et al. 2017a). Images contain 3D-rendered objects of various shapes, materials, colors, and sizes. Questions are multi-step and compositional\nin nature, as shown in Figure 1. They range from counting\nquestions (“How many green objects have the same size as\nthe green metallic block?”) to comparison questions (“Are\nthere fewer tiny yellow cylinders than yellow metal cubes?”)\nand can be 40+ words long. Answers are each one word from\na set of 28 possible answers. Programs are an additional\nsupervisory signal consisting of step-by-step instructions,\nsuch as filter shape[cube], relate[right], and\ncount, on how to answer the question.\nBaselines\nWe compare against the following methods, discussed in detail in the Related Work section:\n• Q-type baseline: Predicts based on a question’s category.\n• LSTM: Predicts using only the question.\n• CNN+LSTM: MLP prediction over CNN-extracted image features and LSTM-extracted question features.\n• Stacked Attention Networks (CNN+LSTM+SA): Linear prediction over CNN-extracted image feature and\nLSTM-extracted question features combined via two\nrounds of soft spatial attention (Yang et al. 2016).\n• End-to-End Module Networks (N2NMN) and Program Generator + Execution Engine (PG+EE): Methods in which separate neural networks learn separate subfunctions and are assembled into a question-dependent\nstructure (Hu et al. 2017; Johnson et al. 2017b).\n• Relation Networks (CNN+LSTM+RN): An approach\nwhich builds in pairwise comparisons over spatial locations to explicitly model reasoning’s relational nature (Santoro et al. 2017).\nResults\nFiLM achieves a new overall state-of-the-art on\nCLEVR, as shown in Table 1, outperforming humans and\nprevious methods, including those using explicit models of\nreasoning, program supervision, and/or data augmentation.\nQ: What shape is the...\n...purple thing? A: cube\n...blue thing? A: sphere\n...red thing right of the\nblue thing? A: sphere\n...red thing left of the\nblue thing? A: cube\nQ:\nHow\nmany\ncyan\nthings are...\n...right of the gray cube?\nA: 3\n...left of the small cube?\nA: 2\n...right of the gray cube\nand left of the small\ncube? A: 1\n...right of the gray cube\nor left of the small cube?\nA: 4 (P: 3)\nFigure 4: Visualizations of the distribution of locations which the model uses for its globally max-pooled features which its ﬁnal\nMLP predicts from. FiLM correctly localizes the answer-referenced object (top) or all question-referenced objects (bottom),\nbut not as accurately when it answers incorrectly (rightmost bottom). Questions and images used match (Johnson et al. 2017b).\nFor methods not using extra supervision, FiLM roughly\nhalves state-of-the-art error (from 4.5% to 2.3%). Note that\nusing pre-trained image features as input can be viewed as a\nform of data augmentation in itself but that FiLM performs\nequally well using raw pixel inputs. Interestingly, the raw\npixel model seems to perform better on lower-level questions (i.e. querying and comparing attributes) while the image features model seems to perform better on higher-level\nquestions (i.e. compare numbers of objects).\n4.2\nWhat Do FiLM Layers Learn?\nTo understand how FiLM visually reasons, we visualize activations to observe the net result of FiLM layers. We also use\nhistograms and t-SNE (van der Maaten and Hinton 2008) to\nﬁnd patterns in the learned FiLM γ and β themselves.\nActivation Visualizations\nFigure 4 visualizes the distribution of locations responsible for the globally-pooled features which the MLP in the model’s ﬁnal classiﬁer uses\nto predict answers. These images reveal that the FiLM\nmodel predicts using features of areas near answer-related\nor question-related objects, as the high CLEVR accuracy\nalso suggests. This ﬁnding highlights that appropriate feature modulation indirectly results in spatial modulation, as\nregions with question-relevant features will have large activations while other regions will not. This observation might\nexplain why FiLM outperforms Stacked Attention, the next\nbest method not explicitly built for reasoning, so signiﬁcantly (21%); FiLM appears to carry many of spatial attention’s beneﬁts, while also inﬂuencing feature representation.\nFigure 4 also suggests that the FiLM-ed network carries\nout reasoning throughout its pipeline. In the top example, the\nFiLM-ed network has localized the answer-referenced object alone before the MLP classiﬁer. In the bottom example,\nthe FiLM-ed network retains, for the MLP classiﬁer, features on objects that are not referred to by the answer but are\nreferred to by the question. The latter example provides evFigure 5: Histograms of γi,c (left) and βi,c (right) values\nover all FiLM layers, calculated over the validation set.\nidence that the ﬁnal MLP itself carries out some reasoning,\nusing FiLM to extract relevant features for its reasoning.\nFiLM Parameter Histograms\nTo analyze at a lower level\nhow FiLM uses the question to condition the visual pipeline,\nwe plot γ and β values predicted over the validation set, as\nshown in Figure 5. γ and β values take advantage of a sizable range, varying from -15 to 19 and from -9 to 16, respectively. γ values show a sharp peak at 0, showing that FiLM\nlearns to use the question to shut off or signiﬁcantly suppress whole feature maps. Simultaneously, FiLM learns to\nupregulate a much more selective set of other feature maps\nwith high magnitude γ values. Furthermore, a large fraction (36%) of γ values are negative; since our model uses\na ReLU after FiLM, γ < 0 can cause a signiﬁcantly different set of activations to pass the ReLU to downstream layers\nthan γ > 0. Also, 76% of β values are negative, suggesting that FiLM also uses β to be selective about which activations pass the ReLU. We show later that FiLM’s success\nis largely architecture-agnostic, but examining a particular\nmodel gives insight into the inﬂuence FiLM learns to exert\nin a speciﬁc case. Together, these ﬁndings suggest that FiLM\nlearns to selectively upregulate, downregulate, and shut off\nfeature maps based on conditioning information.\nFiLM Parameters t-SNE Plot\nIn Figure 6, we visualize\nFiLM parameter vectors (γ, β) for 3,000 random validaFigure 6: t-SNE plots of (γ, β) of the ﬁrst (left) and last (right) FiLM layers of a 6-FiLM layer Network. FiLM parameters\ncluster by low-level reasoning functions in the ﬁrst layer and by high-level reasoning functions in the last layer.\ntion points with t-SNE. We analyze the deeper, 6-ResBlock\nversion of our model, which has a similar validation accuracy as our 4-ResBlock model, to better examine how FiLM\nlayers in different layers of a hierarchy behave. First and\nlast layer FiLM (γ, β) are grouped by the low-level and\nhigh-level reasoning functions necessary to answer CLEVR\nquestions, respectively. For example, FiLM parameters for\nequal color and query color are close for the ﬁrst\nlayer but apart for the last layer. The same is true for shape,\nsize and material questions. Conversely, equal shape,\nequal size, and equal material FiLM parameters\nare grouped in the last layer but split in the ﬁrst layer — likewise for other high level groupings such as integer comparison and querying. These ﬁndings suggest that FiLM layers\nlearn a sort of function-based modularity without an architectural prior. Simply with end-to-end training, FiLM learns\nto handle not only different types of questions differently,\nbut also different types of question sub-parts differently; the\nFiLM model works from low-level to high-level processes\nas is the proper approach. For models with fewer FiLM layers, such patterns also appear, but less clearly; these models\nmust begin higher level reasoning sooner.\n4.3\nAblations\nUsing the validation set, we conduct an ablation study on our\nbest model to understand how FiLM learns visual reasoning.\nWe show results for test time ablations in Figure 7, for architectural ablations in Table 2, and for varied model depths in\nTable 3. Without hyperparameter tuning, most architectural\nablations and model depths outperform prior state-of-the-art\non training from only image-question-answer triplets, supporting FiLM’s overall robustness. Table 3 also shows using\nthe validation set that our results are statistically signiﬁcant.\nEffect of γ and β\nTo test the effect of γ and β separately,\nwe trained one model with a constant γ = 1 and another\nwith β = 0. With these models, we ﬁnd a 1.5% and .5%\naccuracy drop, respectively; FiLM can learn to condition the\nCNN for visual reasoning through either biasing or scaling\nalone, albeit not as well as conditioning both together. This\nFigure 7: An analysis of how robust FiLM parameters are to\nnoise at test time. The horizontal lines correspond to setting\nγ or β to their respective training set mean values.\nresult also suggests that γ is more important than β.\nTo further compare the importance of γ and β, we run\na series of test time ablations (Figure 7) on our best, fullytrained model. First, we replace β with the mean β across\nthe training set. This ablation in effect removes all conditioning information from β parameters during test time, from a\nmodel trained to use both γ and β. Here, we ﬁnd that accuracy only drops by 1.0%, while the same procedure on γ\nresults in a 65.4% drop. This large difference suggests that,\nin practice, FiLM largely conditions through γ rather than β.\nNext, we analyze performance as we add increasingly more\nGaussian noise to the best model’s FiLM parameters at test\ntime. Noise in gamma hurts performance signiﬁcantly more,\nshowing FiLM’s higher sensitivity to changes in γ than in β\nand corroborating the relatively greater importance of γ.\nRestricting γ\nTo understand what aspect of γ is most effective, we train a model that limits γ to (0, 1) using sigmoid, as many models which use feature-wise, multiplicative gating do. Likewise, we also limit γ to (−1, 1) using\ntanh. Both restrictions hurt performance, roughly as much\nas removing conditioning from γ entirely by training with\nγ = 1. Thus, FiLM’s ability to scale features by large magnitudes appears to contribute to its success. Limiting γ to\nModel\nOverall\nRestricted γ or β\nFiLM with β := 0\n96.9\nFiLM with γ := 1\n95.9\nFiLM with γ := σ(γ)\n95.9\nFiLM with γ := tanh(γ)\n96.3\nFiLM with γ := exp(γ)\n96.3\nMoving FiLM within ResBlock\nFiLM after residual connection\n96.6\nFiLM after ResBlock ReLU-2\n97.7\nFiLM after ResBlock Conv-2\n97.1\nFiLM before ResBlock Conv-1\n95.0\nRemoving FiLM from ResBlocks\nNo FiLM in ResBlock 4\n96.8\nNo FiLM in ResBlock 3-4\n96.5\nNo FiLM in ResBlock 2-4\n97.3\nNo FiLM in ResBlock 1-4\n21.4\nMiscellaneous\n1 × 1 conv only, with no coord. maps\n95.3\nNo residual connection\n94.0\nNo batch normalization\n93.7\nReplace image features with raw pixels\n97.6\nBest Architecture\n97.4±.4\nTable 2: CLEVR val accuracy for ablations, trained with the\nbest architecture with only speciﬁed changes. We report the\nstandard deviation of the best model accuracy over 5 runs.\n(0, ∞) with exp also hurts performance, validating the value\nof FiLM’s capacity to negate and zero out feature maps.\nConditional Normalization\nWe perform an ablation\nstudy on the placement of FiLM to evaluate the relationship between normalization and FiLM that Conditional Normalization approaches assume. Unfortunately, it is difﬁcult\nto accurately decouple the effect of FiLM from normalization by simply training our corresponding model without\nnormalization, as normalization signiﬁcantly accelerates,\nregularizes, and improves neural network learning (Ioffe\nand Szegedy 2015), but we include these results for completeness. However, we ﬁnd no substantial performance\ndrop when moving FiLM layers to different parts of our\nmodel’s ResBlocks; we even reach the upper end of the\nbest model’s performance range when placing FiLM after\nthe post-normalization ReLU in the ResBlocks. Thus, we\ndecouple the name from normalization for clarity regarding\nwhere the fundamental effectiveness of the method comes\nfrom. By demonstrating this conditioning mechanism is not\nclosely connected to normalization, we open the doors to applications other settings in which normalization is less common, such as RNNs and reinforcement learning, which are\npromising directions for future work with FiLM.\nRepetitive Conditioning\nTo understand the contribution\nof repetitive conditioning towards FiLM model success, we\ntrain FiLM models with successively fewer FiLM layers.\nModels with fewer FiLM layers, even a single FiLM layer,\nModel\nOverall\nModel\nOverall\n1 ResBlock\n93.5\n6 ResBlocks\n97.7\n2 ResBlocks\n97.1\n7 ResBlocks\n97.4\n3 ResBlocks\n96.7\n8 ResBlocks\n97.6\n4 ResBlocks\n97.4±.4\n12 ResBlocks\n96.9\n5 ResBlocks\n97.4\nTable 3: CLEVR val accuracy by FiLM model depth.\ndo not deviate far from the best model’s performance, revealing that the model can reason and answer diverse questions\nsuccessfully by modulating features even just once. This observation highlights the capacity of even one FiLM layer.\nPerhaps one FiLM layer can pass enough question information to the CNN to enable it to carry out reasoning later in\nthe network, in place of the more hierarchical conditioning\ndeeper FiLM models appear to use. We leave more in-depth\ninvestigation of this matter for future work.\nSpatial Reasoning\nTo examine how FiLM models approach spatial reasoning, we train a version of our best\nmodel architecture, from image features, with only 1 × 1\nconvolutions and without feeding coordinate feature maps\nindicating relative spatial position to the model. Due to the\nglobal max-pooling near the end of the model, this model\ncannot transfer information across spatial positions. Notably, this model still achieves a high 95.3% accuracy, indicating that FiLM models are able to reason about space\nsimply from the spatial information contained in a single location of ﬁxed image features.\nResidual Connection\nRemoving the residual connection\ncauses one of the larger accuracy drops. Since there is a\nglobal max-pooling operation near the end of the network,\nthis ﬁnding suggests that the best model learns to primarily use features of locations that are repeatedly important\nthroughout lower and higher levels of reasoning to make its\nﬁnal decision. The higher accuracies for models with FiLM\nmodulating features inside residual connections rather than\noutside residual connections supports this hypothesis.\nModel Depth\nTable 3 shows model performance by the\nnumber of ResBlocks. FiLM is robust to varying depth but\nless so with only 1 ResBlock, backing the earlier theory that\nthe FiLM-ed network reasons throughout its pipeline.\n4.4\nCLEVR-Humans: Human-Posed Questions\nTo assess how well visual reasoning models generalize\nto more realistic, complex, and free-form questions, the\nCLEVR-Humans dataset was introduced (Johnson et al.\n2017b). This dataset contains human-posed questions on\nCLEVR images along with their corresponding answers.\nThe number of samples is limited — 18K for training, 7K\nfor validation, and 7K for testing. The questions were collected from Amazon Mechanical Turk workers prompted to\nask questions that were likely hard for a smart robot to answer. As a result, CLEVR-Humans questions use more diverse vocabulary and complex concepts.\nQ: What object is the\ncolor\nof\ngrass?\nA:\nCylinder\nQ: Which shape objects\nare partially obscured\nfrom view? A: Sphere\nQ: What color is the\nmatte object farthest to\nthe right? A: Brown\nQ:\nWhat\nshape\nis\nreﬂecting in the large\ncube? A: Cylinder\nQ: If all cubical objects were removed what\nshaped\nobjects\nwould\nthere be the most of? A:\nSphere (P: Rubber)\nFigure 8: Examples from CLEVR-Humans, which introduces new words (underlined) and concepts. After ﬁne-tuning on\nCLEVR-Humans, a CLEVR-trained model can now reason about obstruction, superlatives, and reﬂections but still struggles\nwith hypothetical scenarios (rightmost). It also has learned human preference to primarily identify objects by shape (leftmost).\nTrain\nTrain CLEVR,\nModel\nCLEVR\nﬁne-tune human\nLSTM\n27.5\n36.5\nCNN+LSTM\n37.7\n43.2\nCNN+LSTM+SA+MLP\n50.4\n57.6\nPG+EE (18K prog.)\n54.0\n66.6\nCNN+GRU+FiLM\n56.6\n75.9\nTable 4: CLEVR-Humans test accuracy, before (left) and\nafter (right) ﬁne-tuning on CLEVR-Humans data\nMethod\nTo test FiLM on CLEVR-Humans, we take our\nbest CLEVR-trained FiLM model and ﬁne-tune its FiLMgenerating linguistic pipeline alone on CLEVR-Humans.\nSimilar to prior work (Johnson et al. 2017b), we do not\nupdate the visual pipeline on CLEVR-Humans to mitigate\noverﬁtting to the small training set.\nResults\nOur model achieves state-of-the-art generalization\nto CLEVR-Humans, both before and after ﬁne-tuning, as\nshown in Table 4, indicating that FiLM is well-suited to handle more complex and diverse questions. Figure 8 shows examples from CLEVR-Humans with FiLM model answers.\nBefore ﬁne-tuning, FiLM outperforms prior methods by a\nsmaller margin. After ﬁne-tuning, FiLM reaches a considerably improved ﬁnal accuracy. In particular, the gain in accuracy made by FiLM upon ﬁne-tuning is more than 50%\ngreater than those made by other models; FiLM adapts dataefﬁciently using the small CLEVR-Humans dataset.\nNotably,\nFiLM\nsurpasses\nthe\nprior\nstate-of-the-art\nmethod, Program Generator + Execution Engine (PG+EE),\nafter ﬁne-tuning by 9.3%. Prior work on PG+EEs explains\nthat this neural module network method struggles on questions which cannot be well approximated with the model’s\nmodule inventory (Johnson et al. 2017b). In contrast, FiLM\nhas the freedom to modulate existing feature maps, a fairly\nﬂexible and ﬁne-grained operation, in novel ways to reason\nabout new concepts. These results thus provide some evidence for the beneﬁts of FiLM’s general nature.\nTrain A\nFine-tune B\nMethod\nA\nB\nA\nB\nCNN+LSTM+SA\n80.3\n68.7\n75.7\n75.8\nPG+EE (18K prog.)\n96.6\n73.7\n76.1\n92.7\nCNN+GRU+FiLM\n98.3\n75.6\n80.8\n96.9\nCNN+GRU+FiLM 0-Shot\n98.3\n78.8\n81.1\n96.9\nFigure 9: CoGenT results. FiLM ValB accuracy reported on\nValB without the 30K ﬁne-tuning samples (Figure). Accuracy before and after ﬁne-tuning on 30K of ValB (Table).\n4.5\nCLEVR Compositional Generalization Test\nTo test how well models learn compositional concepts that\ngeneralize, CLEVR-CoGenT was introduced (Johnson et\nal. 2017a). This dataset is synthesized in the same way as\nCLEVR but contains two conditions: in Condition A, all\ncubes are gray, blue, brown, or yellow and all cylinders are\nred, green, purple, or cyan; in Condition B, cubes and cylinders swap color palettes. Both conditions contain spheres of\nall colors. CLEVR-CoGenT thus indicates how a model answers CLEVR questions: by memorizing combinations of\ntraits or by learning disentangled or general representations.\nResults\nWe train our best model architecture on Condition\nA and report accuracies on Conditions A and B, before and\nafter ﬁne-tuning on B, in Figure 9. Our results indicate FiLM\nsurpasses other visual reasoning models at learning general\nconcepts. FiLM learns better compositional generalization\neven than PG+EE, which explicitly models compositionality and is trained with program-level supervision that specifically includes ﬁltering colors and ﬁltering shapes.\nSample Efﬁciency and Catastrophic Forgetting\nWe\nshow sample efﬁciency and forgetting curves in Figure 9.\nFiLM achieves prior state-of-the-art accuracy with 1/3 as\nmuch ﬁne-tuning data. However, our FiLM model still suffers from catastrophic forgetting after ﬁne-tuning.\nZero-Shot Generalization\nFiLM’s accuracy on Condition A is much higher than on B, suggesting FiLM has memorized attribute combinations to an extent. For example, the\nmodel learns a bias that cubes are not cyan, as learning this\ntraining set bias helps minimize training loss.\nTo overcome this bias, we develop a novel FiLM-based\nzero-shot generalization method. Inspired by word embedding manipulations, e.g. “King” - “Man” + “Woman” =\n“Queen” (Mikolov et al. 2013), we test if linear manipulation extends to reasoning with FiLM. We compute (γ, β)\nfor “How many cyan cubes are there?” via the linear combination of questions in the FiLM parameter space: “How\nmany cyan spheres are there?” + “How many brown cubes\nare there?” − “How many brown spheres are there?”. With\nthis (γ, β), our model can correctly count cyan cubes. We\nshow another example of this method in Figure 10.\nWe evaluate this method on validation B, using a parser to\nautomatically generate the right combination of questions.\nWe test previously reported CLEVR-CoGenT FiLM models with this method and show results in Figure 9. With this\nmethod, there is a 3.2% overall accuracy gain when training on A and testing for zero-shot generalization on B. Yet\nthis method could only be applied to 1/3 of questions in\nB. For these questions, model accuracy starts at 71.5% and\njumps to 80.7%. Before ﬁne-tuning on B, the accuracy between zero-shot and original approaches on A is identical,\nlikewise for B after ﬁne-tuning. We note that difference in\nthe predicted FiLM parameters between these two methods\nis negligible, likely causing the similar performance.\nWe achieve these improvements without speciﬁcally\ntraining our model for zero-shot generalization. Our method\nsimply allows FiLM to take advantage of any concept disentanglement in the CNN after training. We also observe\nthat convex combinations of the FiLM parameters – i.e. between “How many cyan things are there?” and “How many\nbrown things are there?” – often monotonically interpolates\nthe predicted answer between the answers to endpoint questions. These results highlight, to a limited extent, the ﬂexibility of FiLM parameters for meaningful manipulations.\nAs implemented, this method has many limitations. However, approaches from word embeddings, representation\nlearning, and zero-shot learning can be applied to directly\noptimize (γ, β) for analogy-making (Bordes et al. 2013;\nGuu, Miller, and Liang 2015; Oh et al. 2017). The FiLM-ed\nnetwork could directly train with this procedure via backpropagation. A learned model could also replace the parser.\nWe ﬁnd such avenues promising for future work.",
        "conclusion": "We show that a model can achieve strong visual reasoning\nusing general-purpose Feature-wise Linear Modulation layers. By efﬁciently manipulating a neural network’s intermediate features in a selective and meaningful manner using\nQuestion\nWhat is the blue big cylinder made of?\n(1) Swap shape\nWhat is the blue big sphere made of?\n(2) Swap color\nWhat is the green big cylinder made of?\n(3) Swap shape/color\nWhat is the green big sphere made of?\nFigure 10: A CLEVR-CoGenT example. The combination\nof concepts “blue” and “cylinder” is not in the training\nset. Our zero-shot method computes the original question’s\nFiLM parameters via linear combination of three other questions’ FiLM parameters: (1) + (2) - (3). This method corrects\nour model’s answer from “rubber” to “metal”.\nFiLM layers, a RNN can effectively use language to modulate a CNN to carry out diverse and multi-step reasoning\ntasks over an image. Our ablation study suggests that FiLM\nis resilient to architectural modiﬁcations, test time ablations,\nand even restrictions on FiLM layers themselves. Notably,\nwe provide evidence that FiLM’s success is not closely connected with normalization as previously assumed. Thus, we\nopen the door for applications of this approach to settings\nwhere normalization is less common, such as RNNs and reinforcement learning. Our ﬁndings also suggest that FiLM\nmodels can generalize better, more sample efﬁciently, and\neven zero-shot to foreign or more challenging data. Overall,\nthe results of our investigation of FiLM in the case of visual\nreasoning complement broader literature that demonstrates\nthe success of FiLM-like techniques across many domains,\nsupporting the case for FiLM’s strength not simply within a\nsingle domain but as a general, versatile approach.",
        "summary_en": "This paper introduces a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. They show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, they show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.",
        "summary_zh": "这篇论文介绍了一种名为FiLM（Feature-wise Linear Modulation）的通用调节方法，旨在解决传统深度学习方法在回答需要多步骤、高层次过程的图像相关问题时表现不佳的问题。FiLM层通过基于调节信息的简单特征仿射变换来影响神经网络计算，极大地提升了视觉推理的效果。论文的实验结果显示，FiLM层在视觉推理任务上取得了显著的成果：（1）将CLEVR基准测试的错误率减少了一半；（2）以一种连贯的方式调制特征；（3）对去除和架构修改具有鲁棒性；（4）能够很好地泛化到具有挑战性的新数据，甚至是从很少的示例甚至零样本学习。"
    },
    {
        "title": "Hierarchical ConViT with Attention-Based Relational Reasoner for Visual Analogical Reasoning",
        "abstract": "Raven’s Progressive Matrices (RPMs) have been widely used to evaluate the visual reasoning ability of humans. To tackle the challenges of visual perception and logic reasoning on RPMs, we propose a Hierarchical ConViT with Attentionbased Relational Reasoner (HCV-ARR). Traditional solution methods often apply relatively shallow convolution networks to visually perceive shape patterns in RPM images, which may not fully model the long-range dependencies of complex pattern combinations in RPMs. The proposed ConViT consists of a convolutional block to capture the low-level attributes of visual patterns, and a transformer block to capture the high-level image semantics such as pattern formations. Furthermore, the proposed hierarchical ConViT captures visual features from multiple receptive fields, where the shallow layers focus on the image fine details while the deeper layers focus on the image semantics. To better model the underlying reasoning rules embedded in RPM images, an Attentionbased Relational Reasoner (ARR) is proposed to establish the underlying relations among images. The proposed ARR well exploits the hidden relations among question images through the developed element-wise attentive reasoner. Experimental results on three RPM datasets demonstrate that the proposed HCV-ARR achieves a significant performance gain compared with the state-of-the-art models. The source code is available at: https://github.com/wentaoheunnc/HCV-ARR.",
        "introduction": "Research in computer vision has advanced significantly recently (Dosovitskiy et al. 2021; He et al. 2016; Zhang et al.\n2022). The research focus is shifting from visual recognition\nof individual objects to visual understanding of image/video\nscenes (Zellers et al. 2019). Visual reasoning, as one of the\nvisual understanding tasks, usually consists of two related\ntasks, “visual perception” and “logical reasoning”. The former perceives the image/video through a perception system\n(Dosovitskiy et al. 2021), and the latter discovers reasoning\nrules through a cognition system (Crouse et al. 2021). A lot\nof research efforts have been devoted to developing a system\nthat can not only visually recognize objects from scenes, but\nalso conduct logical reasoning over the perceived visual information (Sekh et al. 2020; Zhang et al. 2019a).\nConViT\nConViT\nConv\nViT\nConv\nViT\nStage 1\nStage 2\nStage N\nConViT\n…\n…\nARR\nConv\nViT\nARR\nARR\n…\nLocal\nGlobal\nLocal\nGlobal\nFigure 1: RPM problems are challenging as both local\nattributes such as Color, Type and Size and global\nattributes on pattern combinations such as Number and\nPosition need to be extracted simultaneously and a different rule may be applied on each attribute. The proposed\nHCV-ARR consists of a set of ConViT blocks that can simultaneously perceive local attributes through convolutional\nblocks and global attributes through transformer blocks. An\nelement-wise attention-based relational reasoner is then designed to exploit reasoning rules among different attributes.\nRaven’s Progressive Matrix (RPM) problem is one of the\nfrequently-used tests on human’s visual analogical reasoning in cognitive psychology (Raven 2000). An RPM problem is formed by a 3 × 3 pictorial matrix with the last one\nleft blank, as shown in Fig. 1. The objective is to identify the\nmissing entry from eight candidate answers based on the visual context and inductive rules. To minimize the impact of\nlanguage barrier and culture bias, the pictorial matrices are\noften composed of regular polygons. Several RPM databases\n(Barrett et al. 2018; Benny, Pekar, and Wolf 2021; Hu et al.\n2021; Sekh et al. 2020; Teney et al. 2020; Zhang et al.\n2019a) have been developed to evaluate the model capability\nof visual reasoning, i.e., not only visually understand image\nscenes, but also logically conduct inductive reasoning over\npictures (Barrett et al. 2018; Sekh et al. 2020; Teney et al.\n2020; Zhang et al. 2019a). Most existing models for visual\nreasoning (Benny, Pekar, and Wolf 2021; Hu et al. 2021;\nSpratley, Ehinger, and Miller 2020; Zhuo and Kankanhalli\n2021) contain two modules, a perception module that visually perceives the RPM panels to explicitly/implicitly extract\nthe visual attributes, and a logic reasoning module that conducts reasoning over the perceived visual information.\nThe perception module in many existing models (Spratley, Ehinger, and Miller 2020; Zhang et al. 2019b; Zheng,\nZha, and Wei 2019) is built upon shallow convolutional neural networks, as RPM problems are often constructed using\nsimple visual patterns like 2D shapes and lines. But such\nshape patterns are combined to form complex spatial layouts in RPMs, which introduces global layout attributes such\nas Position and Number apart from relatively local attributes such as Color, Size and Type. Applying a shallow network to an image may partially capture local features, but can hardly extract the global panel layouts. Nevertheless, solving an RPM problem requires reasoning over a\nset of relational rules embedded in both global and local attributes. Lacking of “globality” may lose significant reasoning clues on global patterns. More importantly, real-world\nimages are much more complicated than simple shapes. Existing models built on shallow networks may not be able\nto handle the tremendous combinations of complex visual\nrecognition tasks and diversified logical reasoning tasks.\nRecent approaches (Benny, Pekar, and Wolf 2021; Hu\net al. 2021; Spratley, Ehinger, and Miller 2020; Zhuo and\nKankanhalli 2021) often tackle the visual reasoning tasks\nby firstly encoding visual features through convolutions, and\nthen optimizing the “reasoning modules” by computing the\nrow-wise/column-wise similarities of the derived features, to\ndetect the underlying reasoning rules. However, one unique\nreasoning rule can be applied to every visual attribute. After applying a combination of reasoning rules on various attributes, the resulting features may be different across rows/columns. Requesting the resulting features being similar\ndoes not explicitly model the underlying reasoning rules.\nHence, it is critical to build a reasoning module that can simulate a wide range of potential reasoning rules and derive the\nproper rule combinations from a set of RPM images.\nTo tackle the challenges of RPM problems, we propose\nan end-to-end solution model, Hierarchical ConViT with\nAttention-based Relational Reasoner (HCV-ARR). Existing\nperception models for RPMs (Benny, Pekar, and Wolf 2021)\nbuilt on convolutional neural networks cannot completely\nmodel the global dependencies. In this paper, a ConViT\nstructure is proposed, where a convolutional block is designed to capture the low-level visual attributes, and a transformer block is designed to capture the high-level image semantics. Furthermore, we propose to hierarchically recognize the RPM panel in different levels of receptive fields.\nThe hierarchically designed network structure can capture\ndifferent aspects of the RPM panels at different scales, from\noverall global insights of attribute knowledge (e.g., Number\nand Position) to specific local understandings of image\ndetails (e.g., Type and Size). To conduct robust analogical reasoning based on the extracted visual features, instead of simply detecting the common recurrent patterns\namong rows/columns, we design an Attention-based Relational Reasoner (ARR) that dynamically learns the combination of rules applied to attributes across rows/columns. The\ndesigned element-wise attention mechanism better models\nthe non-linear relations in each attribute among images. The\nproposed ARR can uncover a combination of a wide range\nof relational rules in inductive reasoning.\nThe proposed method is compared with state-of-the-art\nmodels on three benchmark datasets. It outperforms the previous best methods in most of the experimental settings on\nthe RAVEN-FAIR (Benny, Pekar, and Wolf 2021), RAVEN\n(Zhang et al. 2019a) and I-RAVEN (Hu et al. 2021) datasets,\nas shown in Tables 3–5. The experimental results demonstrate the effectiveness of the proposed model.\nOur contributions can be summarized as: 1) We propose\nan end-to-end Hierarchical ConViT with an Attention-based\nRelational Reasoner to solve RPM problems. 2) The proposed ConViT can simultaneously extract the global visual features utilizing the self-attention mechanism and local ones utilizing the shallow convolutional layers. 3) The\nhierarchically designed ConViTs can better understand the\nRPM images from different receptive fields. 4) The proposed Attention-based Relational Reasoner can well model\nthe complex relations between rows/columns via the designed element-wise attention-based relational formulation\nand discover a wide range of reasoning rationales.",
        "related work": "Visual Reasoning.\nVisual reasoning visually recognizes\nattributes from scene images and conducts relational reasoning over the derived attributes. In literature, visual reasoning\nspans various tasks, e.g., action recognition (Li et al. 2021;\nWeng et al. 2018, 2020), image captioning (Liu, Ren, and\nYuan 2020; Wu et al. 2017), visual question answering (Antol et al. 2015; Johnson et al. 2017; Teney, Wu, and van den\nHengel 2017; Wu et al. 2017) and visual IQ tests (Benny,\nPekar, and Wolf 2021; Hu et al. 2021; Sekh et al. 2020;\nTeney et al. 2020; Zhang et al. 2019a; Song et al. 2023).\nActivity recognition highly relies on the temporal information, and reasoning the human-object relations over time\nis challenging. Weng et al. (2020) recognize human activities by reasoning over the discriminative channel-level information through a Progressive Enhancement Module to avoid\nrepeating information extraction from different frames. Image/video captioning from a relation-reasoning perspective\nhas received increasing attention. Liu, Ren, and Yuan (2020)\nintroduced a dual-branch Sibling Convolutional Encoder\nwhich combines visual content information with visualsemantic joint embedding using a soft-attention mechanism,\nand an RNN decoder to generate the captions.\nVisual question answering (VQA) is a conventional visual\nreasoning task for machine understanding of scene-level images. The objective is to derive an accurate natural language\nanswer, given an image and a related natural language question. Early VQAs are based on natural scene images (Antol\net al. 2015; Teney, Wu, and van den Hengel 2017). Johnson\net al. (2017) developed the CLEVR dataset by replacing natural images with synthetic images to avoid the misleading\nby background information. Recently, a new form of VQA\ntasks was developed by Zellers et al. (2019) as Visual Commonsense Reasoning, which aims to answer the question and\nprovide an explanation for why the answer is correct.\nSolution Models for RPMs.\nVisual reasoning on RPMs\noften consists of two parts: visual perception and logic reasoning (He et al. 2021). Solution models often use neural\n⟨Q, A⟩\n…\n× 2\nMSA\nMLP\nMerge\nSplit\nConv\nConv\nConViT 1\n𝗭c\n1\n𝗭t\n1\n× 2\nMSA\nMLP\nMerge\n𝗭t\n2\nConv\nConv\n𝗭c\n2\nConViT 2\n× 2\nMSA\nMLP\nMerge\n𝗭t\nN\nConv\nConv\n𝗭c\nN\nConViT N\n…\n…\nPictorial Matrix \nReformulation\n𝗥mix\n1\nARR\n𝗖mix\n1\nARR\nPictorial Matrix \nReformulation\n𝗥mix\n2\nARR\n𝗖mix\n2\nARR\n𝗥mix\nN\n𝗖mix\nN\nPredicted score\nConcat+MLP\n𝗫\nLegends\nConvolution \nblocks\nTransformer \nblocks\n𝗭, 𝗥, 𝗖\nFeature\nElement-wise \nsummation\nHadamard \nproduction\n𝗥1\ni  (or 𝗖1\ni )\nElement-wise Attention-based Reasoning  \nkernel  q\nSoftmax\nkernel  q\nSoftmax\nkernel  q\nSoftmax\nConcat\n𝗥2\ni  (or 𝗖2\ni )\n𝗥3\ni  (or 𝗖3\ni )\n𝗥mix\ni\n (or 𝗖mix\ni\n)\nPictorial Matrix \nReformulation\nARR\nARR\n𝗥1\n𝗖1\n𝗥2\n𝗖2\n𝗥N\n𝗖N\nFigure 2: Block diagram of the proposed HCV-ARR, which consists of a Hierarchical ConViT and an Attention-based Relational Reasoner. It extracts the image details locally through the shallow convolutional blocks and the high-level image semantics globally through the transformer blocks. The proposed ARR extracts the element-wise attentional information between two\nimages, uncovers the relations embedded in the image pair and conducts relational reasoning to derive the correct answer.\nnetworks to extract visual features. CoPINet (Zhang et al.\n2019b) and Rel-AIR (Spratley, Ehinger, and Miller 2020)\nboth utilize the residual network architecture (He et al.\n2016), and MRNet (Benny, Pekar, and Wolf 2021) applies\nmulti-scale convolutional layers to extract features. The neural networks in existing methods (He, Ren, and Bai 2021;\nSpratley, Ehinger, and Miller 2020; Zhang et al. 2019b;\nZheng, Zha, and Wei 2019; Zhuo and Kankanhalli 2021) are\noften relatively shallow, which may not fully capture complex combinations of visual patterns across RPM panels.\nIn literature, row-wise and/or column-wise relations are\noften utilized in solution models to derive the reasoning\nrules, e.g., CoPINet (Zhang et al. 2019b), LEN (Zheng, Zha,\nand Wei 2019), MXGNet (Wang, Jamnik, and Lio 2020),\nRel-AIR (Spratley, Ehinger, and Miller 2020), DCNet (Zhuo\nand Kankanhalli 2021) and MRNet (Benny, Pekar, and Wolf\n2021). The CoPINet (Zhang et al. 2019b) explicitly contrasts the candidate answers and highlights the difference\nbetween options. The LEN (Zheng, Zha, and Wei 2019) utilizes a global encoder that encodes the context and choices to\nderive the row-/column-wise representations. The MXGNet\n(Wang, Jamnik, and Lio 2020) and the Rel-AIR (Spratley, Ehinger, and Miller 2020) subtract the common factors\nfrom all the option representations. The DCNet (Zhuo and\nKankanhalli 2021) implements a dual-contrasting mechanism on both row/column features and choices. The MRNet (Benny, Pekar, and Wolf 2021) applies a multi-scale design and minimizes the squared Euclidean distance between\nrow/column features, to identify recurring patterns. These\nreasoning models often operate on contrastive information\nderived from visual features, either within rows/columns or\nanswer candidates, other than seeking for concrete inductive rationales for visual analogical reasoning. A much more\nsophisticated and comprehensive logic reasoner is needed\nto uncover the combination of reasoning rules embedded in\ndifferent attributes of the RPM problems.",
        "proposed method": "Formally, given a 3 × 3 RPM pictorial matrix with the last\none missing, Q = {q0, q1, · · · , q7}, as shown in Fig. 1, the\ntarget is to find the missing image ˆai from the answer set\nA = {a0, a1, · · · , a7}, forming as a complete RPM sample\n⟨Q, A⟩ with each image of the size of H × W. The same\nreasoning rule regarding one attribute is shared among three\nrows or columns. Note that for one RPM sample, the underlying rules for different attributes can be different. In modern\nRPM solvers, each option image is appended to Q, forming\n8 groups of image tensors Xi ∈ R9×H×W , where i = 0 . . . 7\nindicates the option index.\nAs shown in Fig. 2, the proposed model consists of a Hierarchical ConViT for visual perception and an Attentionbased Relational Reasoner for logic reasoning. The former\nvisually perceives the image contents and extracts the visual information, and the latter conducts reasoning on the\nextracted visual information. The goal of the established network structure is to exploit a discriminative relational mapping that takes the image tensors as the input and outputs the\nprediction score vector for eight options:\nˆy = Fm{X0, X1, . . . , X7; Θ},\n(1)\nwhere Fm is the mapping function, which involves the Hierarchical ConViT for perception and Attention-based Relational Reasoner for reasoning. Θ is the network parameters\nto be optimized. ˆy ∈ [0, 1]8 contains the prediction scores of\nthe 8 candidate options. The final output is the option which\nhas the minimum Binary Cross Entropy loss,\nL = −\n7\nX\ni=0\nσ(yi) · log σ(ˆyi),\n(2)\nwhere σ represents the sigmoid function and yi is the\nground-truth label concerning the i-th option, and it is an\nelement of the one-hot vector y ∈ {0, 1}8.\nHierarchical ConViT for Visual Perception\nAs shown in Fig. 2, the proposed Hierarchical ConViT encoder E contains a set of ConViT blocks to extract the visual\ninformation from multiple receptive fields, and each ConViT\nconsists of a transformer branch and a convolutional branch.\nBoth low-level image details and high-level image semantics have been shown useful for visual reasoning (Benny,\nPekar, and Wolf 2021; Sekh et al. 2020). In Fig. 2, while\nfeature maps of shallow blocks focus more on lower-level\nsemantic relations (Girshick et al. 2014) such as Color and\nType, those of deeper blocks reflect the pattern layout and\ncontain positional relations (Zintgraf et al. 2017; Islam, Jia,\nand Bruce 2020). The proposed Hierarchical ConViT could\nconcurrently capture the visual information from different\nreceptive fields, which is used as multi-receptive clues for\nlogical reasoning at the later stage.\nExisting visual reasoning models (Spratley, Ehinger, and\nMiller 2020; Zhang et al. 2019b; Zheng, Zha, and Wei 2019;\nZhuo and Kankanhalli 2021) often utilize shallow convolutional networks to extract low-level image details, but these\nshallow networks may not capture the complex visual patterns and spatial correlations across different configurations\nin RPMs. On the other hand, transformer models (Han et al.\n2022) perform well on image classification tasks by partitioning the image into sequences of patches and extracting the attentional information among patches. The question\nimage of RPMs is often formed in patches, e.g., there are\n3 × 3 patches in the 3×3Grid setting and 2 × 2 patches\nin the 2×2Grid setting in the RAVEN-like dataset. It is\nhence natural to apply Vision Transformers (ViT) (Dosovitskiy et al. 2021) to better recognize the complex global\npatterns embedded in RPMs. In the design of ConViT, we\nencode the image fine details through the shallow convolutional neural networks in one branch, and exploit the spatial\ncorrelations and long-range dependencies between complex\npatterns in another branch through the patch split-and-merge\noperations and multi-head self-attention.\nMore specifically, denote X ∈ R8×9×H×W as the input\nimage tensors, where H and W are the height and width\nof the image respectively. The output of the convolutional\nbranch Ci at the i-th stage, Zc\ni, can be derived as:\nZc\n1 = C1{X},\nZc\ni = Ci{Zc\ni−1}.\n(3)\nThe output of the transformer branch Ti at the i-th stage,\nZt\ni, can be derived as:\nZt\n0 = PS{X},\nZt\ni = Ti{PM\ni {Zt\ni−1}},\n(4)\nwhere PS, PM\ni\nand Ti denote the patch splitting operation, the patch merging operation and the transformer at\nthe i-th stage, respectively. PM\ni\ncontains an unfold layer\nfor down-sampling and a multi-layer perception. The transformer block Ti propagates a feature map Zt\ni−1 and outputs\nZt\ni at stage i as in Fig. 2. The details of model architecture\nand parameters can be found in Supplementary Materials.\nThe final output of the encoder E is a tuple of tensors,\nrecording the feature maps from both convolution blocks\nand transformer blocks, and concurrently passes it to next\nreasoning modules for further abstraction.\n(Zc\n1 + Zt\n1, . . . , Zc\nN + Zt\nN) = E{X}.\n(5)\nAttention-based Relational Reasoner\nBefore feeding the perceived visual information to the reasoner, we firstly reformulate the derived features according to rows or columns via Pictorial Matrix Reformulation.\nSpecifically, given Zi = Zc\ni + Zt\ni, Zi ∈ R8×9×Ci×Hi×Wi\nfor stage i, where Ci, Hi, Wi are shown in Fig. 2, we reformulate the second dimension of the feature tensor into a\n3×3 matrix. For notation simplicity, stage index i is omitted.\nNext, features among rows and columns from the matrix are\nformed as row features R = [R1; R2; R3] and column features C = [C1; C2; C3], respectively, where R1, R2, R3, C1,\nC2, C3 are in shape of (8, 3, Ci, Hi, Wi).\nHumans often solve visual analogical reasoning tasks by\ninvestigating the common rationale spanning over image\npatterns along rows or columns. The majority of solution\nmodels (Benny, Pekar, and Wolf 2021; Spratley, Ehinger,\nand Miller 2020; Zhang et al. 2019b; Zheng, Zha, and Wei\n2019; Zhuo and Kankanhalli 2021) attempt to model this\nprocess by minimizing the divergence between row/column\nfeature representations. For example, in (Benny, Pekar, and\nWolf 2021), the DIST3 operation within row/column vectors computes the squared Euclidean distance among features. Take row features R = [R1; R2; R3] as an example,\nDIST3(R1; R2; R3)\n= |R1 − R2|2 + |R2 − R3|2 + |R3 − R1|2.\n(6)\nSuch formulation is optimal only for some special cases.\nHowever, a combination of rules applied on different attributes of the problem panel do not necessarily lead to\nsimilar image features, e.g., for Arithmetic rule, image attributes satisfy some arithmetic relations, and for\nDistribute Three rule, three distinct attribute values\nare distributed in images of one row/column. The formulation in Eqn. (6) is not sufficiently expressive to model such\nrelations. To better model the embedded reasoning rules, we\npropose an Attention-based Relational Reasoner.\nAttention mechanism has been widely used in modeling\nthe pairwise relations between two inputs to explore the indepth inter-relations at the feature level (Chen et al. 2019,\n2022; Hori et al. 2017). In RPMs, different inductive rules\nare applied to different attributes. Therefore, unlike the traditional attention mechanism where all dimensions of the\ninput feature vector share the same learnable weight (Chen\net al. 2019), the proposed method weighs each dimension\ndifferently to better model the relations across different attributes. As shown in details from Fig. 2, the element-wise\nattentive relations are learned through a kernel q which has\nthe same dimensionality of the input row/column features.\nTake row features R for instance,\nWi,j = exp(q ⊙ Ri) ⊘ (exp(q ⊙ Ri) + exp(q ⊙ Rj)), (7)\nwhere ⊙ denotes the element-wise (Hadamard) production,\n⊘ denotes element-wise division, and i, j ∈ {1, 2, 3} denotes\nthe row index. The output denoted as Rmix is obtained as,\nRmix = [W1,2 ⊙ R1 + W2,1 ⊙ R2;\nW1,3 ⊙ R1 + W3,1 ⊙ R3;\nW2,3 ⊙ R2 + W3,2 ⊙ R3].\n(8)\nThe same operation is applied to the column features C\nto derive Cmix. Finally, the predicted scores are aggregated\nover N stages through a multilayer perceptron FMLP as,\nˆy = FMLP\n\u0010h\nRmix\n1\n+ Cmix\n1 ; . . . ; Rmix\nN + Cmix\nN\ni\u0011\n.\n(9)\nDiscussion\nAttention in Visual Perception.\nHan et al. (2022) categorizes image representation learning into convolution-based\nand attention-based. With the development of SENet (Hu,\nShen, and Sun 2018), Vision Transformer (ViT) (Dosovitskiy et al. 2021) and its variants, the attention mechanism\noverwhelms traditional convolutional architectures in substantial CV tasks. ViT is good at capturing long-range dependencies between patches, and its variants such as Swin-T\nimprove the modeling capacity for local information (Han\net al. 2022). The proposed HCV simultaneously captures\nboth global and local discriminative information in multiple\nreceptive fields to deeply understand the image contents.\nAttention in Relational Reasoning.\nIn analogical reasoning tasks, Relation Network (Santoro et al. 2017) utilizing\nneural networks has been used to derive the relations of input\nfeatures for various relational reasoning tasks (Barrett et al.\n2018; Santoro et al. 2017). Inspired by the success of the\nattention mechanism in extracting the attentive information\namong elements in a sequence (Vaswani et al. 2017), we propose an attention mechanism to deeply explore the relations\non feature sequences, and subsequently derive the underlying reasoning rules. Traditional attention-based feature fusion scheme in Two-Stream Convolutional Neural Network\n(TSCNN) (Chen et al. 2019) has been proven effective in aggregating two sets of features, which uses the standard selfattention to learn a set of weights {wi, i = 1, 2, . . . , M}\ncorresponding to M sets of features {fi, i = 1, 2, . . . , M}\nto generate the aggregated feature fa = PM\ni σ(q⊤fi)fi.\nTo fuse the two feature representations in (Chen et al. 2019),\nfa = w1f1 +w2f2, the same weight wi is assigned to every\ndimension of the feature vector. Such a mechanism may well\nfuse multi-modal feature representations, but may be ineffective for modeling high-level complex relations using one\nlearnable weight only, as different rules may be applied on\ndifferent attributes. In contrast, the proposed element-wise\nattention-based relational reasoning mechanism assigns a\ndifferent learnable weight to each dimension, which has a\nhigh potential to model the complicated relations embedded\nin different feature dimensions, and consequently induce a\nwide range of abstract relations across different attributes.",
        "experimental results": "The proposed model is evaluated on the RAVEN (Zhang\net al. 2019a), I-RAVEN (Hu et al. 2021) and RAVEN-FAIR\ndatasets (Benny, Pekar, and Wolf 2021). Each dataset is randomly split into 10 folds, with 6 folds for training, 2 folds\nfor validation and 2 folds for testing.\nDatasets\nRAVEN\n(Zhang et al. 2019a) consists of 70,000 question\nsets, where each contains 8 question images, arranged as a\n3 × 3 image matrix with the last one missing, and 8 candidate answers. The candidates are generated by permutation\nfrom the ground-truth answer image, and each permutated\nimage is derived by randomly shifting one attribute value.\nThe dataset is equally distributed into 7 configurations. Each\nquestion contains 6 visual attributes (Angle, Number,\nPosition, Type, Size and Color) and 4 underlying rules (Constant, Progression, Arithmetic and\nDistribute Three). Extra noise is added to attributes to\nmake the problem more challenging.\nI-RAVEN\n(Hu et al. 2021) is developed to fix the problem\nin the original RAVEN dataset that the aggregation of the\nmost common values for each attribute could be the correct\nanswer (Benny, Pekar, and Wolf 2021; Hu et al. 2021). In the\nI-RAVEN dataset, the negative candidate answers are generated by hierarchically permuting one attribute of the groundtruth answer in three iterations. In each iteration, two child\nnodes are generated, where one node remains the same with\nthe parent node while the other permutes one attribute.\nRAVEN-FAIR\n(Benny, Pekar, and Wolf 2021) iteratively\nenlarges the answer set starting with the correct answer only\nand changing one attribute value from either the correct answer or a generated negative answer. Except for the answer\ngeneration, the same settings as in original RAVEN are used\nfor the I-RAVEN and RAVEN-FAIR datasets.\nExperimental Setup\nThe proposed method is compared with the following stateof-the-art solutions.\nCoPINet (Zhang et al. 2019b) models the probability of\neach candidate answer by applying a contrasting module.\nLEN (Zheng, Zha, and Wei 2019) assembles the possible\ncandidate answer embeddings to the 8 question panel embeddings, calculates scores for all possible combinations\n(C3\n9 = 84), and predicts the answer with the highest score.\nRel-AIR (Spratley, Ehinger, and Miller 2020) disentangles\nobjects with an initial unsupervised scene decomposition\nfirst, and then encodes it with additional information such\nas position and scale. A sequence encoder is designed to extract feature relationships and generate the final results.\nSRAN (Hu et al. 2021) utilizes a hierarchical rule embedding module and a gated embedding fusion module to output\nthe rule embedding given two-row sequences.\nDCNet (Zhuo and Kankanhalli 2021) consists of a rule contrast module and a choice contrast module to exploit the inherent structure of RPMs, which compares the latent rules\namong rows/columns and increases the choices differences.\nMRNet (Benny, Pekar, and Wolf 2021) is established by\nusing multi-resolution convolution layers as the perception\nmodule, and the DIST3 row/column operator as the inductive reasoner to conduct relational reasoning.\nThe number of ConViT blocks is set to N = 3. Following\nthe settings in (Benny, Pekar, and Wolf 2021; Zhang et al.\n2019a,b), the input images are resized to 80 × 80 pixels.\nThe maximum number of epochs is 200, and the training is\nstopped if there is no significant improvement on the validation set over 20 epochs. During training, the learning rate\nis set to 0.001, and the Adam optimizer is utilized with a\nweight decay of 1 × 10−5. The batch size is set to 32.\nAblation Study\nTo evaluate the performance gains brought by each of the\ntwo proposed modules, we conduct an ablation study on\nthe RAVEN-FAIR dataset (Benny, Pekar, and Wolf 2021),\nby using the most recent state-of-the-art method MRNet\n(Benny, Pekar, and Wolf 2021) as the baseline model. HCV\nand ARR denote methods of replacing the visual perception\nmodule or reasoning module of MRNet with the proposed\nHCV and ARR, respectively. Besides using just convolutions for perception (i.e., Conv + ARR), the usage of ViT\nonly is also evaluated (i.e., ViT + ARR). We also compare\nthe proposed ARR with the traditional attention-based fusion utilized in TSCNN (Chen et al. 2019) on features extracted from the proposed HCV. The results are in Table 1.\nCompared with MRNet, all the proposed model components receive consistent performance improvements. For\nSetting C, L-R, U-D and O-IC that do not use high-level\nimage semantics such as Number and Position (The regular shapes have a fixed Position and Number) for reaMethods\nAccuracy (%) in Different Configurations\nAvg.\nC\n2x2 3x3 L-R U-D OIC OIG\nMRNet\n86.8\n97.0\n72.7\n69.5\n98.7\n98.9\n97.6\n73.3\nTSCNN\n87.9\n96.8\n73.5\n74.7\n94.4\n91.8\n94.5\n88.9\nHCV\n92.7\n99.9\n85.7\n78.4\n99.9\n99.8\n99.8\n85.4\nConv+ARR 93.4\n99.9\n86.3\n79.8\n99.8\n99.7\n99.6\n88.7\nViT+ARR\n44.9\n52.0\n39.4\n41.0\n35.8\n35.8\n57.8\n50.4\nProposed\n95.4\n99.8\n92.9\n87.9\n99.8\n99.6\n99.7\n88.5\nTable 1: Ablation study on different modules of the proposed architecture. Compared with the baseline method,\nMRNet (Benny, Pekar, and Wolf 2021), both hierarchical\nConViT and ARR bring significant performance improvements across all 7 problem configurations.\nStages\nAccuracy (%) in Different Configurations\n1 2 3\nAvg.\nC\n2x2 3x3 L-R U-D OIC OIG\n\"%%\n74.9\n83.2\n53.1\n58.1\n90.6\n90.3\n87.4\n61.7\n\"\"%\n87.8\n98.9\n68.1\n68.7\n99.3\n99.2\n99.1\n78.1\n\"\"\"\n95.4\n99.8\n92.9\n87.9\n99.8\n99.6\n99.7\n88.5\nTable 2: Ablation study of the depth of the proposed HCVARR on the RAVEN-FAIR dataset.\nsoning, using just convolutions works well, which demonstrates the effectiveness of the convolutions in capturing\nlow-level features for reasoning. Using ViT alone produces\nvery poor results, as most of the attributes, e.g., Color,\nType, Size are low-level features, while ViT focuses more\non the high-level features such as Number and Position.\nWhen both convolutions and ViTs are used, the proposed\nConViT significantly improves the performance on complex\nconfigurations such as 2×2G and 3×3G, by utilizing the\nhigh-level image semantics extracted from ViT blocks.\nBy introducing the HCV module, the hierarchically increased receptive fields can view the question images from\nmultiple scales, and on each scale both local and global features are richly captured. Hence, the inference performance\non all 7 configurations is improved upon the baseline. By\nusing the proposed ARR module, the performance across 7\ndifferent settings is also improved upon MRNet, which indicates the benefits brought by the proposed ARR in handling\nthe diverse reasoning rules across both local and global attributes of various scales. The proposed ARR also achieves a\nsignificant improvement over the traditional attention mechanism of TSCNN (Chen et al. 2019), which assigns the same\nweight across different attributes, while the proposed ARR\ncan capture different rules embedded in different attributes.\nAdditionally, we conduct ablations on the impact of the\ndepth for the proposed HCV-ARR. From Table 2, we can\nobserve the improvements in the reasoning accuracy by considering more receptive fields from deeper ConViT blocks\nand more reasoning clues from deeper ARRs. The accuracy\nincreases by 12.9% on average when the depth N is set from\n1 to 2, and further boosts to 95.4% when N = 3. The experimental results show the benefits of utilizing both global and\nlocal attributes in visual analogical reasoning.\nComparison Results on RAVEN-FAIR Dataset\nThe proposed HCV-ARR is compared with state-of-theart models on the RAVEN-FAIR dataset (Benny, Pekar,\nand Wolf 2021). We implement and evaluate DCNet (Zhuo\nand Kankanhalli 2021) and SRAN (Hu et al. 2021) on the\nRAVEN-FAIR dataset. The results of other compared methods are obtained from (Benny, Pekar, and Wolf 2021). From\nthe results summarized in Table 3, we can see that the proposed method significantly outperforms all the compared\nmethods on all problem configurations. Compared with the\nprevious best model, MRNet (Benny, Pekar, and Wolf 2021),\nthe proposed HCV-ARR achieves a performance gain of\n8.6% on average. The proposed method is particularly competitive for challenging settings, e.g., the performance gains\nMethods\nAccuracy (%) in Different Configurations\nAvg.\nC\n2x2 3x3 L-R U-D OIC OIG\nCoPINet\n36.5\n–\n–\n–\n–\n–\n–\n–\nLEN\n50.9\n–\n–\n–\n–\n–\n–\n–\nDCNet†\n57.0\n57.2\n48.4\n58.2\n57.5\n59.4\n62.0\n56.2\nSRAN†\n76.7\n87.4\n60.4\n62.8\n86.5\n86.7\n77.5\n75.9\nMRNet\n86.8\n97.0\n72.7\n69.5\n98.7\n98.9\n97.6\n73.3\nProposed 95.4\n99.8\n92.9\n87.9\n99.8\n99.6\n99.7\n88.5\nTable 3: Comparison with state-of-the-art on the RAVENFAIR dataset (Benny, Pekar, and Wolf 2021). Results of\nother methods are obtained from (Benny, Pekar, and Wolf\n2021) and † indicates the results by our implementation.\nMethods\nAccuracy (%) in Different Configurations\nAvg.\nC\n2x2 3x3 L-R U-D OIC OIG\nCoPINet\n46.1\n54.4\n36.8\n31.9\n51.9\n52.5\n52.2\n42.8\nLEN\n41.4\n56.4\n31.7\n29.7\n44.2\n44.2\n52.1\n31.7\nDCNet†\n46.6\n56.2\n32.7\n32.9\n54.7\n53.9\n55.9\n39.8\nSRAN\n60.8\n78.2\n50.1\n42.4\n70.1\n70.3\n68.2\n46.3\nMRNet†\n81.0\n99.6\n63.4\n59.2\n98.7\n98.3\n95.7\n51.9\nProposed 93.9\n99.9\n96.2\n75.5\n99.4\n99.6\n99.5\n87.3\nTable 4: Comparison with state-of-the-art models on the IRAVEN dataset. † indicates the results are obtained by us\nand others are from (Hu et al. 2021).\nare 20.2% on 2×2Grid, 18.4% on 3×3Grid, and 15.2%\non Out-InGrid settings, respectively. The underlying reasons are two-fold: 1) The proposed HCV module could better perceive the complex patterns in these settings. 2) The\nproposed ARR could better reason over the combination of\nrules, with one applied to each attribute.\nComparison Results on I-RAVEN Dataset\nWe have implemented and evaluated the DCNet (Zhuo and\nKankanhalli 2021) and MRNet (Benny, Pekar, and Wolf\n2021) on the I-RAVEN dataset, and other results are obtained from (Hu et al. 2021). As shown in Table 4, the proposed model largely outperforms all the compared models.\nCompared to the previously published best result by SRAN\n(Hu et al. 2021), the average accuracy improves from 60.8%\nto 93.9%. The proposed HCV-ARR significantly and consistently outperforms MRNet (Benny, Pekar, and Wolf 2021)\non all the settings. Besides the Center, Left-Right,\nUp-Down and Out-InCenter, HCV-ARR can also effectively solve configurations containing complex spatial relations, such as 2×2Grid and Out-InGrid.\nComparison Results on Original RAVEN Dataset\nWe also conduct comparison experiments on the original\nRAVEN dataset (Zhang et al. 2019a). The original RAVEN\ndataset contains the loophole that the correct answer can be\nderived by simply aggregating the most common properties\nfrom the answer options, without examining the question at\nall (Hu et al. 2021; Benny, Pekar, and Wolf 2021). In the\npast, many approaches took advantage of this shortcut to deMethods\nAccuracy (%) in Different Configurations\nAvg.\nC\n2x2 3x3 L-R U-D OIC OIG\nCoPINet\n18.4\n–\n–\n–\n–\n–\n–\n–\nSRAN†\n46.2\n49.0\n45.4\n52.8\n42.4\n36.0\n49.1\n48.8\nMRNet\n84.0\n–\n–\n–\n–\n–\n–\n–\nProposed 87.3\n99.8\n71.4\n65.9\n99.9\n99.8\n98.0\n76.2\n(a) Without contrasting on candidate answers.\nMethods\nAccuracy (%) in Different Configurations\nAvg.\nC\n2x2 3x3 L-R U-D OIC OIG\nCoPINet\n91.4\n95.1\n77.5\n78.9\n99.1\n99.7\n98.5\n91.4\nLEN\n72.9\n80.2\n57.5\n62.1\n73.5\n81.2\n84.4\n71.5\nRel-AIR\n94.1\n99.0\n92.4\n87.1\n98.7\n97.9\n98.0\n85.3\nDCNet\n93.6\n97.8\n81.7\n86.7\n99.8\n99.8\n99.0\n91.5\nMRNet\n96.6\n–\n–\n–\n–\n–\n–\n–\nProposed\n96.0\n99.4\n86.9\n89.1\n99.9\n99.9\n99.8\n96.8\n(b) With contrasting on candidate answers.\nTable 5: Comparison with state-of-the-art on the RAVEN\ndataset. † indicates the results by our implementation and\nothers are obtained from respective original publications.\nThe proposed HCV-ARR outperforms the previous best\nmethod, MRNet, without using the contrast while achieves\na comparable performance when using the contrast.\nrive a good performance, e.g., by contrasting the candidate\nanswers (Zhang et al. 2019b). We conduct comparison experiments using both settings, with or without the contrast\ninformation. The results are summarized in Table 5.\nFrom Table 5b, we can see that many approaches utilizing the contrast information achieve high accuracy. When\nthe proposed HCV-ARR utilizes the contrast information,\nit achieves slightly poorer than the previous best method,\nMRNet (Benny, Pekar, and Wolf 2021), but outperforms\nother methods. When this loophole is eliminated, the proposed method outperforms MRNet (Benny, Pekar, and Wolf\n2021) by 3.3%, and significantly outperforms other compared methods as shown in Table 5a. These experimental\nresults validate the effectiveness of the proposed method.",
        "conclusion": "In this paper, a Hierarchical ConViT with Attention-based\nRelational Reasoner is proposed to solve RPM problems.\nThe proposed Hierarchical ConViT simultaneously extracts\nthe fine image details and global image semantics through\nshallow convolutional networks and the attention mechanism of vision transformers across multi-level receptive\nfields. The proposed ARR module effectively models the\nunderlying relations via the designed element-wise attention\nmechanism, one rule for each attribute, and discovers a wide\nrange of reasoning rationales for better reasoning. The experimental results on three benchmark datasets demonstrate\nthat the proposed HCV-ARR significantly outperforms the\nstate-of-the-art models in almost all the settings.",
        "summary_en": "Raven’s Progressive Matrices (RPMs) have been widely used to evaluate the visual reasoning ability of humans. To tackle the challenges of visual perception and logic reasoning on RPMs, this paper proposes a Hierarchical ConViT with Attention-based Relational Reasoner (HCV-ARR). Traditional solution methods often apply relatively shallow convolution networks to visually perceive shape patterns in RPM images, which may not fully model the long-range dependencies of complex pattern combinations in RPMs. The proposed ConViT consists of a convolutional block to capture the low-level attributes of visual patterns, and a transformer block to capture the high-level image semantics such as pattern formations. Furthermore, the proposed hierarchical ConViT captures visual features from multiple receptive fields, where the shallow layers focus on the image fine details while the deeper layers focus on the image semantics. To better model the underlying reasoning rules embedded in RPM images, an Attention-based Relational Reasoner (ARR) is proposed to establish the underlying relations among images. The proposed ARR well exploits the hidden relations among question images through the developed element-wise attentive reasoner. Experimental results on three RPM datasets demonstrate that the proposed HCV-ARR achieves a significant performance gain compared with the state-of-the-art models.",
        "summary_zh": "这篇论文介绍了一种名为HCV-ARR（Hierarchical ConViT with Attention-based Relational Reasoner）的模型，旨在解决Raven's Progressive Matrices（RPMs）中的视觉类比推理问题。针对RPMs中的视觉感知和逻辑推理挑战，传统方法通常使用相对较浅的卷积网络来感知RPM图像中的形状模式，但这可能无法充分模拟RPM中复杂模式组合的长程依赖性。该论文提出了一种层次化的ConViT模型，由卷积块和Transformer块组成，以捕获视觉模式低级属性和高级语义。此外，论文还提出了一种基于注意力的关系推理器（ARR），以更好地建模嵌入在RPM图像中的推理规则。实验结果表明，HCV-ARR模型在三个RPM数据集上取得了显著的性能提升，超越了现有模型。"
    },
    {
        "title": "Learning the Dynamics of Visual Relational Reasoning via Reinforced Path Routing",
        "abstract": "Reasoning is a dynamic process. In cognitive theories, the dynamics of reasoning refers to reasoning states over time after successive state transitions. Modeling the cognitive dynamics is of utmost importance to simulate human reasoning capability. In this paper, we propose to learn the reasoning dynamics of visual relational reasoning by casting it as a path routing task. We present a reinforced path routing method that represents an input image via a structured visual graph and introduces a reinforcement learning based model to explore paths (sequences of nodes) over the graph based on an input sentence to infer reasoning results. By exploring such paths, the proposed method represents reasoning states clearly and characterizes state transitions explicitly to fully model the reasoning dynamics for accurate and transparent visual relational reasoning. Extensive experiments on referring expression comprehension and visual question answering demonstrate the effectiveness of our method.",
        "introduction": "Cognitive theories reveal that reasoning is a dynamic process\nwhere the state of the intelligent system is always changing\nover time in its state space (Engelfriet and Treur 1994; Port\nand Van Gelder 1995). The dynamics of reasoning is described by sequences of reasoning states over time after successive reasoning steps (Jonker and Treur 2002). Formally, a\nreasoning state is an intermediate representation of a reasoning process. A transition from one reasoning state to another\nreasoning state formalizes one reasoning step. Modeling the\nreasoning dynamics is critical to simulate the reasoning capability of humans (Jonker and Treur 2003).\nIn this paper, we propose to learn the reasoning dynamics\nof visual relational reasoning by casting it as a path routing task. Visual relational reasoning involves the sequential\nprocessing of visual information such as relationships and\nobjects in images. Figure 1 shows the path routing for visual\nrelational reasoning in the context of visual question answering (VQA). In path routing, we require a reasoning model to\nexplore paths, i.e., sequences of nodes, over a visual graph,\nwhose nodes denote objects in an image and edges denote\nrelationships among the objects. For example, to answer the\nA\n(c) Holistic\nA\n(d) Path Routing (Ours)\nWhat is the person to \nthe left of the glasses \nholding?  (A: Kite)\n(a) Inputs\nI\nG\nQ\nA\nqueryname\n(b) Modular\nI, Q\nG, Q\nG, Q\nrelateholding\nrelateperson, left\nselectglasses \nFigure 1: Illustrations of visual relational reasoning in the\ncontext of visual question answering. (a) An input question,\nan input image, and a graph representing the relational layout of the image. We do not show all edges in the graph\nfor simplicity. (b) The reasoning process of typical modular\nmethods. (c) The reasoning process of typical graph-based\nholistic methods. (d) The path routing on the graph, where\nred circles and red solid arrows comprise the explored path.\nquestion “What is the person to the left of the glasses holding?” in Figure 1 (a), the model should progressively attend\nto the glasses, the person, and the kite on the graph based on\ntheir relationships. Different from existing modular methods (Andreas et al. 2016; Johnson et al. 2017) and holistic\nmethods (Guo, Xu, and Tao 2019; Hu et al. 2019) that focus\non reasoning structure modeling or contextual representations learning, we focus on reasoning dynamics learning in\npath routing shown in Figure 1 (d). By exploring such paths,\nthe reasoning states are represented clearly and the reasoning state transitions are characterized explicitly to enable the\nmodeling of the reasoning dynamics.\nTo achieve this goal, we present a reinforced path routing method, which uses a reinforcement learning (RL) based\nreasoning model to learn multi-step paths, for accurate\nand transparent visual relational reasoning. The proposed\nmethod regards the reasoning model as an agent that navigates on a structured visual graph based on an input sentence to infer the reasoning result. At each time step, the\nagent uses a navigation policy to decide which node should\nbe selected to extend the current path. The navigation policy,\nwhich contains a language attention and a history attention,\nenables the agent to exploit language cues, history states,\nand history decisions in current decision-making. The agent\nwill be rewarded if it ﬁnds a path based on which the correct\nresult is inferred.\nWe further introduce a pre-training strategy that warmstarts the agent via supervised learning (SL). The strategy\nencourages the agent to navigate over the graph from one\nnode to another node in a soft manner. In pre-training, we\nprogressively sharpen the probability distribution of the navigation policy, to encourage the agent to gradually learn to\nfocus on only one node. A coverage loss is used to enforce\nthe model to attend to different nodes at different steps.\nThe pre-training strategy is thus capable of preventing the\nmodel from degradation in the RL stage. Extensive experiments on two visual relational reasoning tasks, referring expression comprehension (Hu et al. 2016; Qiao, Deng, and\nWu 2020), visual question answering (Antol et al. 2015; Wu\net al. 2017), demonstrate our method achieves accurate reasoning with a transparent reasoning process.\nThe contributions of this paper are two-fold:\n1. We are the ﬁrst to learn the dynamics of visual relational\nreasoning by casting it as a path routing task to achieve\naccurate and transparent reasoning.\n2. We present a reinforced path routing method that can\nlearn multi-step paths without any additional annotation.\nThe explored paths can make the reasoning processes of\nour method more human-understandable.",
        "related work": "Visual Relational Reasoning\nExisting visual relational reasoning methods for referring\nexpression comprehension and visual question answering\ncan be divided into two categories: modular and holistic.\nModular methods (Andreas et al. 2016; Johnson et al. 2017;\nHu et al. 2018; Shi, Zhang, and Li 2019; Hong et al. 2019;\nLiu et al. 2019a; Chen et al. 2021) focus on reasoning structure modeling and assembles various neural modules for different input sentence-image pairs. They show good compositionality and interpretability in various tasks but usually\nhave high model complexity and inferior performance on\ntasks over real-world images. Holistic methods (Hudson and\nManning 2018; Perez et al. 2018; Hu et al. 2019; Wang et al.\n2019; Yang, Li, and Yu 2019a; Jing et al. 2020a; Liu et al.\n2020; Liao et al. 2020; Jing et al. 2020b; Deng et al. 2021)\nfocus on contextual representation learning and use a single model for different inputs. They usually stack attention\nmechanisms or graph convolution operations to learn informative representations and perform reasoning in the latent\nspace. By contrast, our method focuses on reasoning dynamics learning by casting the reasoning task as path routing.\nWe explicitly characterize the reasoning states and the state\ntransitions to learn the reasoning dynamics for accurate reasoning with a transparent reasoning process.\nThe NSM (Hudson and Manning 2019b), the XMN (Shi,\nZhang, and Li 2019), and the SGMN (Yang, Li, and Yu\n2020) also build a visual graph and perform reasoning by\ntraversing the graph. Nonetheless, in each step of reasoning,\nthey weighted sum all nodes in the graph to represent the\ncurrent state. Therefore, which node contributes most to the\nnext step of reasoning is not clear, and the reasoning process\nis hard to understand. Our method uses only one attended\nnode to represent the state of each step and combines the\nstate and historical states for the next step of reasoning. Only\nthe attended nodes are involved in reasoning. Thus the path\nformed by the attended nodes serves as an explanation for\nthe reasoning process.\nReinforcement Learning\nReinforcement learning has been widely applied to visionlanguage tasks such as REC and VQA. Nonetheless, most\nof them use the RL as a technique to estimate gradients\nof non-differentiable components to guarantee the models\ncan be optimized in an end-to-end manner. For example,\nneural modular networks (Hu et al. 2017; Johnson et al.\n2017; Mascharka et al. 2018) use the REINFORCE algorithm (Williams 1992) to estimates gradients of the layout generator. Tang et al. (2019) explore tree structure to\nmodel visual context via the REINFORCE for VQA. Wu,\nXu, and Yang (2017) use the RL to learn to move and reshape a bounding box for one-stage REC. Different from\nthese methods, we model the reasoning process as a Markov\ndecision process and use the RL to achieve sequential reasoning for both REC and VQA.",
        "method": "The proposed method learns the dynamics of visual relational reasoning by constructing a graph to represent an input image and encouraging an agent to explore paths on the\ngraph according to an input sentence, as shown in Figure 2.\nIn this section, we ﬁrst formally deﬁne visual relational reasoning as a path routing task and then illustrate the method.\nFormulation\nWe focus on two visual relational reasoning tasks, referring\nexpression comprehension (REC) and visual question answering (VQA). The REC task aims to localize an object\ndescribed by a referring expression L in an image I represented by a set of objects O = {oi}N\ni=1, where N is the number of objects. The VQA task aims to provide an answer for a\nnatural language question L about the image I. For simplicity, here we use the same notations L and I to represent the\ninput sentence (i.e., the referring expression/question) and\nthe image, respectively.\nTo learn the dynamics of visual relational reasoning, we\ncast it as a path routing task on a visual graph G = {V, E}\nrepresenting the relational layout of the image I, where\nQuestion: What is the person to \nthe left of the glasses holding?  \nAnswer\n…\nSupervised Learning\nWord Representations\nQuestion \nRepresentation\nLanguage \nEncoding\nGraph \nConstruction\nVisual graph\n…\nPath Routing\nReinforcement \nLearning\nAgent: \nSupervised learning: \nReinforcement learning: \nAnswer \nClassifier\nFigure 2: Overview of our method for visual question answering. Black arrows show the forward process. Blue arrows and green\narrows show the backward processes of supervised learning and reinforcement learning, respectively. Our method constructs a\nvisual graph from an input image and encodes an input question to obtain the representations of the question and words. An\nagent navigates on the graph according to the question. The learned path is combined with the question to predict the answer.\nA reinforcement learning loss and a supervised learning loss are used to train the agent in an end-to-end manner.\nV = {vi}N\ni=1 is a set of nodes corresponding to the objects\nin I. E = {eij}N\ni,j=1 denotes the relationships among objects. In path routing, a reasoning model learns to navigate\non the graph G according to the sentence L to infer reasoning results. Here we formalize the reasoning dynamics in the\ncontext of path routing and deﬁne three concepts:\n• Reasoning state. In path routing, we deﬁne the reasoning\nstate st, the intermediate representation of a reasoning\nprocess, at each time step t ∈ {0, 1, ..., T} by the current\nnode vut ∈ V . T denotes the number of time steps for\npath routing. ut ∈ {1, 2, ..., N} indicates the index of\nthe node at the time step t.\n• State transition. The reasoning model in path routing\nperforms the transition from one reasoning state to another reasoning state. At each time step t, the model determines the next node vut+1 based on the current node\nvut to obtain the next state st+1.\n• Reasoning dynamics. After the path routing, a path\npT\n=\n{vu1, vu2, ..., vuT } is explored on the visual\ngraph. Thus the reasoning dynamics d = {s1, s2, ..., sT },\na time-indexed sequence of reasoning states, is determined.\nBy exploring paths, the dynamics of visual relational reasoning is supposed to be fully modeled because reasoning\nstates and state transitions are represented and characterized\nclearly. To this end, we present a reinforced path routing\nmethod that models path routing as a Markov Decision Process. The sentence L and the graph G comprise the external environment. An RL-based reasoning model serves as\nthe agent, whose states and actions correspond to reasoning\nstates and state transitions, respectively. In the following, we\nillustrate how we devise, reward, and optimize the reasoning\nmodel for visual relational reasoning.\nModel\nFeature Encoding\nFor a visual graph G = {V, E}, we\nrepresent a node vi via a local feature li ∈ Rdl encoding\nthe appearance information, and a spatial feature bi ∈ Rdb\nencoding its location and size. The two features are concatenated and projected into a common space Rd via a linear\nmapping as vi = Wv[li; bi], where Wv ∈ Rd×(dl+db),\nand\n\u0002\n· ; ·\n\u0003\ndenotes the concatenation operation of two vectors. We use a fully-connected graph to represent the image,\nwhich means there is an undirected edge eij between each\npair of nodes vi and vj. We do not obtain the representation\nof eij as its representation is not used in path routing.\nFor a sentence L which contains a sequence of M words\n{wk}M\nk=1, we use a Bi-LSTM (Schuster and Paliwal 1997)\nto encode the sequence and project it into the common space\nto obtain a sentence-level representation L ∈ Rd. The word\nrepresentation wk ∈ Rd of a word wk is obtained by concatenating corresponding forward and backward hidden vectors and projecting it into the common space. Note that we\nuse bold letters to denote the representations of corresponding non-bold letters throughout this paper.\nPolicy Network\nIn path routing, the agent is supposed to\ndetermine which node should be added to extend the current\npath. A navigation policy πnav is introduced for the agent.\nAt each time step t, the policy generates a probability distribution over all the nodes based on the current state st.\nThe representation of the state is obtained via st = Wsvut,\nwhere vut is the representation of the current node and the\nWs ∈ Rd×d is a learnable matrix. For the initial state s0,\nwe calculate a mean node representation by averaging all\nnode representations to represent it. We use an attentionbased neural network to parameterize the policy network (as\nshown in Figure 3), which is introduced in the following.\nFirstly, a language attention is introduced to enable the\nagent to focus on different parts of the sentence at different\ntime steps. The agent uses the attention mechanism based\nHistory\nCommand\nLanguage  \nAttention\nCurrent\nState\nJoint \nEmbedding\nHistory \nAttention\nHistory States\n…\n…\nAction\nSentence & Words\n…\n…\nFigure 3: Architecture of the policy network. It uses a language attention to generate a textual command by focusing\non important words, and a history attention to fuse the history nodes according to the command and the current state.\nThe history embedding, the command, and the state are combined for action prediction.\non current state st and the sentence-level representation L\nto generate a command embedding:\nct = W0\nM\nX\nk=1\nαL\nt,kwk,\nαL\nt,k = Softmaxk\n\u0000\nW1(wk ◦ W t\n2σ(W3[L; st]))\n\u0001\n,\n(1)\nwhere ◦ denotes the dot product operation of two vectors and\nσ denotes the RELU activation function. wk is the representation of the k-th word in L. W0 ∈ Rd×d, W1 ∈ Rd×d,\nW t\n2 ∈ Rd×d and W 3 ∈ Rd×2d are learnable matrices.\nSpeciﬁcally, W t\n2 denotes a single learnable matrix for each\niteration t. The obtained command embedding serves as an\ninstruction for models to perform state transitions.\nThen, based on the command and the current state, we\nfuse the history states via a history attention to generate the\nhistory embedding:\nht =\nt−1\nX\ni=1\nαH\nt,iW4si,\nαH\nt,i = Softmaxi (W5si ◦ W6[ct; st]) ,\n(2)\nwhere W4 ∈ Rd×d, W5 ∈ Rd×d and W6 ∈ Rd×2d are\nlearnable matrices.\nFinally, the state representation, the command embedding, and the history embedding are fused as a joint embedding gt = [ct; st; ht] to obtain the next state. Concretely,\nthe agent computes the probability of each node for being\nselected to extend the current path as\nαnav\nt,i = Softmaxi (W7gt ◦ (W8[st; vi] + W9ηt,i)) ,\n(3)\nwhere W7 ∈ Rd×3d, W8 ∈ Rd×2d and W9 ∈ Rd×1 are\nlearnable matrices. The vi is the representation of the i-th\nnode. The vector ηt = Pt−1\nj=0 αnav\nj\nis the accumulated probability distribution of previous time steps to enable the agent\nbe aware of the history decisions. The action at is sampled\nfrom the distribution to obtain the next node vut+1.\nOutput Module\nFor each sentence, we use an existing\nlanguage POS tagging method, the Spacy tool (Honnibal\nand Montani 2017), to obtain the part-of-speech (POS) tag\nof each word in it. Then we derive the number of steps T\nfor path routing of the sentence by computing the number\nof nouns in it. The ﬁnal state is thus sT . For REC, we directly output the ﬁnal node vuT as the predicted object for\nan input referring expression L. For VQA, we build a simple task-speciﬁc output module, an answer classiﬁer. The\nanswer classiﬁer projects the inputs into a probability distribution over all possible answers as αans = Wans[sT ; L],\nwhere αans denotes the output probability distribution and\nWans ∈ RN a×2d denotes a learnable matrix. N a is the\nnumber of answers. The answer with the highest probability ˆa is regarded as the predicted answer.\nReward\nThe ultimate goal of the agent is to infer reasoning results that match the objective of the reasoning task.\nFor REC, the objective is achieved if the ﬁnal node vuT is\nthe same as the ground-truth node vgt, while the objective\nof VQA is achieved if the predicted answer ˆa is exactly the\nground-truth answer agt. According to whether the objective\nis achieved, we devise an accuracy reward\nRt =\n\u001a10,\nif the objective is achieved\n0.\notherwise\n(4)\nOptimization\nRecent methods using reinforcement learning in visionlanguage tasks (Nguyen and Daum´e III 2019; Zhou et al.\n2020; Zhao, Wu, and Luo 2021; Wang et al. 2021) reveal\nthat warm-starting the agent with supervised learning can\nguarantee a relatively good policy. However, applying this\nstrategy in path routing is non-trivial. Firstly, the supervision for reasoning dynamics is unavailable, thus we can not\ndirectly optimize the policy network. Secondly, due to the\nnon-differentiability of the action prediction, it’s infeasible\nto train the model with the supervision of reasoning results.\nTo address this issue, we introduce a pre-training strategy\nthat encourages to model to gradually learn to move from\none node to another node in a supervised learning manner. Based on the pre-training strategy, a two-phase learning\nstrategy is developed, which is illustrated in the following.\nSupervised Learning\nThe softmax function is used to\ncompute the probability distribution over all actions in Eq.\n(3). Generally, given the probability distribution, an RLbased agent either uses a deterministic policy by taking the\naction with the highest probability or uses a stochastic policy by randomly selecting an action. Both policies lead to\nnon-differentiability.\nThus we use a differentiable scaled softmax function\nsoftmax(βx), where β > 0 is an scaling parameter, to replace the softmax function in Eq. (3) for the navigation policy. By increasing the scaling parameter, the function will\nbecome more non-smooth and thus can be used to approximate the deterministic policy as (Hinton, Vinyals, and Dean\n2015; Jang, Gu, and Poole 2017).\nAt the beginning of the supervised learning, we set β = 1\nto generate the probability distribution. Based on the distribution, the representation of the next state can be obtained\nby aggregating all node representations. During the training\nprocess, we gradually increase β by setting βi = (1 + γi)λ,\nwhere i is the current iteration number, γ and λ are two\nhyper-parameters. Intuitively, we allow the agent to softly\nnavigate to “multiple” nodes at the beginning but enforce it\ngradually learns to focus on only one node during the training process.\nAfter T rounds of path routing, we compute the taskspeciﬁc objective of supervised learning via a cross-entropy\nloss as\nLSL\ntask = −ytask log\n\u0000\nxtask\n\u0001\n,\n(5)\nwhere task ∈ {rec, vqa}, and ytask is an one-hot label\nwhose element representing the ground truth answer/object\nis 1 and others are 0. xrec = αnav\nT\ndenotes the probability\ndistribution of the navigation policy at the ﬁnal time step.\nxvqa = αans denotes the probability distribution generated\nby the answer classiﬁer.\nBesides, we introduce a visual coverage loss to encourage\nthe agent to focus on different nodes at different time steps in\nsupervised learning, inspired by the coverage mechanism in\ntext summarization (See, Liu, and Manning 2017). The loss\npenalizes the attention distribution that is similar to history\ndistributions and is computed as\nLSL\ncover = µ\nT −1\nX\nt=0\nN−1\nX\ni=0\nmin(ηt,i, αnav\nt,i ),\n(6)\nwhere ηt is the accumulated probability distribution of previous time steps. The coverage loss and the task-speciﬁc\nloss jointly supervise the model learning and µ is a hyperparameter to balance the losses.\nReinforcement Learning\nWe use a policy gradient\nmethod, the advantage actor-critic (A2C) algorithm (Mnih\net al. 2016), to train the policy network. The gradient of reinforcement learning is calculated as\n∇θJ(θ) =\nT\nX\nt=1\n∇θ log πnav (ap\nt | s0:t−1)\n\u0000\nT\nX\ni=t\nRi − bt\n\u0001\n,\n(7)\nwhere θ denotes the parameters of the policy network. bt is\nthe expected accumulated reward learned via a value function. A single fully-connected layer is used to map the current state representation st to the expected reward bt. Note\nthat, for VQA, due to the existence of the answer classiﬁer,\nthe reinforcement learning is combined with the supervised\nanswer classiﬁcation loss to train the model in an end-to-end\nmanner, as shown in Figure 2.",
        "experiment": "We apply the proposed method on two tasks, REC and VQA,\nto evaluate its effectiveness. We ﬁrst evaluate our method on\nREC, which tests the relational reasoning capability of models. We use two REC datasets: the CLEVR-Ref+ (Liu et al.\n2019b) that is a synthetic diagnostic dataset, and the Refreasoning (Yang, Li, and Yu 2020) that contains real images.\nThe reason to choose these two datasets is that they can provide more complex referring expressions that require strong\nreasoning capability. Secondly, we evaluate our method on\nVQA, which tests not only relational reasoning ability but\nalso other capabilities such as question answering and commonsense reasoning. The challenging GQA dataset (Hudson\nand Manning 2019a) that contains compositional questions\nabout real-world images is used. In the following, we illustrate the experimental settings and results for both tasks.\nReferring Expression Comprehension\nDatasets The CLEVR-Ref+ (Liu et al. 2019b) contains synthetic images and automatically generated referring expressions. There are a train split and a val split in the CLEVRRef+ dataset. A uniform sampling strategy is employed to\nguarantee the dataset is approximately unbiased. The RefReasoning dataset is a large-scale real-world dataset. It includes a train split, a val split, and a test split. The expressions of these splits are generated by using diverse expression templates and functional programs over scene graphs\nof images to guarantee diversity. These expressions may involve multiple objects and thus require strong visual reasoning ability to solve.\nImplementation details We use object-level features for the\ntwo datasets because our method explicitly navigates from\none object to another. The Ref-reasoning provides the 2048d object-level features detected by the Faster R-CNN detector (Ren et al. 2015). For the CLEVR-Ref+, we follow\nthe settings of (Hu et al. 2019) and use 1024-d object features extracted from the ResNet-101 (He et al. 2016). The\nground-truth bounding boxes are used for evaluations. For\nthe Ref-reasoning, the hyper-parameters µ, λ and γ are set as\n0.01, 0.5, and 0.01. For the CLEVR-Ref+, the three hyperparameters are set as 0.01, 0.5, and 0.001. The max number\nof time steps is set as 4 for the Ref-reasoning and 3 for the\nCLEVR-Ref+. For both datasets, the dimensions of the spatial feature db and the common space d are set as 128, and\n512, respectively.\nComparisons with state-of-the-art methods The results\nof our method and state-of-the-art methods on the RefReasoning dataset and the CLEVR-Ref+ dataset are listed\nin Table 1 and Table 2, respectively. We found from the tables that our method outperforms the others on both datasets,\nwhich demonstrates the effectiveness of our method for REC\nin both synthetic and real image datasets.\nFor the Ref-Reasoning dataset (from Table 1), the results\non the val split and the test split are presented. For the test\nsplit, the results on four subsets are also listed, where different subsets contain expressions with different numbers\nof objects. It can be seen from the table that our method\nis superior to other methods in both splits. For the test\nsplit, the improvement in the subsets with more objects is\nmore signiﬁcant than that in the subsets with fewer objects,\nwhich demonstrates the multi-step reasoning capability of\nour method. The DGA (Yang, Li, and Yu 2019b), the CMRIN (Yang, Li, and Yu 2019a), and the SGMN (Yang, Li,\nand Yu 2020) also build visual graphs to capture the relational layout of images. The DGA and the CMRIN perform language-guided visual graph convolution over visual\ngraphs, while the SGMN performs modular reasoning over\nvisual graphs. Beneﬁting from path routing, our method is\nMethods\nNumber of Objects\nSplit\none\ntwo\nthree\n≥ four\nval\ntest\nCNN\n10.57\n13.11\n14.21\n11.32\n12.36\n12.15\nCNN+LSTM\n75.29\n51.85\n46.26\n32.45\n42.38\n42.43\nDGA (Yang, Li, and Yu 2019b)\n73.14\n54.63\n48.48\n37.63\n45.37\n45.87\nCMRIN (Yang, Li, and Yu 2019a)\n79.20\n56.87\n50.07\n35.29\n45.43\n45.87\nSGMN (Yang, Li, and Yu 2020)\n79.71\n61.77\n55.57\n41.89\n51.04\n51.39\nOurs\n81.62\n62.43\n56.60\n43.95\n52.22\n53.02\nTable 1: Results of our method and the state-of-the-art methods on the val split and the test split of the Ref-Reasoning dataset.\nMethods\nAccuracy\nStack-NMN (Hu et al. 2018)\n56.5\nSLR (Yu et al. 2017)\n57.7\nMAttNet (Yu et al. 2018)\n60.9\nGroundeR (Rohrbach et al. 2016)\n61.7\nLCGN (Hu et al. 2019)\n74.8\nLCGN†\n76.0\nOurs†\n77.7\nTable 2: Results of our method and the state-of-the-art on the\nval split of the CLEVR-Ref+ dataset. † indicates this method\nuses ground-truth bounding boxes. The other methods use\ngrid features.\ncapable of achieving complex visual relational reasoning\nand thus outperforms all three methods.\nIn the CLEVR-Ref+ dataset (from Table 2), our method\noutperforms other methods. To make fair comparisons, we\ntrain the LCGN (Hu et al. 2019) model with ground truth\nbounding boxes. And our method also outperforms this\nmodel. These results demonstrate the effectiveness of our\nmethod for grounding complex referring expressions on synthetic images.\nAblation Studies We evaluate different variants of our\nmodel by ablating certain components to study the effectiveness of several important components of our method. The results of those models on the test split of the Ref-Reasoning\ndataset are shown in Table 3.\nFirstly, we investigate the inﬂuence of the history states\nand history attention in the policy network. We remove the\nhistory attention of the policy network and obtain a model\ncalled “Ours (w/o history attention)”. We observe the model\nperforms signiﬁcantly worse than our full model, which\ndemonstrates the history states are critical in path routing for\nvisual relational reasoning. Similarly, by removing the term\nabout history decisions in Eq (3), we obtain “Ours (w/o history decisions)”. The comparisons between the model and\nthe full model show that the history decisions are also beneﬁcial for path routing.\nThen, we study the inﬂuence of the pre-training strategy.\nWe train the agent from scratch and obtain a model called\n“Ours (w/o pre-training)”. We ﬁnd that the model performs\nsigniﬁcantly worse than the full model, which demonstrates\nthat the pre-training is indispensable for path routing. The\nmain reason is that the visual and language inputs are noisy\nand directly performing path routing without any initializaMethods\nAccuracy\nOurs (w/o history attention)\n48.40\nOurs (w/o history decision)\n52.15\nOurs (w/o pre-training)\n19.16\nOurs (w/o scaled activation)\n50.80\nOurs (w/o coverage loss)\n49.77\nOurs (w adaptive nodes)\n51.84\nOurs\n53.02\nTable 3: Results of different variants of our model on the test\nsplit of the Ref-Reasoning dataset.\ntion may lead to collapse. By using a vanilla softmax function in pre-training, we obtain the “Ours (w/o scaled activation)”. Besides, we remove the coverage loss and obtain the\n“Ours (w/o coverage loss)”. These models are also inferior\nto the full model, which shows gradually learning to move\nfrom one node to another in pre-training is beneﬁcial.\nFinally, we modify our model to let it adaptively focus\non one to three nodes based on the outputted probability\ndistribution. The representations of the attended nodes are\nsummed to obtain the state representation for further path\nrouting. The obtained model is entitled “Ours (w adaptive\nnodes)”. We observe that although its potential capability is\nstronger, its performance is only comparable with the full\nmodel. The reason is that the dataset mainly focuses on relations of two objects rather than three or more objects (such\nas “surrounded by”). We found that current datasets about\nrelational reasoning hardly contain samples involving relations of three or more objects.\nVisual Question Answering\nDatasets The GQA dataset (Hudson and Manning 2019a) is\na large-scale dataset with 140K real images and 1.7M balanced questions-answer pairs. The dataset has a train split\nfor training, a test-dev split for validation, and a test split for\nonline testing. Each image in the GQA is associated with\na manually annotated scene graph describing the classes, attributes, relations of objects in the image. Based on the scene\ngraphs, diverse compositional questions that require multistep reasoning are generated via a question engine.\nImplementation details In our implementation, the bottomup-attention model (Anderson et al. 2018) is used to extract\n2048-d object-level features. For each image, we keep the\ntop 48 bounding boxes ranked by conﬁdence scores. The\nhyper-parameters µ, λ and γ are set as 0.001, 0.5 and 0.001,\n(a)\n(b)\nQuestion: What kind of sign is \nmade of the same material as the \nvehicle next to the sidewalk?\n(GT: street sign) \nsidewalk (0.5), next (0.1), what (0.06)\nt=1\nsign (0.3), vehicle (0.24), as(0.19)\nt=2\nsign (0.77), what (0.08), kind (0.02)\nt=3\nt=4\n--Language Attention Maps\nPath Routing\nPred: street sign\n2\n1\n3\nQuestion: Does the soccer player \nthat to the right of the ball run on \nthe green grass? (GT: yes) \nball (0.58), run (0.15), of (0.08)\nt=1\nplayer (0.65), soccer (0.12), run (0.1)\nt=2\nplayer (0.31), right (0.31), that (0.12)\nt=3\nt=4\nplayer (0.26), that (0.19), on (0.11)\nLanguage Attention Maps\nPath Routing\nPred: yes\n1\n2\n3\n4\nFigure 4: Qualitative examples from the test-dev split of the GQA. For each example, the upper-left corner shows a question\nand the answer of it. The lower-left corner shows the language attention maps of our method. For each language attention map,\nthe top-3 words with the highest attention values are shown. The right side shows the nodes during the path routing and the\npredicted answer for the question. The node in the t-th time step is marked by a green rectangle with the corresponding number.\nMethods\nAccuracy\nBottom-Up (Anderson et al. 2018)\n49.74\nMAC (Hudson and Manning 2018)\n54.06\nNMN (Andreas et al. 2016)\n55.70\nBAN (Kim, Jun, and Zhang 2018)\n57.10\nGRN (Guo, Xu, and Tao 2019)\n57.04\nLCGN (Hu et al. 2019)\n57.07\nLXMERT (Tan and Bansal 2019)\n60.33\nMMN (Chen et al. 2021)\n60.83\nNSM (Hudson and Manning 2019b)\n63.17\nOurs\n59.43\nTable 4: Results of our method and the state-of-the-art methods on the test split of the GQA dataset.\nrespectively. The max number of time steps is set as 4. The\ndimensions of the spatial feature and the common space are\nset as 96, and 512, respectively. In the supervised learning\nstage, we train the model with the “all” split of questions of\nthe GQA and ﬁne-tune it with the “balanced” split as (Chen\net al. 2021). In supervised learning, we also perform stepwise supervision as (Chen et al. 2021) by training a model\nwith ground-truth scene graphs of the GQA dataset and executing knowledge distillation. The obtained model is then\nused to initialize the agent for reinforcement learning.\nComparisons with state-of-the-art methods The results\nof the proposed method and the state-of-the-art methods\non the test split are listed in Table 4. We observe that our\nmethod achieves comparable performance with other methods, which demonstrates the effectiveness of our method\nfor answering compositional questions through complex\nrelational reasoning. Our method outperforms two graphnetwork-based visual reasoning methods, the GRN (Guo,\nXu, and Tao 2019) and the LCGN (Hu et al. 2019), thanks\nto the learning of dynamics. The main reason that we do\nnot surpass all the state-of-the-art is that the GQA evaluates\nnot only relational reasoning capability but also the question\nanswering. But in this paper, we mainly focus on visual relational reasoning. By contrast, most previous models are tailored for the VQA task and cannot be applied in other tasks\nsuch as REC. The NSM (Hudson and Manning 2019b) relies\non a scene graph generation model (Yang et al. 2018) and the\nLXMERT (Tan and Bansal 2019) uses multiple datasets to\npre-train their model.\nQualitative Results We visualize the reasoning processes of\nour method to demonstrate its transparency. Figure 4 shows\ntwo qualitative examples from the test-dev split of the GQA.\nFor each example, we provide the language attention map\nand the current node of each time step t in path routing. It\nis shown that our method can relatively accurately focus on\nnouns in questions and further localize the corresponding\nobjects in images. In the left example, it gradually localizes\nthe sidewalk, the vehicle, and the sign and then ﬁgures out\nthe correct answer. The overall reasoning process is almost\nfaithful and close to the thinking process of humans.",
        "conclusion": "In this paper, we have presented a reinforced path routing method for visual relational reasoning, providing a new\npoint of view for this area. Our method learns the dynamics\nof reasoning by introducing a reasoning model to explore\npaths over a visual graph based on an input sentence to infer\nreasoning results. Extensive experiments demonstrate that\nour method is capable of achieving accurate visual relational\nreasoning with transparent reasoning processes.\nThe results of this work illustrate that learning the dynamics is an effective and promising way to simulate human reasoning ability. It is worth mentioning that there are two avenues for further studies to learn paths consistent with the\nthinking processes of humans. Firstly, we would like to introduce a rollback mechanism to enable the reasoning model\nback to one previous node when it navigates to a wrong\nnode. Secondly, we plan to curate a visual relational reasoning dataset with human-annotated trajectories of attention to\nsupervise and evaluate path routing.",
        "summary_en": "Reasoning is a dynamic process. In cognitive theories, the dynamics of reasoning refers to reasoning states over time after successive state transitions. Modeling the cognitive dynamics is of utmost importance to simulate human reasoning capability. This paper proposes to learn the reasoning dynamics of visual relational reasoning by casting it as a path routing task. The paper presents a reinforced path routing method that represents an input image via a structured visual graph and introduces a reinforcement learning based model to explore paths (sequences of nodes) over the graph based on an input sentence to infer reasoning results. By exploring such paths, the proposed method represents reasoning states clearly and characterizes state transitions explicitly to fully model the reasoning dynamics for accurate and transparent visual relational reasoning. Extensive experiments on referring expression comprehension and visual question answering demonstrate the effectiveness of the method.",
        "summary_zh": "这篇论文介绍了一种通过强化路径路由来学习视觉关系推理的动态过程。针对视觉关系推理中存在的动态性问题，将其建模为路径路由任务，并提出了一种增强路径路由方法。该方法利用结构化视觉图表示输入图像，并引入基于强化学习的模型来根据输入句子在图上探索路径（节点序列），以推断推理结果。通过探索这些路径，该方法清晰地表示推理状态，并明确地刻画状态转换，以全面地模拟推理动态过程，从而实现准确、透明的视觉关系推理。论文的实验结果在指代表达理解和视觉问答等任务上展示了该方法的有效性。"
    },
    {
        "title": "Visual Concept Reasoning Networks",
        "abstract": "A split-transform-merge strategy has been broadly used as an architectural constraint in convolutional neural networks for visual recognition tasks. It approximates sparsely connected networks by explicitly deﬁning multiple branches to simultaneously learn representations with different visual concepts or properties. Dependencies or interactions between these representations are typically deﬁned by dense and local operations, however, without any adaptiveness or high-level reasoning. In this work, we propose to exploit this strategy and combine it with our Visual Concept Reasoning Networks (VCRNet) to enable reasoning between high-level visual concepts. We associate each branch with a visual concept and derive a compact concept state by selecting a few local descriptors through an attention module. These concept states are then updated by graph-based interaction and used to adaptively modulate the local descriptors. We describe our proposed model by split-transform-attend-interact-modulatemerge stages, which are implemented by opting for a highly modularized architecture. Extensive experiments on visual recognition tasks such as image classiﬁcation, semantic segmentation, object detection, scene recognition, and action recognition show that our proposed model, VCRNet, consistently improves the performance by increasing the number of parameters by less than 1%.",
        "introduction": "Convolutional neural networks have shown notable success\nin visual recognition tasks by learning hierarchical representations. The main properties of convolutional operations,\nwhich are local connectivity and weight sharing, are the key\nfactors that make it more efﬁcient than fully-connected networks for processing images. The local connectivity particularly comes up with a fundamental concept, receptive\nﬁeld, that deﬁnes how far the local descriptor can capture\nthe context in the input image. In principle, the receptive\nﬁeld can be expanded by stacking multiple convolutional\nlayers or increasing the kernel size of them. However, it is\nknown that the effective receptive ﬁeld only covers a fraction of the theoretical size of it (Luo et al. 2016). This eventually restricts convolutional neural networks to capture the\nglobal context based on long-range dependencies. On the\nother hand, most convolutional neural networks are characterized by dense and local operations that take the advantage\nof the weight sharing property. It hence typically lacks an\ninternal mechanism for high-level reasoning based on abstract semantic concepts such as those humans manipulate\nwith natural language and inspired by modern theories of\nconsciousness (Bengio 2017). It is related to system 2 cognitive abilities, which include things like reasoning, planning,\nand imagination, that are assumed to capture the global context from interactions between a few abstract factors and accordingly give feedback to the local descriptor for decisionmaking.\nThere have been approaches to enhance capturing longrange dependencies such as non-local networks (Wang et al.\n2018). The main concept of it, which is related to selfattention (Vaswani et al. 2017), is to compute a local descriptor by adaptively aggregating other descriptors from all\npositions, regardless of relative spatial distance. In this setting, the image feature map is plugged into a fully-connected\ngraph neural network, where all local positions are fully connected to all others. It is able to capture long-range dependencies and extract the global context, but it still works with\ndense operations and lacks high-level reasoning. Both LatentGNN (Zhang, He, and Yan 2019) and GloRe (Chen et al.\n2019) alleviate these issues by introducing compact graph\nneural networks with some latent nodes designed to aggregate local descriptors.\nIn this work, we propose Visual Concept Reasoning Networks (VCRNet) to enable reasoning between high-level visual concepts. We exploit a modularized multi-branch architecture that follows a split-transform-merge strategy (Xie\net al. 2017). While it explicitly has multiple branches\nto simultaneously learn multiple visual concepts or properties, it only considers the dependencies or interactions\nbetween them by using dense and local operations. We\nextend the architecture by split-transform-attend-interactmodulate-merge stages, and this allows the model to capture the global context by reasoning with sparse interactions\nbetween high-level visual concepts from different branches.\nThe main contributions of the paper are:\n• We propose Visual Concept Reasoning Networks (VCRNet) that efﬁciently capture the global context by reasoning over high-level visual concepts.\nFigure 1: A residual block with visual concept reasoning modules.\n• We compactly implement our proposed model by exploiting a modularized multi-branch architecture composed of\nsplit-transform-attend-interact-modulate-merge stages.\n• We showcase that our proposed model improves the performance more than other models by increasing the number of parameters by less than 1% on multiple visual\nrecognition tasks.",
        "related works": "Multi-branch architectures are carefully designed with multiple branches characterized by different dense operations,\nand split-transform-merge stages are used as the building\nblocks. The Inception models (Szegedy et al. 2015) are\none of the successful multi-branch architectures that deﬁne branches with different scales to handle multiple scales.\nResNeXt (Xie et al. 2017) is another version of ResNet (He\net al. 2016) having multiple branches with the same topology in residual blocks, and it is efﬁciently implemented by\ngrouped convolutions. In this work, we utilize this residual\nblock and associate each branch of it with a visual concept.\nThere have been several works to adaptively modulate\nthe feature maps based on the external context or the\nglobal context of input data. Squeeze-and-Excitation networks (SE) (Hu, Shen, and Sun 2018) use a gating mechanism to do channel-wise re-scaling in accordance with the\nchannel dependencies based on the global context. GatherExcite networks (GE) (Hu et al. 2018) further re-scale locally that it is able to ﬁnely redistribute the global context\nto the local descriptors. Convolutional block attention module (CBAM) (Woo et al. 2018) independently and sequentially has channel-wise and spatial-wise gating networks to\nmodulate the feature maps. All these approaches extract\nthe global context by using the global average pooling that\nequally attends all local positions. Dynamic layer normalization (DLN) (Kim, Song, and Bengio 2017) and Featurewise Linear Modulation (FiLM) (Perez et al. 2018) present\na method of feature modulation on normalization layers by\nconditioning on the global context and the external context,\nrespectively.\nContent-based soft-attention mechanisms (Bahdanau,\nCho, and Bengio 2015) have been broadly used on neural networks to operate on a set of interchangeable objects\nand aggregate it. Particularly, Transformer models (Vaswani\net al. 2017) have shown impressive results by using multihead self-attention modules to improve the ability to capture\nlong-range dependencies. Non-local networks (NL) (Wang\net al. 2018) use this framework in pixel-level self-attention\nblocks to implement non-local operations. There are some\nadditional related works that one augments the self-attention\nmodules to convolutional operations (Bello et al. 2019a),\nand another replaces all of them with a form of selfattention (Ramachandran et al. 2019). Global-context networks (GC) (Cao et al. 2019) simplify the non-local networks by replacing the pixel-level self-attention with an attention module having a single ﬁxed query that is globally\nshared and learned. Attention-augmented convolutional networks (Bello et al. 2019b) similarly augment convolutional\noperators with self-attention modules as the non-local networks, but concatenate feature maps from convolution path\nand self-attention path. LatentGNN (Zhang, He, and Yan\n2019) and Global reasoning module (GloRe) (Chen et al.\n2019) differently simpliﬁes the non-local networks that they\nﬁrst map local descriptors into latent nodes, where the number of nodes is relatively smaller than the number of local\npositions, and capture the long-range dependencies from interactions between the latent nodes. Our proposed model is\nsimilar to these two models, but we take the advantage of\nthe multi-branch architecture and the attention mechanism\nto efﬁciently extract a set of distinct visual concept states\nfrom the input data.",
        "methods": "In this section, we introduce our proposed model, Visual\nConcept Reasoning Network (VCRNet), and describe the\noverall architecture and its components in detail. The proposed model is designed to reason over high-level visual\nconcepts and accordingly modulate feature maps based on\nits result. In the following, we assume the input data X ∈\nRHW ×d is a 2D tensor as an image feature map, where\nH, W, and d refer to the height, width, and feature size of X,\nrespectively. Moreover, for simplicity, we denote all modules by a function Ffunc(·; θ), where θ is a learnable parameter and the subscript func brieﬂy explains the functionality\nof it.\nModularized Multi-Branch Residual Block\nResidual blocks are composed of a skip connection and multiple convolutional layers (He et al. 2016). We especially\ntake advantage of using a residual block of ResNeXt (Xie\net al. 2017) that operates by grouped convolutions. This\nblock is explicable by a split-transform-merge strategy and\na highly modularized multi-branch architecture. It has an\nadditional dimension ”cardinality” to deﬁne the number of\nbranches used in the block. The branches are deﬁned by\nseparate networks, which are based on the same topology\nand implemented by grouped convolutions, processing nonoverlapping low-dimensional feature maps. In this work, we\nuse this block by regarding each branch as a network learning representation of a speciﬁc visual concept and, therefore,\nrefer to the cardinality as the number of visuals concepts C.\nThe split-transform-merge strategy can be described by visual concept processing as the following. Each concept c has\na compact concept-wise feature map Zc ∈ RHW ×p, where\np is a lot smaller than d. It is initially extracted from the\ninput data X by splitting it into a low-dimensional feature\nmap ˜Xc ∈ RHW ×p with a 1×1 convolution Fsplit(X; θsplit\nc\n).\nAfterward, it is followed by a concept-wise transformation\nbased on a 3 × 3 convolution Ftrans( ˜Xc; θtrans\nc\n) while keeping\nthe feature size compact. The extracted concept-wise feature maps {Zc}C\nc=1 are then projected back into the input\nspace to be merged as Y = X + PC\nc=1 Fmerge(Zc; θmerge\nc\n).\nThis overall multi-branch procedure interestingly can be\nhighly modularized and parallelized by grouped convolutions. However, it lacks the ability of reasoning over the\nhigh-level visual concepts that captures both local and global\ncontexts. We propose to extend this approach by introducing additional modules to enable visual concept reasoning.\nOur proposed model is based on a new strategy with splittransform-attend-interact-modulate-merge stages. The new\nstages completely work into the residual block with the following modules: (a) concept sampler, (b) concept reasoner,\nand (c) concept modulator. The overall architecture is depicted in Figure 1 showing how it is highly modularized by\nsharing the topology between different concepts. We refer\nto networks having residual blocks with these modules, as\nVisual Concept Reasoning Networks (VCRNet).\nConcept Sampler\nThe concept-wise feature maps {Zc}C\nc=1 are composed of\nall possible pixel-level local descriptors, which contain spatially local feature information, as sets of vectors. To do efﬁcient reasoning over the visual concepts, it ﬁrst requires a\nset of abstract feature vectors representing the visual concepts. Therefore, a form of aggregation mechanism is necessary to derive a set of visual concept states, where each\nstate is a vector, from the concept-wise feature maps. We implement this by presenting a concept sampler (CS) module.\nEach concept c has a separate concept sampler FCS(Zc; θCS\nc )\nthat aggregates the set of local descriptors in Zc and converts it into a concept state hc ∈ R1×˜p, where we set\n˜p = min(p/4, 4). We introduce two types of concept samplers that are based on pooling and attention operations, respectively. Global average pooling is one of the simplest\nFigure 2: Concept samplers with different approaches (⊗ is\na weighted-sum operation).\nways to extract the global context from a feature map without explicitly capturing long-range dependencies. It equally\nand densely attends all local positions to aggregate the local\ndescriptors. Our pooling-based sampler adopts this operation to compute the concept state hc as shown in Figure 2.a,\nand it is formulated as:\nhc = FGAP(Zc)W v\nc =\n\n\n1\nHW\nH\nX\ni=1\nW\nX\nj=1\nZc[i, j]\n\n W v\nc , (1)\nwhere Zc[i, j] ∈ R1×p is a local descriptor at position (i, j),\nand W v\nc ∈ Rp×˜p is a learnable projection weight. In comparison with the attention-based sampler, it is simple and\ncompact having a small number of parameters, but there\nis no data-adaptive process. Due to its simplicity, similar\napproaches have been broadly used in the previous works\nsuch as SENet (Hu, Shen, and Sun 2018) and CBAM (Woo\net al. 2018). The attention mechanism operates by mapping\na query vector and a set of interchangeable key-value vector\npairs into a single vector, which is a weighted sum of value\nvectors. It allows us to aggregate a set of local descriptors\nby sparsely and adaptively selecting them. We hence apply\nthis approach to our concept sampler. For each concept c, the\nquery vector qc ∈ R1×˜p describes what to focus on during\naggregation. The concept-wise feature map Zc converts into\na set of key-value vector pairs that we separately project it\ninto a key map Kc = ZcW k\nc and a value map Vc = ZcW v\nc ,\nwhere W k\nc , W v\nc ∈ Rp×˜p are learnable projection weights.\nThe concept state hc is derived by computing the dot products of the query vector qc with the key map Kc and subsequently applying a softmax function to obtain the attention\nweights over the value map Vc as:\nhc =\n \nsoftmax\n \nqc\n\u0000\nZcW k\nc\n\u0001⊤\n√˜p\n!\nZc\n!\nW v\nc .\n(2)\nThe query vector qc can be either learned as a model parameter or computed by a function of the feature map Zc. The\nformer approach deﬁnes a static query that is shared globally\nover all data. GCNet (Cao et al. 2019) uses this approach, instead of global average pooling, to extract the global context.\nIt can be simpliﬁed and implemented by replacing the term\nqc\n\u0000\nZcW k\nc\n\u0001⊤ in Equation 2 with a 1 × 1 convolution as depicted in Figure 2.b. The latter approach, in contrast, uses a\ndynamic query that varies according to Zc. We set the query\nas an output of the function as qc = FGAP(Zc)W q\nc , which is\nequal to the pool-based sampler, as shown in Equation 1.\nThe concept samplers can be viewed as multi-head attention modules in Transformer models (Vaswani et al. 2017)\nthat we set each concept to be operated by a single-head attention module. However, our concept samplers don’t process the same input feature map as they do. Each concept\nis only accessible to its corresponding feature map, and this\nencourages the concept samplers to attend and process different features. Moreover, we explicitly deﬁne concept-wise\nqueries to aggregate pixel-wise (low-level) descriptors and\nobtain global descriptors (high-level concepts) rather than\nonly capturing long-range dependencies as non-local networks (Wang et al. 2018) work with pixel-level dense selfattention operations.\nConcept Reasoner\nThe visual concept states are derived independently from\nseparate branches in which no communication exists. Therefore, we introduce a reasoning module, Concept Reasoner\n(CR), to make the visual concept states to interact with\nthe others and accordingly update them. We opt for using\na graph-based method by deﬁning a fully-connected graph\nG = (V, E) with nodes V and directional edges E. The node\nvc ∈ V corresponds to a single visual concept c and is described by the visual concept state hc. The edge ecc′ ∈ E\ndeﬁnes the relationship or dependency between visual concepts c and c′. It is further speciﬁed by an adjacency matrix A ∈ RC×C to represent edge weight values in a matrix\nform. Based on this setting, we describe the update rule of\nthe visual concept states as:\n˜hc = ReLU\n \nBN\n \nhc +\nC\nX\nc′=1\nA[c, c′]hc′\n!!\n,\n(3)\nwhere A[c, c′] ∈ R is a edge weight value, and batch normalization (BN) and ReLU activation are used. This can also\nbe implemented in a matrix form as ˜H = ReLU(BN(H +\nAH)), where H = [h1; h2; ...; hC] ∈ RC×˜p is vertically\nstacked concept states. The adjacency matrix A can be\ntreated as a module parameter that is learned during training. This sets the edges to be static that all relationships between visual concepts are consistently applied to all data.\nHowever, we relax this constraint by dynamically computing the edge weights based on the concept states. A function A[c, :] = Fedge(hc; W edge) = tanh(hcW edge), where\nW edge ∈ R˜p×C is a learnable projection weight, is used to\nget all edge weights A[c, :] related to the concept c as shown\nin Figure 3. This function learns how each concept adaptively relates to the others based on its state.\nConcept Modulator\nThe updated concept states are regarding not only a single\nconcept, but also the others as a result of reasoning based\nFigure 3: (Left) Concept reasoner and (right) modulator\non interactions. This information has to be further propagated to local concept features, which are extracted from the\nmainstream of the network. However, this is a non-trivial\nproblem due to dimensional mismatch that the concept states\nare vectors not containing any explicit spatial information.\nWe alleviate this issue by implementing a module, Concept\nModulator (CM), which is based on a feature modulation\napproach. It modulates the concept-wise feature maps by\nchannel-wise scaling and shifting operations. These operations are conditioned on the updated concept states to ﬁnetune the feature maps based on the result of reasoning. We\ndesign this module based on DLN (Kim, Song, and Bengio 2017) and FiLM (Perez et al. 2018). Both models use\nfeature-wise afﬁne transformations on normalization layers\nby dynamically generating the afﬁne parameters instead of\nlearning them. In this way, we deﬁne separate modules for\nthe visual concepts as shown in Figure 3. Each concept-wise\nfeature map Xc is modulated as:\n˜Xc = FCM(˜hc, Xc; θCM\nc\n) = ReLU (αc ⊙ Xc + βc) ,\nαc = ˜hcW scale\nc\n+ bscale\nc\n, βc = ˜hcW shift\nc\n+ bshift\nc\n,\nwhere ⊙ indicates channel-wise multiplication. αc, βc ∈\nR1×p are scaling and shifting parameters, respectively,\nwhich are adaptively computed by linearly mapping the updated concept state ˜hc.\nWe further implement pixel-level concept modulators to\npropagate the global context adaptively and differently into\nlocal descriptors. Each concept state hc is derived by computing the attention map Mc ∈ RHW ×1 from the concept\nsampler as shown in Equation 2, and we assume it contains\nthe spatial information related to the concept c. Therefore,\nwe utilize this attention map for the pixel-level concept modulator. We ﬁrst re-normalize the attention map by its maximum value:\n˜\nMc =\nMc\nmax(Mc), Mc = softmax\n \nqc\n\u0000\nZcW k\nc\n\u0001⊤\n√˜p\n!\n.\nWithout this re-normalization, the learning doesn’t work\nproperly. The re-normalized attention map ˜\nMc is used to\nproject the updated concept state ˜hc into all local positions\nby projection ˜\nMc˜hc ∈ RHW ×˜p. Based on this projection,\nwe are able to do pixel-level feature modulation as:\n˜Xc = FCM(˜hc, ˜\nMc, Xc; θCM\nc\n) = ReLU (αc · Xc + βc)\nαc =\n\u0010\n˜\nMc˜hc\n\u0011\nW scale\nc\n+ bscale\nc\n, βc =\n\u0010\n˜\nMc˜hc\n\u0011\nW shift\nc\n+ bshift\nc\n,\nwhere · is an element-wise multiplication. Both αc and βc\nare having the same size as the feature map Xc so that all local positions have separate scaling and shifting parameters.\nModel\nError (%)\n# of\nParams\nGFLOPs\nTop-1\nTop-5\nResNeXt-50 (Xie et al. 2017)\n21.10\n5.59\n25.03M\n4.24\nResNeXt-50 + SE (Hu, Shen, and Sun 2018)\n20.79\n5.38\n27.56M\n4.25\nResNeXt-50 + CBAM (Woo et al. 2018)\n20.73\n5.36\n27.56M\n4.25\nResNeXt-50 + GC (Cao et al. 2019)\n20.44\n5.34\n27.58M\n4.25\nResNeXt-50 + GloRe (Chen et al. 2019)\n20.15\n5.14\n30.79M\n5.86\nResNeXt-50 + VCR (ours)\n19.97\n5.03\n25.26M\n4.26\nResNeXt-50 + VCR (ours, pixel-level)\n19.94\n5.18\n25.26M\n4.29\nResNeXt-101 (Xie et al. 2017)\n19.82\n4.96\n44.18M\n7.99\nResNeXt-101 + SE (Hu, Shen, and Sun 2018)\n19.39\n4.73\n48.96M\n8.00\nResNeXt-101 + CBAM (Woo et al. 2018)\n19.60\n4.87\n48.96M\n8.00\nResNeXt-101 + GC (Cao et al. 2019)\n19.52\n5.03\n48.99M\n8.00\nResNeXt-101 + GloRe (Chen et al. 2019)\n19.56\n4.85\n49.93M\n9.61\nResNeXt-101 + VCR (ours)\n18.84\n4.48\n44.60M\n8.01\nTable 1: Results of image classiﬁcation on ImageNet validation set\nBackbone Network\nAPbbox\nAPbbox\n50\nAPbbox\n75\nAPmask\nAPmask\n50\nAPmask\n75\n# Params\nResNeXt-50 (Xie et al. 2017)\n40.70\n62.02\n44.49\n36.75\n58.89\n39.03\n43.94M\nResNeXt-50 + SE (Hu, Shen, and Sun 2018)\n41.04\n62.61\n44.45\n37.13\n59.53\n39.79\n46.47M\nResNeXt-50 + CBAM (Woo et al. 2018)\n41.69\n63.54\n45.17\n37.48\n60.27\n39.71\n46.47M\nResNeXt-50 + GC (Cao et al. 2019)\n41.66\n63.76\n45.29\n37.58\n60.36\n39.92\n46.48M\nResNeXt-50 + GloRe (Chen et al. 2019)\n42.31\n64.18\n46.13\n37.83\n60.63\n40.17\n49.71M\nResNeXt-50 + VCR (ours)\n41.81\n63.93\n45.67\n37.71\n60.36\n40.25\n44.18M\nResNeXt-50 + VCR (ours, pixel-level)\n42.02\n64.15\n45.87\n37.75\n60.62\n40.22\n44.18M\nTable 2: Results of object detection and instance segmentation on COCO 2017 validation set",
        "experiments": "In this section, we run experiments on visual recognition tasks such as image classiﬁcation, object detection/segmentation, scene recognition, and action recognition with large-scale datasets. In all experiments, we set\nResNeXt (Xie et al. 2017), which performs better than\nResNet (He et al. 2016) with less parameters, as a base\narchitecture with cardinality = 32 and base width = 4d.\nAs our main contribution is to exploit the multi-branch architecture to enable high-level concept reasoning by implementing split-transform-attend-interact-modulate-merge\nstages, we only use the ResNeXt as a backbone network that\nit is the only one already having the split-transform-merge\nstages allowing us to seamlessly implement our VCRNet.\nFurthermore, our proposed model, VCRNet, is also deﬁned\nby C = 32 concepts in all residual blocks. We also compare VCRNet against other networks (modules), which have\na form of attention or reasoning modules, such as Squeezeand-Excitation (SE) (Hu, Shen, and Sun 2018), Convolutional Block Attention Module (CBAM) (Woo et al. 2018),\nGlobal Context block (GC) (Cao et al. 2019), and Global\nReasoning unit (GloRe) (Chen et al. 2019). All networks\nare implemented in all residual blocks in the ResNeXt except GloRe, which is partially adopted in the second and\nthird residual stages. In all experiments, we mainly set VCRNet with using (1) attention-based concept samplers with\ndynamic queries, (2) concept reasoners with dynamic edge\nweights, and (3) concept modulators with channel-level feature map scaling and shifting.\nImage Classiﬁcation\nWe conduct experiments on a large-scale image classiﬁcation task on the ImageNet dataset (Russakovsky et al. 2015).\nThe dataset consists of 1.28M training images and 50K validation images from 1000 different classes. All networks\nare trained on the training set and evaluated on the validation set by reporting the top-1 and top-5 errors with single\ncenter-cropping. Our training setting is explained in detail\nin Appendix. The overall experimental results are shown in\nTable 1, where all results are reproduced by our training setting for a fair comparison. For evaluation, we always take\nthe ﬁnal model, which is obtained by exponential moving\naverage (EMA) with the decay value 0.9999. VCRNet consistently outperforms than other networks in both ResNeXt50 and ResNeXt-101 settings. Moreover, it is more compact\nthan the others as it only increases the number of parameters by less than 1%(≈ 0.95%). In contrast, GloRe (Chen\net al. 2019), which also does high-level reasoning as our\nmodel, requires more parameters than ours, although it is\npartially applied in the ResNeXt architecture. In addition,\nwe test the pixel-level concept modulators to reuse the attention maps extracted from the concept samplers to modulate\nlocal descriptors at pixel-level as GloRe has a pixel-level reprojection mechanism. The modiﬁcation slightly improves\nthe top-1 performance by using the same number of parameters, but it increases the computational cost (GFLOPs).\nModel\nError (%)\n# of\nParams\nTop-1\nTop-5\nResNeXt-50 (Xie et al. 2017)\n43.49\n13.54\n23.73M\nResNeXt-50 + SE (Hu, Shen, and Sun 2018)\n43.18\n13.41\n26.26M\nResNeXt-50 + CBAM (Woo et al. 2018)\n43.18\n13.45\n26.26M\nResNeXt-50 + GC (Cao et al. 2019)\n43.07\n13.34\n26.28M\nResNeXt-50 + GloRe (Chen et al. 2019)\n42.94\n13.22\n29.48M\nResNeXt-50 + VCR (ours)\n42.92\n12.96\n23.96M\nTable 3: Results of scene recognition on Places-365\nBackbone network\n(Slow-only pathway)\nError (%)\n# of\nParams\nTop-1\nTop-5\nResNeXt-50 (Xie et al. 2017)\n26.41\n9.43\n40.07M\nResNeXt-50 + SE (Hu, Shen, and Sun 2018)\n25.06\n8.70\n42.58M\nResNeXt-50 + CBAM (Woo et al. 2018)\n24.87\n8.81\n42.59M\nResNeXt-50 + GC (Cao et al. 2019)\n25.31\n9.32\n42.60M\nResNeXt-50 + GloRe (Chen et al. 2019)\n25.52\n9.23\n45.81M\nResNeXt-50 + VCR(ours)\n24.73\n8.39\n40.28M\nTable 4: Results of action recognition on Kinetics-400\nModel\n(ResNeXt-50)\nTop-1\n# of\nParams\nError (%)\npool\n20.21\n25.17M\nstatic attn\n20.18\n25.17M\ndynamic attn\n19.97\n25.26M\n(a) Concept Sampler\nModel\n(ResNeXt-50)\nTop-1\n# of\nParams\nError (%)\nno edge\n20.23\n25.26M\nstatic edge\n20.02\n25.28M\ndynamic edge\n19.97\n25.26M\n(b) Concept Reasoner\nModel\n(ResNeXt-50)\nTop-1\n# of\nParams\nError (%)\nonly scale\n20.13\n25.22M\nonly shift\n20.05\n25.22M\nscale + shift\n19.97\n25.26M\n(c) Concept Modulator\nTable 5: Ablation study on VCRNet\nObject Detection and Segmentation\nWe further do some experiments on object detection and\ninstance segmentation on the MSCOCO 2017 dataset (Lin\net al. 2014). MSCOCO dataset contains 115K images over\n80 categories for training, 5K for validation. Our experiments are based on the Detectron2 1. All backbone networks\nare based on the ResNeXt-50 and pre-trained on the ImageNet dataset by default. We employ and train the Mask\nR-CNN with FPN (He et al. 2017). We follow the training\nprocedure of the Detectron2 and use the 1× schedule setting. Furthermore, synchronized batch normalization is used\ninstead of freezing all related parameters. For evaluation,\nwe use the standard setting of evaluating object detection\nand instance segmentation via the standard mean averageprecision scores at different boxes and the mask IoUs, respectively. Table 2 is the list of results by only varying the\nbackbone network. It shows similar tendencies to the results\nof ImageNet. However, GloRe (Chen et al. 2019) is showing\nthe best performance. We assume that this result is from two\nfactors. One is the additional capacity, which is relatively\nlarger than other models, used by Glore. The other is that\nGloRe uses pixel-level re-projection mechanism that applies\nthe result of reasoning by re-computing all local descriptors.\nEspecially, the task requires to do prediction on pixel-level\nso that it would be beneﬁcial to use it. Therefore, we also\nmake our model to use pixel-level feature modulation. It further improves the performance without requiring additional\nparameters.\nScene and Action Recognition\nPlaces365 (Zhou et al. 2017) is a dataset labeled with scene\nsemantic categories for the scene recognition task. This task\nis challenging due to the ambiguity between classes that several scene classes may share some similar objects causing\nconfusion among them. We speciﬁcally use the Places365Standard setting that the train set has up to 1.8M images\nfrom 365 scene classes, and the validation set has 50 images\n1https://github.com/facebookresearch/detectron2\nper each class. All networks are trained from random initialization and evaluated on the validation set by following\nthe setting used in our ImageNet experiments. Additionally,\nwe insert Dropout (Srivastava et al. 2014) layers in residual\nblocks with p = 0.02 to avoid some over-ﬁtting. The human action recognition task is another task appropriate to\ndemonstrate how the network can generalize well not only\nto 2D image data, but also to 3D video data. We use the\nKinetics-400 dataset (Kay et al. 2017) including 400 human action categories with 235K training videos and 20K\nvalidation videos. We follow the slow-only experiment setting used in (Feichtenhofer et al. 2019) that simply takes the\nImageNet pre-trained model with a parameter inﬂating approach (Carreira and Zisserman 2017). Both tasks are classiﬁcation tasks similar to the ImageNet image classiﬁcation,\nand the results shown in Table 3 and 4 explain that our\napproach are generally performing better than other baselines in various visual classiﬁcation tasks. Moreover, action\nrecognition results prove that our model can be generally\napplied to all types of data.\nAblation Study\n(a) Concept Sampler: We have proposed different approaches for the concept sampler (pooling-based and attention based samplers). To compare these approaches, we train\nour proposed networks (ResNeXt-50 + VCR) by having different concept samplers and keeping all other modules ﬁxed.\nTable 5.(a) compares the performance of these approaches\non the ImageNet image classiﬁcation task. The attentionbased approach with dynamic queries (dynamic attn) outperforms the others, and we assume that this is due to having more adaptive power than the others. Furthermore, the\nresults interestingly show that our models consistently perform better than other baseline networks except a network\nwith GloRe, which are shown in Table 1, regardless of the\ntype of concept sampler. (b) Concept Reasoner: To investigate the effectiveness of reasoning based on interactions\nbetween concepts, we conduct some experiments by modifying the concept reasoner. We ﬁrst remove the concept inFigure 4: (Left) t-SNE plots of visual concept states. C = 32 concepts are distinguished by 32 colors. (Right) Visualization of\nattention (projection) maps from VCRNet, GCNet, and GloRe\nFigure 5: Visualization of interactions between concepts.\nteraction term in Equation 3 and evaluate it to measure the\neffectiveness of reasoning. Moreover, we also compare the\nperformance between learned static edges and computed dynamic edges. In Table 5.(b), the results show that the reasoning module is beneﬁcial in terms of the performance.\nNotably, it also reveals that using dynamic edges can improve the reasoning and reduce the number of parameters.\n(c) Concept Modulator: Our feature modulation consists of\nboth channel-wise scaling and shifting operations. Previous\nworks have shown to use only scaling (gating) (Hu, Shen,\nand Sun 2018; Woo et al. 2018; Hu et al. 2018) or only shifting (Cao et al. 2019). We compare different settings of the\nfeature modulation as shown in Table 5.(c). Using only shifting performs better than using only scaling, and combining\nboth operations can be recommended as the best option.\nVisualization\nWe use t-SNE (van der Maaten and Hinton 2008; Chan et al.\n2019) to visualize how visual concept states are existing in\nthe feature space. We collect a set of concept states, which\nare all extracted from the same concept sampler, by doing\ninference with the ImageNet validation set. In Figure 4, it\nis shown that the concept states are clustered and separated\nby concepts. This result can be further explained by observing the attention maps computed from the concept samplers.\nInterestingly, they reveal the fact that the concept samplers\nsparsely attend different regions or objects, and this would\nresult in clustered concept states. This also convinces that\nour proposed architecture is able to learn distinct concepts\nwithout any supervision that explicitly associates branches\nwith certain labeled concepts. We also visualize attention\n(projection) maps from other networks such as GCNet (Cao\net al. 2019) and GloRe (Chen et al. 2019) in Figure 4. GCNet\nonly produces a single attention map, and it tends to sparsely\nattend foreground objects. GloRe similarly computes projection maps as our approach, but the maps are densely attending regions with some redundancies between them. We\nfurthermore extract edge absolute values (interactions) between concepts from different images and visualized them in\nFigure 5. It shows that each image has different interactions\nbetween concepts, and concepts are interacting sparsely that\nmost edge values are near zero.",
        "conclusion": "In this work, we propose Visual Concept Reasoning Networks (VCRNet) that efﬁciently capture the global context\nby reasoning over high-level visual concepts. Our proposed\nmodel precisely ﬁts to a modularized multi-branch architecture by having split-transform-attend-interact-modulatemerge stages. The experimental results shows that it consistently outperforms other baseline models on multiple visual\nrecognition tasks and only increases the number of parameters by less than 1%. We strongly believe research in these\napproaches will provide notable improvements on more difﬁcult visual recognition tasks in the future. As future works,\nwe are looking forward to remove dense interactions between branches as possible to encourage more specialized\nconcept-wise representation learning and improve the reasoning process. Moreover, we expect to have consistent and\nspecialized visual concepts that are shared and updated over\nall stages in the network by removing dense interactions between different branches as possible. Explicitly associating\nbranches in the networks with labeled concepts will also improve the learning of tasks requiring high-level reasoning.",
        "summary_en": "A split-transform-merge strategy has been broadly used as an architectural constraint in convolutional neural networks for visual recognition tasks. It approximates sparsely connected networks by explicitly defining multiple branches to simultaneously learn representations with different visual concepts or properties. Dependencies or interactions between these representations are typically defined by dense and local operations, however, without any adaptiveness or high-level reasoning. This paper proposes to exploit this strategy and combine it with the Visual Concept Reasoning Networks (VCRNet) to enable reasoning between high-level visual concepts. The paper associates each branch with a visual concept and derive a compact concept state by selecting a few local descriptors through an attention module. These concept states are then updated by graph-based interaction and used to adaptively modulate the local descriptors. The paper describes the proposed model by split-transform-attend-interact-modulate-merge stages, which are implemented by opting for a highly modularized architecture. Extensive experiments on visual recognition tasks such as image classification, semantic segmentation, object detection, scene recognition, and action recognition show that the proposed model, VCRNet, consistently improves the performance by increasing the number of parameters by less than 1%.",
        "summary_zh": "这篇论文介绍了一种名为Visual Concept Reasoning Networks（VCRNet）的模型。拆分-变换-合并策略被广泛用于视觉识别任务，其通过明确定义多个分支来同时学习具有不同视觉概念或属性的表征。然而，这些表征之间的依赖关系或交互通常是通过密集的局部操作来定义的，没有任何适应性或高级推理。本文通过将该策略与VCRNet相结合，实现了高层次视觉概念之间的推理。作者将每个分支与一个视觉概念相关联，并通过注意力模块选择一些局部描述符，从而得出一个紧凑的概念状态。然后通过基于图的交互更新这些概念状态，并用于自适应地调节局部描述符。该模型包括拆分-转换-注意-交互-调节-合并等阶段。实验证明，VCRNet模型在图像分类、语义分割、目标检测、场景识别和动作识别等多个视觉识别任务中，仅增加不到1%的参数数量，但在性能上有显著提升。"
    },
    {
        "title": "Context Matters: Graph-based Self-supervised Representation Learning for Medical Images",
        "abstract": "Supervised learning method requires a large volume of annotated datasets. Collecting such datasets is time-consuming and expensive. Until now, very few annotated COVID19 imaging datasets are available. Although self-supervised learning enables us to bootstrap the training by exploiting unlabeled data, the generic self-supervised methods for natural images do not sufﬁciently incorporate the context. For medical images, a desirable method should be sensitive enough to detect deviation from normal-appearing tissue of each anatomical region; here, anatomy is the context. We introduce a novel approach with two levels of self-supervised representation learning objectives: one on the regional anatomical level and another on the patient-level. We use graph neural networks to incorporate the relationship between different anatomical regions. The structure of the graph is informed by anatomical correspondences between each patient and an anatomical atlas. In addition, the graph representation has the advantage of handling any arbitrarily sized image in full resolution. Experiments on large-scale Computer Tomography (CT) datasets of lung images show that our approach compares favorably to baseline methods that do not account for the context. We use the learnt embedding to quantify the clinical progression of COVID-19 and show that our method generalizes well to COVID-19 patients from different hospitals. Qualitative results suggest that our model can identify clinically relevant regions in the images.",
        "introduction": "While deep neural network trained by the supervised approach has made breakthroughs in many areas, its performance relies heavily on large-scale annotated datasets.\nLearning informative representation without human-crafted\nlabels has achieved great success in the computer vision\ndomain (Wu et al. 2018; Chen et al. 2020a; He et al.\n2020). Importantly, the unsupervised approach has the capability of learning robust representation since the features\nare not optimized towards solving a single supervised task.\nSelf-supervised learning has emerged as a powerful way\nof unsupervised learning. It derives input and label from\nan unlabeled dataset and formulates heuristics-based pretext tasks to train a model. Contrastive learning, a more\nprincipled variant of self-supervised learning, relies on instance discrimination (Wu et al. 2018) or contrastive predictive coding (CPC) (Oord, Li, and Vinyals 2018). It has\nachieved state-of-the-art performance in many aspects, and\ncan produce features that are comparable to those produced by supervised methods (He et al. 2020; Chen et al.\n2020a). However, for medical images, the generic formulation of self-supervised learning doesn’t incorporate domainspeciﬁc anatomical context.\nFor medical imaging analysis, a large-scale annotated\ndataset is rarely available, especially for emerging diseases,\nsuch as COVID-19. However, there are lots of unlabeled\ndata available. Thus, self-supervised pre-training presents\nan appealing solution in this domain. There are some existing works that focus on self-supervised methods for learning image-level representations. (Chen et al. 2019) proposed\nto learn image semantic features by restoring computerized\ntomography (CT) images from the corrupted input images.\n(Taleb et al. 2019) introduced puzzle-solving proxy tasks using multi-modal magnetic resonance images (MRI) scans for\nrepresentation learning. (Bai et al. 2019) proposed to learn\ncardiac MR image features from anatomical positions automatically deﬁned by cardiac chamber view planes. Despite their success, current methods suffer from two challenges: (1) These methods do not account for anatomical\ncontext. For example, the learned representation is invariant\nwith respect to body landmarks which are highly informative for clinicians. (2) Current methods rely on ﬁx-sized input. The dimensions of raw volumetric medical images can\nvary across scans due to the differences in subjects’ bodies, machine types, and operation protocols. The typical approach for pre-processing natural images is to either resize\nthe image or crop it to the same dimensions, because the\nconvolutional neural network (CNN) can only handle ﬁxed\ndimensional input. However, both approaches can be problematic for medical images. Taking chest CT for example,\nreshaping voxels in a CT image may cause distortion to the\nlung (Singla et al. 2018), and cropping images may introduce undesired artifacts, such as discounting the lung volume.\nTo address the challenges discussed above, we propose\na novel method for context-aware unsupervised representation learning on volumetric medical images. First, in order\nto incorporate context information, we represent a 3D image as a graph of patches centered at landmarks deﬁned\nby an anatomical atlas. The graph structure is informed\nby anatomical correspondences between the subject’s image and the atlas image using registration. Second, to handle different sized images, we propose a hierarchical model\nwhich learns anatomy-speciﬁc representations at the patch\nlevel and learns subject-speciﬁc representations at the graph\nlevel. On the patch level, we use a conditional encoder to\nintegrate the local region’s texture and the anatomical location. On the graph level, we use a graph convolutional network (GCN) to incorporate the relationship between different anatomical regions.\nExperiments on a publicly available large-scale lung\nCT dataset of Chronic Obstructive Pulmonary Disease\n(COPD) show that our method compares favorably to\nother unsupervised baselines and outperforms supervised\nmethods on some metrics. We also show that features\nlearned by our proposed method outperform other baselines in staging lung tissue abnormalities related to COVID19. Our results show that the pre-trained features on\nlarge-scale lung CT datasets are generalizable and transfer well to COVID-19 patients from different hospitals.\nOur code and supplementary material are available at\nhttps://github.com/batmanlab/Context Aware SSL\nIn summary, we make the following contributions:\n• We introduce a context-aware self-supervised representation learning method for volumetric medical images. The\ncontext is provided by both local anatomical proﬁles and\ngraph-based relationship.\n• We introduce a hierarchical model that can learn both local textural features on patch and global contextual features on graph. The multi-scale approach enables us to\nhandle arbitrary sized images in full resolution.\n• We demonstrate that features extracted from lung CT\nscans with our method have a superior performance in\nstaging lung tissue abnormalities related with COVID19 and transfer well to COVID-19 patients from different\nhospitals.\n• We propose a method that provides task-speciﬁc explanation for the predicted outcome. The heatmap results suggest that our model can identify clinically relevant regions\nin the images.",
        "method": "Our method views images of every patient as a set of nodes\nwhere nodes correspond to image patches covering the lung\nregion of a patient. Larger lung (image) results in more\nspread out patches. We use image registration to an anatomical atlas to maintain the anatomical correspondences between nodes. The edge connecting nodes denote neighboring patches after applying the image deformation derived\nfrom image registration. Our framework consists of two levels of self-supervised learning, one on the node level (i.e.,\npatch level) and the second one on the graph level (i.e., subject level). In the following, we explain each component separately. The schematic is shown in Fig. 1.\nConstructing Anatomy-aware Graph of Patients\nWe use Xi to denote the image of patient i. To deﬁne a standard set of anatomical regions, we divide the atlas image into\na set of N equally spaced 3D patches with some overlap. We\nuse {pj}N\nj to denote the center coordinates of the patches in\nthe Atlas coordinate system. We need to map {pj}N\nj to their\ncorresponding location for each patient. This operation requires transformations that densely map every coordinate of\nthe Atlas to the coordinate on patients. To ﬁnd the transformation, we register a patient’s image to an anatomical atlas\nby solving the following optimization problem:\nmin\nφi\nSim(φi(Xi), XAtlas) + Reg(φi),\n(1)\nwhere Sim is a similarity metric (e.g., ℓ2 norm), φi(·) is the\nﬁtted subject-speciﬁc transformation, Reg(φi) is a regularization term to ensure the transformation is smooth enough.\nThe φi maps the coordinate of the patient i to the Atlas. After solving this optimization for each patient, we can use the\ninverse of this transformation to map {pj}N\nj to each subject\n(i.e., {φ−1\ni (pj)}). We use well-established image registration software ANTs (Tustison et al. 2014) to ensure the inverse transformation exists. To avoid clutter in notation, we\nuse pj\ni as a shorthand for φ−1\ni (pj). In this way, patches with\nthe same index across all subjects map to the same anatomical region on the atlas image:\nφ1(pj\n1) = φ2(pj\n2) = . . . = φi(pj\ni) = pj\n(2)\nTo incorporate the relationship between different anatomical regions, we represent an image as a graph of patches\n(nodes), whose edge connectivity is determined by the Euclidean distance between patches’ centers. With a minor\nabuse of notation, we let Vi = {xj\ni}N\nj\ndenote the set of\npatches that cover the lung region of subject i. More formally, the image Xi is represented as Gi = (Vi, Ei), where\nVi is node (patch) information and Ei denotes the set of\nedges. We use an adjacency matrix Ai ∈ N × N to represent Ei, deﬁned as:\nAjk\ni\n=\n\u001a1,\nif dist(pj\ni, pk\ni ) < ρ\n0,\notherwise\n,\n(3)\nwhere dist(·, ·) denotes the Euclidean distance; pj\ni and pk\ni ∈\nR3 are the coordinates of centers in patches xj\ni and xk\ni , respectively; ρ is the threshold hyper-parameter that controls\nthe density of graph. Note that the adjacency matrix Ai can\nbe different for different subjects because of the deformation\ncaused by image registration.\nLearning Patch-level Representation\nLocal anatomical variations provide valuable information\nabout the health status of the tissue. For a given anatomical region, a desirable method should be sensitive enough to\ndetect deviation from normal-appearing tissue. In addition,\nthe anatomical location of lesion plays a role in patients’ survival outcomes, and the types of lesion vary across different\nanatomical locations in lung. In order to extract anatomyspeciﬁc features, we adopt a conditional encoder E(·, ·) that\nAtlas\nQuery\nKey\nE\nE\naug\nG\nG\nPatch/node\n      Level\nSubject/graph\n          Level\nRegistration\nφ−1\ni\nφ−1\nv\nxj\ni\nxj\nv\nLl\nLg\n×V\n×V\n(Hi, Ai)\n(Hv, Av)\n\u001f\n\u001f\n\u001f\npj\npj\naug\nFigure 1: Schematic diagram of the proposed method. We represent every image as a graph of patches. The context is imposed\nby anatomical correspondences among patients via registration and graph-based hierarchical model used to incorporate the\nrelationship between different anatomical regions. We use a conditional encoder E(·, ·) to learn patch-level textural features\nand use graph convolutional network G(·, ·) to learn graph-level representation through contrastive learning objectives. The\ndetailed architecture of the networks are presented in Supplementary Material.\ntakes both patch xj\ni and its location index j as input. It is\ncomposed with a CNN feature extractor C(·) and a MLP\nhead fl(·), thus we have the encoded patch-level feature:\nhj\ni = E(xj\ni, j) = fl(C(xj\ni) ∥ pj),\n(4)\nwhere ∥ denotes concatenation, .\nWe adopt the InfoNCE loss (Oord, Li, and Vinyals 2018),\na form of contrastive loss to train the conditional encoder on\nthe patch level:\nLl = − log\nexp(qj\ni · k+/τ)\nexp(qj\ni · k+/τ) + P\nk− exp(qj\ni · k−/τ)\n,\n(5)\nwhere qj\ni denotes the representation of query patch xj\ni, k+\nand k− denotes the representation of the positive and the\nnegative key respectively, and τ denotes the temperature\nhyper-parameter. We obtain a positive sample pair by generating two randomly augmented views from the same query\npatch xj\ni, and obtain a negative sample by augmenting the\npatch xj\nv at the same anatomical region j from a random\nsubject v, speciﬁcally:\nqj\ni = fl(C(aug(xj\ni)) ∥ pj),\nk+ = fl(C(aug(xj\ni)) ∥ pj),\nk− = fl(C(aug(xj\nv)) ∥ pj), v ̸= i.\nLearning Graph-level Representation\nWe adopt the Graph Convolutional Network (GCN) (Duvenaud et al. 2015) to summarize the patch-level (anatomyspeciﬁc)\nrepresentation\ninto\nthe\ngraph-level\n(subjectspeciﬁc) representation. We consider each patch as one node\nin the graph, and the subject-speciﬁc adjacent matrix determines the connection between nodes. Speciﬁcally, the GCN\nmodel G(·, ·) takes patch-level representation Hi and adjacency matrix Ai as inputs, and propagates information\nacross the graph to update node-level features:\nH′\ni = concat({h′j\ni}N\nj=1) = σ\n\u0010\nˆD\n− 1\ni 2\nˆAi ˆD\n− 1\ni 2\nHiW\n\u0011\n,\n(6)\nwhere ˆAi = Ai + I, I is an identity matrix, ˆDi is a diagonal node degree matrix of ˆAi, Hi = concat({hj\ni}N\nj=1) is\na N × F matrix containing F features for all N nodes in\nthe image of the subject i, and W is a learnable projection\nmatrix, σ is a nonlinear activation function.\nWe then obtain subject-level representation by global average pooling all nodes in the graph followed by a MLP head\nfg:\nSi = fg(Pool(H′\ni)).\n(7)\nWe adopt the InfoNCE loss to train the GCN on the graph\nlevel:\nLg = − log\nexp(ri · t+/τ)\nexp(ri · t+/τ) + P\nt− exp(ri · t−/τ),\n(8)\nwhere rj\ni denotes the representation of the entire image Xi,\nt+ and t− denotes the representation of the positive and\nthe negative key respectively, and τ denotes the temperature\nhyper-parameter. To form a positive pair, we take two views\nof the same image Xi under random augmentation at patch\nlevel. We obtain a negative sample by randomly sample a\ndifferent image Xv, speciﬁcally:\nri = G(concat({E(aug(xn\ni ), pn)}N\nn=1), Ai),\nt+ = G(concat({E(aug(xn\ni ), pn)}N\nn=1), Ai),\nt− = G(concat({E(aug(xn\nv), pn)}N\nn=1), Av), v ̸= i.\nOverall Model\nThe model is trained in an end-to-end fashion by integrating\nthe two InfoNCE losses obtained from patch level and graph\nlevel. We deﬁne the overall loss function as follows:\nL = Ll(E) + Lg(G).\n(9)\nSince directly backpropagating gradients from Lg(G) to the\nparameters in the conditional encoder E is unfeasible due to\nthe excessive memory footprint accounting for a large number of patches, we propose an interleaving algorithm that alternates the training between patch level and graph level to\nsolve this issue. The algorithmic description of the method\nis shown below:\nAlgorithm 1 Interleaving update algorithm\nRequire: Conditional encoder E(·, ·), GCN G(·, ·).\nInput: Image patch xj\ni, anatomical landmark pj, adjacency matrix Ai.\nfor step t = 1, Tmax do\nfor step tl = 1, Tl do\nRandomly sample a batch of Bl subjects\nfor j = 1, N do\nhj\ni ← E(aug(xj\ni), pj)\nUpdate E by backpropagating Ll\nend for\nend for\nfor step tg = 1, Tg do\nRandomly sample a batch of Bg subjects\nSi←G(concat({E(aug(xn\ni ), pn)}N\nn=1), Ai)\nUpdate G by backpropagating Lg\nend for\nend for\nModel Explanation\nUnderstanding how the model makes predictions is important to build trust in medical imaging analysis. In this section, we propose a method that provides task-speciﬁc explanation for the predicted outcome. Our method is expanded\nfrom the class activation maps (CAM) proposed by (Zhou\net al. 2016). Without loss of generality, we assume a Logistic regression model is ﬁtted for a downstream binary classiﬁcation task (e.g., the presence or absence of a disease) on\nthe extracted subject-level features S′\ni. The log-odds of the\ntarget variable that Yi = 1 is:\nlog P(Yi = 1|S′\ni)\nP(Yi = 0|S′\ni) = β + WS′\ni = β + 1\nN\nN\nX\nj=1\nWh′j\ni\n| {z }\nM j\ni\n(10)\nwhere S′\ni = Pool(H′\ni), the MLP head fg in Eq. 7 is discarded when extracting features for downstream tasks following the practice in (Chen et al. 2020a,b), β and W\nare the learned logistic regression weights. Then we have\nM j\ni = Wh′j\ni as the activation score of the anatomical region j to the target classiﬁcation. We use a sigmoid function\nto normalize {M j\ni }N\nj , and use a heatmap to show the discriminative anatomical regions in the image of subject i.\nImplementation Details\nWe train the proposed model for 30 epochs. We set the learning rate to be 3×10−2. We also employed momentum = 0.9\nand weight decay = 1 × 10−4 in the Adam optimizer. The\npatch size is set as 32×32×32. The batch size at patch level\nand subject level is set as 128 and 16, respectively. We let\nthe representation dimension F be 128. The lung region is\nextracted using lungmask (Hofmanninger et al. 2020). Following the practice in MoCo, we maintain a queue of data\nsamples and use a momentum update scheme to increase the\nnumber of negative samples in training; as shown in previous\nwork, it can improve performance of downstream task (He\net al. 2020). The number of negative samples during training is set as 4096. The data augmentation includes random\nelastic transform, adding random Gaussian noise, and random contrast adjustment. The temperature τ is chosen to be\n0.2. There are 581 patches per subject/graph, this number is\ndetermined by both the atlas image size and two hyperparameters, patch size and step size. The step size controls the\noverlapping between adjacent patches. We select the hyperparameters based on what is suggested in literature (Singla\net al. 2018). The experiments are performed on 2 GPUs,\neach with 16GB memory.",
        "related works": "Unsupervised Learning\nUnsupervised learning aims to learn meaningful representations without human-annotated data. Most unsupervised\nlearning methods can be classiﬁed into generative and discriminative approaches. Generative approaches learn the\ndistribution of data and latent representation by generation.\nThese methods include adversarial learning and autoencoder\nbased methods. However, generating data at pixel space can\nbe computationally intensive, and generating ﬁne detail may\nnot be necessary for learning effective representation.\nDiscriminative approaches use pre-text tasks for representation learning. Different from supervised approaches, both\nthe inputs and labels are derived from an unlabeled dataset.\nDiscriminative approaches can be grouped into (1) pre-text\ntasks based on heuristics, including solving jigsaw puzzles (Noroozi and Favaro 2016), context predication (Doersch, Gupta, and Efros 2015), colorization (Zhang, Isola,\nand Efros 2016), image restoration (Zhou et al. 2019), divergence based methods (Schabdach et al. 2017) and (2) contrastive methods. Among them, contrastive methods achieve\nstate-of-the-art performance in many tasks. The core idea\nof contrastive learning is to bring different views of the\nsame image (called ’positive pairs’) closer, and spread representations of views from different images (called ’negative\npairs’). The similarity is measured by the dot product in feature space (Wu et al. 2018).\nRepresentation Learning for Graph\nGraphs are a powerful way of representing entities with arbitrary relational structure (Battaglia et al. 2018). Several algorithms proposed to use random walk-based methods for\nunsupervised representation learning on the graph (Grover\nand Leskovec 2016; Perozzi, Al-Rfou, and Skiena 2014;\nHamilton, Ying, and Leskovec 2017). These methods are\npowerful but rely more on local neighbors than structural information (Ribeiro, Saverese, and Figueiredo 2017). Graph\nconvolutional network (GCN) (Duvenaud et al. 2015; Kipf\nand Welling 2016) was proposed to generalize convolutional\nneural networks to work on the graphs. Recently, Deep\nGraph Infomax (Velickovic et al. 2019) was proposed to\nlearn node-level representation by maximizing mutual information between patch representations and corresponding\nhigh-level summaries of graphs.",
        "experiments": "We evaluate the performance of the proposed model on two\nlarge-scale datasets of 3D medical images. We compare our\nmodel with various baseline methods, including both supervised approaches and unsupervised approaches.\nDatasets\nThe experiments are conducted on three volumetric medical imaging datasets, including the COPDGene dataset (Regan et al. 2011), the MosMed dataset (Morozov et al. 2020)\nand the COVID-19 CT dataset. All images are re-sampled to\nisotropic 1mm3 resolution. The Hounsﬁeld Units (HU) are\nmapped to the intensity window of [−1024, 240] and then\nnormalized to [−1, 1].\nCOPDGene Dataset\nCOPD is a lung disease that makes\nit difﬁcult to breathe. The COPDGene Study (Regan et al.\n2011) is a multi-center observational study designed to identify the underlying genetic factors of COPD. We use a large\nset of 3D thorax computerized tomography (CT) images of\n9,180 subjects from the COPDGene dataset in our study.\nMosMed Dataset\nWe use 3D CT scans of 1,110 subjects\nfrom the MosMed dataset (Morozov et al. 2020) provided by\nmunicipal hospitals in Moscow, Russia. Based on the severity of lung tissues abnormalities related with COVID-19, the\nimages are classiﬁed into ﬁve severity categories associated\nwith different triage decisions. For example, the patients in\nthe mild category are followed up at home with telemedicine\nmonitoring, while the patients in the critical category are immediately transferred to the intensive care unit.\nCOVID-19 CT Dataset\nTo verify whether the learned\nrepresentation can be transferred to COVID-19 patients\nfrom other sites, we collect a multi-hospital 3D thorax CT\nimages of COVID-19. The combined dataset has 80 subjects, in which 35 positive subjects are from multiple publicly available COVID-19 datasets (Jun et al. 2020; Bell\n2020; Zhou et al. 2020), and 45 healthy subjects randomly\nsampled from the LIDC-IDRI dataset (Armato III et al.\n2011) as negative samples.\nQuantitative Evaluation\nWe evaluate the performance of proposed method by using\nextracted representations of subjects to predict clinically relevant variables.\nCOPDGene dataset\nWe ﬁrst perform self-supervised pretraining with our method on the COPDGene dataset. Then\nwe freeze the extracted subject-level features and use them\nto train a linear regression model to predict two continuous\nclinical variables, percent predicted values of Forced Expiratory Volume in one second (FEV1pp) and its ratio with\nForced vital capacity (FVC) (FEV1/FVC), on the the log\nscale. We report average R2 scores with standard deviations\nin ﬁve-fold cross-validation. We also train a logistic regression model for each of the six categorical variables, including (1) Global Initiative for Chronic Obstructive Lung Disease (GOLD) score, which is a four-grade categorical value\nindicating the severity of airﬂow limitation, (2) Centrilobular emphysema (CLE) visual score, which is a six-grade\ncategorical value indicating the severity of emphysema in\ncentrilobular, (3) Paraseptal emphysema (Para-septal) visual\nscore, which is a three-grade categorical value indicating the\nseverity of paraseptal emphysema, (4) Acute Exacerbation\nhistory (AE history), which is a binary variable indicating\nwhether the subject has experienced at least one exacerbation before enrolling in the study, (5) Future Acute Exacerbation (Future AE), which is a binary variable indicating\nwhether the subject has reported experiencing at least one\nexacerbation at the 5-year longitudinal follow up, (6) Medical Research Council Dyspnea Score (mMRC), which is a\nﬁve-grade categorical value indicating dyspnea symptom.\nWe compare the performance of our method against: (1)\nsupervised approaches, including Subject2Vec (Singla et al.\n2018), Slice-based CNN (Gonz´alez et al. 2018) and (2)\nunsupervised approaches, including Models Genesis (Zhou\net al. 2019), MedicalNet (Chen, Ma, and Zheng 2019),\nMoCo (3D implementation) (He et al. 2020), Divergencebased feature extractor (Schabdach et al. 2017), K-means algorithm applied to image features extracted from local lung\nregions (Schabdach et al. 2017), and Low Attenuation Area\n(LAA), which is a clinical descriptor. The evaluation results\nare shown in Table 1. For all results, we report average test\naccuracy in ﬁve-fold cross-validation.\nThe results show that our proposed model outperforms\nunsupervised baselines in all metrics except for Future AE.\nWhile MoCo is also a contrastive learning based method,\nwe believe that our proposed method achieves better performance for three reasons: (1) Our method incorporates\nanatomical context. (2) Since MoCo can only accept ﬁxedsize input, we resize all volumetric images into 256 × 256 ×\n256. In this way, lung shapes may be distorted in the CT\nimages, and ﬁne-details are lost due to down-sampling. In\ncomparison, our model supports images with arbitrary sizes\nin full resolution by design. (3) Since training CNN model\nwith volumetric images is extremely memory-intensive, we\ncan only train the MoCo model with limited batch size. The\nMethod\nSupervised\nlogFEV1pp\nlogFEV1/FVC\nGOLD\nCLE\nPara-septal\nAE History\nFuture AE\nmMRC\nMetric\nR-Square\n% Accuracy\nLAA-950\n\u0017\n0.44±.02\n0.60±.01\n55.8\n32.9\n33.3\n73.8\n73.8\n41.6\nK-Means\n\u0017\n0.55±.03\n0.68±.02\n57.3\nDivergence-based\n\u0017\n0.58±.03\n0.70±.02\n58.9\nMedicalNet\n\u0017\n0.47±.10\n0.59±.06\n57.0±1.3\n40.3±1.9\n53.1±0.7\n78.7±1.3\n81.4±1.7\n47.9±1.2\nModelsGenesis\n\u0017\n0.58±.01\n0.64±.01\n59.5±2.3\n41.8±1.4\n52.7±0.5\n77.8±0.8\n76.7±1.2\n46.0±1.2\nMoCo\n\u0017\n0.40±.02\n0.49±.02\n52.7±1.1\n36.5±0.7\n52.5±1.4\n78.6±0.9\n82.0±1.2\n46.4±1.7\n2D CNN\n\u0013\n0.53\n51.1\n60.4\nSubject2Vec\n\u0013\n0.67±.03\n0.74±.01\n65.4\n40.6\n52.8\n76.9\n68.3\n43.6\nOurs w/o CE\n\u0017\n0.56±.03\n0.65±.03\n61.6±1.2\n48.1±0.4\n55.5±0.8\n78.8±1.2\n80.8±1.7\n50.4±1.0\nOurs w/o Graph\n\u0017\n0.60±.01\n0.69±.01\n62.5±1.0\n49.2±1.1\n55.8±1.3\n78.7±1.5\n80.7±1.7\n50.6±0.8\nOurs\n\u0017\n0.62±.01\n0.70±.01\n63.2±1.1\n50.4±1.3\n56.2±1.1\n78.8±1.3\n81.1±1.6\n51.0±1.0\n- indicates not reported.\nTable 1: Evaluation on COPD dataset\nsmall batch size may lead to unstable gradients. In comparison, the interleaving training scheme reduces the usage of\nmemory footprint, thus it allows us to train our model with\na much larger batch size.\nOur method also outperforms supervised methods, including Subject2Vec and 2D CNN, in terms of CLE, Paraseptal, AE History, Future AE and mMRC; for the rest clinical variables, the performance gap of our method is smaller\nthan other unsupervised methods. We believe that the improvement is mainly from the richer context information incorporated by our method. Subject2Vec uses an unordered\nset-based representation which does not account for spatial\nlocations of the patches. 2D CNN only uses 2D slices which\ndoes not leverage 3D structure. Overall, the results suggest\nthat representation extracted by our model preserves richer\ninformation about the disease severity than baselines.\nAblation study: We perform two ablation studies to validate the importance of context provided by anatomy and\nthe relational structure of anatomical regions: (1) Removing conditional encoding (CE). In this setting, we replace\nthe proposed conditional encoder with a conventional encoder which only takes images as input. (2) Removing\ngraph. In this setting, we remove GCN in the model and obtain subject-level representation by average pooling of all\npatch/node level representations without propagating information between nodes. As shown in Table 1, both types of\ncontext contribute signiﬁcantly to the performance of the ﬁnal model.\nMosMed dataset\nWe ﬁrst perform self-supervised pretraining with our method on the MosMed dataset. Then we\nfreeze the extracted patient-level features and train a logistic\nregression classiﬁer to predict the severity of lung tissue abnormalities related with COVID-19, a ﬁve-grade categorical\nvariable based on the on CT ﬁndings and other clinical measures. We compare the proposed method with benchmark\nunsupervised methods, including MedicalNet, ModelsGenesis, MoCo, and one supervised model, 3D CNN model. We\nuse the average test accuracy in ﬁve-fold cross-validation as\nthe metric for quantifying prediction performance.\nTable 2 shows that our proposed model outperforms both\nthe unsupervised and supervised baselines. The supervised\nMethod\nSupervised\n% Accuracy\nMedicalNet\n\u0017\n62.1\nModelsGenesis\n\u0017\n62.0\nMoCo\n\u0017\n62.1\n3D CNN\n\u0013\n61.2\nOurs w/o CE\n\u0017\n63.3\nOurs w/o Graph\n\u0017\n64.3\nOurs\n\u0017\n65.3\nTable 2: Evaluation on MosMed dataset\n3D CNN model performed worse than the other unsupervised methods, suggesting that it might not converge well or\nbecome overﬁtted since the size of the training set is limited. The features extracted by the proposed method show\nsuperior performance in staging lung tissue abnormalities related with COVID-19 than those extracted by other unsupervised benchmark models. We believe that the graph-based\nfeature extractor provides additional gains by utilizing the\nfull-resolution CT images than CNN-based feature extractor, which may lose information after resizing or downsampling the raw images. The results of ablation studies support\nthat counting local anatomy and relational structure of different anatomical regions is useful for learning more informative representations for COVID-19 patients.\nCOVID-19 CT Dataset\nSince the size of COVID-19 CT\nDataset is very small (only 80 images are available), we\ndon’t train the networks from scratch with this dataset. Instead, we use models pre-trained on the COPDGene dataset\nand the MosMed dataset to extract patient-level features\nfrom the images in the COVID-19 CT Dataset, and train a\nlogistic regression model on top of it to classify COVID-19\npatients. We compare the features extracted by the proposed\nmethod to the baselines including MedicalNet, ModelsGenesis, MoCo (unsupervised), and 3D CNN (supervised). We\nreport the average test accuracy in ﬁve-fold cross-validation.\nTable 3 shows that the features extracted by the proposed model pre-trained on the MosMed dataset perform\nMethod\nSupervised\n% Accuracy\nMedicalNet\n\u0017\n85.0\nModelsGenesis (Pretrained on COPDGene)\n\u0017\n92.5\nModelsGenesis (Pretrained on MosMed)\n\u0017\n88.7\nMoCo (Pretrained on COPDGene)\n\u0017\n75.0\nMoCo (Pretrained on MosMed)\n\u0017\n86.3\n3D CNN\n\u0013\n77.5\nOurs (Pretrained on COPDGene)\n\u0017\n90.0\nOurs (Pretrained on MosMed)\n\u0017\n96.3\nTable 3: Evaluation on COVID-19 CT dataset\nFigure 2: Embedding of subjects in 2D using UMAP. Each\ndot represents one subject colored by log FEV1pp. Note that\nlower FEV1pp value indicates more severe disease.\nthe best for COVID-19 patient classiﬁcation. They outperform the features extracted by the same model pre-trained\non the COPDGene dataset. We hypothesize that this is because the MosMed dataset contains subjects with COVID19 related pathological tissue, such as ground glass opacities and mixed attenuation. However, the COPDGene dataset\nalso shows great performance for transfer learning with both\nthe ModelsGenesis model and our model, which shed light\non the utility of unlabeled data for COVID-19 analysis.\nModel Visualization\nTo visualize the learned embedding and understand the\nmodel’s behavior, we use two methods to visualize the\nmodel. The ﬁrst one is embedding visualization, we use\nUMAP (McInnes et al. 2018) to visualize the patient-level\nfeatures extracted on the COPDGene dataset in two dimensions. Figure 2 shows a trend, from lower-left to upper-right,\nalong which the value of FEV1pp decreases or the severity\nof disease increases.\nIn addition, we use the model explanation method introFigure 3: An axial view of the activation heatmap on a\nCOVID-19 positive subject in the COVID-19 CT dataset.\nBrighter color indicate higher relevance to the disease severity. The ﬁgure illustrates that high activation region overlaps\nwith the ground glass opacities.\nduced before to obtain the activation heatmap relevant to the\ndownstream task, COVID-19 classiﬁcation. Figure 3 (left)\nshows the axial view of the CT image of a COVID-19 positive patient, and Figure 3 (right) shows the corresponding\nactivation map. The anatomical regions received high activation scores overlap with the peripheral ground glass opacities on the CT image, which is a known indicator of COVID19. We also found that activation maps of non-COVID-19\npatients usually have no obvious signal, which is expected.\nThis result suggests that our model can highlight the regions\nthat are clinically relevant to the prediction. More examples\ncan be found in Supplementary Material.",
        "conclusion": "In this paper, we introduce a novel method for context-aware\nunsupervised representation learning on volumetric medical images. We represent a 3D image as a graph of patches\nwith anatomical correspondences between each patient, and\nincorporate the relationship between anatomical regions.\nIn addition, we introduced a multi-scale model which includes a conditional encoder for local textural feature extraction and a graph convolutional network for global contextual feature extraction. Moreover, we propose a task-speciﬁc\nmethod for model explanation. The experiments on multiple\ndatasets demonstrate that our proposed method is effective,\ngeneralizable and interpretable.",
        "summary_en": "Supervised learning method requires a large volume of annotated datasets. Collecting such datasets is time-consuming and expensive. Until now, very few annotated COVID-19 imaging datasets are available. Although self-supervised learning enables us to bootstrap the training by exploiting unlabeled data, the generic self-supervised methods for natural images do not sufficiently incorporate the context. For medical images, a desirable method should be sensitive enough to detect deviation from normal-appearing tissue of each anatomical region; here, anatomy is the context. This paper introduces a novel approach with two levels of self-supervised representation learning objectives: one on the regional anatomical level and another on the patient-level. The paper use graph neural networks to incorporate the relationship between different anatomical regions. The structure of the graph is informed by anatomical correspondences between each patient and an anatomical atlas. In addition, the graph representation has the advantage of handling any arbitrarily sized image in full resolution. Experiments on large-scale Computer Tomography (CT) datasets of lung images show that the approach compares favorably to baseline methods that do not account for the context. The paper uses the learnt embedding to quantify the clinical progression of COVID-19 and show that the method generalizes well to COVID-19 patients from different hospitals. Qualitative results suggest that the model can identify clinically relevant regions in the images.",
        "summary_zh": "这篇论文介绍了一种基于图的自监督表示学习方法，用于医学图像。自监督学习可以利用无标记数据进行自引导训练，但用于自然图像的通用自监督方法并没有充分结合上下文。为了解决这一问题，论文提出了一种双层自监督表示学习目标，分别在区域解剖水平和患者水平上进行学习。利用图神经网络结合解剖图谱中的解剖对应关系，将不同解剖区域之间的关系纳入学习过程中。实验结果表明，该方法在处理肺部CT图像数据集时表现优异，且能有效量化COVID-19的临床进展，具有良好的泛化性能。"
    },
    {
        "title": "Self-Supervised Graph Learning for Long-Tailed Cognitive Diagnosis",
        "abstract": "Cognitive diagnosis is a fundamental yet critical research task in the field of intelligent education, which aims to discover the proficiency level of different students on specific knowledge concepts. Despite the effectiveness of existing efforts, previous methods always considered the mastery level on the whole students, so they still suffer from the Long Tail Effect. A large number of students who have sparse interaction records are usually wrongly diagnosed during inference. To relieve the situation, we proposed a Self-supervised Cognitive Diagnosis (SCD) framework which leverages the self-supervised manner to assist the graph-based cognitive diagnosis, then the performance on those students with sparse data can be improved. Specifically, we came up with a graph confusion method that drops edges under some special rules to generate different sparse views of the graph. By maximizing the cross-view consistency of node representations, our model could pay more attention on long-tailed students. Additionally, we proposed an importance-based view generation rule to improve the influence of long-tailed students. Extensive experiments on real-world datasets show the effectiveness of our approach, especially on the students with much sparser interaction records. Our code is available at https://github.com/zeng-zhen/SCD.",
        "introduction": "Cognitive diagnosis (CD) is an essential part of the intelligent education system (Liu 2021; Anderson et al. 2014).\nFurther works in intelligent education, i.e., knowledge tracking (Piech et al. 2015), exercise recommendation (Wu et al.\n2020) are based on the results of cognitive diagnosis. In the\ncognitive diagnosis system, there are a series of students,\nexercises, and knowledge concepts. For a given student, the\ncognitive diagnosis task aims to predict the knowledge mastery level on knowledge concepts based on his historical interaction records (Lord 1952; Wang et al. 2020).\nThere has been a great development on cognitive diagnosis, such as linear model ITR (Lord 1952), MIRT (Reckase\n2009), and neural network-based model NCD (Wang et al.\n2020). Recent work RCD (Gao et al. 2021) explored the application of graph convolutional networks (GCN) on cognitive diagnosis and achieved the state-of-the-art results. How0-5\n5-10\n10-15\n15-20\n20-25\n25-30\n30-35\n35Number of interactions per student in the group\n0\n0.1\n0.2\n0.3\n0.4\nNumber of students(%)\n0\n0.2\n0.4\n0.6\n0.8\n1\nNumber of interactions(%)\njunyi_student\nASSIST_student\njunyi_interaction\nASSIST_interaction\nFigure 1: Statistics of the number of students and interactions grouping by interaction records. The horizontal coordinate is the number of interactions per student in each group.\nThe left vertical coordinates indicate the percentage of students in each group to the total students. And the right vertical coordinate indicates the percentage of interactions of all\nstudents in each group to the total interactions.\never, according to our research, there is a serious long-tailed\nproblem in student-exercise interactions, which is largely\noverlooked by existing efforts. Statistics of two real-word\ndatasets, i.e., junyi (Chang, Hsu, and Chen 2015) and ASSIST (Feng, Heffernan, and Koedinger 2009), are showed in\nfigure 1. Specifically, a minority students have the vast majority interaction records. Existing models based on overall\noptimization will favor these students and ignore the longtailed students.\nIn order to address the long-tailed problem in cognitive diagnosis, the long-tailed nodes that occupy much sparser interaction records should be paid more attention during training. Inspired by the SGL (Wu et al. 2021), we aim to generate sparse views of the original student-exercise interaction graph. In the specially designed sparse views, the graph\ndropped edges based on the degree of nodes, so that the longtailed nodes could achieve a stronger influence. Additionally, we use the sparse views to construct an auxiliary selfsupervised task, the information discarded in the process of\nedge dropout will have few impact on other nodes. Specifically, we construct a graph contrastive learning for cognitive\ndiagnosis, which provides self-supervised signals by performing self-discrimination of nodes on sparse views. The\nwhole process can be summarized as follows. Firstly, in the\ngraph constructed by student-exercise interaction records,\nwe measure the edge importance based on the degree of\nnodes. Then we generate different sparse views for the interaction graph according to the edge importance. Different\nfrom the view generation rules in previous self-supervised\ngraph learning work (Wu et al. 2021; Zhu et al. 2021), longtailed nodes have greater weight in our generated views.\nBy maximizing the cross-view consistency of the node representations, the model could pay more attention on longtailed students during the stage of graph learning.\nOur key contributions are summarized as follows:\n• We propose a new cognitive diagnosis framework SCD\nbased on self-supervised graph learning, which leverages\nthe auxiliary self-supervised signals to alleviate the data\nsparse problem.\n• In SCD, we design an importance-based view generation\nrule to enhance the influence of long-tailed students.\n• We have conducted extensive experiments on real-world\ndatasets to validate the effectiveness of our SCD, especially on long-tailed students.",
        "related work": "Cognitive Diagnosis\nCognitive diagnosis (CD) is the first step of intelligent education. Existing CD methods mainly depend on the assumption that the cognitive states of the student maintain\nstability in a stable scene. IRT (Lord 1952) is considered\na milestone of CD, it predicted the accuracy of the student\nthrough a manually designed function. Then, MIRT (Reckase 2009) extended the latent features of students and exercised in IRT to multi-dimensions. However, the performance\nof these traditional models is limited by the design of functions. To overcome this limitation, NCD (Wang et al. 2020)\nleveraged the neural networks to model the interaction between students and exercises. RCD (Gao et al. 2021) first\nmodeled the relationships between knowledge concepts and\nbrought in the graph models to cognitive diagnosis. Another\nmainstream foundational paradigm is DINA (De La Torre\n2009). It directly modeled the relationship between students\nand knowledge concepts, and took the slip and guessing into\naccount. An improved model of DINA is FuzzyCDM (Liu\net al. 2018), which introduced the fuzzy logic control and\nrealized that the knowledge mastery state could keep a constant value. Existing CD models have achieved good performance for overall students. However, none of them takes the\nlong-tailed problem into account.\nGraph Representation Learning\nGraph-based representation learning has become a popular\ntopic due to the ability of capturing interactions between\ndata. GNN (Scarselli et al. 2008; Zhou et al. 2020) first introduced the information propagation mechanism of neural\nnetworks into graph embedding. GNN-based models (Kipf\nand Welling 2016) had shown great success, such as graphsage (Hamilton, Ying, and Leskovec 2017) and other applications (Yang, Du, and Wang 2020). GAT (Veliˇckovi´c et al.\n2017) introduced the attention mechanism to capture the interactions between neighboring nodes. Recently, the methods (e.g., lightGCN (He et al. 2020)) of message passing on\nheterogeneous graphs are explored. Heterogeneous graphs\ncould represent many real-world interactions, such as the\nstudent-exercise relationship in cognitive diagnostics. However, except RCD (Gao et al. 2021), there is few related work\non cognitive diagnosis.\nSelf-Supervised Learning\nSelf-supervised learning could alleviate the problem of\nsparse labeling. Research on self-supervised learning can\nbe divided into three categories: context-based, temporalbased, and contrastive-based. Context-based and temporalbased learning are always suitable for Euclidean data, such\nas text (Mikolov et al. 2013; Devlin et al. 2018), images (Doersch, Gupta, and Efros 2015; Noroozi and Favaro\n2016; Pathak et al. 2016), videos (Sermanet et al. 2018;\nWang and Gupta 2015; Misra, Zitnick, and Hebert 2016),\netc. Contrastive-based learning is usually applied on nonEuclidean data such as graph structure data. Graph contrastive learning (Hjelm et al. 2018; Oord, Li, and Vinyals\n2018; Zhu et al. 2021) constructs two different views for the\nsame graph structure, and uses contrastive loss constraints to\ncapture the consistency of feature representations under different views. Benefiting from its good performance in data\nsparseness, graph contrastive learning has achieved good\nperformance in recommendation systems (Wu et al. 2021;\nXia et al. 2022). Inspired by it, to address the long-tailed\nproblem in cognitive diagnosis, we constructed a suitable\nself-supervised model to learn student and exercise features.",
        "preliminaries": "We formally define the cognitive diagnosis task. To apply\ngraph contrastive learning on cognitive diagnostic task, we\nconstruct a cognitive diagnostic relationship graph.\nProblem Statement\nLet S = {s1, s2, . . . , sM}, E = {e1, e2, . . . , eN} and C =\n{c1, c2, . . . , cK} be the set of students, exercises and knowledge concepts, respectively. Q-matrix Q ∈ {0, 1}N×K\nwhich contains the relation of exercises with concepts is usually labeled by experts. Qi,j = 1 denotes that exercise ei is\nrelated to knowledge concept cj and the reverse Qi,j = 0.\nThe response records of the students are given in the form\nof triplet (s, e, r), where s ∈ S, e ∈ E, and r is the score\nthat the student s got on exercise e. R is the set of response\nrecords. Given the students’ response records set R and Qmatrix Q, the goal of cognitive diagnosis task is to diagnose\nstudents’ proficiency on knowledge concepts.\nRelation Graph\nFollowing the student-exercise-concept relation graph proposed in RCD (Gao et al. 2021), we construct a relation\ngraph applicable to graph contrastive learning. The knowledge concept nodes C act as intermediate nodes to associate\nexercises that have the same knowledge concept. Two subgraphs are constructed, which are the student-exercise interEdge Dropout\n𝑠𝑖\n𝑒𝑗\n𝑝 𝑒j\n𝑝 𝑠𝑖\n𝑒𝑗\n𝑠𝑖\n𝑐𝑘\nGenerated View\n𝑒𝑗\n𝑠𝑖\n𝑐𝑘\nGenerated View\nℒmain\nDiagnosis Layer\n𝐹\ny\nℒssl\n𝑒𝑗\n𝑙+1\n𝑠𝑖\n𝑙+1\nℎ𝑖\n𝑠\nℎ𝑗\n𝑒\nNegative\npair\nPositive\npair\nShare Parameters\nGCN \nLayer\nL-layer GCN\nShare Parameters\nGCN \nLayer\nGCN \nLayer\nGCN \nLayer\nGCN \nLayer\nGCN \nLayer\n𝑒𝑗\n′\n…\n𝑒𝑘\n′\n𝑒𝑙\n′\n𝑒𝑗\n′′\n…\n𝑒𝑘\n′′\n𝑒𝑙\n′′\n…\n𝑠𝑚′\n𝑠𝑛′\n…\n𝑠𝑚′′ 𝑠𝑛′′\n𝑠𝑖\n′\n𝑠𝑖\n′′\n𝑒𝑗\n𝑠𝑖\n𝑐𝑘\nRelation Graph\nEmbedding Layer\n1 0 0 0 0\n×E\n𝑦𝑗\n0 0 1 0 0\n×S\n𝑥𝑖\n0 1 0 0 0\n×C\n𝑧𝑘\n𝑒𝑗\n𝑠𝑖\n𝑐𝑘\nRelation Graph\nKnowledge\nConcept\nExercise\nStudent\nFigure 2: Overview of SCD. The upper part shows the workflow of the cognitive diagnosis main task. And the bottom illustrates\nthe workflow of the self-supervised graph learning task. The main task uses the original relation graph to predict student\nperformance, and the self-supervised graph learning task uses the generated view from edge dropout for contrastive learning.\nThe two tasks share the graph convolutional network.\naction subgraph Gse and the exercise-concept relation subgraph Gec.\nStudent-Exercise Interaction Subgraph\nWe model the\ninteraction records of students and exercises as a bipartite\ngraph Gse = (Vse, Rse), where the node set Vse = S ∪ E\ninvolves all students and exercises, and edge rse ∈ Rse denotes that student s has answered exercises e.\nExercise-Concept Relation Subgraph\nThe construction\nof the exercise-concept relation graph relies on the Q-matrix.\nWe denote it as Gec = (Vec, Rec), where the node set Vec =\nE∪C is the union of exercises and knowledge concepts, and\nedge set Rec is generated from the Q-matrix. As the above\nrec ∈ Rec indicates that exercise e involves the knowledge\nconcept c.",
        "methodology": "In this paper, we propose a Self-supervised graph learning\nfor Cognitive Diagnosis (SCD), which assists the main supervised task by self-supervised signals. Firstly, the task of\nour method is briefly introduced. Then we present the main\ntask and self-supervised task separately.\nOverview\nAs shown in Figure 2, our SCD model can be divided into\ntwo tasks: the main supervised task and the auxiliary selfsupervised task. Two tasks share the same GCN model. The\nmain task is a common graph-based cognitive diagnostic\ntask which leverages the student embeddings and exercise\nembeddings from the GCN output to make predictions on\nstudent scores. For self-supervised graph learning, we constructed two different views based on the relational graph\nby the proposed rule of dropping edges. By maximizing the\ncross-view consistency between the representations of the\nsame node on two generated views and minimizing those\nof different nodes, we could enable GCN to focus more on\nlong-tailed nodes.\nEmbedding Layer\nEmbedding layer encodes the students, exercises, and\nknowledge concepts into embedding vectors. Let S\n=\nRM×d, E = RN×d and C = RK×d be trainable matrix,\nwhere M, N and K denote the number of students, exercises\nand knowledge concepts respectively. d is the dimension of\nthe embedding vector, which is usually set to the same size\nwith the number of knowledge concepts. Student si is encoded as one-hot vector xi. We can get its embedding vector\ns1\ni by multiplying one-hot vector xi with trainable matrix\nS. Similarly, the embedding vector of the exercise ej and\nknowledge concept ck can be obtained in the same way. The\nembedding layer can be modeled as:\ns1\ni = xT\ni S, e1\nj = yT\nj E, c1\nk = zT\nk C,\n(1)\nwhere xi ∈ {0, 1}M, yj ∈ {0, 1}N and zk ∈ {0, 1}K are all\none-hot vectors, s1\ni , e1\nj, c1\nk ∈ Rd.\nGCN Model\nThe input of the first layer in the GCN model is the output of\nthe embedding layer. The graph convolution is a process of\nmessage passing and feature aggregation based on the graph\nstructure. The representation of each node is the aggregation of the neighbor representations in the previous layer. As\nshown in figure 2, the outputs of the GCN based on the original relational graph and the generated views are leveraged\nfor the cognitive diagnosis main task and the self-supervised\nlearning task respectively. Let sl\ni, el\nj, cl\nk ∈ Rd be the input\nstudent, exercise and knowledge concept embedding vectors\nof the l-th GCN layer respectively. sl+1\ni\n, el+1\nj\n, cl+1\nk\n∈ Rd\nare updated after the neighbor aggregation. As for student\nsi, the embedding is updated in layer l by aggregating the\nembeddings of the neighborhood exercise nodes:\nsl+1\ni\n=\nX\nj∈N e\nsi\nαl\nijel\nj + sl\ni,\n(2)\nwhere N e\nsi are exercises that student si interacted with, sl\ni\nis residual connection and αl\nij is the node-level attention\nweight. The attention weight can be calculated as:\nαl\nij =\nexp(Fse([sl\ni, el\nj]))\nP\na∈N e\nsi exp(Fse([sl\ni, ela])),\n(3)\nwhere Fse represents a fully connected layer and [·] denotes\na concatenation operation.\nThe embeddings of exercises and knowledge concepts are\nupdated in the same way. Note that the exercise nodes aggregate information from both student and knowledge concept nodes. Stacking the l-layer GCN, the embedding vectors sl+1, el+1, cl+1 can be obtained by incorporating the relation information.\nDiagnosis Layer\nLet hs\ni ∈ (0, 1)K be the mastery level of student si on each\nknowledge concept, and he\nj ∈ (0, 1)K be the difficulty of\nexercise ej. After obtaining the embedding vectors of the\nexercises and students, we can diagnose the cognitive state\nof students and the difficulty of the exercises through the\ndiagnosis layer:\nhs\ni = σ(Fs(sl+1\ni\n)),\n(4)\nhe\nj = σ(Fe(el+1\nj\n)),\n(5)\nTo verify the correctness of the diagnosis, a prediction\nfunction is used to predict the scores of students:\nyij =\n1\n|N cej|\nX\nk∈N c\nej\nzT\nk σ(Fpredict(hs\ni − he\nj)),\n(6)\nwhere yij is the accuracy that the student si answers the exercise ej. Fpredict is a fully connected layer. N c\nej is the set\nof knowledge concepts involved in exercise ej. | · | is the\ncounting symbol, and zk is the one-hot code of knowledge\nconcepts ck. The loss function of the main task is a crossentropy loss between predicted score y and true scores r.\nLmain = −\nX\ni\n(ri log yi + (1 − ri) log(1 − yi)).\n(7)\nSelf-Supervised Graph Learning\nAs shown in figure 2, self-supervised graph learning is added\nto the main task as an auxiliary branch. It helps GCN to\nbetter capture the interaction of sparse nodes and provides\nself-supervised signals for the model training. Two different views are generated for the relational graph by dropping\nedges, which have fewer edges but retain as much important\ngraph structure information as possible. The detailed process\nof generating views is described below:\nEdge Dropout\nTo make GCN focus on long-tailed nodes\nand prevent them from being biased toward nodes with highdegree, we generate sparse views for the original graph. In\nparticular, during the generation process, we remove some\nof the edges connected to high-degree nodes, while keeping the edges of the long-tailed nodes to prevent them from\nInteraction \nSubgraph\nExercise to \nStudent\nStudent to \nExercise\n𝑝 𝑒j\n𝑝 𝑠𝑖\n𝑒𝑗\n𝑠𝑖\n𝑠𝑖\n𝑒𝑗\nEdge \nDropout\nGenerated View\nGenerated View\nFigure 3: The detailed process of edge dropout. The studentexercise interaction subgraph is split into two directed\ngraphs. The retained probability of the exercise-to-student\nedge is generated based on the degree of the student node,\nand does the same for the student-to-exercise edge.\nbecoming isolated nodes. Considering the edge on which\nthe degrees of two nodes are quite different, retaining such\nedges would result in generating views that are not sparse\nenough, while deleting them would produce isolated nodes.\nTo address this problem, we divide each edge into two directed edges based on the message passing procedure of\nGCN and process them separately.\nFigure 3 shows the process of edge dropout. The target of\nedge dropout is student-exercise interaction edges, so here\nwe only present the student-exercise interaction subgraph.\nFirstly, we split the student-exercise interaction subgraph\ninto two directed bipartite graphs: exercise to student and\nstudent to exercise. The former one contains only the edges\nfrom exercises to student and is used in the GCN layer for\nstudent aggregation. The latter one has opposite edges and\nis used for exercise aggregation. Let d(si) be indegree of\nsi. For each edge whose vertex is si in exercise to student\nsubgraph, we calculate their importance t(si) by:\nt(si) =\nk\nln(d(si) + θ),\n(8)\nwhere k is a hyperparameter which controls the overall probability of edge dropout, θ is a small positive value to avoid\nthe numerator being 0. The probability p(si) of retaining\neach edge is calculated by t(si):\np(si) =\n\n\n\npmin,\nif t(si) ≤ pmin\nt(si),\nif pmin < t(si) ≤ 1\n1,\nif 1 < t(si)\n(9)\nwhere pmin is the hyperparameter to control the minimum\nprobability that each edge is retained to avoid the generated\nview from discarding too much information.\nIn student to exercise subgraph, we generate the retention probabilities for each edge by the same approach as in\nEq. (8, 9):\nt(ej) =\nk\nln(d(ej) + θ),\n(10)\np(ej) =\n\n\n\npmin,\nif t(ej) ≤ pmin\nt(ej),\nif pmin < t(ej) ≤ 1\n1,\nif 1 < t(ej)\n(11)\nDataset\njunyi\nASSIST\nStudents\n10000\n3644\nExercises\n835\n14439\nKnowledge Concepts\n835\n123\nInteractions\n220799\n281890\nInteractions per student\n22.07\n77.36\nDensity\n0.02644\n0.00536\nTable 1: Statistics of experimental datasets.\nBased on the probability p for each edge to be retained,\nthe different views of the student-exercise interaction subgraph can be generated.\nContrastive Learning\nIn each epoch, two views are generated for the student-exercise interaction subgraph and replace the original subgraph in the relationship graph. By performing neighbor aggregation on the generated relational\ngraph, the representations of the same node on different\nviews can be obtained: s′\ni, s′′\ni for si and e′\nj, e′′\nj for ej. We\ntreat the representations of the same node under different\nviews as being positive pairs (i.e., (s′\ni, s′′\ni )), and the representations of different nodes as negative pairs (i.e., (s′\ni, s′′\nj ), i ̸=\nj). To maximize the cross-view consistency of the positive class samples while minimizing that of the negative\nclass samples, we adopt contrastive loss adjusted from InfoNCE (Oord, Li, and Vinyals 2018) as self-supervised loss:\nLs\nssl = 1\nM\nX\nsi∈S\n− log\nexp(sim(s′\ni, s′′\ni )/τ)\nP\nsj∈S,i̸=j exp(sim(s′\ni, s′′\nj )/τ),\n(12)\nLe\nssl = 1\nN\nX\nei∈E\n− log\nexp(sim(e′\ni, e′′\ni )/τ)\nP\nej∈E,i̸=j exp(sim(e′\ni, e′′\nj )/τ),\n(13)\nLssl = Ls\nssl + Le\nssl,\n(14)\nwhere M, N are the number of students and exercises respectively, sim(·) is the similarity function which is set as\ncosine distance, and τ is hyperparameter temperature which\ncan mine difficult negative pairs.\nTraining\nSelf-supervised graph learning is performed as an auxiliary\ntask, in parallel with the main cognitive diagnosis task. They\nare optimized with a multi-task training strategy at the same\ntime. The total loss of our method is:\nL = Lmain + λ1Lssl + λ2∥Θ∥2,\n(15)\nwhere Θ represents learnable model parameters. The Adam\noptimizer is adopted to minimize L.",
        "experiments": "The experiments are conducted on real-world cognitive diagnostic datasets to answer the following research questions:\n• RQ1: How does SCD perform compared to the state-ofthe-art cognitive diagnosis models?\n• RQ2: How does SCD perform on students with sparse\ninteraction data?\n• RQ3: What is the advantage of our edge dropout strategy\nover random strategy?\n• RQ4: How about the interpretability of SCD diagnostic\nresults?\nExperimental Settings\nDatasets\nWe conduct experiments on two real-world\ndatasets: junyi 1 and ASSIST 2. Junyi dataset is collected\nfrom the Chinese e-learning website Junyi Academy. ASSIST is a publicly available dataset collected by ASSISTments online tutoring system for student performance prediction, and we choose the “skill-builder data 2009-2010”\nversion. Both two datasets contain students’ interaction\nrecords with exercises and exercises’ relation with knowledge concepts. To demonstrate the ability of SCD on sparse\ndata, we retain students with the number of interaction\nrecords above 5. Detailed statistical information of the processed data is shown in table 1. To explore the effect of different sparse data on the experimental results, we divided\nthe data set into different proportions.\nBaseline\nWe compare our SCD with the following CD\nmodels:\n• IRT (Lord 1952). IRT is a basic CD model which leverages a linear function to predict the probability that a student correctly answers a question.\n• MIRT (Reckase 2009). MIRT extends the representation\nof students and exercises in IRT from one-dimensional to\nmultidimensional.\n• NCD (Wang et al. 2020). NCD introduces nonlinearity\nand replaces the manually designed prediction function\nwith a neural network.\n• RCD (Gao et al. 2021). RCD is the state-of-the-art CD\nmodel. It introduces relations between knowledge concepts and models the relations using the graph structure.\nHyperparameter Settings and Metrics\nWe implement\nour SCD with PyTorch. For IRT, MIRT and NCD, we use\nthe code provided by EduCDM (bigdata ustc 2021). We reimplement RCD to make it easy to train and achieve the\nsame results as the original code. For each model we set the\nbatch size to 256. As for graph-based models, i.e. RCD and\nSCD, we set the layers of the graph network to 2.\nWe adopt RMSE (Root Mean Square Error) and\nACC (Prediction Accuracy) to evaluate the models. In addition, for the long-tailed problem, we design RMSE50 and\nACC50 to focus more on the diagnosis of long-tailed students and mitigate the effect of a small number of students\nwith the majority of interaction data on the results. Specifically, we calculate RMSE50 and ACC50 as the average of\nthe RMSE and ACC over the top 50% students with much\nfewer interaction data, respectively:\n1https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=\n1198\n2https://sites.google.com/site/assistmentsdata/home/20092010-assistment-data/skill-builder-data-2009-2010\ntrain:test\n5:5\n6:4\n7:3\n8:2\nmethods\nacc\nrmse acc50 rmse50\nacc\nrmse acc50 rmse50\nacc\nrmse acc50 rmse50\nacc\nrmse acc50 rmse50\nIRT\n67.90 45.82 65.74 47.06\n68.58 45.44 66.61 46.99\n69.39 45.07 66.20 47.12\n69.73 44.69 66.83 46.73\nMIRT\n70.53 48.98 69.33 45.25\n70.07 47.98 69.91 45.37\n71.38 47.08 70.21 44.66\n72.00 46.27 70.83 45.72\nNCD\n71.88 43.73 70.18 46.46\n72.01 43.49 71.18 45.11\n72.79 43.08 71.21 44.25\n72.77 42.74 71.77 43.82\nRCD\n71.97 43.29 71.06 41.65\n72.14 43.12 72.06 41.22\n72.69 42.73 72.18 40.81\n72.82 42.50 71.93 39.23\nSCD\n72.24 43.15 73.00 40.94\n72.40 42.93 73.08 40.18\n72.88 42.64 73.15 39.46\n73.17 42.33 72.94 38.34\n(a)ASSIST\ntrain:test\n5:5\n6:4\n7:3\n8:2\nmethods\nacc\nrmse acc50 rmse50\nacc\nrmse acc50 rmse50\nacc\nrmse acc50 rmse50\nacc\nrmse acc50 rmse50\nIRT\n78.93 39.18 74.27 44.00\n79.13 38.87 76.31 41.84\n79.35 38.41 77.75 40.37\n79.45 38.30 76.59 39.52\nMIRT\n79.91 37.72 75.86 40.48\n80.14 37.43 76.47 42.20\n80.35 37.28 77.49 40.83\n80.54 37.22 76.93 38.34\nNCD\n77.93 38.90 75.00 40.80\n77.42 39.97 76.02 40.49\n78.12 38.90 77.39 39.61\n77.74 38.97 77.54 39.54\nRCD\n80.86 36.79 77.21 36.65\n80.85 36.85 77.31 35.87\n80.95 36.63 77.83 33.57\n81.04 36.55 77.96 32.97\nSCD\n81.16 36.69 77.75 34.79\n81.00 36.69 77.88 34.01\n81.14 36.55 78.41 32.95\n81.17 36.54 78.44 31.72\n(b)junyi\nTable 2: Experimental results on student performance prediction in percentage. The bold indicates the best result.\nRMSE50 =\n1\nN50\nX\nsi∈S50\nRMSE(si)\n(16)\nACC50 =\n1\nN50\nX\nsi∈S50\nACC(si),\n(17)\nwhere S50 are the top 50% students with fewer interaction\ndata, and N50 is the amount of these students.\nPerformance Comparison (RQ1)\nTable 2 shows the results comparison between our SCD\nand four baselines, and the best results are shown in bold.\nThe experimental results show that SCD outperforms all\nbaselines. Especially, compared with the same graph based\nmethod RCD, we find that our SCD outperforms RCD on\nACC and RMSE and achieves more significant improvements on ACC50 and RMSE50. This indicates that:\n• There is a serious long-tailed problem in the cognitive\ndiagnostic system, i.e., many students have very little interaction data. Those long-tailed students would be ignored when evaluating the model using common metrics,\nwhere their results will only have a very small impact on\nthe whole results.\n• Our SCD can pay more attention on the long-tailed students. The results on ACC50 and RMSE50 show that\nthe self-supervised learning auxiliary task can effectively\nimprove the performance of the model on these students.\nPerformance on Long-Tailed Students(RQ2)\nTo answer RQ2 and explore how SCD benefits from selfsupervised learning, we group students according to the\nnumber of interactions and measure the performance of\nRCD and SCD on each group of students separately. Specifically, we divide the students into eight groups, and the\namount of interaction data per student in the group is within\na specific range. The number of students in each group, the\n0-5\n5-10\n10-15\n15-20\n20-25\n25-30\n30-35\n3576\n77\n78\n79\n80\n81\n82\n83\n84\nAccuracy\n0-5\n5-10\n10-15\n15-20\n20-25\n25-30\n30-35\n3533\n34\n35\n36\n37\nRMSE\nRCD\nSCD\n(a) junyi\n0-5\n5-10\n10-15\n15-20\n20-25\n25-30\n30-35\n3568\n69\n70\n71\n72\n73\n74\n75\nAccuracy\n0-5\n5-10\n10-15\n15-20\n20-25\n25-30\n30-35\n3539\n40\n41\n42\n43\n44\n45\nRMSE\n(b) ASSIST\nFigure 4: Performance comparison over different student\ngroups. The horizontal coordinate is the number of interactions per student in each group.\ntotal amount of interactions, and the amount of interaction\ndata per student are shown in table 1.\nFigure 4 shows the results for grouped students. We find\nthat SCD achieves a large improvement on groups with a\nlimited number of interactions (e.g., 0-5, 5-10), while these\ngroups make up the majority of students. Furthermore, SCD\nperforms similarly or even better than RCD on groups where\nthe interaction data are not sparse (e.g., 30-35, 35-). This\nindicates that the introducing of self-supervision positively\naffects all data.\nStrategy of Edge Dropout(RQ3)\nIn SCD, we design an importance-based edge dropout strategy. Specifically, we generate the retention probability of\neach edge based on the degree of its vertices. To investiD\nASSIST\njunyi\nM\nacc\nrmse acc50 rmse50\nacc\nrmse acc50 rmse50\ntrain : test = 5 : 5\nrand 71.01 43.34 71.17\n41.77\n80.82 36.91 77.12\n36.83\nSCD 72.24 43.15 73.00\n40.96\n81.16 36.69 77.75\n34.79\ntrain : test = 6 : 4\nrand 72.20 43.13 72.07\n43.20\n80.95 36.78 77.35\n35.70\nSCD 72.40 42.93 73.08\n40.18\n81.00 36.69 77.88\n34.01\ntrain : test = 7 : 3\nrand 72.63 42.71 72.04\n40.78\n81.00 36.63 77.85\n33.54\nSCD 72.88 42.64 73.15\n39.46\n81.14 36.55 78.41\n32.95\ntrain : test = 8 : 2\nrand 72.95 42.38 72.04\n39.19\n81.10 36.57 77.97\n33.02\nSCD 73.17 42.33 72.94\n38.34\n81.17 36.54 78.44\n31.72\nTable 3: Results of study on edge dropout in percentage. The\nbold indicates the best results. D denotes datasets and M\ndenotes methods.\ngate the effectiveness of our strategy, we designed rand as a\nvariant: we retain all edges in the student-exercise interaction subgraph with the same probability p, and p is adjusted\nso that the view generated by rand has a similar number of\nedges as the view generated from SCD.\nTable 3 shows the comparison between SCD and rand.\nSCD outperforms rand in all metrics. And SCD has a greater\nadvantage on ACC50 and RMSE50. From the comparison\nwe can see that rand as a self-supervised learning method\ndoes not have an advantage on the long-tailed problem. This\nis because completely random strategy may lose essential\ngraph structure information. An equal-probability dropout\nof all edges does not help the long-tailed nodes but makes\ntheir information much sparser. Even worse, the nodes can\nbecome isolated. Comparing with rand, the edge dropout\nstrategy in our SCD can better enhance the weight of longtailed nodes in the generated view, which improves the performance on long-tailed students.\nDiagnostic Case Studie(RQ4)\nTo answer RQ4, we construct a diagnostic case study. We\nchose two students and three exercises that they had answered, from which student#2 is from the long-tailed student\npopulation. Table 4 shows the statistical information of these\nstudents and exercises. Exercise#1 is related to Knowledge\nConcept A&B, while exercise#2 is related to Knowledge\nConcept C, and D&E for exercise#3. All students’ scores\non the exercises have been given.\nFigure 5 shows the diagnostic results of RCD and SCD.\nAs for the diagnosis result of SCD, student#1 has mastered\nall the knowledge concepts at a higher level than the difficulty of the corresponding exercises, and all three exercises\nare correct; student#2 has mastered the knowledge concepts\nrelated to exercise#1, but not exercises#2 and#3, so the exercise#1 is right and it is failed on the others. RCD has been\nequally successful in diagnosing Student#1. In the RCD’s\ndiagnosis results for Student#2, he has a higher level of\nmastery of knowledge concept C than the difficulty of exercise#2. This is a failed diagnosis for RCD. The difference in\nExercise#1\nExercise#2\nExercise#3\nConcept\nA,B\nC\nD,E\nStudent#1\n✔\n✔\n✔\nStudent#2\n✔\n✘\n✘\nTable 4: Statistics of diagnosis case study. The knowledge\nconcepts associated with each exercise and the true score of\neach interaction are given.\nA\nB\nC\nD\nE\nknowledge concept\n0.2\n0.4\n0.6\n0.8\n1\nmastery level of student\nexer1_RCD\nexer2_RCD\nexer3_RCD\nexer1_SCD\nexer2_SCD\nexer3_SCD\nstu1_RCD\nstu2_RCD\nstu1_SCD\nstu2_SCD\n0.2\n0.4\n0.6\n0.8\n1\ndifficulty of exercise\nFigure 5: Result of diagnosis case study. The vertical coordinate shows the diagnosed student’s knowledge proficiencies\n(bars) and knowledge difficulties of each exercise (points).\nThe suffixes in the figure legends indicate the diagnosis results of different models.\ndiagnostic results for long-tailed students demonstrates the\nstrength of SCD’s diagnostic capabilities for these students.",
        "conclusion": "We proposed a self-supervised graph learning framework for\ncognitive diagnosis (SCD), which applies contrastive learning to provide self-supervised signals for the cognitive diagnosis. It assists the main task to improve the diagnosis ability on long-tailed students with sparser interaction records.\nSpecifically, we generate different views of the studentexercise graph by discarding the student-exercise relationship edges in a way that retains the important edges. The\nconsistency between the representations of the same nodes\non different sparse views are maximized to make the model\nfocus more on long-tailed nodes and provide self-supervised\nsignals. Experiments on real-world datasets demonstrate the\neffectiveness in alleviating the long-tailed issue and the advantage of our view generation strategy. We hope to conduct further studies of view generation strategies based on\nthe characteristics of cognitive diagnostic task.",
        "summary_en": "Cognitive diagnosis is a fundamental yet critical research task in the field of intelligent education, which aims to discover the proficiency level of different students on specific knowledge concepts. Despite the effectiveness of existing efforts, previous methods always considered the mastery level on the whole students, so they still suffer from the Long Tail Effect. A large number of students who have sparse interaction records are usually wrongly diagnosed during inference. To relieve the situation, this paper proposed a Self-supervised Cognitive Diagnosis (SCD) framework which leverages the self-supervised manner to assist the graph-based cognitive diagnosis, then the performance on those students with sparse data can be improved. Specifically, the paper came up with a graph confusion method that drops edges under some special rules to generate different sparse views of the graph. By maximizing the cross-view consistency of node representations, the model could pay more attention on long-tailed students. Additionally, the paper proposed an importance-based view generation rule to improve the influence of long-tailed students. Extensive experiments on real-world datasets show the effectiveness of the approach, especially on the students with much sparser interaction records.",
        "summary_zh": "这篇论文介绍了一种名为自监督图学习的方法，用于解决智能教育中长尾认知诊断的问题。传统方法往往无法准确诊断稀疏交互记录的学生，因此提出了自监督认知诊断（SCD）框架，利用自监督方式辅助基于图的认知诊断，通过特殊规则下的图混淆方法生成图的不同的稀疏视图，并最大化跨视图节点表示的一致性，从而更加关注长尾学生。实验结果表明该方法在真实数据集上取得了良好效果，尤其对于交互记录较少的学生。"
    },
    {
        "title": "Augmentation-Free Self-Supervised Learning on Graphs",
        "abstract": "Inspired by the recent success of self-supervised methods applied on images, self-supervised learning on graph structured data has seen rapid growth especially centered on augmentation-based contrastive methods. However, we argue that without carefully designed augmentation techniques, augmentations on graphs may behave arbitrarily in that the underlying semantics of graphs can drastically change. As a consequence, the performance of existing augmentationbased methods is highly dependent on the choice of augmentation scheme, i.e., hyperparameters associated with augmentations. In this paper, we propose a novel augmentation-free self-supervised learning framework for graphs, named AFGRL. Specifically, we generate an alternative view of a graph by discovering nodes that share the local structural information and the global semantics with the graph. Extensive experiments towards various node-level tasks, i.e., node classification, clustering, and similarity search on various realworld datasets demonstrate the superiority of AFGRL. The source code for AFGRL is available at https://github.com/ Namkyeong/AFGRL.",
        "introduction": "Recently, self-supervised learning paradigm (Liu et al.\n2021), which learns representation from supervision signals derived from the data itself without relying on\nhuman-provided labels, achieved great success in many domains including computer vision (Gidaris, Singh, and Komodakis 2018; Noroozi and Favaro 2016), signal processing (Banville et al. 2021, 2019), and natural language processing (Devlin et al. 2018; Brown et al. 2020). Specifically, contrastive methods, which are at the core of selfsupervised learning paradigm, aim to build effective representation by pulling semantically similar (positive) pairs together and pushing dissimilar (negative) pairs apart (Hjelm\net al. 2018; Oord, Li, and Vinyals 2018), where two augmented versions of an image are considered as positives, and\nthe remaining images are considered as negatives. Inspired\nby the success of the contrastive methods in computer vision applied on images, these methods have been recently\nadopted to graphs (Xie et al. 2021).\nRandom Cropping\nColor Distortion\nO\nO\nOH\nO\nO\nO\nOH\nO\nO\nO\nO\nO\nDrop Node\nO\nO\nOH\nO\nPerturb Edge\naspirin\nalkene\naspirin\n𝜸 - lactone\nDrop Edge\n(a) Image\n(b) Molecular\n(c) Social Graph\nCommunity 1\nCommunity 2\nCommunity 3\nBob\nAlice\nFigure 1: Augmentations on images ((a)) keep the underlying semantics, whereas augmentations on graphs ((b),(c))\nmay unexpectedly change the semantics.\nAlthough self-supervised contrastive methods have been\nshown to be effective on various graph-related tasks, they\npay little attention to the inherent distinction between images and graphs: while augmentation is well defined on images, it may behave arbitrarily on graphs. For example, in\nthe case of images, even after randomly cropping and rotating them, or distorting their color, their underlying semantic is hardly changed (Figure 1 (a)), and even if the semantic changes, humans can readily recognize the change\nvisually, and choose an alternative augmentation approach\nthat preserves the semantic. On the other hand, when we\nperturb (drop or add) edges/nodes, and their features of a\ngraph, we cannot ascertain whether the augmented graph\nwould be positively related to the original graph, and what\nis worse, it is non-trivial to verify the validity of the augmented graph since graphs are hard to visualize. For example, in molecular graphs, dropping a carbon atom from the\nphenyl ring of aspirin breaks the aromatic system and results in a alkene chain (Figure 1(b)). Moreover, perturbing\nthe connection of aspirin might introduce a molecule of totally different property, namely, five-membered lactone (Sun\net al. 2021a). Likewise, in social graphs, randomly dropping\nedges might also lead to semantic changes, especially when\nthese edges are related to hub nodes. For example, as shown\nin Figure 1(c), if the edge between Bob and Alice is dropped,\nit would take much longer distance for Bob to reach Community 3 (i.e., from 2-hops to 5-hops), which also alters the\nrelationship between Community 1 and Community 3. We\nargue that this is mainly because graphs contain not only the\nsemantic but also the structural information.\nDue to the aforementioned arbitrary behavior of augmentation on graphs, the quality of the learned graph representations of previous augmentation-based contrastive methods (Hassani and Khasahmadi 2020; Zhu et al. 2020, 2021;\nThakoor et al. 2021; Sun et al. 2021a; Veliˇckovi´c et al. 2018)\nis highly dependent on the choice of the augmentation\nscheme. More precisely, in order to augment graphs, these\nmethods perform various augmentation techniques such as\nnode/edge perturbation and node feature masking, and the\namount by which the graphs are augmented is controlled by\na set of hyperparameters. However, these hyperparameters\nshould be carefully tuned according to which datasets, and\nwhich downstream tasks are used for the model evaluation,\notherwise the model performance would vary greatly (You\net al. 2021). Moreover, it is also shown that the performance\non downstream tasks highly resort to which combinations\nof the augmentation techniques (You et al. 2020; Sun et al.\n2021a) are used.\nFurthermore, even after discovering the best hyperparameters for augmentations, another limitation arises due to the\ninherent philosophy of contrastive learning. More precisely,\ninheriting the principle of instance discrimination (Wu et al.\n2018), contrastive methods treat two samples as a positive\npair as long as they are two augmented versions of the same\ninstance, and all other pairs are treated as negatives. Although this approach is effective for learning representations\nof images (Chen et al. 2020; He et al. 2020), simply adopting it to graphs by treating all other nodes apart from\nthe node itself as negatives overlooks the structural information of graphs, and thus cannot benefit from the relational inductive bias of graph-structured data (Battaglia et al.\n2018). Lastly, due to the nature of contrastive methods, a\nlarge amount of negative samples is required for improving the performance on the downstream tasks, requiring high\ncomputational and memory costs, which is impractical in reality (Thakoor et al. 2021).\nContribution\n. We propose a self-supervised learning\nframework for graphs, called Augmentation-Free Graph\nRepresentation Learning (AFGRL), which requires neither\naugmentation techniques nor negative samples for learning\nrepresentations of graphs. Precisely, instead of creating two\narbitrarily augmented views of a graph and expecting them\nto preserve the semantics of the original graph, we use the\noriginal graph per se as one view, and generate another view\nby discovering, for each node in the original graph, nodes\nthat can serve as positive samples via k-nearest-neighbor\n(k-NN) search in the representation space. Then, given the\ntwo semantically related views, we aim to predict, for each\nnode in the first view, the latent representations of its positive nodes in the second view. However, naively selecting\npositive samples based on k-NN search to generate an alternative view can still alter the semantics of the original graph.\nHence, we introduce a mechanism to filter out false positives from the samples discovered by k-NN search. In a nutshell, we consider a sample to be positive only if either 1)\nit is a neighboring node of the target node in the adjacency\nmatrix (local perspective), capturing the relational inductive\nbias inherent in the graph-structured data, or 2) it belongs\nto the same cluster as the target node (global perspective).\nMoreover, by adopting BYOL (Grill et al. 2020) as the backbone of our model, negative samples are not required for the\nmodel training, thereby avoiding the “sampling bias” (Lin\net al. 2021), i.e. the negative samples may have the same\nsemantics with the query node, which would result in less\neffective representation (Saunshi et al. 2019).\nOur extensive experiments demonstrate that AFGRL outperforms a wide range of state-of-the-art methods in terms of\nnode classification, clustering and similarity search. We also\ndemonstrate that compared with existing methods that heavily depend on the choice of hyperparameters, AFGRL is stable over hyperparameters. To the best of our knowledge, AFGRL is the first work that learns representations of graphs\nwithout relying on manual augmentation techniques and\nnegative samples.",
        "related work": "Recently, motivated by the great success of self-supervised\nmethods on images, contrastive methods have been increasingly adopted to graphs. DGI (Veliˇckovi´c et al. 2018), a\npioneering work highly inspired by Deep InfoMax (Hjelm\net al. 2018), aims to learn node representations by maximizing the mutual information between the local patch\nof a graph. i.e., node, and the global summary of the\ngraph, thereby capturing the global information of the graph\nthat is overlooked by vanilla graph convolutional networks\n(GCNs) (Kipf and Welling 2016; Veliˇckovi´c et al. 2017).\nDGI is further improved by taking into account the mutual\ninformation regarding the edges (Peng et al. 2020) and node\nattributes (Jing, Park, and Tong 2021). Inspired by SimCLR (Chen et al. 2020), GRACE (Zhu et al. 2020) first creates two augmented views of a graph by randomly perturbing\nnodes/edges and their features. Then, it learns node representations by pulling together the representation of the same\nnode in the two augmented graphs, while pushing apart representations of every other node. This principle (Wu et al.\n2018) has also been adopted for learning graph-level representations of graphs that can be used for graph classification, (Sun et al. 2019; You et al. 2020; Hassani and Khasahmadi 2020). Despite the success of contrastive methods on\ngraphs, they are criticized for the problem raised by the\n“sampling bias” (Bielak, Kajdanowicz, and Chawla 2021).\nMoreover, these methods require a large amount of negative\nsamples for the model training, which incurs high computational and memory costs (Grill et al. 2020).\nTo address the sampling bias issue, BGRL (Thakoor et al.\n2021) learns node representations without using negative\nsamples. It learns node representations by encoding two augmented versions of a graph using two separate encoders:\none is trained through minimizing the cosine loss between\nthe representations generated by the two encoders, while the\nother encoder is updated by an exponential moving average\nof the first encoder. Although the sampling bias has been\naddressed in this way, BGRL still requires augmentations\non the original graph, which may lead to semantic drift (Sun\net al. 2021a) as illustrated in Figure 1. On the other hand,\nour proposed method learns node representations without\nany use of negative samples or augmentations of graphs.\nAugmentations on Graphs.\nMost recently, various augmentation techniques for graphs have been introduced. e.g.,\nnode dropping (You et al. 2020), edge modification (Jin et al.\n2021; Qiu et al. 2020; Zhao et al. 2020), subgraph extraction (Jiao et al. 2020; Sun et al. 2021b), attribute masking\n(Zhu et al. 2020, 2021) and others (Hassani and Khasahmadi\n2020; Kefato and Girdzijauskas 2021; Suresh et al. 2021).\nGRACE (Zhu et al. 2020) randomly drops edges and masks\nnode features to generate two augmented views of a graph.\nGCA (Zhu et al. 2021) further improves GRACE by introducing advanced adaptive augmentation techniques that take\ninto account both structural and attribute information. However, due to the complex nature of graphs, the performance\non downstream tasks is highly dependent on the selection\nof the augmentation scheme, as will be shown later in our\nexperiments (Table 1). Moreover, previous work (You et al.\n2020; Sun et al. 2021a) have shown that there is no universally outperforming data augmentation scheme for graphs.\nLastly, Sun et al. (2021a) demonstrates that infusing domain knowledge is helpful in finding proper augmentations,\nwhich preserves biological assumption in molecular graph.\nHowever, domain knowledge is not always available in reality. In this work, we propose a general framework for generating an alternative view of the original graph without relying on existing augmentation techniques that may either\n1) change the semantics of the original graph or 2) require\ndomain knowledge.",
        "problem statement": "Notations.\nLet G = (V, E) denote a graph, where V =\n{v1, ..., vN} represents the set of nodes, and E ⊆ V × V\nrepresents the set of edges. G is associated with a feature\nmatrix X ∈ RN×F , and an adjacency matrix A ∈ RN×N\nwhere Aij = 1 iff (vi, vj) ∈ E and Aij = 0 otherwise.\nTask: Unsupervised Graph Representation Learning.\nGiven a graph G along with X and A, we aim to learn a encoder f(·) that produces node embeddings H = f(X, A) ∈\nRN×D, where D << F. In particular, our goal is to learn\nnode embeddings that generalize well to various downstream tasks without using any class information.",
        "preliminary: bootstrap your own latent": "Before explaining details of our proposed method, we begin\nby introducing BYOL (Grill et al. 2020), which is the backbone of our proposed framework. The core idea of BYOL\nis to learn representations of images without using negative samples (Grill et al. 2020). Given two augmented views\nof an image, BYOL trains two separate encoders, i.e., online encoder fθ and target encoder fξ, and learns representations of images by maximizing the similarity of the two\nrepresentations produced by each encoder. More formally,\nBYOL generates two views x1 ∼ t(x), and x2 ∼ t′(x)\nof an image x given a set of transformations t ∼ T and\nt′ ∼ T , and these two generated views of an image are\nfed into the online and target encoders. Precisely, the online\nencoder fθ produces online representation h1 = fθ(x1),\nwhile the target encoder fξ produces target representation\nh2 = fξ(x2). Then, both online and target representations\nare projected to smaller representations z1 = gθ(h1) and\nz2 = gξ(h2) using projectors gθ and gξ, respectively. Finally, an additional predictor qθ is applied on top of the\nprojected online representation, i.e., z1, to make the architecture asymmetric. The objective function is defined as\nLθ,ξ = ∥¯qθ(z1) − ¯z2∥2, where ¯qθ(z1) and ¯z2 denote l2normalized form of qθ(z1) and z2, respectively. A symmetric loss eLθ,ξ is obtained by feeding x2 into the online encoder and x1 into the target encoder, and the final objective\nis to minimize LBYOL\nθ,ξ\n= Lθ,ξ + eLθ,ξ. At each training iteration, a stochastic optimization step is performed to minimize\nLBYOL\nθ,ξ\nwith respect to θ only, while ξ is updated using the\nexponential moving average (EMA) of θ, which is empirically shown to prevent the collapsing problem (Chen and He\n2021). More formally, the parameters of BYOL are updated\nas θ ← optimizer\n\u0010\nθ, ∇θLBYOL\nθ,ξ\n, η\n\u0011\n, ξ ← τξ + (1 − τ)θ,\nwhere η is learning rate for online network, and τ ∈ [0, 1] is\nthe decay rate that controls how close ξ remains to θ.",
        "proposed method": "We first introduce how BYOL has been previously employed\non graphs (Thakoor et al. 2021), and discuss about the several limitations of augmentation-based methods for graphs.\nFinally, we present our proposed method, called AFGRL.\nGenerating\nAlternative\nViews\nvia\nAugmentation.\nBGRL (Thakoor et al. 2021) is a recently proposed fully\nnon-contrastive method for learning node representations\nthat does not leverage negative samples benefiting from\nthe framework of BYOL (Grill et al. 2020). Precisely,\nBGRL generates two different views of a graph via manual\naugmentations, i.e., node feature masking and edge masking\nas done by previous methods (Zhu et al. 2020, 2021),\nand the amount by which the graphs are augmented is\ncontrolled by a set of hyperparameters. Then, two encoders,\ni.e., online and target encoders, generate embeddings given\nthe augmented views of a graph as inputs, and the two\ngenerated embeddings are learned to be close to each other.\nTo prevent the representations from collapsing to trivial\nsolutions, BGRL introduces a symmetry-breaking technique\n(refer to Section for more detailed explanation). It is also\nworth noting that BGRL intentionally considers simple\naugmentation techniques to validate the benefit of fully\nnon-contrastive scheme applied on graphs.\nLimitation of Augmentation-based Methods on Graphs.\nAlthough BGRL has been shown to be effective in a fully\nnon-contrastive manner, i.e., without using negative samples, we observe that the quality of the learned node representations relies on the choice of the augmentation scheme.\nIn other words, performance on various downstream tasks\nevaluated based on the representations learned by BGRL\nvaries greatly according to the choice of hyperparameters\nassociated with augmentations, and the best hyperparameters are different for different datasets. This phenomenon\nEMA\n𝑓𝑓𝜃𝜃\n𝑓𝑓𝜉𝜉\n(𝑿𝑿, 𝑨𝑨)\n(𝑯𝑯𝜃𝜃, 𝑨𝑨)\n(𝑯𝑯𝜉𝜉, 𝑨𝑨)\n(𝒁𝒁𝜃𝜃, 𝑨𝑨)\n𝑞𝑞𝜃𝜃\n𝑘𝑘-NN(𝑩𝑩𝑖𝑖)\nK-means(𝑪𝑪𝑖𝑖)\n(𝑵𝑵𝑖𝑖)\nAdj.\n𝑷𝑷𝑖𝑖\n𝐱𝐱𝑖𝑖\n𝒛𝒛𝑖𝑖\n𝜃𝜃\n𝐡𝐡𝑖𝑖\n𝜃𝜃\n𝐡𝐡𝑖𝑖\nξ\n𝐡𝐡𝑗𝑗\nξ\nLocal (𝑵𝑵𝑖𝑖 ∩ 𝑩𝑩𝑖𝑖)\nGlobal (𝑩𝑩𝑖𝑖 ∩ 𝑪𝑪𝑖𝑖)\nStop-Gradient\n− 1\n𝑁𝑁 ෍\n𝑖𝑖 = 1\n𝑁𝑁\n෍\n𝑗𝑗 ∈ 𝐏𝐏𝑖𝑖\n𝐳𝐳i\n𝜃𝜃𝐡𝐡𝑗𝑗\nξ⊤\nฮ\nฮ\n𝐳𝐳i\n𝜃𝜃 ฮ\nฮ\n𝐳𝐳i\n𝜃𝜃\n𝐡𝐡𝑗𝑗\nξ\nFigure 2: The overall architecture of AFGRL. Given a graph, fθ and fξ generate node embeddings Hθ and Hξ both of which\nare used to obtain k-NNs for node vi, i.e., Bi. Combining it with Ni, we obtain local positives, i.e., Bi ∩ Ni. To obtain global\npositives for node vi, K-means clustering is performed on Hξ, and the result Ci is combined with Bi, i.e., Bi ∩Ci. Finally, we\ncombine local and global positives to obtain real positives, i.e., Pi. A predictor qθ projects Hθ to Zθ, which is used to compute\nthe final loss along with Hξ. Note that fθ is updated via gradient descent of the loss, whereas fξ is updated via EMA of fθ.\nComp.\nPhoto\nCS\nPhysics\nNode\nClassi.\nBGRL\n-4.00%\n-1.06%\n-0.20%\n-0.69%\nGCA\n-19.18%\n-5.48%\n-0.27%\nOOM\nNode\nClust.\nBGRL\n-11.57%\n-13.30%\n-0.78%\n-6.46%\nGCA\n-26.28%\n-23.27%\n-1.64%\nOOM\nTable 1: Performance sensitivity according to the hyperparameters for augmentations (i.e., edge drop and node feature masking) on node classification and clustering. Each\nvalue indicates the relative performance difference between\nthe best vs. worst performing cases, i.e. − (best−worst)\nbest\n× 100.\nThe hyperparameters (i.e., probability of edge drop and node\nfeature masking) are chosen within the range from 0.0 to 0.5\nto prevent a significant distortion of graphs.\nbecomes even clearer when stronger augmentations, such\nas diffusion (Hassani and Khasahmadi 2020), adaptive techniques (Zhu et al. 2021) and the infusion of domain knowledge (Sun et al. 2021a) are applied. Table 1 shows how\nthe performance of augmentation-based methods varies according to the hyperparameters associated with augmentations. More precisely, we report the relative performance of\nthe best performing case compared to the worst performing\ncase, i.e., -4.00% indicates that the worst case performs 4%\nworse than the best case. We observe that the performance\nin both tasks is sensitive to the hyperparameters, and that it\naggravates when a stronger augmentation technique is employed, i.e., GCA, in which case the role of augmentation\nbecomes even more important. Thus, we need a more stable\nand general framework for generating an alternative view\nof the original graph without relying on augmentation techniques introduced in existing works.\nAugmentation-Free GRL (AFGRL)\nWe propose a simple yet effective self-supervised learning\nframework for generating an alternative view of the original graph taking into account the relational inductive bias of\ngraph-structured data, and the global semantics of graphs.\nFor each node vi ∈ V in graph G, we discover nodes that\ncan serve as positive samples based on the node representations learned by the two encoders. i.e., online encoder fθ(·)\nand target encoder fξ(·)1. More precisely, these encoders\ninitially receive the adjacency matrix A and the feature matrix X of the original graph as inputs, and compute the online and target representations. i.e., Hθ = fθ(X, A) and\nHξ = fξ(X, A) whose i-th rows, hθ\ni and hξ\ni , are representations for node vi ∈ V. Then, for a given query node vi ∈ V,\nwe compute the cosine similarity between all other nodes in\nthe graph as follows:\nsim(vi, vj) =\nhθ\ni · hξ\nj\n∥hθ\ni ∥∥hξ\nj∥\n, ∀vj ∈ V\n(1)\nwhere the similarity is computed between the online and the\ntarget representations. Given the similarity information, we\nsearch for k-nearest-neighbors for each node vi, and denote\nthem by a set Bi, which can serve as positive samples for\nnode vi. Essentially, we expect the nearest neighbors in the\nrepresentation space to belong to the same semantic class as\nthe query node, i.e., vi in this case. Although Bi can serve\nas a reasonable set of positive candidates for node vi, 1) it\nis inherently noisy as we do not leverage any label information, i.e., Bi contains samples that are not semantically\nrelated to the query node vi. Moreover, only resorting to the\nnearest neighbors in the representation space may not only\noverlook 2) the structural information inherent in the graph,\ni.e., relational inductive bias, but also 3) the global semantics of the graph. To address these limitations, we introduce\na mechanism to filter out false positives from the samples\ndiscovered by k-NN search, while also capturing the local\nstructural information and the global semantics of graphs.\n1AFGRL adopts the architecture of BGRL (Thakoor et al.\n2021), which is slightly modified from BYOL (Grill et al. 2020). In\nparticular, projection networks, i.e., gθ(·) and gξ(·) are not used.\nFigure 3: Analysis on the ratio of its neighboring nodes being the same label as the query node across different ks.\nCapturing Local Structural Information.\nRecall that we\nexpected the nearest neighbors found by k-NN search, i.e.,\nBi, to share the same class label as the query node vi. To\nverify whether our expectation holds, we perform analysis on two datasets, i.e., Amazon Computers and WikiCS\ndatasets as shown in Figure 3. First, we obtain node embeddings from a randomly initialized 2-layer GCN (Kipf and\nWelling 2016), i.e., HRand-GCN = Rand-GCN(X, A), and\nperform k-NN search for each node given the node embeddings HRand-GCN. Then, for each node, we compute the ratio\nof its neighboring nodes being the same label as the query\nnode. In Figure 3, we observe that although the ratio is high\nwhen considering only a small number of neighbors, e.g.,\nk = 4, the ratio decreases as k gets larger in both datasets.\nThis implies that although our expectation holds to some extent, there still exists noise.\nHence, to filter out false positives from the nearest neighbors found by k-NN search, i.e., Bi for each node vi, we\nleverage the local structural information among nodes given\nin the form of an adjacency matrix. i.e., relational inductive bias. More precisely, for a node vi, its adjacent nodes\nNi tend to share the same label as the query node vi,\ni.e., smoothness assumption (Zhu, Ghahramani, and Lafferty\n2003). In Figure 3, we indeed observe that the ratio of the adjacent nodes being the same label as the query node (“Adj”)\nis about 70% in both datasets, which demonstrates the validity of the smoothness assumption. Therefore, to capture the\nrelational inductive bias reflected in the smoothness assumption, while filtering out false positives from noisy nearest\nneighbors, we compute the intersection between the nearest\nneighbors and adjacent nodes, i.e., Bi ∩ Ni. We denote the\nset of these intersecting nodes as local positives of vi. Indeed, Figure 3 shows that the local positives (“Rand. GCN\n+ Adj”) consistently maintain high correct ratio even when\nk increases.\nCapturing Global Semantics.\nTo capture the semantics\nof nodes in a global perspective, we leverage clustering techniques. The intuition is to discover non-adjacent nodes that\nshare the global semantic information with the query node.\nFor example, in an academic collaboration network whose\nnodes denote authors and edges denote collaboration between authors, even though two authors work on the same\nresearch topic (i.e., same label), they may not be connected\nin the graph since they neither collaborated in the past nor\nCluster 1\nAdjacency (𝑵!)\nNearest Neighbors (𝑩!)\nCluster 2\nCluster 3\nQuery Node (𝑣!)\n𝑣$\nNode (\n\\𝑣!)\nLocal Positive (𝑩! ∩ 𝑵!)\nGlobal Positive (𝑩! ∩ 𝑪!)\nReal Positive (𝑷!)\nSame cluster as 𝑣! (𝑪!)\n𝑪$\n𝑩$\nFigure 4: An overview of obtaining real positives of node vi.\nshare any collaborators. We argue that such semantically\nsimilar entities that do not share an edge can be discovered via clustering in a global perspective. In this regard,\nwe apply K-means clustering algorithm on the target representation Hξ to cluster nodes into a set of K clusters, i.e.\nG = {G1, G2, ..., GK}, and c(hξ\ni ) ∈ {1, ..., K} denotes\nthe cluster assignment of hξ\ni , i.e., vi ∈ Gc(hξ\ni ). Then, we\nconsider the set of nodes that belong to the same cluster as\nvi, i.e., Ci = {vj|vj ∈ Gc(hξ\ni )}, as its semantically similar nodes in the global perspective. Finally, we obtain the\nintersection between the nearest neighbors and the semantically similar nodes in the global perspective, i.e., Bi ∩ Ci,\nand we denote the set of these intersecting nodes as global\npositives of vi. In other words, nodes that are among the\nnearest neighbors of vi and at the same time belong to the\nsame cluster as vi are considered as globally positive neighbors. It is important to note that as K-means clustering algorithm is sensitive to the cluster centroid initialization, we\nperform multiple runs to ensure robustness of the clustering results. Specifically, we perform K-means clustering M\ntimes and obtain M sets of clusters, i.e., {G(j)}M\nj=1, where\nG(j) =\nn\nG(j)\n1 , G(j)\n2 , ..., G(j)\nK\no\nis the result of j-th run of\nthe clustering. Then, we define Ci = SM\nj=1 G(j)\nc(j)(hξ\ni ), where\nc(j)(hξ\ni ) ∈ {1, ..., K} denotes the cluster assignment of hξ\ni\nin the j-th run of clustering.\nObjective Function.\nIn order to consider both the local\nand global information, we define the set of real positives\nfor node vi as follows:\nPi = (Bi ∩ Ni) ∪ (Bi ∩ Ci)\n(2)\nOur objective function aims to minimize the cosine distance\nbetween the query node vi and its real positives Pi:\nLθ,ξ = − 1\nN\nN\nX\ni=1\nX\nvj∈Pi\nzθ\ni hξ⊤\nj\n\r\rzθ\ni\n\r\r\n\r\r\rhξ\nj\n\r\r\r\n,\n(3)\nwhere zθ\ni = qθ(hθ\ni ) ∈ RD is the prediction of the online embedding hθ\ni ∈ RD, and qθ(·) is the predictor network. Following BYOL, AFGRL’s online network is updated based on the gradient of its parameters with respect\nto the loss function (Equation 3), while the target network\nWikiCS\nComputers\nPhoto\nCo.CS\nCo.Physics\nSup. GCN\n77.19 ± 0.12\n86.51 ± 0.54\n92.42 ± 0.22\n93.03 ± 0.31\n95.65 ± 0.16\nRaw feats.\n71.98 ± 0.00\n73.81 ± 0.00\n78.53 ± 0.00\n90.37 ± 0.00\n93.58 ± 0.00\nnode2vec\n71.79 ± 0.05\n84.39 ± 0.08\n89.67 ± 0.12\n85.08 ± 0.03\n91.19 ± 0.04\nDeepWalk\n74.35 ± 0.06\n85.68 ± 0.06\n89.44 ± 0.11\n84.61 ± 0.22\n91.77 ± 0.15\nDW + feats.\n77.21 ± 0.03\n86.28 ± 0.07\n90.05 ± 0.08\n87.70 ± 0.04\n94.90 ± 0.09\nDGI\n75.35 ± 0.14\n83.95 ± 0.47\n91.61 ± 0.22\n92.15 ± 0.63\n94.51 ± 0.52\nGMI\n74.85 ± 0.08\n82.21 ± 0.31\n90.68 ± 0.17\nOOM\nOOM\nMVGRL\n77.52 ± 0.08\n87.52 ± 0.11\n91.74 ± 0.07\n92.11 ± 0.12\n95.33 ± 0.03\nGRACE\n77.97 ± 0.63\n86.50 ± 0.33\n92.46 ± 0.18\n92.17 ± 0.04\nOOM\nGCA\n77.94 ± 0.67\n87.32 ± 0.50\n92.39 ± 0.33\n92.84 ± 0.15\nOOM\nBGRL\n76.86 ± 0.74\n89.69 ± 0.37\n93.07 ± 0.38\n92.59 ± 0.14\n95.48 ± 0.08\nAFGRL\n77.62 ± 0.49\n89.88 ± 0.33\n93.22 ± 0.28\n93.27 ± 0.17\n95.69 ± 0.10\nTable 2: Performance on node classification (OOM: Out of memory on 24GB RTX3090).\nis updated by smoothing the online network. We also symmetrize the loss function. In the end, the online embeddings,\ni.e., Hθ ∈ RN×D are used for downstream tasks. Figure 4\nillustrates the overview of obtaining real positives of node\nvi.\nIn summary, 1) AFGRL does not rely on arbitrary augmentation techniques for the model training, thereby achieving stable performance. 2) AFGRL filters out false positives from the samples discovered by k-NN search, while\nalso capturing the local structural information, i.e., relational\ninductive bias, and the global semantics of graphs. 3) AFGRL does not require negative samples for the model training, thereby avoiding sampling bias and alleviating computational/memory costs suffered by previous contrastive\nmethods.",
        "experiments": "Experimental Setup\nDatasets.\nTo\nevaluate\nAFGRL,\nwe\nconduct\nexperiments\non\nfive\nwidely\nused\ndatasets,\nincluding\nWikiCS (Mernyei and Cangea 2020), Amazon (Computers\nand Photo) (McAuley et al. 2015), Coauthor (Co.CS and\nCo.Physics) (Sinha et al. 2015).\nMethods\nCompared.\nWe\nprimarily\ncompare\nAFGRL against GRACE (Zhu et al. 2020), BGRL (Thakoor\net al. 2021) and GCA (Zhu et al. 2021), which are the\ncurrent state-of-the-art self-supervised methods for learning\nrepresentations of nodes in a graph. For all baselines but\nBGRL, we use the official code published by the authors. As\nthe official code for BGRL is not available, we implement\nit by ourselves, and try our best to reflect the details provided in the original paper (Thakoor et al. 2021). We also\nreport previously published results of other representative\nmethods, such as DeepWalk (Perozzi, Al-Rfou, and Skiena\n2014), DGI (Veliˇckovi´c et al. 2018), GMI (Peng et al. 2020),\nand MVGRL (Hassani and Khasahmadi 2020), as done in\n(Thakoor et al. 2021; Zhu et al. 2021).\nEvaluation protocol.\nWe evaluate AFGRL on three nodelevel tasks, i.e., node classification, node clustering and node\nGRACE\nGCA\nBGRL\nAFGRL\nWikiCS\nNMI\n0.4282\n0.3373\n0.3969\n0.4132\nHom.\n0.4423\n0.3525\n0.4156\n0.4307\nComputers\nNMI\n0.4793\n0.5278\n0.5364\n0.5520\nHom.\n0.5222\n0.5816\n0.5869\n0.6040\nPhoto\nNMI\n0.6513\n0.6443\n0.6841\n0.6563\nHom.\n0.6657\n0.6575\n0.7004\n0.6743\nCo.CS\nNMI\n0.7562\n0.7620\n0.7732\n0.7859\nHom.\n0.7909\n0.7965\n0.8041\n0.8161\nCo.Physics\nNMI\nOOM\nOOM\n0.5568\n0.7289\nHom.\nOOM\nOOM\n0.6018\n0.7354\nTable 3: Performance on node clustering in terms of NMI\nand homogeneity.\nsimilarity search. We first train all models in an unsupervised\nmanner. For node classification, we use the learned embeddings to train and test a simple logistic regression classifier (Veliˇckovi´c et al. 2018). We report the test performance\nwhen the performance on validation data gives the best result. For node clustering and similarity search, we perform\nevaluations on the learned embeddings at every epoch and\nreport the best performance.\nImplementation details.\nWe use a GCN (Kipf and\nWelling 2016) model as the encoders, i.e., fθ(·) and fξ(·).\nMore formally, the encoder architecture is defined as:\nH(l) = GCN(l)(X, A) = σ( ˆD−1/2 ˆA ˆD−1/2XW(l)),\n(4)\nwhere H(l) is the node embedding matrix of the l-th layer for\nl ∈ [1, ..., L], ˆA = A + I is the adjacency matrix with selfloops, ˆD = P\ni ˆAi is the degree matrix, σ(·) is a nonlinear\nactivation function such as ReLU, and W(l) is the trainable\nweight matrix for the l-th layer. We perform grid-search on\nseveral hyperparameters, such as learning rate η, decay rate\nτ, node embedding dimension size D, number of layers of\nGCN encoder L, for fair comparisons.\nGRACE\nGCA\nBGRL\nAFGRL\nWikiCS\nSim@5\n0.7754\n0.7786\n0.7739\n0.7811\nSim@10\n0.7645\n0.7673\n0.7617\n0.7660\nComputers\nSim@5\n0.8738\n0.8826\n0.8947\n0.8966\nSim@10\n0.8643\n0.8742\n0.8855\n0.8890\nPhoto\nSim@5\n0.9155\n0.9112\n0.9245\n0.9236\nSim@10\n0.9106\n0.9052\n0.9195\n0.9173\nCo.CS\nSim@5\n0.9104\n0.9126\n0.9112\n0.9180\nSim@10\n0.9059\n0.9100\n0.9086\n0.9142\nCo.Physics Sim@5\nOOM\nOOM\n0.9504\n0.9525\nSim@10\nOOM\nOOM\n0.9464\n0.9486\nTable 4: Performance on similarity search. (Sim@n: Average ratio among n nearest neighbors sharing the same label\nas the query node.)\nPerformance Analysis\nOverall evaluation.\nTable 2 shows the node classification\nperformance of various methods. We have the following\nobservations: 1) Our augmentation-free AFGRL generally\nperforms well on all datasets compared with augmentationbased methods, i.e., GRACE, GCA and BGRL, whose reported results are obtained by carefully tuning the augmentation hyperparameters. Recall that in Table 1 we demonstrated that their performance is highly sensitive to the\nchoice of augmentation hyperparameters. This verifies the\nbenefit of our augmentation-free approach. 2) We also evaluate AFGRL on node clustering (Table 3) and similarity search (Table 4). Note that the best hyperparameters\nfor node classification task were adopted. Table 3 shows\nthat AFGRL generally outperforms other methods in node\nclustering task. We argue that this is mainly because AFGRL also considers global semantic information unlike the\ncompared methods. 3) It is worth noting that methods built\nupon instance discrimination principle (Wu et al. 2018), i.e.,\nGRACE and GCA, are not only memory consuming (OOM\non large datasets), but also generally perform worse than\ntheir counterparts on various tasks (especially on clustering). This indicates that instance discrimination, which treats\nall other nodes except itself as negatives without considering the graph structural information, is not appropriate for\ngraph-structured data, especially for clustering in which the\nglobal structural information is crucial. 4) AFGRL generally performs well on node similarity search (Table 4). This\nis expected because AFGRL aims to make nearest neighbors of each node share the same label with the query node\nby discovering the local and the global positives.\nAblation Studies.\nTo verify the benefit of each component of AFGRL, we conduct ablation studies on two datasets\nthat exhibit distinct characteristics, i.e., Amazon Computers\n(E-commerce network) and WikiCS (Reference network).\nIn Figure 5, we observe that considering both local structural and global semantic information shows the best performance. Moreover, we observe that the global semantic information is more beneficial than the local structural information. This can be explained by the performance of “k-NN\nFigure 5: Ablation study on AFGRL.\nFigure 6: Sensitivity analysis.\nonly” variant, which performs on par with “k-NN + Adj”\nvariant. That is, we conjecture that performing k-NN on\nthe node representations learned by our framework can capture sufficient local structural information contained in the\nadjacency matrix. Based on the ablation studies, we argue\nthat AFGRL still gives competitive performance even when\nthe adjacency matrix is sparse, which shows the practicality\nof our proposed framework. Finally, the low performance of\n“Clus-only” variant implies the importance of considering\nthe local structural information in graph-structured data.\nHyperparameter Analysis.\nFigure 6 shows the sensitivity\nanalysis on the hyperparameters k and M of AFGRL. We\nobserve that k = 4 and M > 1 generally give the best performance, while the performance is rather stable over various Ms. This verifies that our augmentation-free approach\ncan be easily trained compared with other augmentationFigure 7: Effect of embedding dimension size (D).\nSampled\nSampled\n(a) GCA\n(b) AFGRL\nFigure 8: t-SNE embeddings of nodes in Photo dataset.\nbased methods, i.e., stable over hyperparameters, while outperforming them in most cases. Moreover, in Figure 7, we\nconduct experiments across various sizes of node embedding dimension D. We observe that AFGRL benefits from\nhigh-dimensional embeddings, while other methods rapidly\nsaturate when the dimension of embeddings increase. Note\nthat Zbontar et al. (2021) recently showed similar results indicating that methods based on instance discrimination (Wu\net al. 2018) is prone to the curse of dimensionality.\nVisualization of embeddings.\nTo provide a more intuitive\nunderstanding of the learned node embeddings, we visualize\nnode embeddings of GCA (Figure 8(a)) and AFGRL (Figure 8(b)) by using t-SNE (Van der Maaten and Hinton 2008).\nEach point represents a node, and the color represents the\nnode label. We observe that node embeddings generated by\nboth methods are grouped together according to their corresponding node labels. However, the major difference is\nthat AFGRL captures more fine-grained class information\ncompared with GCA. That is, for AFGRL, there tend to be\nsmall clusters within each label group. To emphasize this,\nwe sample the same set of nodes from each label, and compare their embeddings (Figure 8 bottom). We clearly observe\nthat nodes are more tightly grouped in AFGRL compared\nwith GCA, which implies that AFGRL captures more finegrained class information.",
        "conclusion": "In this paper, we propose a self-supervised learning framework for graphs, which requires neither augmentation techniques nor negative samples for learning representations of\ngraphs. Instead of creating two arbitrarily augmented views\nof a graph and expecting them to preserve the semantics of\nthe original graph, AFGRL discovers nodes that can serve\nas positive samples by considering the local structural information and the global semantics of graphs. The major\nbenefit of AFGRL over other self-supervised methods on\ngraphs is its stability over hyperparameters while maintaining competitive performance even without using negative\nsamples for the model training, which makes AFGRL practical. Through experiments on multiple graphs on various\ndownstream tasks, we empirically show that AFGRL is superior to the state-of-the-art methods that are sensitive to\naugmentation hyperparameters.",
        "summary_en": "Inspired by the recent success of self-supervised methods applied on images, self-supervised learning on graph structured data has seen rapid growth especially centered on augmentation-based contrastive methods. However, this paper argues that without carefully designed augmentation techniques, augmentations on graphs may behave arbitrarily in that the underlying semantics of graphs can drastically change. As a consequence, the performance of existing augmentation-based methods is highly dependent on the choice of augmentation scheme, i.e., augmentation hyperparameters and combinations of augmentation. The paper proposes a novel augmentation-free self-supervised learning framework for graphs, named AFGRL. Specifically, the paper generate an alternative view of a graph by discovering nodes that share the local structural information and the global semantics with the graph. Extensive experiments towards various node-level tasks, i.e., node classification, clustering, and similarity search on various real-world datasets demonstrate the superiority of AFGRL.",
        "summary_zh": "这篇论文介绍了一种新颖的无增强自监督学习框架（AFGRL），用于在图上进行自监督学习，旨在解决基于增强的对比方法需要精心设计增强技术的问题。该方法通过发现与图共享局部结构信息和全局语义的节点，生成图的另一种视图。在各种真实数据集上进行了广泛实验，包括节点分类、聚类和相似性搜索等任务，结果显示了AFGRL的优越性能。"
    },
    {
        "title": "Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labeled Nodes",
        "abstract": "Graph Convolutional Networks (GCNs) play a crucial role in graph learning tasks, however, learning graph embedding with few supervised signals is still a difﬁcult problem. In this paper, we propose a novel training algorithm for Graph Convolutional Network, called Multi-Stage SelfSupervised (M3S) Training Algorithm, combined with selfsupervised learning approach, focusing on improving the generalization performance of GCNs on graphs with few labeled nodes. Firstly, a Multi-Stage Training Framework is provided as the basis of M3S training method. Then we leverage DeepCluster technique, a popular form of self-supervised learning, and design corresponding aligning mechanism on the embedding space to reﬁne the Multi-Stage Training Framework, resulting in M3S Training Algorithm. Finally, extensive experimental results verify the superior performance of our algorithm on graphs with few labeled nodes under different label rates compared with other state-of-theart approaches.",
        "introduction": "With great expressive power, graphs have been employed\nas the representation of a wide range of systems across\nvarious areas, including social network (Kipf and Welling\n2016; Hamilton, Ying, and Leskovec 2017), physical systems (Battaglia et al. 2016; Sanchez-Gonzalez et al. 2018),\nprotein-protein interaction networks (Hamaguchi et al.\n2017) and knowledge graph (Fout et al. 2017). Recently, research of analyzing graphs with machine learning has been\nreceived more and more attention, mainly focusing on node\nclassiﬁcation (Kipf and Welling 2016), link prediction (Zhu,\nSong, and Chen 2016) and clustering tasks (Fortunato 2010).\nGraph convolution can be regarded as the extension\nof standard convolution from Euclidean to non-Euclidean\ndomain. Graph Convolutional Networks (GCNs) (Kipf\nand Welling 2016) generalize convolutional neural networks (CNNs) to graph-structured data from the perspective\nof spectral theory based on prior works (Bruna et al. 2013;\nDefferrard, Bresson, and Vandergheynst 2016). GCNs naturally integrate the connectivity patterns and feature attributes of graph-structured data and it has been demonstrated that GCNs and their variants (Hamilton, Ying, and\nLeskovec 2017; Velickovic et al. 2017; Dai et al. 2018;\nChen and Zhu 2017) signiﬁcantly outperform traditional\nmulti-layer perceptron (MLP) models and traditional graph\nembedding approaches (Tang et al. 2015; Perozzi, Al-Rfou,\nand Skiena 2014; Grover and Leskovec 2016).\nNevertheless, it is well known that deep neural networks\nheavily depend on a large amount of labeled data. The requirement of large-scale data might not be met in many real\nscenarios for graphs with sparse labeled nodes. GCNs and\ntheir variants are mainly established on semi-supervised setting where the graph usually has relative plenty of labeled\ndata. However, to the best of our knowledge, there is hardly\nany work about graphs focusing on weakly supervised setting (Zhou 2017), especially learning a classiﬁcation model\nwith few examples from each class. In addition, the GCNs\nare usually with shallow architectures due to its intrinsic limitation (Li, Han, and Wu 2018), thereby restricting the efﬁcient propagation of label signals. To address this issue,\n(Li, Han, and Wu 2018) proposed Co-Training and SelfTraining to enlarge training dataset in a boosting-like way.\nAlthough these methods can partially improve the performance of GCNs with few labeled data, it is difﬁcult to pick\nsingle one consistently efﬁcient algorithm in real applications since these proposed methods (Li, Han, and Wu 2018)\nperform inconsistently across distinct training sizes.\nOn the other hand, a recent surge of interest has focused\non the self-supervised learning, a popular form of unsupervised learning, which uses pretext tasks to replace the labels annotated by humans by “pseudo-label” directly computed from the raw input data. On the basis of the analysis\nabove, there are mainly two issues worthy to explore further.\nFirstly, since it is hard to change the innate shallow architectures of GCNs, how to design a consistently efﬁcient training algorithm based on GCNs to improve its generalization\nperformance on graphs with few labeled nodes? Secondly,\nhow to leverage the advantage of self-supervised learning\napproaches based on a large amount of unlabeled data, to\nreﬁne the performance of proposed training algorithm?\nIn this paper, we ﬁrstly analyze the Symmetric Laplacian\nSmoothing (Li, Han, and Wu 2018) of GCNs and show that\nthis intrinsic property determines the shallow architectures\nof GCNs, thus restricting its generalization performance on\nonly few labeled data due to the inefﬁcient propagation of\nlabel information. Then we show the layer effect of GCNs\non graph with few labeled nodes: to maintain the best generalization, it requires more layers for GCNs with fewer\nlabeled data in order to propagate the weak label signals\nmore broadly. Further, to overcome the inefﬁcient propagation of label information with few labels for shallow architectures of GCNs, we ﬁrstly propose a more general training algorithm of GCNs based on Self-Training (Li, Han, and\nWu 2018), called Multi-Stage Training Framework. Furthermore, we apply DeepCluster (Caron et al. 2018), a popular\nmethod of self-supervised learning, on the graph embedding\nprocess of GCNs and design a novel aligning mechanism on\nclusters to construct pseudo-labels in classiﬁcation for each\nunlabeled data in the embedding space. Next we incorporate\nDeepCluster approach and the aligning mechanism into the\nMulti-Stage Training Framework in an elegant way and formally propose Multi-Stage Self-Supervised (M3S) Training\nAlgorithm. Extensive experiments demonstrate that our M3S\napproach are superior to other state-of-the-art approaches\nacross all the considered graph learning tasks with limited\nnumber of labeled nodes. In summary, the contributions of\nthe paper are listed below:\n• We ﬁrst probe the existence of Layer Effect of GCNs\non graphs with few labeled nodes, revealing that GCNs\nrequires more layers to maintain the performance with\nlower label rate.\n• We propose an efﬁcient training algorithm, called M3S,\ncombining the Multi-Stage Training Framework and\nDeepCluster approach. It exhibits state-of-the-art performance on graphs with low label rates.\n• Our M3S Training Algorithm in fact can provide a more\ngeneral framework that leverages self-supervised learning\napproaches to improve multi-stage training framework to\ndesign efﬁcient algorithms on learning tasks with only\nfew labeled data.",
        "our approach": "Before introducing our M3S training algorithm, we will\nﬁrstly elaborate the issue of inefﬁcient propagation of information from limited labeled data due to the essence of symmetric laplacian smoothing of GCNs, which forms the motivation of our work. Then a multi-stage training framework\nand DeepCluster approach are proposed, respectively, composing the basic components of our M3S algorithm. Finally,\nwe will formally provide multi-stage self-supervised (M3S)\ntraining algorithm in detail, a novel and efﬁcient training\nmethod of GCNs focusing on graphs with few labeled nodes.\nSymmetric Laplacian Smoothing of Graph\nConvolutional Networks\nIn the GCNs model (Kipf and Welling 2016) of semisupervised classiﬁcation, the graph embedding Z of nodes\nwith two convolutional layers is formulated as:\nZ = softmax( ˆA ReLU( ˆAXW (0))W (1)),\n(1)\nwhere ˆA = ˜D− 1\n2 ˜A ˜D− 1\n2 , ˜A = A + I and ˜D is the degree\nmatrix of ˜A. X and A denote the feature and the adjacent\nmatrix, respectively. W (0) is the input-to-hidden weight matrix and W (1) is the hidden-to-output weight matrix.\nRelated work (Li, Han, and Wu 2018) pointed out the reason why the GCNs work lies in the Symmetric Laplacian\nSmoothing of this spectral convolutional type, which is the\nkey for the huge performance gain. We simplify it as follows:\nzi =\n\u0002\nj\n˜aij\n\u0003 ˜di\n\u0004\n˜dj\nxj\n(for 1 ≤ i ≤ n),\n(2)\nwhere n is the size of nodes and zi is the ﬁrst-layer embedding of node i from input features x. Its corresponding\nmatrix formulation is as follows:\nZ = ˜D− 1\n2 ˜A ˜D− 1\n2 X,\n(3)\nwhere Z is the one-layer embedding matrix of feature matrix X. In addition, (Li, Han, and Wu 2018) showed that by\nrepeatedly applying Laplacian smoothing many times, the\nembedding of vertices will ﬁnally converge to the proportional to the square root of the vertex degree, thus restricting\nthe enlargement of convolutional layers.\nIn this case, a shallow GCN cannot sufﬁciently propagate the label information to the entire graph with only a\nfew labels, yielding the unsatisfying performance of GCNs\non graphs with few labeled nodes. To tackle this deﬁcit of\nGCNs, we propose an effective training algorithm based on\nGCNs especially focusing on graphs with only a small number of labels, dispensing with the inconsistent performance\nof four algorithms proposed in (Li, Han, and Wu 2018).\nOn the other hand, as shown in Figure 2, the requirement\nof number of graph convolutional layers for the best performance differs for the different label rates. Concretely speaking, the lower label rate of a graph has, the more graph convolutional layers are required for the purpose of more efﬁcient propagation of label information.\nMulti-Stage Training Framework\nInspired by the Self-Training algorithm proposed by (Li,\nHan, and Wu 2018), working by adding the most conﬁdent\npredictions of each class to the label set, we propose a more\ngeneral Multi-Stage Training Framework described in Algorithm 1.\nIn contrast with original Self-Training that explores the\nmost conﬁdent nodes and adds them with predicted virtual\nlabels only once, Multi-Stage Training Algorithm executes\nthis process K times. On graphs with limited labels, this algorithm framework repeatedly adds more conﬁdent labeled\ndata and facilitates the propagation of label information, resulting in the better performance compared with original approaches.\nfind top t confident nodes\nAligning Mechanism\nEnlarge Labeled \nData with\nVirtual Labels\nDeepCluster\nPseudo Labels\nGCNs\nMultiStage Self-Training Framework\nSelf-Checking Mechanism\nFigure 1: Flow chart of Multi-Stage Self-Supervised (M3S) Training Algorithm.\nNevertheless, the core of Multi-Stage Training Framework lies in the accuracy of selected nodes with virtual labels based on the conﬁdence and thus it is natural to incorporate self-checking mechanism that can guarantee the precision of chosen labeled data.\nAlgorithm 1 Multi-Stage Training Algorithm\nInput: Features matrix X, adjacent matrix A, labeled and\nunlabeled set L0, U0, graph convolution network fθ\nOutput: Graph Embedding Z = fθ(X, A)\n1: Train a ﬁxed number of epoches on the initial labeled\nand unlabeled set L0, U0\n2: for each stage k do\n3:\nSort vertices on conﬁdence in unlabeled set Uk−1.\n4:\nfor each class j do\n5:\nFind the top t vertices in Zi,j.\n6:\nAdd them to labeled set Lk−1 with virtual labels j.\n7:\nDelete them from unlabeled set Uk−1.\n8:\nend for\n9:\nTrain a ﬁxed number of epoches on the new labeled\nand unlabeled set Lk, Uk\n10: end for\n11: return Accuracy based on the ﬁnal Z.\nDeepCluster\nRecently, self-supervised learning (Doersch, Gupta, and\nEfros 2015), a popular form of unsupervised learning, shows\nits power in the ﬁeld of computer vision, which utilizes\npretext tasks to replace the labels annotated by human by\n“pseudo-labels”. A neat and effective approach of selfsupervised learning is DeepCluster (Caron et al. 2018) that\ntakes a set of embedding vectors produced by ConvNet F\nas input and groups them into k distinct clusters based on a\ngeometric criterion.\nMore concretely, DeepCluster jointly learns a d × k centroid matrix C and the cluster assignment yn of each data\npoint n such as image, by solving the following problem:\nmin\nC∈Rd×k\n1\nN\nN\n\u0002\nn=1\nmin\nyn∈{0,1}k ∥F(xn) − Cyn∥2\n2\ns.t.\nyT\nn 1k = 1\n(4)\nSolving this problem provides a set of optimal assignments (y∗\nn)n≤N and a centroid matrix C∗. These assignments are then used as pseudo-labels. In particular, DeepCluster alternates between clustering the embedding vectors\nproduced from ConvNet into pseudo-labels and updating parameters of the ConvNet by predicting these pseudo-labels.\nFor the node classiﬁcation task in a graph, the representation process can also be viewed as graph embedding (Zhou\net al. 2018), allowing the DeepCluster as well. Thus, we\nharness the innate property of graph embedding in GCNs\nand execute k-means on the embedding vectors to cluster\nall nodes into distinct categories based on embedding distance. Next, an aligning mechanism is introduced to classify\nthe nodes in each cluster to the nearest class in classiﬁcation on the embedding space. Finally, the obtained pseudolabels are leveraged to construct the self-checking mechanism of Multi-Stage Self-Supervised Algorithm as shown in\nFigure 1.\nAligning Mechanism\nThe target of aligning mechanism\nis to transform the categories in clustering to the classes\nin classiﬁcation based on the embedding distance. For each\ncluster l in unlabeled data after k-means, the computation of\naligning mechanism is:\nc(l) = arg min\nm\n∥vl − μm∥2,\n(5)\nwhere μm denotes centroids of class m in labeled data, vl\ndenotes the centroid of cluster l in unlabeled data and c(l)\nrepresents the aligned class that has the closest distance from\nvl among all centroids of class in the original labeled data.\nThrough the aligning mechanism, we are capable of classifying nodes of each cluster to a speciﬁc class in classiﬁcation and then construct pseudo-labels for all unlabeled nodes\naccording to their embedding distance.\nExtension\nIn fact, DeepCluster is a more general and economical form of constructing self-checking mechanism via\nembedding distance. The naive self-checking way is to compare the distance of each unlabeled node to centroids of\nclasses in labeled data since distance between each unlabeled data and training centroids is a more precise measure\nthan the class centriods of unlabeled data. However, when\nthe number of clusters is equivalent to the amount of all\nunlabeled nodes, our self-checking mechanism via DeepCluster is the same as the naive way. Considering the expensive computation of the naive self-checking, DeepCluster performs more efﬁciently and ﬂexibly in the selection of\nnumber of clusters.\nM3S Training Algorithm\nIn this section, we will formally present our Multi-Stage\nSelf-Supservised (M3S) Training Algorithm, a novel training method on GCN aiming at addressing the inefﬁcient\npropagation of label information on graphs with few labeled\nnodes. The ﬂow chart of our approach is illustrated in Figure 1.\nThe crucial part of M3S Training Algorithm compared\nwith Multi-Stage Training is additionally utilizing the information of embedding distance to check the accuracy of selected nodes with virtual labels from Self-Training based on\nthe conﬁdence. Speciﬁcally speaking, M3S Training Algorithm elegantly combines DeepCluster self-checking mechanism with Multi-Stage Training Framework to choose nodes\nwith more precise virtual labels in an efﬁcient way. We provide a detailed description of M3S approach in Algorithm 2.\nFor M3S Training Algorithm, ﬁrstly we train a GCN\nmodel on an initial dataset to obtain meaningful embedding\nvectors. Then we perform DeepCluster on the embedding\nvectors of all nodes to acquire their clustering labels. Furthermore, we align their labels of each cluster based on the\nembedding distance to attain the pseudo-label of each unlabeled node. In the following Self-Training process, for the\nselected top conﬁdent nodes of each class, we perform selfchecking based on pseudo-labels to guarantee they belong\nto the same class in the embedding space, then add the ﬁltered nodes to the labeled set and execute a new stage SelfTraining.\nAvoiding Trivial Solutions\nIt should be noted that the categorically balanced labeled set plays an important role on\ngraphs with low label rate. In addition, DeepCluster tends\nto be caught in trivial solutions that actually exist in various methods that jointly learns a discriminative classiﬁer\nand the labels (Caron et al. 2018). Highly unbalanced data\nof per class is a typical trivial solution of DeepCluster, which\nhinders the generalization performance with few supervised\nAlgorithm 2 M3S Training Algorithm\nInput: Features Matrix X, adjacent matrix A, labeled and\nunlabeled set L0, U0, graph convolution network fθ.\nOutput: Graph Embedding Z = fθ(X, A)\n1: Train a ﬁxed number of epoches on the initial labeled\nand unlabeled sets L0, U0.\n2: for each stage k do\n3:\n% Step 1: Deep Clustering\n4:\nExecute K-means based on embedding Z of all data\nand obtain pseudo labels of each data point for clustering.\n5:\n% Step 2: Aligning Mechanism\n6:\nCompute centroids μm of each class m in labeled\ndata.\n7:\nCompute centroids vl of each cluster l in unlabeled\ndata.\n8:\nfor each cluster l of unlabeled set do\n9:\nAlign label of lth cluster on the embedding space.\nc(l) = arg min\nm\n∥vl − μm∥2\n10:\nSet unlabeled data in lth cluster with pseudo label\nc(l).\n11:\nend for\n12:\n% Step 3: Self-Training\n13:\nSort vertices according to the conﬁdence in unlabeled\nset Uk−1.\n14:\nfor each class j do\n15:\nFind the top t vertices in Zi,j.\n16:\nfor each vertice of selected t vertices do\n17:\nif pseudo label c of the vertice equals j then\n18:\nAdd it to labeled set Lk−1 with virtual label\nj.\n19:\nDelete it from unlabeled set Uk−1.\n20:\nend if\n21:\nend for\n22:\nend for\n23:\nTrain a ﬁxed number of epoches on the new labeled\nand unlabeled set Lk, Uk\n24: end for\n25: return Accuracy based on the ﬁnal Z.\nsignals. In this paper we provide a simple and elegant solution by enlarging the number of clusters in K-means. For the\none hand, setting more clusters allows higher probability of\nbeing evenly classiﬁed to all categories. For the other hand,\nit contributes to more precise computation in embedding distance from the perspective of extension of DeepCluster selfchecking mechanism. These are dicussed in the experimental part.",
        "experiments": "In this section we conduct extensive experiments to demonstrate the effectiveness of our proposed M3S Algorithm on\ngraphs with few labeled nodes. For the graph dataset, we select the three commonly used citation networks: CiteSeer,\nCora and PubMed (Sen et al. 2008). Dateset statistics are\nFigure 2: The change of accuracy for GCNs model with different layers under different label rates.\nsummarized in Table 1.\nAs for the baselines, we opt the Label Propagation (LP)\n(Wu et al. 2012) using ParWalks; Graph Convolutional Networks (GCNs) (Kipf and Welling 2016); Self-Training, CoTraining, Union and Intersection (Li, Han, and Wu 2018)\nall based on the conﬁdence of prediction. On graphs with\nlow label rates, we compare both our Multi-Stage Training\nFramework and M3S Algorithm with other state-of-the-art\napproaches by changing the label rate for each dataset. We\nreport the mean accuracy of 10 runs in all result tables to\nmake fair comparison. Our implementationm, including the\nsplitting of train and test datasets, adapts from original version in (Li, Han, and Wu 2018).\nDateset\nNodes\nEdges\nClasses\nFeatures\nLabel\nRate\nCiteSeer\n3327\n4732\n6\n3703\n3.6%\nCora\n2708\n5429\n7\n1433\n5.2%\nPubMed\n19717\n44338\n3\n500\n0.3%\nTable 1: Dateset statistics\nLayer Effect on Graphs with Few Labeled Nodes\nBefore comparing our algorithms with other methods, we\npoint out the layer effect of GCNs for different label rates:\nto maintain the best performance, a GCN model in semisupervised task with a lower label rate requires more graph\nconvolutional layers.\nFigure 2 presents some empirical evidence to demonstrate\nthe layer effect on graphs with few labels. We test the performance of GCNs with different layers in distinct label rates in\nFigure 2 and it is apparent to note that the number of layer\nunder the best performance exhibits a descending trend as\nthe label rate increases.\nThe existence of layer effect demonstrates the urge of\npropagation of label information by stacking more convolutional layers. In the original GCNs (Kipf and Welling 2016),\nthe authors argued to apply two graph convolutional layers\nfor standard node classiﬁcation tasks. However, due to the\nexistence of Layer Effect, we are expected to choose proper\nnumber of layers especially on graphs with low label rates.\nIn the following experiments, we all choose the best number\nof layer to compare the best performance for each method.\nPerformance of Multi-Stage Training Framework\nTo gain a better understanding of the advantage of MultiStage Training Framework, we make an extensive comparison between Multi-Stage Framework of different stages with\nthe Self-Training approach under different label rates.\nFrom Figure 3, it is easy to observe that all self-training\nmethods outperform the original GCNs with a large margin,\nespecially when the graph has low label rate, which usually\nhappens in real applications. In addition, Multi-Stage Training is superior to traditional Self-Training especially when\nthere are fewer labeled nodes and more stages are inclined to\nbring more improvement. Nevertheless, the discrepancy between the Multi-Stage Training algorithm and Self-Training\nalgorithm narrows down as the label rate increases. Moreover, the improvement of all self-training methods over\nGCNs diminishes as well with the increasing of label rate.\nAs for the reason, we argue that with the enlargement of labeled nodes, the accuracy of the learned GCN model also\nincreases, while the accuracy of explored nodes via selftraining tends to approach the accuracy of current GCN, resulting in the diminishment of improvement. However, the\nlimited precision of selected nodes only based on the conﬁdence of prediction is just what M3S Training Algorithm is\ndevoted to improve.\nFigure 3: Multi-Stage Training vs Self-training.\nPerformance of M3S Training Algorithm\nIn this section, we conduct experiments by comparing MultiStage Self-Training Algorithm and M3S Training Algorithm\nwith other state-of-the-art approaches under different label\nrates across the three datasets.\nExperimental Setup\nAll the results are the mean accuracy\nof 10 runs and the number of clusters in DeepCluster is ﬁxed\n200 for all datasets to avoid trivial solutions. We select the\nbest number of layers for different label rates. In particular,\nthe best layer in Cora and CiteSeer is 4,3,3,2,2 and 3,3,3,2,2\nrespectively for 0.5%,1%,2%,3%,4% label rates and ﬁxed\n4 for 0.03%,0.05%,0.1% label rates on PubMed. The number of epochs of each stage in Multi-Stage Training Framework, M3S and other approaches is set as 200. For all methods involved in GCNs, we use the same hyper-parameters\nas in (Kipf and Welling 2016): learning rate of 0.01, 0.5\ndropout rate, 5×10−4 L2 regularization weight, and 16 hidden units without validation set for fair comparison (Li, Han,\nand Wu 2018). For the option of K stages, we view it as a\nhyper-parameter. For CiteSeer dataset we ﬁx K = 3 and for\nPubMed dataset we ﬁx K = 4, in which the result of our\nproposed algorithms have already outperformed other apCora Dataset\nLabel Rate\n0.5%\n1%\n2%\n3%\n4%\nLP\n57.6\n61.0\n63.5\n64.3\n65.7\nGCN\n50.6\n58.4\n70.0\n75.7\n76.5\nCo-training\n53.9\n57.0\n69.7\n74.8\n75.6\nSelf-training\n56.8\n60.4\n71.7\n76.8\n77.7\nUnion\n55.3\n60.0\n71.7\n77.0\n77.5\nIntersection\n50.6\n60.4\n70.0\n74.6\n76.0\nMultiStage\n61.1\n63.7\n74.4\n76.1\n77.2\nM3S\n61.5\n67.2\n75.6\n77.8\n78.0\nTable 2: Classiﬁcation Accuracy on Cora.\nproaches easily. For Cora dataset we choose K as 5,4,4,2,2\nas the training size increases, since higher label rate usually\nmatches with a smaller K.\nResults shown in Tables 2, 3 and 4 verify the effectiveness\nof our M3S Training Algorithm, consistently outperforming\nother state-of-the-art approaches to a large margin on a wide\nrange of label rates across the three datasets. More speciﬁcally, we make four observations from the results:\n• It is apparent to note that the performance of GCN significantly declines when the labeled data is scarce due to the\ninefﬁcient propagation of label information. For instance,\nCiteSeer Dataset\nLabel Rate\n0.5%\n1%\n2%\n3%\n4%\nLP\n37.7\n41.6\n41.9\n44.4\n44.8\nGCN\n44.8\n54.7\n61.2\n67.0\n69.0\nCo-training\n42.0\n50.0\n58.3\n64.7\n65.3\nSelf-training\n51.4\n57.1\n64.1\n67.8\n68.8\nUnion\n48.5\n52.6\n61.8\n66.4\n66.7\nIntersection\n51.3\n61.1\n63.0\n69.5\n70.0\nMultiStage\n53.0\n57.8\n63.8\n68.0\n69.0\nM3S\n56.1\n62.1\n66.4\n70.3\n70.5\nTable 3: Classiﬁcation Accuracy on CiteSeer.\nPubMed Dataset\nLabel Rate\n0.03%\n0.05%\n0.1%\nLP\n58.3\n61.3\n63.8\nGCN\n51.1\n58.0\n67.5\nCo-training\n55.5\n61.6\n67.8\nSelf-training\n56.3\n63.6\n70.0\nUnion\n57.2\n64.3\n70.0\nIntersection\n55.0\n58.2\n67.0\nMultiStage\n57.4\n64.3\n70.2\nM3S\n59.2\n64.4\n70.6\nTable 4: Classiﬁcation Accuracy on PubMed.\nFigure 4: Relation between Accuracy and Max-Min Ratio with the increasing of Clusters K. All values are the mean accuracy\nor max-min ratio of 10 runs.\non Cora and PubMed datasets, the performance of GCN is\neven inferior to Label Propagation (LP) when the training\nsize is relative small.\n• Previous state-of-the-art algorithms, namely Co-training,\nSelf-training, Union and Intersection exhibit inconsistent\nperformance compared with GCNs, thus it is hard to employ one single algorithm from them in real scenarios.\n• Multi-Stage Training Framework tends to be superior to\nSelf-Training especially on fewer labeled data, demonstrating the effectiveness of this framework on graphs\nwith few labeled nodes.\n• M3S Training Algorithm leverages both the advantage\nof Multi-Stage Training Framework and self-checking\nmechanism constructed by DeepCluster, consistently outperforming other state-of-the-art approaches on all label\nrates. Additionally, it turns out that the lower label rate the\ngraph has, the larger improvement of M3S Training Algorithm can produce, perfectly adapting on graphs with few\nlabeled nodes.\nSensitivity Analysis of Number of Clusters\nSensitivity\nanalysis of number of clusters is regarded as the extensive discussion of our M3S Training Algorithm, where we\npresent the inﬂuence of number of clusters in DeepCluster\non the balance of each class and the ﬁnal performance of\nGCN. We leverage “Max-Min Ratio” to measure the balance level of each class, which is computed by the subtraction between max ratio and min ratio of categories of\nunlabeled data after the aligning mechanism, and the lower\n“Max-Min Ratio” represents the higher balance level of categories. We choose two labeled nodes of each class across\nthree datasets. As shown in Figure 4 where each column\npresents the change of a speciﬁc dataset, with the increasing\nof number of clusters, categories tend to be more balanced\nuntil the number of clusters is large enough, facilitating the\nﬁnal performance of M3S Training Algorithm. These results\nempirically demonstrate that more clusters are beneﬁcial to\navoid trivial solutions in DeepCluster, thus enhancing the\nperformance of our method.",
        "discussions": "Although in this work we employ only one kind of selfsupervised approach on the graph learning task, the introduction of self-checking mechanism constructed by DeepCluster in fact provides a more general framework on\nweakly supervised signals for a wide range of data types.\nOn the one hand, it is worthy of exploring the avenue to\nutilize the pseudo-labels produced by self-supervised learning more efﬁciently on few supervised labels, for instance,\ndesigning new aligning mechanism or applying better selfsupervised learning approach. On the other hand, how to extend similar algorithm combined with self-supervised learning methods to other machine learning task such as image\nclassiﬁcation and sentence classiﬁcation, requires more endeavours in the future.",
        "conclusion": "In this paper, we ﬁrstly clarify the Layer Effect of GCNs\non graphs with few labeled nodes, demonstrating that it is\nexpected to stack more layers to facilitate the propagation\nof label information with lower label rate. Then we propose Multi-Stage Training Algorithm Framework on the basis of Self-Training, adding conﬁdent data with virtual labels to the labeled set to enlarge the training set. In addition,\nwe apply DeepCluster on the graph embedding process of\nGCNs and design a novel aligning mechanism to construct\nself-checking mechanism to improve MultiStage Training\nFramework. Our ﬁnal proposed approach, M3S Training\nAlgorithm, outperforms other state-of-the-art methods with\ndifferent label rates across all the considered graphs with few\nlabeled nodes. Overall, M3S Training Algorithm is a novel\nand efﬁcient algorithm focusing on graphs with few labeled\nnodes.",
        "summary_en": "Graph Convolutional Networks (GCNs) play a crucial role in graph learning tasks, however, learning graph embedding with few supervised signals is still a difficult problem. This paper proposes a novel training algorithm for Graph Convolutional Network, called Multi-Stage Self-Supervised (M3S) Training Algorithm, combined with self-supervised learning approach, focusing on improving the generalization performance of GCNs on graphs with few labeled nodes. Firstly, a Multi-Stage Training Framework is provided as the basis of M3S training method. Then the paper leverages DeepCluster technique, a popular form of self-supervised learning, and design corresponding aligning mechanism on the embedding space to refine the Multi-Stage Training Framework, resulting in M3S Training Algorithm. Finally, extensive experimental results verify the superior performance of the algorithm on graphs with few labeled nodes under different label rates compared with other state-of-the-art approaches.",
        "summary_zh": "这篇论文介绍了一种名为多阶段自监督学习的图卷积网络训练算法，为了解决目前在少量标记节点上学习图嵌入仍然困难的问题，旨在提高GCNs在少量标记节点图上的泛化性能。首先作者提供了一个多阶段训练框架作为M3S训练方法的基础。然后，利用DeepCluster技术，在嵌入空间上设计相应的对齐机制，对多阶段训练框架进行完善改进，形成M3S训练算法。最后，大量的实验结果验证了该算法与其他先进方法相比，在不同标签率下，在标注节点较少的图中表现出了卓越的性能。"
    },
    {
        "title": "Wiener Graph Deconvolutional Network Improves Graph Self-Supervised Learning",
        "abstract": "Graph self-supervised learning (SSL) has been vastly employed to learn representations from unlabeled graphs. Existing methods can be roughly divided into predictive learning and contrastive learning, where the latter one attracts more research attention with better empirical performance. We argue that, however, predictive models weaponed with powerful decoder could achieve comparable or even better representation power than contrastive models. In this work, we propose a Wiener Graph Deconvolutional Network (WGDN), an augmentation-adaptive decoder empowered by graph wiener filter to perform information reconstruction. Theoretical analysis proves the superior reconstruction ability of graph wiener filter. Extensive experimental results on various datasets demonstrate the effectiveness of our approach.",
        "introduction": "Self-Supervised Learning (SSL), which extracts informative\nknowledge through well-designed pretext tasks from unlabeled data, has been extended to graph data recently due to\nits great success in computer vision (CV) (He et al. 2020)\nand natural language processing (NLP) (Devlin et al. 2019).\nWith regard to the objectives of pretext tasks, graph SSL\ncan be divided into two major categories: predictive SSL\nand contrastive SSL (Liu et al. 2022). Predictive models\nlearn informative properties generated from graph freely via\nprediction tasks, while contrastive models are trained on\nthe mutual information between different views augmented\nfrom the original graph. As the dominant technique, contrastive SSL has achieved state-of-the-art performance empirically (Xu et al. 2021; Thakoor et al. 2022; Lee, Lee, and\nPark 2022) for graph representation learning. In contrast, the\ndevelopment of predictive SSL has lagged behind over the\npast few years.\nGraph reconstruction is a natural self-supervision, and\nthus most methods in predictive SSL employ graph autoencoder (GAE) as their backbones (Wang et al. 2017; Hu et al.\n2020b; Li et al. 2020b). The work of GraphMAE (Hou et al.\n2022) re-validates the potentials of reconstruction paradigm.\nDespite recent advancements, the importance of graph\ndecoder has been largely ignored. Most existing works\nFigure 1: Comparison of different variants of GALA against\nlatent Gaussian augmentation with magnitude β.\nleverage trivial decoders, such as multi-layer perceptron\n(MLP) (Kipf and Welling 2016; Pan et al. 2018; You et al.\n2020b), which under-exploit graph topology information,\nand thus may lead to the degradation in learning capability.\nVanilla graph neural networks (GNNs), such as GCN (Kipf\nand Welling 2017), are inappropriate for decoding due to\ntheir Laplacian-smooth essence. To overcome such inherent\nlimitation of GCN, GALA (Park et al. 2019) adopts spectral counterpart of GCN to facilitate the learning, but may\ntake the risk of unstable learning due to its poor resilience to\ndata augmentation (See Figure 1). GAT (Veliˇckovi´c et al.\n2018) is employed as decoder in recent works including\nGATE (Salehi and Davulcu 2020) and GraphMAE (Hou\net al. 2022). Although attention mechanism enhances model\nflexibility, recent work (Balcilar et al. 2021) shows GAT acts\nlike a low-pass filter and cannot well reconstruct the graph\nspectrum. As an inverse to GCN (Kipf and Welling 2017),\ngraph deconvolutional network (GDN) could be expected\nto further boost the performance of reconstruction (Li\net al. 2021), which may substantially benefit the context of\nrepresentation learning. We present a summary of different decoders of predictive graph SSL in Table 1. Given the\naforementioned observations, a natural question comes up,\nthat is, can we improve predictive SSL by a framework with\npowerful decoder?\nTypically, a powerful decoder should at least remain effective against augmentations. Motivated by recent advancement of wiener in deep image reconstruction (Dong, Roth,\nand Schiele 2020), we introduce the classical deconvolutional technique, wiener filter, into GDN, which is the theoModel\nDecoder\nFeature\nStructure\nDeconv.\nAugmentation\nSpectral\nSpace\nLoss\nLoss\nDecoder\nAdaption\nKernel\nVGAE (Kipf and Welling 2016)\nDP\nCE\n✗\n✗\n✗\nO(N 2)\nARVGA (Pan et al. 2018)\nDP\nCE\n✗\n✗\n✗\nO(N 2)\nMGAE (Wang et al. 2017)\nMLP\nMSE\n✗\n✗\n✗\nO(N)\nAttrMask (Hu et al. 2020b)\nMLP\nCE\n✗\n✗\n✗\nO(N)\nGALA (Park et al. 2019)\nGNN\nMSE\n✓\n✗\n✗\nO(N)\nGraphMAE (Hou et al. 2022)\nGNN\nSCE\n✗\n✓\n✗\nO(N)\nWGDN\nGNN\nMSE\n✓\n✓\n✓\nO(N)\nTable 1: Technical components comparison within predictive SSL approaches. DP: Non-parametric Dot Product. CE: CrossEntropy Error. MSE: Mean Square Error. SCE: Scaled-Cosine Error.\nretical optimum for restoring augmented signals with respect\nto mean square error (MSE). We propose a GAE framework (Li et al. 2020b), named Wiener Graph Deconvolutional Network (WGDN), which utilizes graph wiener filter\nto facilitate representation learning with graph spectral kernels. We first derive the graph wiener filter and prove its superiority in theory. We observe that, however, directly using\nthe explicit graph wiener filter induces low scalability due\nto indispensable eigen-decomposition and may not be applicable to large-scale datasets. Therefore, we adopt average\ngraph spectral energy and Remez polynomial (Pachon and\nTrefethen 2009) for fast approximation.\nWe evaluate the learned representation quality on two\ndownstream tasks: node classification and graph classification. Empirically, our proposed WGDN achieves better results over a wide range of state-of-the-art benchmarks of\ngraph SSL with efficient computational cost. Particularly,\nWGDN yields up to 1.4% higher accuracy than runnerup model, and requires around 30% less memory overhead\nagainst the most efficient contrastive counterpart.",
        "related work": "Graph self-supervised learning.\nAccording to recent surveys (Liu et al. 2022; Xie et al. 2022), works in graph SSL\ncan be classified into two categories: contrastive learning\nand predictive learning. Contrastive SSL attracts more attention currently due to the state-of-the-art performance on\nrepresentation learning. Early efforts focus on the design of\nnegative sampling and augmentation schemes, such as corruptions in DGI (Veliˇckovi´c et al. 2019), graph diffusion\nin MVGRL (Hassani and Khasahmadi 2020) and masking\nin GRACE (Zhu et al. 2020) and GCA (Zhu et al. 2021).\nRecent works have attempted for negative-sample-free contrastive SSL. For example, BGRL (Thakoor et al. 2022)\nadapts BYOL (Grill et al. 2020) for graph representation\nlearning, CCA-SSG (Zhang et al. 2021) conducts feature\ndecorrelation, and AFGRL (Lee, Lee, and Park 2022) obtains positive pairs via latent space clustering. Despite their\nadvancement, intricate architecture designs are required.\nAs for predictive learning, predicting node features and\nneighborhood context is a traditional pretext task with graph\nautoencoder (GAE). For instance, VGAE (Kipf and Welling\n2016) and ARVGA (Pan et al. 2018) learn missing edges\nprediction by structural reconstruction. Moreover, one representative manner (You et al. 2020b) follows the perturbthen-learn strategy to predict the corrupted information,\nsuch as attribute masking\n(Hu et al. 2020b) and feature\ncorruption (Wang et al. 2017). Recently, GraphMAE (Hou\net al. 2022) implements a masking strategy and scaled cosine\nerror for feature reconstruction and achieves great success\nto match state-of-the-art contrastive SSL approaches. However, it ignores the potential benefit leveraging graph spectral\ntheory. In this work, we propose an augmentation-adaptive\nGAE framework that unleashes the power of graph spectral\npropagation.\nGraph deconvolutional network.\nRegarding graph deconvolution, early research (Yang and Segarra 2018)\nformulates the deconvolution as a pre-processing step.\nGALA (Park et al. 2019) performs Laplacian sharpening to\nrecover information. Recent work (Zhang et al. 2020) employs GCN (Kipf and Welling 2017) to reconstruct node features from the latent representations. All these works, however, neglect the influence of augmentation. Another GDN\nframework (Li et al. 2021) is designed via a combination\nof inverse filters in spectral domain and denoising layers in\nwavelet domain, which is sub-optimal regarding signal reconstruction. Wiener filtering, as an alternative, executes an\noptimal trade-off between signal recovering and denoising.\nIt has been introduced to deconvolutional networks (Dong,\nRoth, and Schiele 2020; Son and Lee 2017) for image deblurring. However, its effectiveness on graph structure has\nnot been well investigated yet.",
        "preliminaries": "Under a generic self-supervised graph representation learning setup, we are given an attributed graph G = (V, A, X)\nconsisting of: (1) V = {v1, v2, ..., vN} is the set of nodes;\n(2) A ∈ RN×N is the adjacency matrix where Aij ∈ {0, 1}\nrepresents whether an undirected edge exists between vi\nand vj; and (3) X ∈ RN×D denotes the feature matrix.\nOur objective is to learn an autoencoder with encoder E :\n(RN×N, RN×D) 7→ RN×D′ and decoder D : RN×D′ 7→\nRN×D to produce node embedding, or graph embedding\nupon a pooling function. H = E(A, X) ∈ RN×D′ represents the learned embedding in low dimensional space,\nwhich can be used for various downstream tasks.\nFigure 2: The autoencoder framework of WGDN for graph SSL. Given the augmented latent representations, graph wiener filter\nis approximated via estimating spectral energy and augmentations adaptively. With such, WGDN permits the stable feature\nreconstruction from the augmented latent space for representation learning.\nGraph convolution.\nConvolutional operation in graph can\nbe interpreted as a special form of Laplacian smoothing on\nnodes. From the spectral perspective, graph convolution on\na signal x ∈ RN with a filter gc is defined as\nh = gc ∗ x = Udiag(gc(λ1), ..., gc(λN))UT x\n= Ugc(Λ)UT x = gc(L)x,\n(1)\nwhere {λi}N\ni=1 and U represent the eigenvalues and\neigenvectors of normalized Laplacian matrix L = I −\nD− 1\n2 AD− 1\n2 = UΛUT respectively. D denotes the Degree matrix. ∗ denotes convolutional operator. We consider\n(1) GCN (Kipf and Welling 2017), it is a low-pass filter in\nspectral domain with gc(λi) = 1 − λi shown by (Wu et al.\n2019); (2) GDC (Klicpera, Weißenberger, and G¨unnemann\n2019) and Heatts (Li et al. 2020a), both use heat kernel gc(λi) = e−tλi; (3) APPNP (Gasteiger, Bojchevski,\nand G¨unnemann 2019), it leverages personalized pagerank\n(PPR) kernel gc(λi) =\nα\n1−(1−α)(1−λi).\nGraph deconvolution.\nAs an inverse to convolution,\ngraph deconvolution aims to recover the input attributes\ngiven the smoothed node representation. From the spectral\nperspective, graph deconvolution on a smoothed representation h ∈ RN with filter gd is defined as\nˆx = gd ∗ h = Udiag(gd(λ1), ..., gd(λN))UT h\n= Ugd(Λ)UT h = gd(L)h.\n(2)\nA trivial selection of gd is the inverse function of gc, e.g.,\ngd(λi) =\n1\n1−λi for GCN (Li et al. 2021), gd(λi) = etλi for\nheat kernel, or gd(λi) = 1−(1−α)(1−λi)\nα\nfor PPR kernel.",
        "the proposed framework": "In this section, we first extend classical wiener filter to graph\ndomain and demonstrate its superiority in reconstructing\ngraph features. Then, we propose Wiener Graph Deconvolutional Network (WGDN), an efficient and augmentationadaptive framework empowered by graph wiener filter.\nWiener Filter on Graph\nIn this work, we follow the settings in previous papers (Jin\nand Zhang 2019; Cheung and Yeung 2021) and introduce\nadditive latent augmentations in model training due to its\nflexible statistical characteristics, such as unbiasedness and\ncovariance-preserving (Zhang et al. 2022). Combining with\nthe graph convolution in Eq. 1, augmented representation ˆh\nin graph is similarly defined as\nˆh = Ugc(Λ)UT x + ϵ,\n(3)\nwhere x ∈ RN denotes input features and ϵ ∈ RN is assumed to be any i.i.d. random augmentation with E[ϵi] = 0\nand VAR[ϵi] = σ2. In contrast to the isolated data augmentations in graph topology and features, ϵ indirectly represents joint augmentations to both (Jin and Zhang 2019).\nNaturally, feature recovered by graph deconvolution is formulated by\nˆx = Ugd(Λ)gc(Λ)UT x + Ugd(Λ)UT ϵ.\n(4)\nProposition 1. Let ˆxinv be recovered features by inverse\nfilter gd(λi) = g−1\nc (λi) = ginv(λi). For common lowpass filters satisfying gc : [0, 2] 7→ [−1, 1], such as GCN,\nHeat and PPR, the reconstruction MSE is dominated by\namplified augmentation MSE(ˆxinv) = E∥x − ˆxinv∥2\n2 =\nPN\ni=1\nσ2\ng2\nc(λi).\nThe proof is trivial and illustrated in Appendix A for\ndetails. Based on Proposition 1, feature reconstruction becomes unstable and even ineffective if augmentation exists.\nTo well utilize the power of augmentation, our goal is to\nstabilize the reconstruction paradigm, which resembles the\nclassical restoration problems. In signal deconvolution, classical wiener filter (Wiener 1964) is able to produce a statistically optimal estimation of the real signals from the augmented ones with respect to MSE. With this regard, we are\nencouraged to extend wiener filter to graph domain (Perraudin and Vandergheynst 2017). Assuming the augmentation to be independent from input features, graph wiener filter can be similarly defined by projecting MSE into graph\nspectral domain\nMSE(ˆx)\n= E∥ˆx − x∥2\n2 = E\n\r\rUT ˆx − UT x\n\r\r2\n2\n=\nN\nX\ni=1\n(gd(λi)gc(λi) − 1)2E[x∗2\ni ] + g2\nd(λi)E[ϵ∗2\ni ]\n=\nN\nX\ni=1\nS(λi, x∗\ni , σ, gc, gd),\n(5)\nwhere x∗ = UT x = {x∗\n1, x∗\n2, ..., x∗\nN} and ϵ∗ = UT ϵ =\n{ϵ∗\n1, ϵ∗\n2, ..., ϵ∗\nN} represent graph spectral projection of the input and augmentation respectively. We denote E[x∗2\ni ] and\nS(λi, x∗\ni , σ, gc, gd) as the spectral energy and spectral reconstruction error of spectrum λi. Considering the convexity of\nEq. 5, MSE is minimized by setting the derivative with respect to gd(λi) to zero and thus we obtain the graph wiener\nfilter gw(λi) as\ngw(λi) =\ngc(λi)\ng2c(λi) + σ2/E[x∗2\ni ],\n(6)\nwhere σ2 = VAR[ϵ∗\ni ] = E[ϵ∗2\ni ] and σ2/E[x∗2\ni ] is denoted\nas the Augmentation-to-Energy Ratio (AER) of particular\nspectrum λi, which represents the relative magnitude of augmentation.\nProposition 2. Let ˆxw be recovered features by gw(λi),\nwhere gw(λi) is a graph wiener filter, then the reconstruction MSE and variance of ˆxw are less than ˆxinv.\nPlease refer to Appendix B for details. Proposition 2\nshows graph wiener filter has better reconstruction property than inverse filter, which promotes the resilience to latent augmentations and permits stable model training. We\nobserve that, in Eq. 2 and 6, eigen-decomposition is indispensable in computations of spectral energy and deconvolutional filter. However, in terms of scalability, an important\nissue for large-scale graphs is to avoid eigen-decomposition.\nNote that PN\ni=1 E[x∗2\ni ] = PN\ni=1 E[x2\ni ] due to orthogonal\ntransformation, we propose the modified graph wiener filter ¯gw,γ with average spectral energy ¯x∗2\nγ\n= γ · ¯x∗2 =\nγ · 1\nN\nPN\ni=1 E[x∗2\ni ] as\n¯gw,γ(λi) =\ngc(λi)\ng2c(λi) + σ2/¯x∗2\nγ\n,\n(7)\nwhere γ is a hyper-parameter to adjust AER. As a natural\nextension of Proposition 2, ¯gw,γ owns the following proposition.\nProposition 3. Let ˆxw,γ be the recovered features by modified graph wiener filter ¯gw,γ(λi), then the variance of ˆxw,γ\nis less than ˆxinv. In addition, S(λi, x∗\ni , σ, gc, ¯gw,γ1) ≤\nS(λi, x∗\ni , σ, gc, ¯gw,γ2) ≤ S(λi, x∗\ni , σ, gc, ginv) if E[x∗2\ni ] ≤\n¯x∗2\nγ1 ≤ ¯x∗2\nγ2.\nPlease refer to Appendix C for details. Proposition 3\ndemonstrates that ¯gw,γ attends to spectral reconstructions\nover different ranges of spectra, depending on the selection\nof γ. The graph wiener kernel Dγ = U¯gw,γ(Λ)UT can also\nbe reformatted as matrix multiplication\nDγ = U(g2\nc(Λ) + σ2\n¯x∗2\nγ\nI)−1gc(Λ)UT .\n(8)\nNote that gc can be arbitrary function and support of λi\nis restricted to [0, 2], we adopt Remez polynomial (Pachon\nand Trefethen 2009) to approximate ¯gw,γ(λi), which mitigates the need of eigen-decomposition and matrix inversion\nin Eq. 8.\nDefinition 1 (Remez Polynomial Approximation). Given\nan arbitrary continuous function ζ(t) on t ∈ [a, b], the Remez polynomial approximation for ζ(t) is defined as\npK(t) :=\nK\nX\nk=0\ncktk,\n(9)\nwhere coefficients c0, . . . , cK and leveled error e are obtained by resolving linear system\nζ(tj) = pK(tj) + (−1)je,\n(10)\nwhere {tj}K+1\nj=0 are interpolation points within [a, b].\nLemma 1. If interpolation points {tj}K+1\nj=0 are Chebyshev\nnodes, the interpolation error |ζ(t) − pK(t)| of Remez polynomial pK(t) is minimized.\nThe proof is trivial and illustrated in detail as Corollary\n8.11 in (Burden, Faires, and Burden 2015). Following Definition 1, the Kth order Remez approximation of Dγ is formulated as\nDγ = UpK(Λ)UT =\nK\nX\nk=0\nck,γLk,\n(11)\nwhere Dγ is approximated adaptively in each epoch.\nWiener Graph Deconvolutional Network\nGraph encoder.\nTo incorporate both graph features X and\nstructure A in a unified framework, we employ M layers of\ngraph convolution neural network as our graph encoder. For\nm = 0, ..., M − 1,\nH(m+1) = ϕ(gc(L)H(m)W(m)),\n(12)\nwhere H(0) = X, ϕ is the activation function such as\nPReLU and gc(λi) = 1 − λi as GCN (Kipf and Welling\n2017), gc(λi)\n=\ne−tλi as heat kernel or gc(λi)\n=\nα\n1−(1−α)(1−λi) as PPR kernel.\nRepresentation augmentation.\nFor simplicity, Gaussian\nnoise is employed as latent augmentations to the node embedding generated by the last layer encoder\nˆH(M) = H(M) + βE,\n(13)\nwhere E\n=\n{ϵ1, ..., ϵN}, ϵi\n∼\nN(0, σ2\nP I), σ2\nP\n=\nVAR[H(M)] and β is a hyper-parameter to adjust the magnitude of augmentations.\nGraph wiener decoder.\nThe decoder aims to recover original features given the augmented representation ˆH. Our\nprevious analysis demonstrates the superiority of wiener kernel to permit reconstruction-based representation learning\nwith augmented latent space. Considering the properties of\nspectral reconstruction error from Proposition 3, we symmetrically adopt M layers of graph deconvolution as the\ndecoder, where each layer consists of q channels of graph\nwiener kernels. For m = 1, ..., M and i = 1, ..., q,\nZ(m−1)\ni\n= ϕ(D(m)\nγi\nˆH(m)W(m)\ni\n),\nˆH(m−1) = AGG([Z(m−1)\n1\n, ..., Z(m−1)\nq\n]),\n(14)\nwhere ˆX = ˆH(0) and AGG(·) is aggregation function such\nas summation. Note that the actual value of ¯x∗2 and σ2 of\nD(m)\nγi\nare unknown, we estimate ¯x∗2 following its definition and leverage neighboring information for σ2 estimation.\nFurther details are presented in Appendix D.\nOptimization and inference.\nOur model is optimized following the convention of reconstruction-based SSL, which\nis simply summarized as\nL = ||X − ˆX||F .\n(15)\nFor downstream applications, we treat the fully trained\nH(M) as the final node embedding. For graph-level tasks,\nwe adopt a non-parametric graph pooling (readout) function R, e.g. MaxPooling, to generate graph representation\nhg = R(H(M)).\nComplexity analysis.\nThe most intensive computational\ncost of our proposed method is kernel approximation in\nEq. 11. Note that kernel approximation is a simple Kth order\npolynomial of graph convolution. By sparse-dense matrix\nmultiplication, graph convolution can be efficiently implemented, which take O(K|E|) (Kipf and Welling 2017) for a\ngraph with |E| edges.",
        "experiments": "In this section, we investigate the benefit of our proposed\napproach by addressing the following questions:\nQ1. Does WGDN outperform self-supervised and semisupervised counterparts?\nQ2. Do the key components of WGDN contribute to representation learning?\nQ3. Can WGDN be more efficient than competitive baselines?\nQ4. How do the hyper-parameters impact the performance of our proposed model?\nExperimental Setup\nDatasets.\nWe conduct experiments on both node-level\nand graph-level representation learning tasks with benchmark datasets across different scales and domains, including PubMed (Sen et al. 2008), Amazon Computers,\nPhoto (Shchur et al. 2018), Coauthor CS, Physics (Shchur\net al. 2018), and IMDB-B, IMDB-M, PROTEINS, COLLAB, DD, NCI1 from TUDataset (Morris et al. 2020). Detailed statistics are presented in Table 8 and Table 9 of Appendix F.\nBaselines.\nWe compare WGDN against representative\nmodels from the following five different categories: (1) traditional models including Node2Vec (Grover and Leskovec\n2016), Graph2Vec (Narayanan et al. 2017), DeepWalk (Perozzi, Al-Rfou, and Skiena 2014), (2) graph kernel models\nincluding Weisfeiler-Lehman sub-tree kernel (WL) (Shervashidze et al. 2011), deep graph kernel (DGK) (Yanardag\nand Vishwanathan 2015), (3) predictive SSL models including GAE (Kipf and Welling 2016), GALA (Park et al. 2019),\nGDN (Li et al. 2021), GraphMAE (Hou et al. 2022), (4) contrastive SSL models including DGI (Veliˇckovi´c et al. 2019),\nMVGRL (Hassani and Khasahmadi 2020), GRACE (Zhu\net al. 2020), GCA (Zhu et al. 2021), BGRL (Thakoor\net al. 2022), AFGRL (Lee, Lee, and Park 2022), CCASSG (Zhang et al. 2021), InfoGraph (Sun et al. 2019),\nGraphCL (You et al. 2020a), JOAO (You et al. 2021), SimGRACE (Xia et al. 2022), InfoGCL (Xu et al. 2021) and (5)\nsemi-supervised models including GCN (Kipf and Welling\n2017), GAT (Veliˇckovi´c et al. 2018) and GIN (Xu et al.\n2019).\nEvaluation protocol.\nWe closely follow the evaluation\nprotocol in recent SSL researches. For node classification,\nthe node embedding is fed into a logistic regression classifier (Veliˇckovi´c et al. 2019). We run 20 trials with different\nseeds and report the mean classification accuracy with standard deviation. For graph classification, we feed the graph\nrepresentation into a linear SVM, and report the mean 10fold cross-validation accuracy with standard deviation after\n5 runs (Xu et al. 2021). Please refer to Appendix F.1 for further details.\nExperiment settings.\nWe use the official implementations\nfor all baselines in node classification and follow the suggested hyper-parameter settings, whereas graph classification results are obtained from original papers if available.\nFor spectral filter, we consider heat kernel gc(λi) = e−tλi\nwith diffusion time t = 1 and PPR kernel gc(λi) =\nα\n1−(1−α)(1−λi) with teleport probability α = 0.2. In node\nclassification training, we use the public split for PubMed\nand follow 10/10/80% random split for the rest. Further details of model configurations (e.g., hyper-parameters selection) can be found in Appendix F.2.\nPerformance Comparison (Q1)\nThe node classification performances are reported in Table 2. We find that WGDN outperforms the predictive SSL\nmethods by a large margin over all datasets. For fair comparisons, we report the best results of recent methods using\ndiffusion kernels (denoted with ∗). WGDN performs competitively with contrastive SSL methods, achieving state-ofthe-art performances across all datasets. For instance, our\nmodel WGDN is able to improve by a margin up to 0.9%\non accuracy over the most outstanding contrastive counterpart CCA-SSG on PubMed. Moreover, when compared to\nsemi-supervised models, WGDN consistently generates better performance than both GCN and GAT.\nTable 3 lists the graph classification performance across\nvarious methods. We observe that our approach achieves\nModel\nPubMed\nComputers\nPhoto\nCS\nPhysics\nSelf-supervised\nNode2Vec\n66.6 ± 0.9\n84.39 ± 0.08\n89.67 ± 0.12\n85.08 ± 0.03\n91.19 ± 0.04\nDeepWalk + Feat.\n74.3 ± 0.9\n86.28 ± 0.07\n90.05 ± 0.08\n87.70 ± 0.04\n94.90 ± 0.09\nGAE\n72.1 ± 0.5\n85.27 ± 0.19\n91.62 ± 0.13\n90.01 ± 0.71\n94.92 ± 0.07\nGALA\n75.9 ± 0.4\n87.61 ± 0.06\n91.27 ± 0.12\n92.48 ± 0.07\n95.23 ± 0.04\nGDN\n76.4 ± 0.2\n87.67 ± 0.17\n92.84 ± 0.07\n92.93 ± 0.18\n95.22 ± 0.05\nDGI\n76.8 ± 0.6\n83.95 ± 0.47\n91.61 ± 0.22\n92.15 ± 0.63\n94.51 ± 0.52\nMVGRL\n80.1 ± 0.7\n87.52 ± 0.11\n91.74 ± 0.07\n92.11 ± 0.12\n95.33 ± 0.03\nGRACE\n80.5 ± 0.4\n86.25 ± 0.25\n92.15 ± 0.24\n92.93 ± 0.01\n95.26 ± 0.02\nGCA\n80.2 ± 0.4\n88.94 ± 0.15\n92.53 ± 0.16\n93.10 ± 0.01\n95.73 ± 0.03\nBGRL∗\n79.8 ± 0.4\n89.70 ± 0.15\n93.37 ± 0.21\n93.51 ± 0.10\n95.28 ± 0.06\nAFGRL∗\n79.9 ± 0.3\n89.58 ± 0.45\n93.61 ± 0.20\n93.56 ± 0.15\n95.74 ± 0.10\nCCA-SSG∗\n81.0 ± 0.3\n88.15 ± 0.35\n93.25 ± 0.21\n93.31 ± 0.16\n95.59 ± 0.07\nWGDN\n81.9 ± 0.4\n89.72 ± 0.48\n93.89 ± 0.31\n93.67 ± 0.14\n95.76 ± 0.11\nSupervised\nGCN\n79.1 ± 0.3\n86.51 ± 0.54\n92.42 ± 0.22\n93.03 ± 0.31\n95.65 ± 0.16\nGAT\n79.0 ± 0.3\n86.93 ± 0.29\n92.56 ± 0.35\n92.31 ± 0.24\n95.47 ± 0.15\nTable 2: Node classification accuracy of all compared methods. The best and runner up models in self-supervised learning are\nhighlighted in boldface and underlined.\nModel\nIMDB-B\nIMDB-M\nPROTEINS\nCOLLAB\nDD\nNCI1\nSelf-supervised\nWL\n72.30 ± 3.44\n46.95 ± 0.46\n72.92 ± 0.56\n79.02 ± 1.77\n79.43 ± 0.55\n80.01 ± 0.50\nDGK\n66.96 ± 0.56\n44.55 ± 0.52\n73.30 ± 0.82\n73.09 ± 0.25\n80.31 ± 0.46\nGraph2Vec\n71.10 ± 0.54\n50.44 ± 0.87\n73.30 ± 2.05\n73.22 ± 1.81\nMVGRL\n74.20 ± 0.70\n51.20 ± 0.50\nInfoGraph\n73.03 ± 0.87\n49.69 ± 0.53\n74.44 ± 0.31\n70.65 ± 1.13\n72.85 ± 1.78\n76.20 ± 1.06\nGraphCL\n71.14 ± 0.44\n48.58 ± 0.67\n74.39 ± 0.45\n71.36 ± 1.15\n78.62 ± 0.40\n77.87 ± 0.41\nJOAO\n70.21 ± 3.08\n49.20 ± 0.77\n74.55 ± 0.41\n69.50 ± 0.36\n77.32 ± 0.54\n78.07 ± 0.47\nSimGRACE\n71.30 ± 0.77\n75.35 ± 0.09\n71.72 ± 0.82\n77.44 ± 1.11\n79.12 ± 0.44\nInfoGCL\n75.10 ± 0.90\n51.40 ± 0.80\n80.00 ± 1.30\n80.20 ± 0.60\nGraphMAE\n75.52 ± 0.66\n51.63 ± 0.52\n75.30 ± 0.39\n80.32 ± 0.46\n78.86 ± 0.35\n80.40 ± 0.30\nWGDN\n75.76 ± 0.20\n51.77 ± 0.55\n76.53 ± 0.38\n81.76 ± 0.24\n79.54 ± 0.51\n80.70 ± 0.39\nSupervised\nGCN\n74.0 ± 3.4\n51.9 ± 3.8\n76.0 ± 3.2\n79.0 ± 1.8\n75.9 ± 2.5\n80.2 ± 2.0\nGIN\n75.1 ± 5.1\n52.3 ± 2.8\n76.2 ± 2.8\n80.2 ± 1.9\n75.3 ± 2.9\n82.7 ± 1.7\nTable 3: Graph classification accuracy of all compared methods.\nstate-of-the-art results compared to existing SSL baselines\nin all datasets. Besides, WGDN outperforms the best kernel methods up to a large margin. Even when compared to\nsemi-supervised models, our model achieves the best results\nin 4 out of 6 datasets and the gaps for the rest are relatively\nminor.\nIn brief, our model consistently achieves comparable performance with the cutting-edge SSL and semi-supervised\nmethods across node-level and graph-level tasks. Particularly, the significant improvements demonstrate the effectiveness of WGDN in boosting the learning capability under\nGAE framework.\nEffectiveness of Key Components (Q2)\nTo validate the benefit of introducing graph wiener decoder,\nwe conduct ablation studies on node and graph classification tasks with five datasets that exhibit distinct characteristics (e.g., citation, social and bioinformatics). For clarity,\nWGDN-A and WGDN-W are denoted as the models removing augmentation or substituting graph wiener decoder\nwith inverse decoder. WGDN-AW is the plain model without both components. Specifically, heat kernel is selected\nas the backbone of encoder for node-level datasets, and we\nadopt PPR kernel for graph-level datasets.\nThe results are illustrated in Figure 3, from which we\nmake several observations. (1) WGDN-W may underperform WGDN-AW. This observation validates that deterministic inverse decoder is ill-adapted to augmented latent\nspace and may lead to degraded learning quality, which\nis consistent with our theoretical analysis. (2) Compared\nwith WGDN-AW, WGDN-A improves model performance\nacross all datasets, which suggests that graph wiener decoder is able to benefit representation learning even without augmentation. (3) The performance of WGDN is signifFigure 3: Ablation study of graph wiener decoder. Complete\nmodel consistently boosts model performance across different datasets.\nicantly higher than other counterparts. For instance, WGDN\nhas a relative improvement up to 6% over WGDN-AW on\nPubMed. It can be concluded that the graph wiener decoder allows the model to generate more semantic embedding from the augmented latent space.\nEfficiency Analysis (Q3)\nTo evaluate the computational efficiency, we compare the\ntraining speed and GPU overhead of WGDN against BGRL\nand GraphMAE on datasets of different scales, including\nComputers and OGBN-Arxiv (Hu et al. 2020a). For fair\ncomparisons, we set the embedding size of all models as\n512 and follow their suggested hyper-parameters settings.\nIt is evident from Table 4 that the memory requirement of\nWGDN is significantly reduced up to 30% compared to\nBGRL, the most efficient contrastive benchmark. In addition, as WGDN is a GAE framework without computationally expensive add-on, its computational cost is shown to\nbe comparable to GraphMAE. Considering that memory is\nusually the bottleneck in graph-based applications, WGDN\ndemonstrates a practical advantage when limited resources\nare available.\nDataset\nModel\nSteps/Second\nMemory\nComputers\nBGRL\n17.27\n3.01 GB\nGraphMAE\n19.47\n2.03 GB\nWGDN\n19.62\n2.20 GB\nOGBN-Arxiv\nBGRL\n2.52\n9.74 GB\nGraphMAE\n3.13\n8.01 GB\nWGDN\n3.16\n7.35 GB\nTable 4: Comparison of computational efficiency on benchmark datasets.\nHyper-Parameter Analysis (Q4)\nMagnitude of augmentation β.\nIt is expected that introducing adequate augmentation enriches the sample distribution in the latent space, which contributes to learning more\nexpressive representations. Figure 4 shows that the classification accuracy generally reaches the peak and drops gradually when the augmentation size β increases, which aligns\nFigure 4: Downstream tasks performance versus varied augmentation magnitude β in training.\nFilter\nGCN\nHeat\nPPR\nPubMed\n80.2 (0.019)\n81.9 (0.011)\n81.4 (0.013)\nComputers\n89.03 (0.417)\n89.72 (0.375)\n89.59 (0.405)\nCS\n92.48 (0.263)\n93.67 (0.241)\n92.75 (0.245)\nIMDB-B\n75.46 (0.102)\n75.71 (0.098)\n75.76 (0.093)\nDD\n79.29 (0.118)\n79.36 (0.104)\n79.54 (0.074)\nTable 5: Performance and training loss of WGDN with different convolution filter gc.\nwith our intuition. We also observe that the optimal augmentation magnitudes are relatively smaller for node-level\ndatasets, which may be related to the semantics level of\ngraph features. Input features of graph-level datasets are\nless informative and latent distribution may still preserve\nwith stronger augmentations. Besides, the stable trend further verifies that graph wiener decoder is well adapted to\naugmentation in representation learning.\nConvolution filter gc.\nTable 5 shows the influence of\ndifferent convolution filters. It is observed that diffusionbased WGDN outperforms its trivial version with GCN filter\nacross different applications. Specifically, heat kernel generates better results in node classification and PPR kernel\nis more suitable for graph-level tasks. We conjecture that\nsparse feature information may be better compressed via\npropagation with PPR kernel. In addition, we also find that\ntraining loss of diffusion models is consistently lower. Both\nphenomena indicate that the superior information aggregation and powerful reconstruction of diffusion filters jointly\ncontribute to learning a more semantic representation.",
        "conclusion": "In this paper, we propose Wiener Graph Deconvolutional\nNetwork (WGDN), a predictive self-supervised learning\nframework for graph-structured data. We introduce graph\nwiener filter and theoretically validate its superior reconstruction ability to facilitate reconstruction-based representation learning. By leveraging graph wiener decoder, our\nmodel can efficiently learn graph embedding with augmentation. Extensive experimental results on various datasets\ndemonstrate that WGDN achieves competitive performance\nover a wide range of self-supervised and semi-supervised\ncounterparts.",
        "summary_en": "Graph self-supervised learning (SSL) has been vastly employed to learn representations from unlabeled graphs. Existing methods can be roughly divided into predictive learning and contrastive learning, where the latter one attracts more research attention with better empirical performance. This paper argues that, however, predictive models weaponed with powerful decoder could achieve comparable or even better representation power than contrastive models. The proposes a Wiener Graph Deconvolutional Network (WGDN), an augmentation-adaptive decoder empowered by graph wiener filter to perform information reconstruction. Theoretical analysis proves the superior reconstruction ability of graph wiener filter. Extensive experimental results on various datasets demonstrate the effectiveness of the approach.",
        "summary_zh": "这篇论文介绍了一种Wiener图解卷积网络（WGDN），用于改善图自监督学习。现有的图自监督学习方法大致可分为预测学习和对比学习，后者经验表现更好。但作者认为，预测模型加上强大的解码器，可以达到与对比模型相当甚至更好的表征能力。该模型是一种增强型自适应解码器，利用维纳图滤波器来执行信息重构。理论分析证明了Wiener图滤波器的优越的重构能力。在各种数据集上进行了广泛的实验，证明了该方法的有效性。"
    },
    {
        "title": "Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks",
        "abstract": "In recent years, visual recognition on challenging long-tailed distributions, where classes often exhibit extremely imbalanced frequencies, has made great progress mostly based on various complex paradigms (e.g., meta learning). Apart from these complex methods, simple reﬁnements on training procedure also make contributions. These reﬁnements, also called tricks, are minor but effective, such as adjustments in the data distribution or loss functions. However, different tricks might conﬂict with each other. If users apply these long-tail related tricks inappropriately, it could cause worse recognition accuracy than expected. Unfortunately, there has not been a scientiﬁc guideline of these tricks in the literature. In this paper, we ﬁrst collect existing tricks in long-tailed visual recognition and then perform extensive and systematic experiments, in order to give a detailed experimental guideline and obtain an effective combination of these tricks. Furthermore, we also propose a novel data augmentation approach based on class activation maps for long-tailed recognition, which can be friendly combined with re-sampling methods and shows excellent results. By assembling these tricks scientiﬁcally, we can outperform state-of-the-art methods on four long-tailed benchmark datasets, including ImageNet-LT and iNaturalist 2018. Our code is open-source and available at https://github.com/zhangyongshun/BagofTricks-LT.",
        "introduction": "Computer vision has achieved great progress with the development of convolutional neural networks (CNNs) trained on\nbalanced distributed datasets (Deng et al. 2009; Krizhevsky\nand Hinton 2009). But in real-world scenarios, large scale\ndatasets (Zhou et al. 2017; Van Horn et al. 2018; Lin et al.\n2014) naturally exhibit the imbalanced and long-tailed distributions, where a few categories (majority categories) occupy\nmost of the data while most categories (minority categories)\nare under-represented. CNNs trained on these long-tailed\ndatasets deliver poor recognition accuracy, especially for\nunder-represented minority categories. Dealing with such\nlong-tailed distributions is indispensable in real-world applications, such as object detection (Lin et al. 2017; Ouyang\net al. 2016), instance segmentation (Wang et al. 2019; Gupta,\nDollar, and Girshick 2019), visual recognition (Zhang et al.\n2017; Cui et al. 2019), etc. In this paper, we focus on the\nfundamental long-tailed visual recognition problem.\nRecently, long-tailed visual recognition has attracted increasing attentions. Various methods belonging to different\nparadigms, e.g., metric learning (Wang et al. 2018; Cao et al.\n2019), meta learning (Liu et al. 2019; Peng et al. 2019; Jamal et al. 2020) and knowledge transfer (Wang, Ramanan,\nand Hebert 2017), have been successfully explored. Although\nthese methods bring a steady trend of accuracy improvements\non long-tailed datasets, they often suffer from high sensitivity\nto hyper-parameters (Cao et al. 2019; Yan et al. 2019) or high\ncomplexity in the training procedures (Wang, Ramanan, and\nHebert 2017; Liu et al. 2019; Xiang, Ding, and Han 2020). Besides, it causes difﬁculties to efﬁciently apply these methods\nin various real-world scenarios. Apart from these methods,\nexisting training tricks in long-tailed visual recognition also\nplay a major role, which just make simple reﬁnements to\nthe vanilla training procedure, such as adjustments in loss\nfunctions or data sampling strategies. These tricks are simple\nbut make big differences. However, different tricks might\nhurt each other during training when they were employed\ninappropriately. For instance, re-sampling (Buda, Maki, and\nMazurowski 2018; Japkowicz and Stephen 2002) and reweighting (Mikolov et al. 2013; Cui et al. 2019) are two commonly used tricks to alleviate the imbalance of long-tailed\ndistributions. Re-sampling tries to get balanced datasets, and\nre-weighting assigns weights to categories determined by\ninversion of class frequencies. Since both re-sampling and reweighting try to enlarge the inﬂuence of minority categories,\napplying re-weighting and re-sampling simultaneously will\nobtain similar or even worse accuracy than using them alone.\nSimilar to re-weighting and re-sampling, when we apply two\nor more long-tail related tricks, it would be great to know\nwhich of them can be combined synergistically and also\nwhich of them might conﬂict with others. Yet, no guideline\nis available in the literature. Although there are several good\nDatasets\nCIFAR-10-LT\nCIFAR-100-LT\niNaturalist 2018\nImageNet-LT\nImbalance factor\n100\n50\n100\n50\nBackbones\nResNet-32\nResNet-32\nResNet-50\nResNet-10\nBaseline (Vanilla ResNet)\n30.18\n24.78\n61.73\n57.90\n39.89\n65.99\nFocal loss (Lin et al. 2017)\n29.62\n24.75\n61.90\n57.56\n39.70\n67.36\nCB Focal (Cui et al. 2019)\n25.43\n20.73\n60.40\n53.79\n38.88\n–\nFeature space augmentation (Chu et al. 2020)\n–\n–\n–\n–\n34.09\n64.80\nMeta-learning (Jamal et al. 2020)†\n20.00\n17.77\n55.92\n50.84\n32.45\n70.10\nLDAM with DRW (Cao et al. 2019)\n22.97\n20.70\n57.96\n54.92\n32.00\n63.97\nDecoupling learning (Kang et al. 2020)\n–\n–\n–\n–\n30.70\n58.20\nMulti-experts (Xiang, Ding, and Han 2020)†‡\n–\n–\n57.70\n–\n–\n61.20\nBBN (Zhou et al. 2020)\n20.18\n17.82\n57.44\n52.98\n30.38\n–\nBaseline + tricks (Ours)\n19.97\n16.41\n52.17\n48.31\n29.13\n56.87\n† : Results on CIFAR-10-LT and CIFAR-100-LT are obtained by incorporating LDAM (Cao et al. 2019).\n‡ : Results on ImageNet-LT are obtained by incorporating OLTR (Liu et al. 2019).\nTable 1: Top-1 error rates on long-tailed benchmark datasets. Our bag of tricks obtains signiﬁcant accuracy gains compared with\nstate-of-the-art methods. (Best results are marked in bold.)\nsurveys about class imbalance learning (More 2016; Buda,\nMaki, and Mazurowski 2018; Japkowicz and Stephen 2002),\nthey could be further comprised of effective tricks in the deep\nlearning era. More importantly, they lack the comprehensive empirical studies of combining and evaluating a set of\nlong-tail related tricks quantitatively.\nIn this paper, we focus on exploring commonly used, easily equipped, and hyper-parameters insensitive tricks in longtailed visual recognition. Also, we conduct extensive experiments to provide valuable practical guidelines for future\nresearches. These long-tail related tricks are separated into\nfour families, i.e., re-weighting, re-sampling, mixup training, and two-stage training. Particularly, we add mixup training (Zhang et al. 2018; Verma et al. 2019) into long-tail related tricks because we ﬁnd that mixup training delivers good\nresults in long-tailed visual recognition, especially when combined with re-sampling. In each trick family, we introduce\ncommonly used tricks and compare the results on long-tailed\nbenchmark datasets. Furthermore, to overcome the lack of\ndiscriminative information in existing re-sampling methods,\nwe propose a novel data augmentation approach based on\nclass activation maps (CAM) (Zhou et al. 2016), which is tailored for two-stage training and generates discriminative images by transferring foregrounds while keeping backgrounds\nunchanged. It can be friendly combined with existing resampling methods and exhibits excellent results, which is\ntermed as “CAM-based sampling”. Also, we explore the\nconﬂicts between tricks of different families to ﬁnd the optimal combination of tricks, named bag of tricks. Top-1 error\nrates on long-tailed CIFAR and two large scale datasets (e.g.,\nImageNet-LT and iNaturalist 2018) are shown in Table 1,\nwhich shows signiﬁcant accuracy gains of our bag of tricks\ncompared with state-of-the-art methods.\nThe major contributions of our work can be summarized:\n• We comprehensively explore existing simple, hyperparameters insensitive, long-tail related tricks and provide\na valuable practical guideline for future researches.\n• We propose a novel CAM-based sampling approach tailored for two-stage training, which is simple but effective\nfor long-tailed visual recognition.\n• We conduct extensive experiments and ﬁnd the optimal\ncombination of tricks. Our bag of tricks achieves outperforming recognition results compared with state-of-the-art\nmethods on four long-tailed benchmark datasets without\nintroducing extra FLOPs.",
        "datasets and baseline settings": "In this section, we describe the long-tailed datasets used in\nexperiments as well as baseline training settings, e.g., backbone network, data augmentation, etc. For fair comparisons,\nwe keep our experimental settings consistent with previous\nworks (Cao et al. 2019; Cui et al. 2019; Zhou et al. 2020).\nDatasets\nLong-Tailed CIFAR\nThe long-tailed versions of CIFAR10 and CIFAR-100 datasets (CIFAR-10-LT and CIFAR-100LT) (Cui et al. 2019) are benchmark datasets for long-tailed\nrecognition. As the original CIFAR datasets (Krizhevsky\nand Hinton 2009), the long-tailed versions contain the same\ncategories. However, they are created by reducing the number\nof training samples per class according to an exponential\nfunction n = nt × µt, where t is the class index (0-indexed)\nand nt is the original number of training images with µ ∈\n(0, 1). The test set remains unchanged. The imbalance factor\nof a long-tailed CIFAR dataset is deﬁned as the number of\ntraining samples in the largest class divided by that of the\nsmallest, which ranges from 10 to 200. In the literature, the\nimbalance factor of 50 and 100 are widely used, with around\n12,000 training images under each imbalance factor.\niNaturalist 2018\nThe iNaturalist species classiﬁcation\ndatasets (Van Horn et al. 2018) are large-scale real-world\ndatasets that suffer from extremely imbalanced label distributions. The most challenging dataset of iNaturalist is the\nDatasets\nCIFAR-10-LT\nCIFAR-100-LT\niNaturalist 2018\nImageNet-LT\nImbalance factor\n100\n50\n100\n50\nBackbones\nResNet-32\nResNet-32\nResNet-50\nResNet-10\nBaseline (Vanilla ResNet)\n30.18\n24.78\n61.73\n57.90\n39.89\n65.99\nReference (Cui et al. 2019; Liu et al. 2019)\n29.64\n25.19\n61.68\n56.15\n42.86\n64.40\nTable 2: Top-1 error rates of reference implementations and our baseline.\n2018 version, which contains 437,513 images from 8,142\ncategories. Besides the extreme imbalance, the iNaturalist\ndatasets also face the ﬁne-grained problem (Wei, Wu, and Cui\n2019). We follow the ofﬁcial training and validation splits of\niNaturalist 2018 in our experiments.\nLong-Tailed\nImageNet\nThe\nlong-tailed\nImageNet\n(ImageNet-LT) is derived from the original ImageNet2012 (Deng et al. 2009) by sampling a subset following the\nPareto distribution from 1,000 categories, with maximally\n1,280 images per class and minimally 5 images per class.\nThe test set is balanced by following (Liu et al. 2019).\nBaseline Settings\nBackbones\nWe adopt deep residual networks (He et al.\n2016) as backbones. Speciﬁcally, we follow (Cui et al.\n2019) to use the residual network with 32 layers (ResNet32) and the residual network with 50 layers (ResNet-50)\nfor long-tailed CIFAR and iNaturalist datasets, respectively.\nFor ImageNet-LT, according to (Liu et al. 2019), we adopt\nResNets with 10 layers (ResNet-10) for fair comparisons.\nTraining Details\nAll backbones are trained from scratch.\nWe adopt the initialization method in (He et al. 2015). We\ntrain ResNet-32 on long-tailed CIFAR datasets by stochastic\ngradient descent (SGD) with momentum of 0.9 and weight\ndecay of 2 × 10−4. We follow the data augmentation in\n(He et al. 2016). The number of training epochs is 200 and\nthe batch size is 128. Learning rate is initialized to 0.1 and\ndivided by 100 at the 160th and 180th epoch, respectively. We\nuse warm-up (Goyal et al. 2017) for the ﬁrst ﬁve epochs.\nFor iNaturalist 2018 and ImageNet-LT, we follow the same\ntraining strategy with Goyal et al. (2017). We follow the data\naugmentation in (Goyal et al. 2017). Backbones are trained\nwith batch size of 512. The number of training epochs is 90,\nand the learning rate is initialized to 0.2 and divided by 10\nat the 30th, 60th and 80th epoch without warm-up. SGD is\nadopted with momentum of 0.9 and weight decay of 1×10−4.\nTop-1 error rates of baseline training are shown in Table 2,\nand our results are mostly consistent with references (Cui\net al. 2019; Liu et al. 2019). For slightly inconsistent ones,\nsuch as iNaturalist 2018 and ImageNet-LT, they might be\ncaused by running environment (e.g., the version of CUDA\nand deep learning frameworks), because we keep training\nand validation settings consistent with references.",
        "trick gallery": "We divide the long-tail related tricks into four families: reweighting, re-sampling, mixup training, and two-stage training. We take mixup training as a long-tail related trick, because we ﬁnd that mixup training (Zhang et al. 2018; Verma\net al. 2019) delivers good recognition accuracy in longtailed visual recognition, especially when combined with\nre-sampling. In each trick family, we introduce commonly\nused tricks and compare their accuracy.\nIn addition, we propose a simple yet effective data augmentation approach tailored for two-stage training. The\nproposed approach is based on the class activation maps\n(CAM) (Zhou et al. 2016), which can be friendly combined\nwith re-sampling and termed as “CAM-based sampling”.\nRe-Weighting Methods\nCost-sensitive re-weighting methods are commonly adopted\nmethods in the long-tailed literature. These methods guide\nthe network to pay more attention on minority categories by\nassigning different weights to different classes.\nFormally, for each image with label c ∈ {1, 2, . . . , C}, we\nset the predicted outputs as z = [z1, z2, . . . , zC]⊤, where C\nis the total number of classes. We deﬁne nc as the number of\ntraining images in class c and nmin as the number of training\nimages in the smallest class. Softmax cross-entropy loss (CE)\nis used as the baseline, which is deﬁned as\nLCE(z, c)=− log\n \nexp (zc)\nPC\ni=1 exp (zi)\n!\n.\n(1)\nExisting Re-Weighting Methods\nWe review commonly\nused re-weighting methods, including cost-sensitive softmax cross-entropy loss (Japkowicz and Stephen 2002), focal loss (Lin et al. 2017), and the recently proposed classbalanced loss (Cui et al. 2019).\n• Cost-sensitive softmax cross-entropy loss (CS CE) (Japkowicz and Stephen 2002) is deﬁned as\nLCS CE(z, c)=−nmin\nnc\nlog\n \nexp (zc)\nPC\ni=1 exp (zi)\n!\n.\n(2)\n• Focal loss (Lin et al. 2017) adds an adjusting factor to\nthe sigmoid cross-entropy loss to focus training on difﬁcult\nsamples. We denote pi = sigmoid(zi) =\n1\n1+exp(−zi) and\ndeﬁne pt\ni as\npt\ni =\n\u001a\npi,\ni = c\n1 − pi,\ni ̸= c ,\n(3)\nDatasets\nCIFAR-10-LT\nCIFAR-100-LT\nImbalance factor\n100\n50\n100\n50\nCE\n30.18\n24.78\n61.73\n57.90\nCB CE\n28.26\n22.76\n66.40\n63.48\nCS CE\n29.07\n23.74\n70.92\n63.78\nFocal loss\n29.62\n24.75\n61.90\n57.56\nCB Focal\n27.02\n22.03\n62.36\n57.24\nTable 3: Top-1 error rates of re-weighting methods. It shows\ndirectly applying re-weighting is inappropriate, especially\nwhen the number of classes increases.\nand then the focal loss can be written as\nLFocal(z, c)=−\nC\nX\ni=1\n\u0000\n1−pt\ni\n\u0001γ log\n\u0000\npt\ni\n\u0001\n,\n(4)\nwhere γ is a hyper-parameter to control the importances of\ndifferent samples.\n• Class-balanced loss (Cui et al. 2019) considers the real\nvolumes of different classes, named effective numbers, rather\nthan the nominal numbers of images provided by datasets.\nWith the theory of effective numbers, the class-balanced focal\nloss (CB Focal) and class-balanced softmax cross-entropy\nloss (CB CE) are deﬁned as\nLCB Focal(z, c)=− 1−β\n1−βnc\nC\nX\ni=1\n\u0000\n1−pt\ni\n\u0001γ log\n\u0000\npt\ni\n\u0001\n,\n(5)\nLCB CE(z, c)=− 1−β\n1−βnc log\n \nexp (zc)\nPC\ni=1 exp (zi)\n!\n,\n(6)\nwhere γ and β are two hyper-parameters. We set γ and β on\ndifferent long-tailed datasets according to (Cui et al. 2019).\nExperimental Results\nWe evaluate re-weighting methods\non long-tailed CIFAR datasets. As shown in Table 3, we discover that re-weighting delivers lower error rates on CIFAR10-LT, but obtains worse results on CIFAR-100-LT compared with vanilla ResNet-32. This indicates that applying\nre-weighting directly in the training procedure is not a proper\nchoice, especially when the number of categories increases\nand data becomes more imbalanced.\nIn the later section of “Two-stage training procedures”, we\nwill describe the two-stage training strategy for long-tailed\nvisual recognition, which demonstrates an effective strategy\nto apply re-weighting.\nRe-Sampling Methods\nRe-sampling is popular used for dealing with long-tailed\nproblems, which attempts to sample the data to get an evenlydistributed dataset.\nExisting Re-Sampling Methods\nWe review existing simple and commonly used re-sampling methods as follows.\n• Random over-sampling (Buda, Maki, and Mazurowski\n2018) is one of the representative re-sampling methods,\nwhich replicates randomly sampled training images from minority classes. Random over-sampling is effective, but might\nlead to overﬁtting (Saraﬁanos, Xu, and Kakadiaris 2018).\nDatasets\nCIFAR-10-LT\nCIFAR-100-LT\nImbalance factor\n100\n50\n100\n50\nBaseline (Vanilla ResNet)\n30.18\n24.78\n61.73\n57.90\nRandom under-sampling\n34.14\n26.91\n67.23\n60.98\nRandom over-sampling\n33.24\n26.53\n67.00\n61.11\nClass-balanced sampling\n30.44\n23.97\n67.34\n61.48\nSquare-root sampling\n31.36\n24.84\n64.47\n59.82\nP-B sampling\n32.91\n25.03\n61.41\n57.09\nTable 4: Top-1 error rates of re-sampling methods. It demonstrates directly applying re-sampling methods brings slight\nimprovements. “P-B” represents “progressively-balanced”.\n• Random under-sampling (More 2016) randomly removes\ntraining images of majority classes until all classes become\nbalanced. Drummond and Holte (2003) show that undersampling can be preferable to over-sampling in some situations.\n• Class-balanced sampling (Kang et al. 2020) makes each\nclass to have an equal probability of being selected. The\nprobability pCB\nj\nof each class j is given by the following\nEq. (7) with q = 0. Speciﬁcally, class-balance sampling\nﬁrstly samples a class uniformly and then an instance from\nthe chosen class is uniformly sampled:\npj =\nnq\nj\nΣC\ni=1nq\ni\n,\n(7)\nwhere j is the current class, and ni is the number of samples\nin class i with q ∈ [0, 1]. C is the number of total classes.\n• Square-root sampling (Kang et al. 2020) sets q to 1\n2 in\nEq. (7), which aims to return a lighter imbalanced dataset.\n• Progressively-balanced sampling (Kang et al. 2020) progressively changes the sampling probabilities of classes from\nvanilla imbalanced sampling to class-balanced sampling. The\ncorresponding sampling probability pj of class j can be calculated by Eq. (8) for the current epoch t:\npP B\nj\n= (1 − t\nT )\nnj\nΣC\ni=1ni\n+ t\nT\n1\nC ,\n(8)\nwhere T is the total epochs.\nFurthermore, there are also other sampling methods that\ncreate artiﬁcial samples or sample based on gradients and\nfeatures (Yan et al. 2019; Chawla et al. 2002; Shen, Lin, and\nHuang 2018; Han, Wang, and Mao 2005; Perez-Ortiz et al.\n2019; Yu and Lam 2019). However, these methods are usually\ncomplicated and likely to introduce noisy data (Yu and Lam\n2019). Therefore, we have not considered these methods in\nthis paper which targets on simple tricks.\nExperimental Results\nTable 4 shows the error rates of\ndifferent re-sampling methods on long-tailed CIFAR datasets.\nIt can be observed that directly applying re-sampling to the\ntraining procedure gets slight improvements.\nAlso, we will show in the section of “Two-stage training\nprocedures” that combining re-sampling methods with twostage training obtains signiﬁcant improvements.\nDatasets\nCIFAR-10-LT\nCIFAR-100-LT\nImbalance factor\n100\n50\n100\n50\nBaseline (Vanilla ResNet)\n30.18\n24.78\n61.73\n57.90\nInput mixup (α=2)\n28.65\n24.90\n60.38\n56.39\nInput mixup (α=1)\n26.99\n22.91\n59.66\n55.75\nMM on layer3 (α=2)\n27.14\n22.31\n60.73\n56.52\nMM on layer3 (α=1)\n27.30\n22.41\n60.81\n56.68\nMM on FC layer (α=2)\n27.79\n21.87\n60.21\n57.09\nMM on FC layer (α=1)\n26.64\n22.55\n60.20\n56.72\nMM on pooling layer (α=2)\n27.67\n22.02\n60.45\n56.45\nMM on pooling layer (α=1)\n26.61\n21.50\n60.14\n56.44\nTable 5: Top-1 error rates of mixup methods. α is the hyperparameter of the Beta distribution. “FC” represents “fullyconnected”. “MM” represents “Manifold mixup”. We can\nsee that input mixup and manifold mixup are comparable.\nMixup Training\nMixup training can be viewed as a data augmentation trick,\nwhich aims to regularize CNNs. We ﬁnd mixup training delivers good accuracy in long-tailed visual recognition, especially\nwhen combined with re-sampling.\nExisting Mixup Methods\nWe introduce two mixup methods in this section: input mixup (Zhang et al. 2018) and\nmanifold mixup (Verma et al. 2019).\n• Input mixup has been proved effective to alleviate adversarial perturbations in CNNs (He et al. 2019a; Zhang et al.\n2019). In details, each new example is formed with two randomly sampled examples (xi, yi) and (xj, yj), by a weighted\nlinear interpolation as follows\nbx = λxi + (1 − λ)xj ,\n(9)\nby = λyi + (1 − λ)yj ,\n(10)\nwhere λ is randomly sampled from a Beta distribution. We\nonly use (bx, by) when training with input mixup.\n• Manifold mixup encourages neural networks to predict\nless conﬁdently on interpolations of hidden representations,\nwhich leverages semantic interpolations as additional training\nsignals. The mixed example is produced by\nbgk = λgk(xi) + (1 − λ)gk(xj) ,\n(11)\nby = λyi + (1 − λ)yj ,\n(12)\nwhere (gk(xi), yi) and (gk(xj), yj) are intermediate outputs\nof two randomly sampled examples (xi, yi) and (xj, yj) after\nlayer k, and λ is the mixing coefﬁcient sampled from a Beta\ndistribution. We apply manifold mixup on only one layer in\nour experiments.\nFine-Tuning after Mixup Training\nHe et al. (2019b)\nshow that the results of models trained by mixup can be\nfurther improved if we remove the mixup in last several\nepochs. In our experiments, we use the mixup training ﬁrstly,\nand then ﬁne-tune the models trained by mixup for several\nepochs in order to obtain further improvements, which is\nnamed “ﬁne-tuning after mixup training”.\nDatasets\nCIFAR-10-LT\nCIFAR-100-LT\nImbalance factor\n100\n50\n100\n50\nBaseline (Vanilla ResNet)\n30.18\n24.78\n61.73\n57.90\nInput mixup (α=1)\n26.99\n22.91\n59.66\n55.75\nInput mixup (α=1)\n+ ft. after mixup training\n26.27\n20.32\n58.21\n53.97\nMM on pooling layer (α=1)\n26.61\n21.50\n60.14\n56.44\nMM on pooling layer (α=1)\n+ ft. after mixup training\n28.88\n22.59\n61.16\n57.43\nTable 6: Top-1 error rates of ﬁne-tuning after mixup training.\nFine-tuning the models trained with input mixup obtains\nfurther improvements. “ft.” represents “ﬁne-tuning”. “MM”\nrepresents “Manifold mixup”.\nCNN\nEach sampled \nimage \nGround truth label !\nForeground \naugmentation\nObtain\nforeground\nCAM \nactivation\nGenerated \nimage\nPooling\nFC layer ’s \nweights of label !\nImbalanced \ndataset\n\"#\n$\n\"%$\n$\n&\nhead           tail \nAugmented\ndataset\nFC\n…\n!\n'\nRe-sampling\nFigure 1: Overview of our proposed CAM-based sampling.\nFor each image sampled by re-sampling, CAM is ﬁrstly generated based on feature maps and FC weights of ground truth\nlabel c. We separate the foreground and background based on\nthe average of its CAM values, and subsequently we transform foreground while keeping background unchanged to get\nthe generated informative sampled dataset.\nExperimental Results\nExperiments of mixup methods are\nshown in Table 5. Especially, we do not try all possible values\nof hyper-parameter α for the Beta distribution, which is not\nthe main purpose of our work. We can discover from Table 5\nthat 1) both input mixup and manifold mixup deliver better\nresults over baseline, and 2) when α is 1 and mixing up\nlocation is set to the pooling layer, input mixup and manifold\nmixup achieve comparable results, which need to conduct\nmore experiments with other tricks.\nThe results of ﬁne-tuning after mixup training are shown\nin Table 6. We can ﬁnd that ﬁne-tuning after input mixup\ntraining can obtain further improvements, but ﬁne-tuning\nafter manifold mixup gets worse results.\nTwo-Stage Training Procedures\nTwo-stage training consists of imbalanced training and balanced ﬁne-tuning. In this section, we focus on exploring\ndifferent methods of balanced ﬁne-tuning. We ﬁrstly describe\nexisting ﬁne-tuning methods and then present our CAMbased sampling approach.\nBalanced\nFine-Tuning\nafter\nImbalanced\nTraining\nCNNs trained on imbalanced datasets without any reFirst\nstage\nSecond stage\nof DRS\nCIFAR-10-LT\nCIFAR-100-LT\nImbalance factor\n100\n50\n100\n50\nCE\nBaseline\nwithout re-sampling\n30.18\n24.78\n61.73\n57.90\nRandom\nunder-sampling\n28.18\n21.33\n60.21\n55.97\nRandom\nover-sampling\n28.88\n21.52\n59.76\n55.90\nClass-balanced\nsampling\n29.04\n21.34\n59.56\n55.67\nSquare-root\nsampling\n31.31\n22.21\n61.02\n57.05\nP-B\nsampling\n33.48\n24.58\n61.35\n56.93\nCAM-based\nunder-sampling\n24.98\n19.15\n58.99\n54.17\nCAM-based\nover-sampling\n24.87\n18.82\n58.45\n54.36\nCAM-based\nbalance-sampling\n24.63\n18.60\n58.27\n54.05\nCAM-based\nsquare-sampling\n28.14\n20.69\n60.07\n55.61\nCAM-based\nprogressive-sampling\n27.39\n19.46\n59.67\n55.28\nImageTrans\nbalance-sampling\n28.10\n21.60\n59.28\n55.05\nTable 7: Top-1 error rates of different re-sampling methods\nused in DRS. The proposed CAM-based sampling delivers\nbetter results. In particular, CAM-based balance-sampling\nobtains the best results. “P-B” represents “Progressivelybalanced”.\nFirst\nstage\nSecond stage\nof DRW\nCIFAR-10-LT\nCIFAR-100-LT\nImbalance factor\n100\n50\n100\n50\nCE\nCE\n30.18\n24.78\n61.73\n57.90\nFocal loss\n29.71\n23.77\n61.74\n57.32\nCB Focal\n25.62\n21.25\n61.99\n55.54\nCS CE\n25.31\n20.81\n58.92\n54.57\nTable 8: Top-1 error rates of different re-weighting methods\nused in DRW. CS CE obtains the best results in DRW training\nschedule.\nweighting or re-sampling method learn good feature\nrepresentations but suffer poor recognition accuracy\non under-represented tail categories. Cui et al. (2018)\nﬁne-tune these networks on balanced subsets to make the\nlearned features from imbalanced datasets be transferred\nand re-balanced among all categories. These ﬁne-tuning\nmethods (Cao et al. 2019) can be divided into two sections:\ndeferred re-balancing by re-sampling (DRS) and by\nre-weighting (DRW).\n• DRS uses the vanilla training schedule ﬁrstly, and then\napplies re-sampling for balanced ﬁne-tuning. In order to get\na balanced subset for ﬁne-tuning, re-sampling methods inFirst\nstage\nSecond\nstage\nCIFAR-10-LT\nCIFAR-100-LT\nImbalance factor\n100\n50\n100\n50\nCE\nCE\n30.18\n24.78\n61.73\n57.90\nCS CE\n25.31\n20.81\n58.92\n54.57\nCAM-BS\n24.63\n18.60\n58.27\n54.05\nCS CE + CAM-BS\n24.82\n18.96\n58.36\n54.09\nTable 9: Top-1 error rates of different strategies to apply DRW\nand DRS. Applying DRS (CAM-based balance-sampling)\nonly shows the best result. “CAM-BS” represents “CAMbased balance-sampling”.\nTraining\nscheduler\nMixup\ntraining\nCIFAR-10-LT\nCIFAR-100-LT\nImbalance factor\n100\n50\n100\n50\nDRS with\nCAM-BS\nManifold mixup\n22.65\n19.17\n57.20\n56.94\nInput mixup\n21.88\n17.94\n53.94\n50.04\nTable 10: Top-1 error rates of combining mixup methods\nwith other best tricks. We can easily ﬁnd that input mixup\nobtains larger gains over manifold mixup. “CAM-BS” represents “CAM-based balance-sampling”. In mixup, α is 1 and\nmainifold mixup’s location is set to the pooling layer.\ntroduced in the section of “Re-sampling methods” will be\napplied. Furthermore, we propose a sample yet effective generative sampling method termed “CAM-based sampling”.\n• DRW uses the vanilla training schedule in the ﬁrst stage,\nand then applies re-weighting methods in the second stage.\nRe-weighting methods introduced in the section of “Reweighting methods” will be applied in the second stage.\nThe Proposed CAM-Based Sampling for DRS\nExisting\nre-sampling methods used in DRS only replicate or remove\nrandomly selected samples from the original dataset to generate balanced subsets, which deliver limited improvements during balanced ﬁne-tuning. In order to generate discriminative\ninformation, inspired by class activation maps (CAM) (Zhou\net al. 2016), we propose CAM-based sampling, which shows\na signiﬁcant accuracy improvement over existing methods\nwith a marginal extra cost.\nAs illustrated in Figure 1, we ﬁrstly apply re-sampling to\nget balanced sampled images. For each sampled image, we\nuse the parameterized model trained in the ﬁrst training stage\nto generate CAM based on its ground truth label and corresponding fully-connected layer’s weights. The foreground\nand background are separated based on the average value\nof its CAM, where the foreground contains pixels larger\nthan the average and the background contains the rest (Wei\net al. 2017). Finally, we apply transformations to the foreground while keeping the background unchanged. The transformation (implemented by Huawei MindSpore) includes\nhorizontal ﬂipping, translation, rotating and scaling, and we\nrandomly choose only one transformation for each image.\nIn concretely, we combine CAM with random oversampling, random under-sampling, class-balanced sampling,\nsquare-root sampling, and progressively-balanced sampling,\nDatasets\nCIFAR-10\nCIFAR-100\niNat 18\nImageNet-LT\nImbalance factor\n100\n50\n100\n50\nBaseline (Vanilla ResNet)\n30.18\n24.78\n61.73\n57.90\n39.89\n65.99\n+ IM & DRS with CAM-BS\n21.88\n17.94\n53.94\n50.04\n29.72\n58.13\n+ ft. after mixup training\n19.97\n16.41\n52.17\n48.31\n29.13\n56.87\nTable 11: Reductions of top-1 error rates with incremental tricks. Our bag of tricks shows a steady trend of accuracy improvement,\nwhich proves the effectiveness of our tricks on both small and large scale real-world datasets. “iNat 18” represents “iNaturalist\n2018” and “IM” represents “input mixup”. α is 1 in input mixup. “CAM-BS” represents “CAM-based balance-sampling”\nwhich are named “CAM-based over-sampling”, “CAM-based\nunder-sampling”, “CAM-based balance-sampling”, “CAMbased square-sampling”, and “CAM-based progressivesampling”, respectively.\nExperimental Results\nThe results of re-sampling methods\nin DRS are shown in Table 7. We add a sampling method\nnamed image transferring balance-sampling (ImageTrans\nbalance-sampling) to prove the effectiveness of our CAMbased balance-sampling. Its pipeline is the same as CAMbased balance-sampling, but without using CAM to separate\nthe foreground and background.\nFrom the results in Table 7, we have the following observations: 1) Compared with applying re-sampling directly in\nTable 4, applying re-sampling in DRS delivers better results.\n2) Our proposed CAM-based sampling obtains substantially\nlarge gains. 3) In CAM-based sampling, CAM-based balancesampling delivers the best results. 4) The results of ImageTrans balance-sampling prove the effectiveness of CAM used\nin our CAM-based balance-sampling.\nTable 8 shows the results of different re-weighting methods\nin DRW. From the results, we observe that: 1) compared\nwith apply re-weighting directly in Table 3, combining reweighting with DRW delivers better results, and 2) DRW\nwith CS CE obtains the best results.",
        "trick combinations": "In this section, we ﬁrst review the conﬂictual tricks in each\ntrick family, which obtain comparable results. We combine\nthese conﬂictual tricks with other best tricks across trick families, in order to ﬁnd the best trick combination. Furthermore,\nwe apply the best trick combination incrementally to show\nthe negligible conﬂicts between these tricks.\nRemoving Conﬂictual Tricks in Each Trick Family\nExperiments in the section of “Two stage training procedures”\nhave shown the best training schedule of two-stage training\nis DRS with CAM-based balance-sampling and DRW with\nCS CE, but DRS and DRW are both two-stage training tricks,\nwe need more experiments to explore the best strategy to\napply them. Moreover, in mixup training, input mixup and\nmanifold mixup achieve comparable results, as shown in\nTable 5. Thus, we conduct more experiments to compare\ntheir results when they are combined with other tricks.\nResults in Table 9 show that the best strategy of applying two-stage training is DRS with CAM-based balancesampling. We can also ﬁnd that combining CS CE and CAMbased balance-sampling together cannot further improve the\naccuracy, since both of them try to enlarge the inﬂuence of\ntail classes and the joint use of the two could cause an accuracy drop due to the overﬁtting problem. Furthermore, from\nTable 10, we observe that input mixup obtains substantially\nlarger gains over manifold mixup when combined with other\nbest tricks.\nFrom experiments in each trick family and trick combinations, we ﬁnd the optimal trick combination is input mixup,\nDRS with CAM-based balance-sampling, and ﬁne-tuning\nafter mixup training, which we name as bag of tricks.\nApplying the Best Tricks Incrementally\nIn order to demonstrate the performances and negligible conﬂicts of our bag of tricks, we apply these tricks incrementally on long-tailed datasets, including large scale real-world\ndatasets iNaturalist 2018 and ImageNet-LT. By considering\nthat we use CAM-based balance-sampling in DRS with input\nmixup, in ﬁne-tuning after mixup training, we also adopt\nclass-balanced sampling to maintain the learned features.\nThe results are shown in Table 11. From the results, we\nﬁnd that 1) by stacking input mixup, DRS with CAM-based\nbalance-sampling, ﬁne-tuning after mixup training, the results are steadily improved, 2) the results on iNaturalist 2018\nand ImageNet-LT demonstrate the effectiveness of our bag of\ntricks on real-world large scale datasets clearly, and 3) with\nall of our tricks, we reduce about 10% error rates on all longtailed datasets, which demonstrates signiﬁcant improvements\ncompared with existing state-of-the-art methods.",
        "conclusion": "In this paper, we systematically explored existing simple\nyet effective long-tail related tricks and provided a scientiﬁc\nexperimental guideline for long-tailed visual recognition. Furthermore, we found that existing simple sampling methods\nare lack of discriminative information. Motivated by this,\nwe proposed a novel data augmentation approach based on\nthe class activation maps and combined it with existing resampling methods. By conducting extensive experiments,\nwe obtain the optimal trick combination, i.e., bag of tricks,\ncontained negligible conﬂicts and achieved the best results\non long-tailed benchmarks without introducing extra FLOPs.\nWe also release our source codes as a scientiﬁc and practical\ntoolbox, which could beneﬁt future researches of long-tailed\nvisual recognition. In the future, we attempt to explore bag\nof tricks in other challenging long-tailed tasks, e.g., detection\nand segmentation.",
        "summary_en": "In recent years, visual recognition on challenging long-tailed distributions, where classes often exhibit extremely imbalanced frequencies, has made great progress mostly based on various complex paradigms (e.g., meta learning). Apart from these complex methods, simple refinements on training procedures also make contributions. These refinements, also called tricks, are minor but effective, such as adjustments in the data distribution or loss functions. However, different tricks might conflict with each other. If users apply these long-tail related tricks inappropriately, it could cause worse recognition accuracy than expected. Unfortunately, there has not been a scientific guideline of these tricks in the literature. Therefore, this paper first collects existing tricks in long-tailed visual recognition and then perform extensive and systematic experiments, in order to give a detailed experimental guideline and obtain an effective combination of these tricks. Furthermore, the paper also proposes a novel data augmentation approach based on class activation maps for long-tailed recognition, which can be friendly combined with re-sampling methods and shows excellent results. By assembling these tricks scientifically, the paper can outperforms state-of-the-art methods on four long-tailed benchmark datasets, including ImageNet-LT and iNaturalist 2018.",
        "summary_zh": "这篇论文研究了深度卷积神经网络在长尾分布上的视觉识别任务。近年来，长尾分布上的视觉识别取得了很大的进步，这主要是基于各种复杂的方法，但简单的训练技巧也同样发挥了作用。然而，不同的技巧可能会相互冲突，若不恰当地应用这些长尾相关技巧，可能导致比预期更差的识别准确度。论文收集了长尾视觉识别中现有的技巧，并进行了广泛而系统的实验，提供了详细的实验指南并获得了这些技巧的有效组合。此外，论文还提出了一种基于类激活图的长尾识别数据增强方法，可与重新采样方法友好地结合，并展现出卓越的效果。通过科学地组合这些技巧，该方法在ImageNet-LT和iNaturalist 2018等四个长尾基准数据集上的表现超越了最先进的方法。"
    },
    {
        "title": "Progress and Limitations of Deep Networks to Recognize Objects in Unusual Poses",
        "abstract": "Deep networks should be robust to rare events if they are to be successfully deployed in high-stakes real-world applications. Here we study the capability of deep networks to recognize objects in unusual poses. We create a synthetic dataset of images of objects in unusual orientations, and evaluate the robustness of a collection of 38 recent and competitive deep networks for image classification. We show that classifying these images is still a challenge for all networks tested, with an average accuracy drop of 29.5% compared to when the objects are presented upright. This brittleness is largely unaffected by various design choices, such as training losses, architectures, dataset modalities, and data-augmentation schemes. However, networks trained on very large datasets substantially outperform others, with the best network tested—Noisy Student trained on JFT-300M—showing a relatively small accuracy drop of only 14.5% on unusual poses. Nevertheless, a visual inspection of the failures of Noisy Student reveals a remaining gap in robustness with humans. Furthermore, combining multiple object transformations—3D-rotations and scaling—further degrades the performance of all networks. Our results provide another measurement of the robustness of deep networks to consider when using them in the real world. Code and datasets are available at https://github.com/amro-kamal/ObjectPose.",
        "introduction": "In real-world applications, deep networks are often deployed\nand used to detect harder examples than those seen in the\ndevelopment test set. This has led researchers to investigate the performance of networks using more challenging\ntest examples, in the so-called out-of-distribution (OOD)\nregime [Hendrycks et al. 2021b, Hendrycks and Dietterich\n2019, Hendrycks et al. 2021a, Wang et al. 2019, Dodge and\nKaram 2017, Recht et al. 2019a, Shankar et al. 2021, Geirhos\net al. 2018, 2021]. Many of the previous studies on out-ofdistribution generalization have focused on measuring the\ngeneralization capabilities of networks to distorted images.\nNotably, [Geirhos et al. 2021] show that the newest generation of very large deep networks is closing the humanmachine robustness gap on 17 different out-of-distribution\nimage distortion types. However, most of the transformations\nused to generate these datasets are local distortions that only\naffect the texture of objects, but do not change the global\nstructure of the image. [Taori et al. 2020] shows that robustness to these kinds of distortions does not transfer entirely to\nnatural shifts and does not represent a comprehensive measure of the network’s robustness.\nPose transformations (e.g., a bus seen upside-down)\nrepresent an interesting case-study for networks’ robustness,\nas (1) unlike simple distortions, these transformations\naffect the global structure of the image, and (2) it would\nbe technically challenging to augment an image dataset\nat scale with this type of 3D transformations. In a rare\nstudy of its kind, [Alcorn et al. 2019] show that two\ndeep networks, Inception-V3 [Szegedy et al. 2016] and\nResNet50 [He et al. 2016], drastically fail to recognize objects in unusual poses, incorrectly classifying\nmost of the poses explored. Later, [Madan et al. 2021]\nalso reveal a brittleness of ResNet18 and CLIP [Radford\net al. 2021] to small changes in pose, in an adversarial setting.\nIn this study, we revisit the question of networks’ robustness to unusual poses using a diverse set of the latest and best\npublicly available networks for image classification. We test\n38 networks with a variety of different architectures, sizes,\ntraining datasets, and training objectives on a custom dataset\nof images of objects in unusual poses. Our contributions are:\n• We observe that, on unusual poses, the networks of our\ncollection suffer from a 14.5% to 45.5% accuracy drop\ncompared to usual poses. A visual inspection of networks’\nfailures reveal that, even for the best models, a robustness\ngap remains with the human visual system.\n• We provide a detailed study of the effect of in-(image)plane vs. out-of-plane object rotations, backgroundforeground congruency, and rotation angle on performance. We show that the best networks rely on a different strategy than weaker networks when it comes to\nincorporating information from the background.\n• By combining multiple unusual transformations—such\nas object rotations and scaling—we show that such combinations lead to further performance degradation for all\nnetworks, as predicted by a combinatorial model of error.\n• In an effort to go beyond synthetic datasets, we test the\nnetworks on images from the Common Objects in 3D\nDataset (CO3D), a dataset of objects seen in various poses,\nFigure 1: Performance of all networks on ObjectPose. (A) Rotating the objects in unusual poses (ObjectPose, colored bars)\ninduces a top-1 accuracy drop of 14.5%-45.5% compared to when objects are presented upright (ObjectPose +-10, grey bars).\nBar colors indicate different architectures. Blue dots: Top-1 accuracy on ImageNet (as reported in the papers). (B) Accuracy on\nusual vs. unusual poses. Networks trained on ImageNet1k cluster together with low accuracy on ObjectPose. Networks trained\non ImageNet21k perform better, and networks trained on extremely large datasets—Noisy Student models (300M images) and\nSWAG (3.6B images)—perform the best, with the exception of CLIP models (400M images), which were not fine-tuned on\nImageNet categories.\nand show a generalization gap of 5.2% on average across\nnetworks compared to a benchmark of objects presented\nin their usual pose, ImageNetV2.",
        "dataset and networks": "ObjectPose Dataset\nWe generated a synthetic dataset of\nobjects in unusual poses, ObjectPose. The dataset contains\n27,540 images of 17 high-quality 3D objects rendered in a\nrange of different orientations and over different background\nimages, following the pipeline of [Alcorn et al. 2019]. Briefly,\nthe object is first placed in an initial upright position. We then\nchoose one of the YAW, ROLL, or PITCH axes to rotate the\nobject along it and render it on top of a background image.\nWe used three different background images with each object.\nTwo of the backgrounds are images chosen manually from the\ninternet to match the object’s usual context and not to contain\nany other ImageNet object. We chose the third background\nto be grey with all its RGB pixel values equal to (0.485,\n0.456, 0,406), corresponding to the average pixel color of\nImageNet images. In order to focus on robustness to unusual\nposes, each object is chosen carefully so that the resulting\nimages for that object are correctly classified with > 90%\naccuracy by a ResNet-50 when the object’s orientation is\nless than 10° apart from its upright pose. We gather these\nimages where the object is rotated by -10° to 10° only in\nthe ObjectPose +-10 dataset (1,683 images in total), and\nexclude them from ObjectPose, such that ObjectPose only\ncontains unusual poses (11° to 349° from the upright pose).\nEach of the 17 objects belonging to one of 1000 ImageNet\n[Russakovsky et al. 2015] classes.\nDeep Networks We tested a collection of 38 networks on\nObjectPose. We chose a diverse set of networks with different\narchitectures, training datasets sizes, number of parameters,\nand training objectives. The networks have varying number\nof parameters (from 22M up to 645M), and different architectures including convolutional neural networks (CNNs) [He\net al. 2016, Xie et al. 2020, Kolesnikov et al. 2020, Chen\net al. 2020b, Liu et al. 2022], Vision Transformers (ViTs)\n[Dosovitskiy et al. 2020, Chen, Hsieh, and Gong 2021, Liu\net al. 2021] [Bao, Dong, and Wei 2021, Touvron et al. 2021],\nand MLP-Mixers [Tolstikhin et al. 2021]. We also include\nConViT [d’Ascoli et al. 2021], a hybrid CNN-ViT architecture that adds a convolutional inductive bias to the Vision\nTransformer.\nWe chose networks trained under different objectives, including (1) Supervised learning, including convolutional architectures, such as [He et al. 2016, Xie et al. 2020, Liu et al. 2022,\nKolesnikov et al. 2020], Vision Transformers, such as [Dosovitskiy et al. 2020, Chen, Hsieh, and Gong 2021, Bao, Dong,\nand Wei 2021, Touvron et al. 2021, Liu et al. 2021, 2022],\nand MLP-mixer [Tolstikhin et al. 2021], (2) Self-supervised\nlearning, such as SimCLR [Chen et al. 2020a], and BEiT\n[Bao, Dong, and Wei 2021], (3) Semi-weakly supervised\nlearning, such as SWSL-ResNet50 and SWSL-ResNeXt101\nFigure 2: Presentation of the 17 3D objects composing ObjectPose. These objects were carefully selected from\nhttps://sketchfab.com/.\nFigure 3: Selected failures of Noisy Student EfficientNet-L2—the best-performing network of our collection—on ObjectPose.\nLeft column: Objects presented upright and top-5 predictions from the network (as measured by the softmax layer activations).\nOther columns: Objects presented in incorrectly classified poses and top-5 predictions from the network. Some of these errors\nreveal a brittleness compared to the human visual system (e.g., a tank at 90° is confused with a shield).\n[Chen et al. 2020b], and weakly supervised learning, such\nas SWAG [Singh et al. 2022], (4) Text supervision, such as\nCLIP [Radford et al. 2021].\nThe networks were trained on datasets with different sizes\nranging from 1M to 3.6B images. Among the networks we\nuse, Noisy Student EfficientNet [Xie et al. 2020] (300M\nimages) and SWAG [Singh et al. 2022] (3.6B images) are\nthe only networks pretrained on extremely large datasets\nand fine-tuned on ImageNet. Although CLIP [Radford\net al. 2021] was pretrained on a very large dataset (400M\nimage-caption examples), it was not fine-tuned on ImageNet.\nThe best network of our collection according to its performance on ImageNet is the SWAG-RegNetY-128GF-384\nmodel [Singh et al. 2022], pretrained with a weaklysupervised learning approach on 3.6B Instagram images (IG).\nIt achieves 88.55% ImageNet top-1 accuracy.",
        "results": "All networks exhibit a performance drop on unusual\nposes compared to usual poses (Fig. 1).\nWe measure\nnetworks’ robustness to unusual object poses by testing\ntheir accuracy on our synthetic dataset, ObjectPose. Our\ncollection of networks show a top-1 accuracy drop in a\nrange of 14.5%-45.5% on unusual poses compared to usual\nposes (ObjectPose +-10). Examples of network failures are\nshown in Fig. 3 for the best model tested, Noisy Student\nEfficientNet-L2.\nScaling both training dataset size and network capacity is helpful on ObjectPose. Networks trained on larger\ndatasets show a narrower performance gap than networks\ntrained on smaller datasets (Fig. 1B), with the exception of\nCLIP models which were not fine-tuned on ImageNet. It is\nalso important for the network to have a sufficient size (i.e.\nnumber of parameters) to benefit from the large dataset. This\nis deduced by the performance of the EfficientNet-L2 model\n(480M parameters) compared to EfficientNet-B7 (66M parameters), both trained using the Noisy Student method on the\nJFT-300M dataset. While EfficientNet-L2 outperforms the\nrest of the networks on ObjectPose, EfficientNet-B7 performs\non par with many networks pretrained on smaller datasets\n(such as ImageNet21k).\nAlthough SWAG models are pretrained on more images\n(3.6B) and have more parameters, they do not outperform\nNoisy Student EfficientNet-L2. This also shows that scaling\nthe dataset is not the only factor that matters, here the training\nprocedure and architecture also play an important role. In\nparticular, Noisy student was trained using the RangAugment\ndata augmentation method [Cubuk et al. 2020] which applies\nFigure 4: (A) In-plane rotation accuracy. Most networks are slightly more robust to in-plane rotations (colored bars) than\nout-of-plane rotations (grey bars). Exceptions are marked by red arrows. (B) Image Rotation. The best networks benefit from the\nbackground image being rotated with the object (colored bars), unlike most weaker networks which prefer an upright background\n(in-plane condition, grey bars), revealing a difference in strategy between these two groups of networks.\nintensive data augmentation including rotations and shears.\nIn contrast, SWAG only used a limited augmentation strategy of cropping and flipping and not including rotations and\nshears.\nA visual inspection of networks’ failures reveals room\nfor improvement even for the best networks tested (Fig.\n3). By visually inspecting the errors made by the networks\n(Fig. 3), we find that even the best model tested, Noisy\nStudent EfficientNet-L2 (NS), makes errors that a human\nobserver would not.\nWhich types of rotation are most problematic for deep\nnetworks (Fig. 4)? We compare the effect of object rotations\nin the plane of the image, vs. out-of-plane rotations seen\nin ObjectPose (Fig. 4A). We find that both conditions are\nproblematic for all networks, with the out-of-plane condition\nonly slightly worse than the in-plane condition for most\nnetworks. We then compare the in-plane condition with\nsimple image rotations, where the background is rotated with\nthe foreground object (Fig. 4B). By comparing these two\nconditions, we find that the best networks on ObjectPose\nrely on a different strategy than the weaker networks when\nit comes to incorporating background information. Indeed,\nweak networks perform better on in-plane rotations than on\nimage rotations, in contrast to the best networks (e.g., Noisy\nStudent EfficientNets and SWAG) which perform better on\nimage rotations. Our interpretation is that weaker networks\nbenefit from seeing features of the background upright,\nwhereas the best networks suffer from the incongruency\nbetween the upright background and the rotated foreground\nobject. The interpretation that the best networks suffer from\nthe incongruency of the background is reinforced by the\nobservation that they are the only ones to see an increase\nin accuracy on ObjectPose when the natural backgrounds\nare removed altogether. The interpretation that the weaker\nnetworks benefit from seeing the background features\nupright is confirmed by the observation that their accuracy\ndrops when the background alone is rotated.\nWe next study the relation between network accuracy\nand rotation angle for different rotation types (Fig. 5).\nWe find that all networks are most fragile when the object is\nrotated by 90° in the out-of-plane condition (ObjectPose). We\nFigure 5: Accuracy decreases as we increase rotation angle, with a low point at 90°. Networks are less robust to (A) out-of-plane\nrotations than (B) in-plane rotations and (C) image rotations. Noisy Student EfficientNet models are very robust to image rotations,\nperhaps for the reason that they were trained with this type of augmentation (RandAugment). Yet they are not completely robust\nto the two other types of rotations.\nalso find that the best networks are more robust across the full\nrange of rotation angles in the in-plane and image-rotation\nconditions than in the out-of-plane condition. Noisy Student\nEfficientNets are especially robust to image rotations, which\nmight be explained by the fact that they are trained with\nimage-rotation augmentations. Yet, we see that this type of\ndata augmentation is not enough to guarantee full robustness\nto in-plane and out-of-plane rotations.\nCombining more than one transformation degrades performance of all networks further, as predicted by a combinatorial model of error (Fig. 6). We next sought to study\nhow the combination of multiple transformations would affect the performance of the networks. For this set of experiments, we use a grey background for all images in order to\navoid complex interferences between foreground and background. First, we try combining rotations along the three axes\nof rotations, YAW, PITCH, and ROLL together. We find that\nthis combination leads to a degradation of performance for\nall networks (Fig. 6A) in a range 6%-16.5% compared to the\ncondition where only one axis is rotated at a time. Next we\ninvestigate the effect of combining three-axes rotations with\nscaling (Fig. 6C). We find that this combination of transformations further degrades the performance of all networks,\nwith an accuracy drop in the range 24.5%-78.2% compared\nto usual poses, larger than the accuracy drop seen for ObjectPose in the range 14.5%-45%. We find that a simple combinatorial model of errors (grey bars in Fig. 6C) recapitulates\nthe accuracy drop well for most of the networks: this model\nassumes that the probability of the network being correct in\nthe scaled-rotated condition (panel C) is simply the product\nof the probabilities of being correct in the scaled-only (panel\nB) and rotated-only (panel A) conditions respectively (see\nDiscussion for implications).\nHow well do our findings on synthetics datasets transfer to real-world datasets (Fig. 7)? In an attempt to go\nbeyond synthetic datasets, we explore the robustness of our\ncollection of networks to a dataset of real objects filmed from\nvarious points of view, the Common Objects in 3D Dataset\n(CO3D) [Reizenstein et al. 2021]. Originally designed for\n3D reconstruction and new-view synthesis tasks, this dataset\nwas collected by workers turning around and filming common objects from their environment. We sample 1000 images\nfrom each of 10 categories common to CO3D and ImageNet,\nto get a total of about 10,000 images. We then estimate the\nperformance of our collection of networks on these images\n(Fig. 7). As a point of comparison, we measure the accuracy\nof networks on images of the same object categories taken\nfrom ImageNetV2, a datatset where objects are mostly presented in their usual canonical view. ImageNetV2 can be seen\nas a fairer comparison benchmark to CO3D than ImageNet,\nas networks were not overfitted to the the exact statistics of\nImageNetV2 [Recht et al. 2019b]. We observe an average accuracy drop across networks of 10.4% on ImageNetV2 over\nImageNet, and an average accuracy drop of 5.2% on CO3D\nover ImageNetV2 (Fig. 7A). However, four networks trained\non very large datasets perform nearly as well on both datasets:\nCLIP-RN-101, CLIP-ViT-B/16, Noisy Student EfficientNetB7, and SWAG-ViT (Fig. 7B). In summary, the variety of\nFigure 6: Effect of combining multiple transformations on performance. (A) Combining rotations along the three axes ROLL,\nPITCH and YAW decreases the accuracy of all networks (colored bars) compared to the single-axis rotation condition (grey\nbars). (B) Scaling the object size in the image decreases the accuracy severely for most networks (colored bars) compared to\nthe fixed-scaled upright condition (grey bars), but only slightly for the best networks. (C) Combining three-axes rotations and\nscaling strongly affects the accuracy of all networks (colored bars), with the best network (Noisy Student) at 75% accuracy only\nin this condition. The grey bars represent a combinatorial model of error which predicts performance degradation well (see text).\nobject views seen in CO3D seems less problematic for our\ncollection of networks than the views from of our synthetic\ndataset ObjectPose. This discrepancy could be due to the fact\nthat although the workers turn around the objects in CO3D,\nthey do not necessarily explore all the unusual views that we\ncan explore in our synthetic dataset.\nFigure 7: Going beyond synthetic datasets. Most networks perform worse on CO3D (colored bars), a dataset exploring various\nobject views, than on ImageNetV2 (grey bars), which mostly presents objects in their usual canonical views (average accuracy\ndrop of 5.2% between the two datasets). ImageNetV2 is a fairer comparison benchmark to CO3D than ImageNet as there was no\noverfitting it.",
        "discussion": "Probing the robustness of deep networks to objects in unusual poses is interesting for multiple reasons. First, this type\nof transformation changes the global structure of the image,\nwhich might represent a bigger challenge for deep networks\nthan local image distortions. Second, data-augmentation\nstrategies do not provide an easy fix to the problem of generalization to unusual poses. Indeed, a classical mitigation\nstrategy in deep learning consists in augmenting the dataset\nwith the out-of-distribution case that one would like to become robust to, effectively making that case in-distribution.\nFor most image distortions, this augmentation strategy is easy\nto implement. But for objects in unusual poses, one would\nneed to collect a very large number of images of objects in unusual poses, which is practically challenging, especially at the\nscale needed to compete with the best current networks (e.g.,\nNoisy Student is trained on JFT-300M comprising 300 million images). Alternatively, one could synthetically generate\nsuch a dataset with 3D models of objects, but this would require a very large database of high-quality 3D models which\nis currently lacking.\nA striking result of our study is that very large networks\ntrained on very large datasets (e.g., Noisy Student trained\non JFT-300M, SWAG trained on 3.6B Instagram images)\nare quite robust to unusual poses. However, a careful visual\ninspection of the errors made by these networks reveal that\nthey do not yet meet the robustness of the human visual\nsystem. A thorough comparison to human would be useful to\nestimate the fraction of the errors that are due to the images\nthemselves not containing enough information about the class\n(e.g., a jeep seen from the bottom could be any sort of car) vs.\nout-of-distribution generalization errors that a human would\nnot fall into (e.g., a cannon seen at a 90° angle is confused by\nthe network with a harp). In future work, we plan to establish\nthis human benchmark on our custom dataset ObjectPose\nin order to precisely quantify the gap in robustness between\nhumans and networks.\nWhen combining object rotations and scaling, we find that\na combinatorial model of error—which assumes that the probability of the network being correct on the combination of\ntransformations is equal to the product of the probabilities of\nit being correct on each respective transformation—accounts\nwell for the degradation of performance of most networks.\nIt would be interesting to study whether networks’ performance degrade according to this combinatorial model when\ncombining even more transformations, such as translations,\ntexture-removals etc. Indeed, an implication of this combinatorial model of error is that networks should be brittle in the\nface of a large combination of transformations, as each factor\nof variation adds its own source for potential errors.\nIt is an open question how the human visual system builds\nrobustness to unusual poses. When performing mental rotation, a task consisting in comparing two 3D shapes in different orientations, human subjects take a time to respond that\nscales linearly with the angle of rotation between the two\nshapes [Shepard and Metzler 1971]. A similar phenomenon\nhappens when recognizing every-day-life objects in unusual\norientations [Jolicoeur 1985]: the recognition time is again\nlinear with the angle of the object with respect to its upright\npose. These observations are striking as they suggest that\ndifferent mechanisms take place in the brain compared to\nfeed-forward deep networks, where the processing time is\nfixed and does not depend on the complexity of the input.\nWe hypothesize that recurrent mechanisms may play a key\nrole in recognizing objects in unusual poses in the brain, as\nthere is mounting evidence that such mechanisms are critical\nfor recognizing images that are challenging to deep networks\n[Kar et al. 2019, Bonnen, Yamins, and Wagner 2021].",
        "conclusion":"",
        "summary_en": "Deep networks should be robust to rare events if they are to be successfully deployed in high-stakes real-world applications. This paper studies the capability of deep networks to recognize objects in unusual poses. The paper creates a synthetic dataset of images of objects in unusual orientations, and evaluate the robustness of a collection of 38 recent and competitive deep networks for image classification. The paper shows that classifying these images is still a challenge for all networks tested, with an average accuracy drop of 29.5% compared to when the objects are presented upright. This brittleness is largely unaffected by various design choices, such as training losses, architectures, dataset modalities, and data-augmentation schemes. However, networks trained on very large datasets substantially outperform others, with the best network tested—Noisy Student trained on JFT-300M—showing a relatively small accuracy drop of only 14.5% on unusual poses. Nevertheless, a visual inspection of the failures of Noisy Student reveals a remaining gap in robustness with humans. Furthermore, combining multiple object transformations—3D-rotations and scaling—further degrades the performance of all networks. The results of the paper provide another measurement of the robustness of deep networks to consider when using them in the real world.",
        "summary_zh": "这篇论文研究了深度网络在识别不寻常姿势物体时的局限性。作者创建了一个包含不寻常方向物体图像的合成数据集，用来评估最新的38个深度网络在图像分类方面的鲁棒性。实验结果表明，对所有网络来说，分类这些图像仍是一个挑战，与物体直立呈现时相比，平均准确率下降了 29.5%。然而，在大型数据集上训练出来的网络表现更好，表现最好的网络是在 JFT-300M 上训练的 Noisy Student网络，其准确率下降相对较小，仅为 14.5%，但该网络的鲁棒性与人类相比仍有差距。此外，结合多种对象变换的图像会进一步降低所有网络的表现。本文的研究结果提供了另一种衡量深度网络鲁棒性的方法，供在现实世界中使用它们时参考。"
    },
    {
        "title": "Quantum-Inspired Representation for Long-Tail Senses of Word Sense Disambiguation",
        "abstract": "Data imbalance, also known as the long-tail distribution of data, is an important challenge for data-driven models. In the Word Sense Disambiguation (WSD) task, the long-tail phenomenon of word sense distribution is more common, making it difﬁcult to effectively represent and identify LongTail Senses (LTSs). Therefore exploring representation methods that do not rely heavily on the training sample size is an important way to combat LTSs. Considering that many new states, namely superposition states, can be constructed from several known states in quantum mechanics, superposition states provide the possibility to obtain more accurate representations from inferior representations learned from a small sample size. Inspired by quantum superposition states, a representation method in Hilbert space is proposed to reduce the dependence on large sample sizes and thus combat LTSs. We theoretically prove the correctness of the method, and verify its effectiveness under the standard WSD evaluation framework and obtain state-of-the-art performance. Furthermore, we also test on the constructed LTS and the latest cross-lingual datasets, and achieve promising results.",
        "introduction": "Data imbalance is very common in the ﬁeld of data mining (Abd Elrahman and Abraham 2013). For real data, the\nsample sizes of different categories are generally not uniform distributions, but unbalanced, such as the most common long-tailed distribution. Large datasets often exhibit\nsuch a long tail phenomenon (Yang and Xu 2020). For datadriven models, data imbalance directly leads to difﬁculty\nin obtaining effective representation and recognition for tail\ncategories (Ntoutsi, Fafalios, and Gadiraju 2020).\nWord Sense Disambiguation (WSD) is to determine the\nword sense of the target word according to the given context, which belongs to the basic research topic in natural\nlanguage processing (Bevilacqua et al. 2021; Navigli 2009).\nBut for high-level tasks based on natural language understanding, an accurate identiﬁcation of word senses at the lexical level can effectively improve the overall performance.\nDue to the long-tail phenomenon of word sense distribution\nin linguistics, the Long-Tailed Senses (LTSs) lack sufﬁcient\ntraining samples to complete effective feature representation, and then obtain correct recognition (Aliwy and Taher\n2019; Bevilacqua et al. 2021).\nResearchers employ data augmentation methods, such as\noversampling (Fujii et al. 1998), data synthesis (Bolshina\nand Loukachevitch 2020), multilingualism (Scarlini, Pasini,\nand Navigli 2020a), etc., to improve the disadvantaged position of LTSs. These methods are not universally effective\nand destroy the statistical laws inherent in the data. Methods of changing learning strategies have also been extensively studied, such as transfer learning (Kohli 2021), metalearning (Holla et al. 2020), few-shot learning (Kumar et al.\n2019), etc.\nKumar et al. (Kumar et al. 2019) adjusted the discrete label space often used in the WSD task into a continuous sense\nembedding space, resulting in state-of-the-art performance\nin experiments. The success of this scheme is not only because the sense embeddings replace the original labels, but\nalso more importantly because the continuous space constraint helps to obtain correct sense representations. Compared with the Euclidean space without constraints, continuous space constraints can compress the range of parameter\nvalues and reduce the number of parameters.\nConsidering that quantum states are constructed in Hilbert\nspace, it naturally has the advantage of continuous space\nconstraints. In addition, considering that many new quantum\nstates, that is, superposition states (Nielsen and Chuang\n2002), can be constructed from several known quantum\nstates in quantum mechanics, the concept of superposition\nstates provides the possibility to obtain more accurate representations from inferior representations learned from small\nsample sizes. In this article, we leverage quantum states to\ndescribe representations learned from insufﬁcient training\nsamples, and further obtain more accurate representations\nin their quantum state form by constructing superposition\nstates.\nIn quantum mechanics, quantum states, including superposition states, are points on the sphere of Hilbert space (the\nspecial case of entangled states is not discussed here), so\nthe representations described by quantum states have the advantage of continuous space constraints. Furthermore, learning the representations does not require adding additional\nparameters. Because building a superposition state requires\nan additional parameter, the continuous space constraint can\nsave a parameter. In general, the quantum-inspired representation method provides the ability to obtain more accurate\nrepresentations under continuous space constraints without\nincreasing the amount of parameters. See Theoretical Analysis and Methodology section for theoretical proofs and speciﬁc implementation details respectively.\nOur contributions can be summarized as follows:\n• Propose a method to obtain more accurate representations in the case of insufﬁcient training samples, inspired\nby quantum superposition states, which can alleviate the\ndependence of data-driven models on large sample sizes;\n• Theoretically prove that the quantum-inspired representation method is better than the classical one;\n• Implement experiments under the standard evaluation\nframework for the WSD task and obtain state-of-the-art\nperformance; Execute experiments under the constructed\nlong-tail sense and the latest cross-lingual datasets and\nalso achieve promising results.",
        "related work": "Long-tail Word Sense Disambiguation\nWith the continuous optimization and improvement of intelligent algorithms, high-frequency word senses can be accurately identiﬁed. At this stage, the focus of the WSD\ncommunity is gradually converging on long-tail word sense\ndisambiguation (Bevilacqua et al. 2021; Aliwy and Taher\n2019).\nBlevins et al. (Blevins and Zettlemoyer 2020) ﬁrst realized that the focus of the current WSD task should be on\nLTSs, and proposed joint training of dual encoders to enhance the representation of word sense with the knowledge\nof training samples. Kumar et al. (Kumar et al. 2019) paid\nattention to the representation and recognition of zero-shot\nword sense disambiguation, and proposed to use continuous word sense embedding space to replace discrete label\nspace to deal with unseen word senses. In addition, there\nare Refs. (Wang and Wang 2021; Wang, Zhang, and Wang\n2021a; Berend 2020; Zhang et al. 2022a).\nThe representation method proposed in this article is specially designed for the scarcity of training samples of LTSs,\nand also adopts the characteristics of continuous space,\nnamely Hilbert space.\nQuantum-inspired Models for WSD\nQuantum theory is believed to be able to reveal human cognitive behavior (Busemeyer and Bruza 2012; Bruza, Wang,\nand Busemeyer 2015), and the mathematical principles of\nquantum mechanics (namely, quantum probability theory)\nhave been extensively studied because of their superiority (Li et al. 2018; Liu, Hou, and Song 2021; Zhang et al.\n2020, 2022b,c).\nOn the WSD task, Tamburini (Tamburini 2019) adopted\nquantum probability theory to calculate the similarity between the complex word or sentence embeddings. Although\nthe model is relatively simple, it has the advantage that it\ndoes not take a lot of time during the training phase. In addition, Kumar et al. (Kumar et al. 2019) adjusted the discrete\nlabel space into a continuous sense embedding space, resulting in state-of-the-art performance in the experiments. Although this work does not mention quantum probability theory, the pure state in quantum mechanics is also established\nin a continuous space, indicating that continuous space constraints are indeed beneﬁcial to the representation and recognition of word senses.\nInspired by the above work, this article leverages the\nmathematical form of quantum superposition state to obtain\nan optimal representation under the premise of suboptimal\nrepresentation, that is, to search for a better representation\nbased on the original representation.",
        "theoretical analysis": "Preliminaries\nThis section introduces the background knowledge of Quantum Probability Theory (QPT) necessary to understand theoretical proof and model design. Note that QPT is a more\ngeneral probability theory and is perfectly compatible with\nClassical Probability Theory (CPT), so it can be used as a\nmodeling tool in information systems. See Ref. (Nielsen and\nChuang 2002) for more details.\nQuantum Events: QPT assigns probabilities to events\nlike CPT, but it deﬁnes events in the subspace of the multidimensional complex Hilbert space H ∈ Cn, unlike CPT\nthat deﬁnes events as sets.\nQuantum States: In QPT, the quantum system (i.e.,\nquantum state) is deﬁned as a complex vector |ψ⟩ ∈ H with\n∥|ψ⟩∥ = 1 using the Dirac1 notation. A more general formalization can be deﬁned as\n|ψ⟩ = φ1|e1⟩ + φ2|e2⟩ + ... + φi|ei⟩ + ...\n(1)\nwhere (φ1)2 +(φ2)2 +...+(φi)2 +... = 1, φi is a complex\nnumber called the probability amplitude, φi = ⟨ei|ψ⟩, and\n|ei⟩ is the basis of the space H. This state is called a superposition state, and its basis vectors are the basic states. It\nis also possible to construct a superposition state from other\nsuperposition states,\n|ψ⟩ = φ1|ψ1⟩ + φ2|ψ2⟩ + ... + φi|ψi⟩ + ....\n(2)\nQuantum Measurements: There is more than one type\nof quantum measurement in quantum mechanics, such as\ngeneral measurement, projection measurement and POVM\nmeasurement (Nielsen and Chuang 2002). The textbook\nmainly introduces projection measurement, while the ﬁeld\nof quantum information processing mostly uses general\nmeasurement.\nGeneral measurement is described by a set of measurement operators {Mm}, where m refers to the possible result. For example, the status of the system is |ψ⟩, and the\nprobability of the measured result m is\np(m) = p(m; Mm) = ⟨ψ|M †\nmMm|ψ⟩.\n(3)\n1In Dirac notation, |·⟩ is a column vector (or called ket), while\n⟨·| is a row vector (or called bra). Using these symbols, the inner\nproduct can be expressed as ⟨x|y⟩ and the outer product as |x⟩⟨y|.\nAlso ⟨x| = |x⟩†, where “†” marks the conjugate transpose operation on vectors or matrices.\n|𝜓⟩\n$𝜓%\n|Ψ⟩\n|𝜙⟩\n（1）\n|𝜓⟩\n$𝜓%\n|Ψ⟩\n|𝜙⟩\n（2）\nFigure 1: Illustration of the superposition state on the Bloch\nsphere. (1) refers to high-dimensional space and (2) to plane.\nAfter the measurement, the status of the system changes to\n|ψ′⟩ =\nMm|ψ⟩\nq\n⟨ψ|M †\nmMm|ψ⟩\n.\n(4)\nNote that {Mm} needs to satisfy the completeness,\nP\nm M †\nmMm = I, which reveals the fact that the sum of\nthe probabilities is 1,\nX\nm\np(m) =\nX\nm\n⟨ψ|M †\nmMm|ψ⟩ = 1.\n(5)\nTheoretical Analysis for Quantum-inspired\nRepresentation\nWhen the word or text vector ψ obtained with an insufﬁcient\ntraining sample size is represented by the quantum state |ψ⟩,\nwe can obtain a more correct representation through the superposition state constructed by |ψ⟩ and its anti-state |ψ⟩\n(the method of obtaining |ψ⟩ is given in Methodology section). The formal deﬁnition and proof of this conclusion are\nas follows.\nConclusion 1. Assuming that |φ⟩ is the correct state and |ψ⟩\nis obtained when the training sample size is insufﬁcient, the\nsuperposition state\n|Ψ⟩ = cos(θ)|ψ⟩ + sin(θ)|ψ⟩\n(6)\ncan be better than |ψ⟩, that is, there is\np(|ψ⟩; |φ⟩⟨φ|) ≤ p(|Ψ⟩; |φ⟩⟨φ|) ≤ 1\n(7)\nwhere |φ⟩⟨φ| represents a measurement operator, Mφ =\n|φ⟩⟨φ|, and this probability operation can be deﬁned as\np(|·⟩; Mφ) = ⟨·|M †\nφMφ|·⟩ = ⟨·|φ⟩⟨φ|·⟩ = (⟨φ|·⟩)2\n(8)\nby general measurement theory.\nProof. In the high-dimensional Hilbert space, the superposition states composed of |ψ⟩ and its anti-state |ψ⟩ can form\na plane, which is a great circle under the Bloch sphere description method.\n• When |φ⟩ is not on the plane formed by the superposition states, as shown in Fig. 1 (1), some superposition states on the plane are closer to |φ⟩ than |ψ⟩, i.e.,\np(|ψ⟩; Mφ) ≤ p(|Ψ⟩; Mφ).\n• When |φ⟩ is on the plane formed by superposition states,\nas shown in Fig. 1 (2), |φ⟩ can be characterized by one\nof the superposition states, i.e., p(|Ψ⟩; Mφ) = 1, and in\nother cases, it is p(|Ψ⟩; Mφ) < 1.\nNote that the proof shows that some better representations\ncan be obtained based on superposition states, but it does\nnot deny that there are worse cases. However, in speciﬁc applications, the algorithm can ﬁnally obtain a more correct\nrepresentation described by the superposition state through\noptimization learning.\nMoreover, it needs to be emphasized that this article uses\nthe method of increasing the system dimension to replace the\ncomplex number representation of the quantum system, and\ntheir functions are equivalent in the form of a single quantum system (Hardy 2001; Janotta and Hinrichsen 2014).",
        "methodology": "This section is divided into four parts: a formal description of the WSD task is given; the quantum-inspired representation method is formalized; the WSD model under\nthe Quantum-inspired Representation (called QR-WSD) is\nconstructed; the loss function and optimization method are\ngiven.\nWord Sense Disambiguation\nWSD belongs to a standard classiﬁcation task, and its core\ntask is to learn a mapping model from the target word to\nword senses. When using pre-trained language models to\nvectorize the target word and word senses, the core task is\nto match the most similar word sense embeddings embi\nsense\nfor the target word embedding embtarget,\nMin{..., d(embtarget, embi\nsense), ...}\n(9)\nwhere d(·) represents a distance metric function.\nQuantum-inspired Representation\nIn some tasks of Natural Language Processing (NLP), texts\nor words are vectorized, such as text or word embedding\nV = [..., vi, ...], and on this basis, we can construct quantum\nsystems (i.e., quantum states) by imposing the constraint\nC(V ) = |V ⟩ =\n1\npP\ni v2\ni\nV\n(10)\nwith ∥|V ⟩∥ = 1. By the constraint C, all vectors can be\nrepresented as quantum systems. The system generated by\nthis method does not conﬂict with the system constructed\nfrom the basis vectors, that is, it can be decomposed into a\ncomposite form of the basis vectors, as shown in Eq. (1).\nNote that quantum states of single systems, i.e., points\non the sphere of Hilbert space, can be obtained by this\nmethod (Nielsen and Chuang 2002).\nBased on the above quantum system, the superposition\nstate needed in this article can be constructed,\n|V⟩ = cos(θ)|V ⟩ + sin(θ)|V ⟩\n(11)\nwhere θ ∈ R and |V ⟩ = X|V ⟩. X is the NOT gate of quantum mechanics, and its high-dimensional form is shown as\nX =\n\n\n1\n· · ·\n1\n\n .\n(12)\nFor the parameter θ in Eq. (11), we can obtain it in many\nways in practice, such as setting it as an independent parameter, or setting it as a non-independent parameter learned\nbased on other features. We here obtain θ from V through a\nlinear layer of the neural network,\nL(V ) = θ ∈ R.\n(13)\nTherefore, the mapping from word or text vectors to superposition states can be formalized as\nF(V ) = cos(L(V))C(V)+sin(L(V))XC(V) = |V⟩.\n(14)\nAt this point, we can use quantum measurement theory to\ncalculate the similarity between the representation obtained\nfrom the superposition state and the representation of the\nlabel. Since the output of the quantum measurement is the\nprobability, it can be directly used as the score of the corresponding label. Here we deﬁne the correct state |φ⟩ (which\ncan be the vectorized form of the label in speciﬁc applications) as a measurement operator Mφ = |φ⟩⟨φ|, and the\nprobability of |V⟩ can be obtained from the general measurement theory,\np(|V⟩; Mφ)=⟨V|M †\nφMφ|V⟩=(⟨φ|V⟩)2.\n(15)\nArchitecture of QR-WSD\nBased on the quantum-inspired representation method, a\nWSD model is constructed, called QR-WSD, and its architecture is shown in Fig. 2.\nThe overall structure of the model can be divided into two\nparts, namely the classical processing method part and the\nquantum processing method part. Previous work (Kumar\net al. 2019; Blevins and Zettlemoyer 2020) has veriﬁed that\nthe classic method can effectively deal with head senses (that\nis, most frequent senses). This paper will integrate the quantum method, which is good at processing tail senses, to make\nup for the shortcomings of previous work. Both quantum\nand classical processing methods rely on the embeddings extracted by BERT (Devlin et al. 2019) from the target word,\nglosses and example sentences, where glosses and example\nsentences come from WordNet (Miller 1998). It should be\nemphasized that although this article uses the glosses and\nexample sentences, the difference is that we only use them\nfor training label vectors instead of directly using them as\ntraining data. In the end, these two methods together determine the ﬁnal output of the model.\nClassical Processing Method:\nThe classical processing\nmethod part uses the embeddings obtained by the two\nBERTs as input. One BERT is used to extract the target word\nand example sentence embeddings (in Fig. 2, for the convenience of understanding, we repeat the BERT), and the other\nBERT to extract gloss embeddings. The generation process\nof the embeddings is described below, and they are also applicable to the quantum processing method part.\n• Target word embedding: The training set is used as the\ninput of BERT to obtain the embedding of the target\nword. The text is processed following the uniﬁed regulations of BERT, such as adding [CLS] and [SEP] marks\nto the beginning and end of the text respectively.\n• Gloss embeddings: Gloss texts in WordNet are used to\nextract word sense embeddings, that is, the vector form\nof labels. The processing method of these texts is also in\naccordance with the uniﬁed regulations of BERT. Since\nthe entire text needs to be represented, the output vector\nof [CLS] is used to represent the entire text according to\nthe convention.\n• Example sentence embeddings: The example sentences\nin WordNet are used to extract the word sense embeddings, which are another form of word sense deﬁnitions.\nGloss texts are a conceptual formal description of sense,\nwhile example sentences are a description of its applicable scenario. They complement each other, but each has\nits own focus. In order to extract the scenario information\nof the target word in the example sentence, we use the\n[MASK] mark to cover the target word. The vector corresponding to [MASK] in the example sentence is used\nas the embedding of the example sentence.\nAt this point, the similarity between the target word\nWtarget and each sense Si can be calculated, where i ∈\n{1, ..., |S|} is the index of the list of candidate senses. The\nlist of senses is represented by glosses Si\ngloss and example\nsentences Si\nexam. The score of each sense of the target word\ncan be calculated:\nScorei\ngloss = VWtarget ⊙ VSi\ngloss\n(16)\nScorei\nexam = VWtarget ⊙ VSi\nexam\n(17)\nwhere V represents the corresponding vectorization (i.e.,\nembedding), and ⊙ represents the inner product operation.\nQuantum Processing Method:\nUnder the quantum processing method part, gloss embeddings VSi\ngloss and example\nsentence embeddings VSi\nexam are constructed as superposition states,\n|VSi\ngloss⟩ = F(VSi\ngloss),\n(18)\n|VSi\nexam⟩ = F(VSi\nexam),\n(19)\nand the target word embedding VWtarget as a measurement\noperator,\nMWtarget = |Wtarget⟩⟨Wtarget|\n(20)\n= C(VWtarget)(C(VWtarget))†.\n(21)\nAt this point, the score of each sense of the target word\ncan be calculated, i.e., Eq. (15),\nQ-Scorei\ngloss = p(|VSi\ngloss⟩; MWtarget),\n(22)\nQ-Scorei\nexam = p(|VSi\nexam⟩; MWtarget).\n(23)\nCombined with the scores obtained by the classic method,\nthey are collectively used as the parameters of the loss function of QR-WSD.\n[CLS]\na\nliving\nplant\nsprouted\nThe\nTo\nplant\nfish\nScore of Sense Labels\nQuantum Measurement\nQuantum processing method\nClassic processing method\nM M M M\nM M M\nM\nInner Product\nFigure 2: Illustration of model structure. The model consists of two parts, quantum part and classical part. The ﬁgure shows\nthree BERTs, but in practice, the BERT of the training target word and the BERT of the training examples are combined into\none. The notation ⊙ represents the inner product operation. The circled M stands for quantum measurement.\nModel Training\nWe employ a cross-entropy loss on the scores of the candidate senses of the target word to train QR-WSD,\nScorei =Scorei\ngloss + Scorei\nexam\n(24)\n+ Q-Scorei\ngloss + Q-Scorei\nexam.\nThe loss function is\nLoss(Score, index)\n(25)\n= − log\n\u0012 exp(Score[index])\nP\ni=1 exp(Score[i])\n\u0013\n(26)\n= −Score[index] + log\nX\ni=1\nexp(Score[i])\n(27)\nwhere index is the index of the list of the candidate senses.\nQR-WSD employs the Adam optimizer (Kingma and Ba\n2015) to update the parameters, and the speciﬁc settings of\nthe optimizer will be given in the experimental section.",
        "experiments": "Datasets and Model Settings\nDatasets:\nTo evaluate the effectiveness of QR-WSD, we\ncarried out experiments under two evaluation settings,\nnamely the standardized evaluation setting and the enhanced evaluation setting. The standardized setting includes only SemCor2 in the training set; the enhanced setting\n2http://lcl.uniroma1.it/wsdeval/training-data\nincludes SemCor and WNGT3 in the training set. Both settings use the same development set and test sets. SemEval07 (SE7; Pradhan et al. (2007)), following convention (Kumar et al. 2019; Blevins and Zettlemoyer 2020), is regarded\nas the development set. Senseval-2 (SE2; Edmonds and Cotton (2001)), Senseval-3 (SE3; Snyder and Palmer (2004)),\nSemEval-13 (SE13; Navigli, Jurgens, and Vannella (2013)),\nSemEval-15 (SE15; Moro and Navigli (2015)) and their\nconcatenation (ALL) are designated as the test sets.\nIn addition, it needs to be explained that the candidate list\nof the senses of the target word is all the word senses of\nWordNet 3.0. Evaluation metrics and other unlisted information are set according to the WSD evaluation framework\nproposed by Navigli et al. (Navigli, Camacho-Collados, and\nRaganato 2017).\nModel Settings:\nThe computing platform of the program\nis Ubuntu 18.04, which is equipped with two Tesla P40\nGPUs. The program is developed based on the Pytorch\n1.8 framework and written in Python 3.6. Moreover, WordNet 3.0 is provided by NLTK 3.5, and bert-base-uncased\nand bert-large-uncased are provided by Transformers 4.5.\nThe learning rate, epoch and batch size of the model are\n{1e-5, 5e-6}, 20 and 4 respectively. Other hyperparameters\nnot listed will be given in the published code.\nBaselines\nAccording to the traditional comparison scheme, we choose\ntwo versions of BERT (namely BERT-base and BERT-large)\n3https://wordnetcode.princeton.edu/glosstag.shtml\nas the encoder to build our model (called QR-WSDbase and\nQR-WSDlarge) to compete with the baseline models. The\ncomparison models are divided into models under the standardized setting and under the enhanced setting.\nUnder the Standardized Setting:\nOur model is compared\nwith EWISE (Kumar et al. 2019), LMMS (Loureiro and\nJorge 2019), SREF (Wang and Wang 2020), ARES (Scarlini, Pasini, and Navigli 2020b), BEM (Blevins and Zettlemoyer 2020), EWISER (Bevilacqua and Navigli 2020), SyntagRank (Scozzafava et al. 2020), COF (Wang, Zhang, and\nWang 2021b), ESR (Song et al. 2021), Z-Reweighting (Su\net al. 2022), and QWSD (Tamburini 2019). Their experimental results come from the values published in the original\npaper.\nUnder the Enhanced Setting:\nWe choose the three\nmost representative models as the baselines, they are\nSparseLMMS (Berend 2020), EWISER (Bevilacqua and\nNavigli 2020) and ESR (Song et al. 2021). Their experimental results also come from the values published in the\noriginal paper.\nResults and Analysis\nUnder the Standardized Setting:\nThe experimental results under the standardized evaluation setting are shown\nin the ﬁrst block of Tab. 1. From the overall performance\npoint of view, our model is superior to the baseline models in\nmultiple test sets. Compared with QWSD (Tamburini 2019),\nwhich is a quantum-inspired model, our model is far superior to the performance of QWSD. A notable reason is that\nQWSD does not use learnable embeddings, and we choose\nthe current mainstream pre-training language model. Compared with EWISE (Kumar et al. 2019), which is a model in\na continuous sense space, our model also performs outstandingly. The reason is that our model not only uses continuous space to constrain embeddings, but also can ﬁnd better\nrepresentations through the quantum-inspired representation\nmethod.\nIn terms of overall performance, our results are already\nin a high position, but still cannot compete with the excellent performance of BEM (Blevins and Zettlemoyer 2020)\non multiple test sets, such as Adj. and Adv.. By analyzing\nthe proportion of head and tail senses in the datasets, it is\nfound that head senses in these datasets have a higher proportion, which does not give full play to the advantages of\nthe quantum-inspired representation.\nUnder the Enhanced Setting:\nThe experimental results\nunder the enhanced evaluation setting are shown in the second block of Tab. 1. From the experimental results, in addition to the poor performance on the test set Adj. and Adv.,\nour model is better than the comparison models under other\ntest sets. By comparing QR-WSDbase and QR-WSDlarge, we\nﬁnd that based on a more powerful pre-training model will\nsigniﬁcantly improve the performance.\nAblation Study under MFSs and LTSs\nFor a more detailed analysis of the contribution of the\nquantum-inspired representation to the model, we perform\nFigure 3: Experimental results of XLMR-Base (which results from data published by the evaluation framework), QRWSD+, and QR-WSD– under the cross-lingual WSD evaluation framework.\nablation experiments.\nIn terms of models, we chose the model with the quantum processing method removed as the experimental group,\ncalled QR-WSD–, and the original model as the control\ngroup, called QR-WSD+. In terms of datasets, the ﬁrst set\nof experiments employs the original training set (i.e., SemCor) and test sets; the second set of experiments constructs\nthe Most Frequent Sense (MFS) training set and test sets by\nselecting the head senses in the original training set and test\nsets respectively; the third set of experiments constructs the\nLong Tail Sense (LTS) training set and test sets by selecting the tail senses in the original training set and test sets\nrespectively. In the implementation process, we classify the\nsenses that appear more than 3 times in the dataset as MFSs,\nand vice versa as LTSs. Other settings are the same as in the\nmain experiment.\nThe experimental results are shown in Tab. 2. Because the\ndatasets used in the ﬁrst set of experiments are the original\ndata, the results show the performance of the models under\nthe real data. From these results, it can be seen that the quantum representation method has advantages under real data,\nindicating that the quantum representation has the ability to\ndeal with the head senses and the tail senses at the same\ntime.\nUnder the MFS datasets, the model with quantum representation does not have a signiﬁcant advantage. But this result is in line with reality, because when there are only head\nsenses, the classical method can effectively deal with them,\nand the quantum method is difﬁcult to play its advantages.\nUnder the LTS datasets, QR-WSD+ is better than QRWSD–. The reason for presenting this result is that the classical method is not suitable for small sample tasks, while the\nadvantages of the quantum method can be exerted. However,\nfrom the overall performance of the experimental results, the\nquantum representation method is also affected by the number of samples.\nExperiments under Cross-Lingual Datasets\nTo evaluate the robustness of the model and the applicability of the quantum-inspired representation method, we\nconduct experiments on the latest cross-lingual evaluation\nModels\nDev set\nTest sets\nConcatenation of all test sets\nSE7\nSE2\nSE3\nSE13\nSE15\nNouns\nVerbs\nAdj.\nAdv.\nALL\nTraining data: SemCor\nEWISE (ACL 2019)\n67.3\n73.8\n71.1\n69.4\n74.5\n74.0\n60.2\n78.0\n82.1\n71.8\nLMMS (ACL 2019)\n68.1\n76.3\n75.6\n75.1\n77.0\n75.4\nSREF (EMNLP 2020)\n72.1\n78.6\n76.6\n78.0\n80.5\n80.6\n66.5\n82.6\n84.4\n77.8\nARES (EMNLP 2020b)\n71.0\n78.0\n77.1\n77.3\n83.2\n80.6\n68.3\n80.5\n83.5\n77.9\nBEM (ACL 2020)\n74.5\n79.4\n77.4\n79.7\n81.7\n81.4\n68.5\n83.0\n87.9\n79.0\nEWISER (ACL 2020)\n71.0\n78.9\n78.4\n78.9\n79.3\n81.7\n66.3\n81.2\n85.8\n78.3\nSyntagRank (ACL 2020)\n59.3\n71.6\n72.0\n72.2\n75.8\n71.2\nCOF (EMNLP 2021b)\n69.2\n76.0\n74.2\n78.2\n80.9\n80.6\n61.4\n80.5\n81.8\n76.3\nESR (EMNLP 2021)\n75.4\n80.6\n78.2\n79.8\n82.8\n82.5\n69.5\n82.5\n87.3\n79.8\nZ-Reweighting (ACL 2022)\n71.9\n79.6\n76.5\n78.9\n82.5\n–\n–\n–\n–\n78.6\nQuantum-inspired models\nQWSD (RANLP 2019)\n70.5\n69.8\n69.8\n73.4\n73.6\n54.4\n77.0\n80.6\n70.6\nQR-WSDbase\n74.5\n80.6\n79.1\n80.0\n84.7\n83.7\n71.4\n82.8\n86.7\n80.5\nQR-WSDlarge\n75.4\n81.8\n80.2\n81.1\n85.1\n84.9\n72.1\n84.4\n88.0\n81.7\nTraining data: SemCor + WNGT\nSparseLMMS (EMNLP 2020)\n73.0\n79.6\n77.3\n79.4\n81.3\n78.8\nEWISER (ACL 2020)\n75.2\n80.8\n79.0\n80.7\n81.8\n81.7\n66.3\n81.2\n85.8\n80.1\nESR (EMNLP 2021)\n77.4\n81.4\n78.0\n81.5\n83.9\n83.1\n71.1\n83.6\n87.5\n80.7\nQR-WSDbase\n78.1\n81.5\n79.0\n82.0\n84.4\n83.1\n71.2\n80.9\n84.1\n81.0\nQR-WSDlarge\n78.8\n82.4\n81.0\n83.4\n85.6\n84.0\n73.0\n84.7\n87.0\n82.1\nTable 1: F1-score (%) on the English all-words WSD task. The comparison models are divided into two groups: those under the\nstandardized evaluation setting (i.e., using only SemCor) and those under the enhanced evaluation setting (i.e., using SemCor\nand WNGT). SOTA performance is bold compared to QR-WSDbase and underlined compared to QR-WSDlarge.\nTest sets\nSE2\nSE3\nSE13\nSE15\nALL\nDataset: SemCor\nQR-WSD+\n80.6\n79.1\n80.0\n84.7\n80.5\nQR-WSD–\n77.7\n75.3\n76.1\n82.0\n77.1\nDataset: MFS\nQR-WSD+\n92.7\n89.3\n92.0\n95.8\n92.4\nQR-WSD–\n93.1\n89.0\n91.7\n95.8\n92.2\nDataset: LTS\nQR-WSD+\n63.3\n67.6\n71.6\n71.5\n70.7\nQR-WSD–\n57.1\n64.4\n63.2\n67.7\n66.8\nTable 2: The experimental results of ablation experiments\nunder the original, MFS and LTS datasets.\nframework proposed by Pasini et al. (Pasini, Raganato, and\nNavigli 2021).\nTo clearly demonstrate the effectiveness of the quantuminspired representation method, we still use the settings of\nthe models in the ablation experiments, namely QR-WSD+\nand QR-WSD–. The encoders of the model use bert-basemultilingual-cased. Furthermore, we adopt the model employed in the evaluation framework as the baseline model,\nXLMR-Base (Conneau et al. 2020), and the experimental\nresults are also derived from the results published in the paper. Note that since the cross-lingual datasets are constructed\nbased on BabelNet, the glosses and example sentences in\nthis section are from BabelNet. Moreover, since most small\nlanguages in BabelNet use deﬁnitions in English, we directly\nuse glosses and example sentences in English to provide a\ncandidate list of senses.\nThe experimental results are shown in Fig. 3. Comparing QR-WSD+ with XLMR-Base, QR-WSD+ is better\nthan XLMR-Base on multiple datasets, which shows that\nour model has a certain robustness, and that the quantuminspired representation method has a wide range of applicability. Comparing QR-WSD+ with QR-WSD–, QR-WSD+ is\nfar superior to QR-WSD–, which shows that quantum representation is helpful for the representation and identiﬁcation\nof long-tail senses.",
        "conclusion": "Inspired by the quantum superposition state, this article proposes a novel quantum-inspired representation method. This\nmethod attempts to obtain a relatively correct representation\nfor long-tail senses with only a small sample size. Moreover,\na WSD model is constructed based on the proposed representation to verify the effectiveness in speciﬁc applications.\nThe experimental results show that the model is better than\nthe baseline models and achieves comparable performance.\nIn the constructed long-tail sense dataset and the recently\nproposed cross-lingual datasets, the quantum-inspired representation shows advantages over the traditional methods,\nindicating that the representation has a certain potential for\nsolving the long-tail phenomenon of data distribution.\nThe signiﬁcance of this article is to propose an effective\nrepresentation method inspired by quantum mechanics and\nto explore new ways to solve the data imbalance. In future\nwork, we will further study the application of multiple quantum systems in few-shot tasks, and explore the potential of\nquantum mechanics in traditional tasks.",
        "summary_en": "Data imbalance, also known as the long-tail distribution of data, is an important challenge for data-driven models. In the Word Sense Disambiguation (WSD) task, the long-tail phenomenon of word sense distribution is more common, making it difficult to effectively represent and identify Long-Tail Senses (LTSs). Therefore exploring representation methods that do not rely heavily on the training sample size is an important way to combat LTSs. Considering that many new states, namely superposition states, can be constructed from several known states in quantum mechanics, superposition states provide the possibility to obtain more accurate representations from inferior representations learned from a small sample size. Inspired by quantum superposition states, a representation method in Hilbert space is proposed by this paper to reduce the dependence on large sample sizes and thus combat LTSs. The paper theoretically proves the correctness of the method, and verify its effectiveness under the standard WSD evaluation framework and obtain state-of-the-art performance. Furthermore, the paper also tests on the constructed LTS and the latest cross-lingual datasets, and achieve promising results.",
        "summary_zh": "这篇论文研究了量子启发式的表示方法在长尾词义消歧任务中的应用。在词义消歧（WSD）任务中，词义分布的长尾现象较为普遍，难以有效表示和识别长尾词义（LTS），作者受量子力学启发，提出了一个表示方法以应对长尾词义的识别困难。该方法通过在希尔伯特空间中构建叠加态，降低了对大样本量的依赖，并在标准词义消歧评估框架下取得了最先进的性能。此外，论文还在构建的长尾词义数据集和最新的跨语言数据集上进行了测试，并取得了令人满意的结果。"
    },
    {
        "title": "A Gift from Label Smoothing: Robust Training with Adaptive Label Smoothing via Auxiliary Classifier under Label Noise",
        "abstract": "As deep neural networks can easily overfit noisy labels, robust training in the presence of noisy labels is becoming an important challenge in modern deep learning. While existing methods address this problem in various directions, they still produce unpredictable sub-optimal results since they rely on the posterior information estimated by the feature extractor corrupted by noisy labels. Lipschitz regularization successfully alleviates this problem by training a robust feature extractor, but it requires longer training time and expensive computations. Motivated by this, we propose a simple yet effective method, called ALASCA, which efficiently provides a robust feature extractor under label noise. ALASCA integrates two key ingredients: (1) adaptive label smoothing based on our theoretical analysis that label smoothing implicitly induces Lipschitz regularization, and (2) auxiliary classifiers that enable practical application of intermediate Lipschitz regularization with negligible computations. We conduct wide-ranging experiments for ALASCA and combine our proposed method with previous noise-robust methods on several synthetic and real-world datasets. Experimental results show that our framework consistently improves the robustness of feature extractors and the performance of existing baselines with efficiency.",
        "introduction": "While deep neural networks (DNNs) have high expressive\npower that leads to promising performances, the success of\nDNNs heavily relies on the quality of training data, in particular, accurately labeled training examples. Unfortunately,\nlabeling large-scale datasets is a costly and error-prone process, and even high-quality datasets contain incorrect labels (Nettleton, Orriols-Puig, and Fornells 2010; Zhang et al.\n2017). Hence, mitigating the negative impact of noisy labels\nis critical, and many approaches have been proposed to improve robustness against noisy data for learning with noisy\nlabels (LNL).\nRobustness to label noise is typically pursued by identifying noisy samples to reduce their contribution to the\nloss (Han et al. 2018; Mirzasoleiman, Cao, and Leskovec\n2020), correcting labels (Yi and Wu 2019; Li, Socher, and\nHoi 2020), utilizing a robust loss function (Zhang and\nSabuncu 2018; Wang et al. 2019). However, one of the\nbiggest challenges of LNL methods involves providing a dependable criterion for distinguishing clean data from noisy\ndata, such that clean data is fully exploited while filtering\nnoisy data. While these existing methods are partially effective in mitigating label noise, their criterion for identifying noisy examples uses biased posterior information from\na linear classifier or the penultimate layer of the corrupted\nnetwork. These unpredictable biases can lead to a reduction\nin the network’s ability to separate clean and noisy instances\n(Nguyen et al. 2020; Kim et al. 2021a).\nTo solve this undesired bias, several regularization methods (Xia et al. 2021; Cao et al. 2021) have been proposed\nto enhance the robustness of the feature extractor. However,\nwhile existing regularization-based learning frameworks alleviate the degradation, these methods require multiple training stages and considerable computational costs and are difficult to apply in practice. Cao et al. (2021) used two-stage\ntraining to compute the relative data-dependent regularization power to conduct Lipschitz regularization (LR) on intermediate layers. Xia et al. (2021) identified and regularized\nthe non-critical parameters that tend to fit noisy labels and\nrequire longer training time. Some studies (Zhang and Yao\n2020; Zheltonozhskii et al. 2022) have designed contrastive\nlearning frameworks to generate high-quality feature extractors using unsupervised approaches, which require considerable computations for high performance.\nTo mitigate these impractical issues, we provide a simple yet effective learning framework for a robust feature extractor, Adaptive LAbel Smoothing via auxiliary ClAssifier\n(ALASCA), with theoretical guarantee and small additional\ncomputation. Our proposed method is robust to label noise\nitself and can further enhance the performance of existing\nLNL methods. Our main contributions are as follows:\n• We theoretically explain that label smoothing (LS) implicitly induces LR, which is known to enable robust\ntraining with noisy labels (Finlay et al. 2018; Cao et al.\n2021). Through theoretical motivations, we empirically\nshow that adaptive LS (ALS) can regularize noisy examples while fully exploiting clean examples.\n• To practically implement adaptive LR on the intermediate layers, we propose ALASCA, which combines ALS\nwith auxiliary classifiers. To the best of our knowledge,\nthis is the first study to apply auxiliary classifiers under\nlabel noise with theoretical evidence.\n• We experimentally demonstrate that ALASCA is universal by combining various LNL methods and validating that ALASCA consistently boosts robustness on\nbenchmark-simulated and real-world datasets.\n• We verify that ALASCA effectively enhances the robustness of feature extractors by comparing the quality of\nsubsets on sample-selection methods and robustness to\nthe hyperparameter selection of LNL methods.",
        "related works": "2.1\nLearning with Noisy Labels\nZhang et al. (2017) empirically demonstrated that convolutional neural networks trained with stochastic gradient methods easily memorize random labeling of the training data. To\naddress this, numerous studies have examined the classification task with noisy labels. Existing methods address this\nproblem by (1) filtering noisy examples and training using\nonly clean examples (Han et al. 2018; Mirzasoleiman, Cao,\nand Leskovec 2020; Kim et al. 2021a) or (2) relabeling noisy\nexamples using the model itself or another model trained\nonly on the clean dataset (Lee et al. 2018; Li, Socher, and\nHoi 2020). Some approaches focus on designing loss functions with robust behaviors and provable tolerance to label\nnoise (Ghosh, Kumar, and Sastry 2017; Zhang and Sabuncu\n2018; Wang et al. 2019). We fully describe these previous\nworks in Appendix B.\nRegularization-based Methods.\nAnother line of work\nhas attempted to design regularization-based techniques. For\nexample, some studies have stated and theoretically analyzed how early-stopped model can prevent the memorization phenomenon of noisy labels (Arpit et al. 2017; Song\net al. 2019). Based on this, Liu et al. (2020) proposed an\nearly learning regularization (ELR) loss function that avoids\nmemorizing noisy data by leveraging semi-supervised learning (SSL) techniques. Xia et al. (2021) clarified that neural network parameters cause memorization and proposed\na robust training method for these parameters. Developing\nregularization at the prediction level has been addressed by\nsmoothing one-hot vectors (Lukasik et al. 2020) and distilling the rescaled predictions of other models (M¨uller, Kornblith, and Hinton 2019; Kim et al. 2021b). Recently, Cao\net al. (2021) proposed a heteroskedastic adaptive regularization that applies stronger regularization to noisy instances.\n2.2\nLabel Smoothing\nLS (Szegedy et al. 2016; M¨uller, Kornblith, and Hinton\n2019) is commonly used to construct a generalized DNN\nmodel by preventing over-confident predictions. This regularization technique facilitates generalization by softening a\nground-truth one-hot vector y with a weighted mixture of\nhard targets:\nyLS := (1 − α) · y + α\nL · 1L,\nwhere L denotes the number of classes, 1L denotes an allone vector in RL, and α ∈ [0, 1] is a smoothing parameter.\nLukasik et al. (2020) claimed that LS denoises label noise by\ncausing label correction and weight-shrinkage regularization\neffects. However, Wei et al. (2021a) recently show that LS\ntends to over-smooth the estimated posterior under high levels of label noise, which can hurt robustness. Moreover, several studies (Szegedy et al. 2016; Pereyra et al. 2017; M¨uller,\nKornblith, and Hinton 2019; Chorowski and Jaitly 2017)\nhave validated that LS boosts model generalizability, and Li,\nDasarathy, and Berisha (2020) proposed the need for datadependent smoothing. Li, Dasarathy, and Berisha (2020)\nproposed structural LS, which selects smoothing strength\ndata-dependently that minimizes the Bayes error rate bias.\nGhoshal et al. (2021) derived PAC Bayesian generalization\nbounds for LS and proposed adaptive smoothing for the latent structure of the label space.",
        "methodology: alasca": "LR has been shown to be effective for DNNs (Gouk et al.\n2021). Wei and Ma (2019a,b) theoretically and empirically\nshow that LR for all intermediate layers improves generalization of DNNs. Furthermore, many studies show that different regularization strengths along the data points are essential. Wang, Du, and Shen (2013) and Tibshirani (2014)\nstated that smoothing splines with different smoothing parameters perform well in regression problems. Recently, Cao\net al. (2021) showed that applying strong LR to highly uncertain data points improves generalization. We recall key\ntakeaways from Cao et al. (2021) that motivated our work.\nRemark 3.1 (Cao et al. 2021). In a binary classification\nproblem on one-dimensional data, the authors i) derived the\nformula for the asymptotic mean squared error (MSE) on\nthe test set, ii) with some simplifications, showed that the\nasymptotic MSE is minimized when the smoothing parameter is proportional to the 3\n5-th power of the label uncertainty.\nThe exact theorem statement and detailed explanation\nmay be found in Appendix A.1. However, this explicit regularization requires multiple training phases to estimate and\napply relative regularization power for different data points.\nFurthermore, computing the Hessian matrix resulting from\ndirectly regularizing the norm of Jacobian matrices increases\nthe computational cost (Filiposka, Djuric, and ElMaraghy\n2014; Nesser et al. 2021). In this section, we present that\nLS implicitly incurs LR and introduce the simple unified\nframework for efficient learning with label noise, ALASCA.\nIn principle, our method can be used with most LNL methods, such as noise-robust loss function (Zhang and Sabuncu\n2018; Wang et al. 2019) and sample-selection methods (Han\net al. 2018; Kim et al. 2021a).\n3.1\nLabel Smoothing as Lipschitz Regularization\nIn this section, we analytically present our motivation that\nLS implicitly encourages LR. Here, we formally define the\nnotation and terminology for our problem.\nNotation.\nWe focus on multiclass classification with L\nclasses. Assume that the data points and labels lie in X × Y,\nwhere the feature space X ⊂ RD and label space Y =\n \n \n \n \n \n \n \n \n \n  \n            \n \n \n  \n  \n  \n                    \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n                     \n \n   \n    \n    \n    \n             \n   \n   \n   \n   \nFigure 1: Comparison of class-wise representation vector norm (left; Theorem 3.5), and distribution of Jacobian matrix norm\nfor penultimate layer (right; Theorem 3.7) across different smoothing factors on CIFAR-10.\n{0, 1}L. A single data point x and its label y follow a distribution (x, y) ∼ PX×Y. We aim to find a predictor f :\nX → RL minimizing the risk of E(x,y)∼PX×Y [ℓ(f(x), y)]\nwith loss function ℓ : RL × RL → R+.\nDefinition 3.2 (Lipschitzness). A function f is called Lipschitz continuous with a Lipschitz constant Lf ∈ [0, ∞) if\n∥f(y) − f(x)∥ ≤ Lf∥y − x∥,\nfor all x, y ∈ dom f.\nDefinition 3.3 (Lipschitz Regularization). Let F be a twicedifferentiable model family from RD to RL. Lipschitz regularization aims to optimize the function with a smoothness\npenalty as follows:\nmin\nf∈F\n1\nN\nN\nX\nn=1\nℓ(f(xn), yn) + λ∥Jf(xn)∥F ,\nwhere λ is the regularization coefficient, N the number of\ntraining data points, Jf the Jacobian matrix of f, and ∥ · ∥F\nthe Frobenius norm.\nHere, we focus only on ℓ as cross-entropy (CE) loss. Compared with the CE loss with one-hot vector ℓ(f(x), y), the\nCE loss with LS ℓ(f(x), yLS) of factor α can be presented\nas follows:\nℓ(f(x), yLS) = (1 − α) · ℓ(f(x), y) + α\nL · ℓ(f(x), 1L)\n∝ ℓ(f(x), y) +\nα\n(1 − α) · L · Ω(f).\nBy denoting fi(·) as the i-th element of logit vector f(·), the\nregularization term of LS is\nΩ(f) = L · log\n\" L\nX\ni=1\nefi(·)\n#\n−\nL\nX\ni=1\nfi(·).\n(1)\nPreviously, Lukasik et al. (2020) suggested that LS encourages weight shrinkage in DNNs; however, this interpretation\nwas validated only for linear models. To better understand\nLS, we consider that the DNN function can be presented\nas arbitrary surrogate models. We suppose that the surrogate model f(·) = g ◦ h(·) consists of an arbitrary twicedifferentiable feature extractor h : X → RQ and fixed linear\nclassifier g(z) = W⊺z with W ∈ RQ×L and z ∈ RQ. First,\nwe find a minimizer of the regularization term of LS. The\nfollowing assumption is required to guarantee the uniqueness of the minimizer.\nAssumption 3.4. W1, W2, · · · , WL is an affine basis of\nRQ, where Wi is the i-th column of W (i.e., W2 −\nW1, W3 − W1, · · · , WL − W1 are linearly independent).\nTheorem 3.5. h = 0 is a minimizer of Ω◦g. If Assumption\n3.4 holds, h = 0 is the unique minimizer.\nNote that h = 0 is always a minimizer of Equation\n(1) without any assumptions. The takeaway from Theorem 3.5 is that the regularization term of LS encourages\nh(x1), . . . , h(xN) to shrink to zero. However, this does not\nensure the shrinkage of the Jacobian matrix of f. The following theorem shows that this is true under a common Lipschitz assumption. We state the assumption and present our\nnext main result.\nAssumption 3.6. Each gradient ∇hi(x) is Lipschitz continuous with a Lipschitz constant Lh for all i, where h(x) =\n(h1(x), h2(x), · · · , hQ(x)).\nTheorem 3.7. Consider a bounded feature space X and\nsuppose that Assumption 3.6 is satisfied. If h(xn) = 0 for\n1 ≤ n ≤ N, ∥Jf(xn)∥F → 0 as N → ∞ holds for\n1 ≤ n ≤ N.\nTheorem 3.7 states that the smoothness of the feature extractor function in the training set induces LR. By combining Theorem 3.5 and 3.7, we conclude that the regularization\nterm of LS encourages LR. The detailed proof of the theorems may be found in Appendix A.2.To validate our theoretical perspective, we conducted the following exploratory experiments: compare the (1) class-wise average norm values\nof representation vectors and (2) Jacobian matrix norm from\nthe penultimate layer of ResNet34 trained on CIFAR-10. As\nFigure 1 shows, both the representation and Jacobian matrix\nnorms decrease to zero as the smoothing factor increases,\nindicating that the LS regularization term induces the representation vector to the origin (Theorem 3.5) and implicitly\nincurs LR (Theorem 3.7).\nAdaptive Regularization.\nFrom our perspective of LS,\nwe can apply adaptive LR using different smoothing factors. Cao et al. (2021) shows that applying stronger LR\nto highly uncertain data points improves generalization on\nnoisy datasets. To implement adaptive LR through LS, we\ndesign the smoothing factor of LS proportional to the 1 −\nPr(y|x) for each instance. While Cao et al. (2021) suggests\nthat the optimal smoothing parameter is proportional to the\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n500\n1000\n1500\n2000\nLow Conf.\n= Strong Reg.\nHigh Conf.\n= Weak Reg.\nClean\nNoisy\n(a) Concept of ALS\nCE\nLS (0.1)\nLS (0.3)\nLS (0.5)\nALS\n0\n2\n4\nNorm. (× 1e-3)\nMean Norm (LS_0.3)\nMean Norm (ALS)\nClean\nNoise\n(b) Jacobian Norm\nFigure 2: (a) The concept of ALS. Because noisy instances tend to have lower confidence, we conduct stronger regularization\non lower confidence instances. (b) Comparison of Jacobian matrix norms for the penultimate representation on CIFAR-10 with\n50% of symmetric noise.\n0\n20\n40\n60\n80\n100\n120\nEpoch.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n20\n40\n60\n80\n100\n120\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 3: Dynamic patterns (mean ± std) of instantaneous (left) and EMA confidence (right) suggested in ALASCA for CIFAR10 under 40% of asymmetric noise. The blue and orange lines are corresponding to clean and noisy instances.\n3/5-th power of the label uncertainty, our proposed strategy\nshows similar performances despite its simplicity. Consequently, we use the following loss function for ALS:\nℓALS\n˜α(x)(f(x), y) = (1− ˜α(x))·ℓ(f(x), y)+ ˜α(x)·Ω(f), (2)\nwhere ˜α(x) is the y-th element of 1 − S(f(x)), and S\nis the softmax function. Figure 2 shows that the proposed\nALS enables us to mainly regularize noisy examples because\nwe conduct stronger regularization for lower confidence examples, where noisy examples take up a large proportion.\nWhile uniform LS (ULS) equally regularizes the smoothness\nof both clean and noisy examples, we validate that ALS can\napply the appropriate LR for each example.\n3.2\nCombination with Additional Techniques\nNow, we integrate ALS with auxiliary classifiers (AC) and\nthe exponential moving averaged (EMA) confidence to practically regularize the smoothness of intermediate layers. We\ndescribe the overall algorithm of ALASCA in Algorithm 1.\nUse of Auxiliary Classifiers.\nRecent studies have emphasized LR of intermediate layers. Wei and Ma (2019a,b)\nshowed that LR for all intermediate layers improves generalization of DNNs. Sokoli´c et al. (2017) and Cao et al.\n(2021) showed similar results in data-limited and distribution shift setups, respectively. Inspired by these works, we\nuse LS as an intermediate LR by utilizing an AC commonly\nused in deep learning. While ACs work well on various domains such as self-distillation (Zhang et al. 2019) and classimbalance (Lee, Shin, and Kim 2021), our work first proAlgorithm 1: ALASCA\nRequire: {xi, yi}, 1 ≤ i ≤ N\nRequire: L\n▷ Loss function for existing LNL methods\nRequire: {Θk}K\nk=0\n▷ Parameters for main and ACs\nRequire: β, τ ▷ EMA weight and temperature of ALASCA\nRequire: λ\n▷ Coefficient for power of regularization\nOutput: Θ0\n1: ti ← 0N.\n▷ Initialize EMA confidence\n2: for each minibatch B do\n3:\nti ← βti + (1 − β)fΘ0(xi)\n▷ EMA (Averaging)\n4:\n˜α(xi) ← 1 − S(ti/τ)\n▷ EMA (Sharpening)\n5:\nloss ← − 1\n|B|\nP|B|\ni=1 L (fΘ0(xi), yi)\n6:\nloss ← + λ\n|B|\nPK\nk=1\nP|B|\ni=1 ℓALS\n˜α(xi) (fΘk(xi), yi)\n7:\n/* Compute loss by using Eq. (2) */\n8:\nupdate Θ0 and {Θk}K\nk=1 using SGD\n9: end for\nposes using ACs with noisy labels based on theoretical motivation. We compare our method with Zhang et al. (2019)\nin D.2.We apply bottleneck (Howard et al. 2017) architecture as ACs and attach them at the end of all blocks of backbone by our results in Section 4.3. By applying ACs, we can\nencourage greater LR to noisy instances and weaker LR to\nclean instances with only a small additional computation.\nFurthermore, this approach has two additional advantages.\nWei et al. (2021b) showed that the benefits of LS disappear\nin a high-level noise, as applying LS tends to over-smooth\nDataset\nCIFAR-10\nCIFAR-100\nNoisy Type\nSymm.\nAsym.\nInst.\nSymm.\nAsym.\nNoise Ratio\n50%\n80%\n40%\n40%\n20%\n50%\n80%\n40%\nStandard\n78.2 ± 0.8\n53.8 ± 1.0\n85.0 ± 0.1\n74.1 ± 2.9\n58.7 ± 0.3\n42.5 ± 0.3\n18.1 ± 0.8\n42.7 ± 0.6\n+ ALASCA\n88.0 ± 0.3\n70.3 ± 0.4\n90.3 ± 0.3\n81.4 ± 0.3\n70.6 ± 0.2\n59.7 ± 0.4\n26.1 ± 0.9\n59.0 ± 0.6\nGCE\n86.5 ± 0.2\n64.1 ± 1.4\n76.7 ± 0.6\n55.7 ± 0.2\n66.8 ± 0.4\n57.3 ± 0.3\n29.2 ± 0.7\n47.2 ± 1.2\n+ ALASCA\n88.4 ± 0.3\n73.7 ± 1.5\n90.2 ± 0.1\n73.5 ± 1.2\n70.8 ± 0.5\n60.1 ± 0.4\n32.2 ± 0.5\n57.3 ± 0.5\nSCE\n84.7 ± 0.3\n68.1 ± 0.8\n82.5 ± 0.5\n71.4 ± 1.3\n70.4 ± 0.1\n48.8 ± 1.3\n25.9 ± 0.4\n48.4 ± 0.9\n+ ALASCA\n88.5 ± 0.1\n71.7 ± 0.8\n89.4 ± 0.2\n78.0 ± 0.6\n71.4 ± 0.2\n61.3 ± 0.6\n28.7 ± 0.6\n57.3 ± 0.7\nELR\n88.2 ± 0.1\n72.9 ± 0.6\n90.1 ± 0.5\n79.8 ± 0.2\n74.2 ± 0.2\n59.1 ± 0.8\n29.8 ± 0.6\n73.3 ± 0.4\n+ ALASCA\n89.5 ± 0.3\n74.2 ± 1.2\n90.4 ± 0.2\n82.2 ± 0.5\n74.8 ± 0.1\n63.6 ± 0.6\n34.4 ± 0.4\n73.9 ± 0.5\nCo-teaching\n83.3 ± 0.6\n66.3 ± 1.5\n88.4 ± 2.8\n70.5 ± 0.5\n63.4 ± 0.0\n49.1 ± 0.4\n20.5 ± 1.3\n47.7 ± 1.2\n+ ALASCA\n90.1 ± 1.5\n71.1 ± 1.2\n91.2 ± 0.1\n76.8 ± 0.4\n75.5 ± 0.3\n68.1 ± 0.4\n42.2 ± 1.2\n64.7 ± 0.4\nCRUST\n87.0 ± 0.1\n64.8 ± 1.1\n82.4 ± 0.0\n64.7 ± 2.1\n69.3 ± 0.2\n62.3 ± 0.2\n21.7 ± 0.7\n56.1 ± 0.5\n+ ALASCA\n87.6 ± 0.2\n71.5 ± 1.5\n90.2 ± 0.2\n71.6 ± 1.1\n70.4 ± 0.1\n64.1 ± 0.9\n25.5 ± 0.7\n58.3 ± 0.5\nFINE\n87.3 ± 0.2\n69.4 ± 1.1\n89.5 ± 0.1\n82.4 ± 0.5\n70.3 ± 0.2\n64.2 ± 0.5\n25.6 ± 1.2\n61.7 ± 1.0\n+ ALASCA\n88.0 ± 0.1\n70.6 ± 0.9\n90.3 ± 0.2\n84.3 ± 1.2\n70.9 ± 0.3\n65.8 ± 0.2\n29.4 ± 1.5\n63.5 ± 0.7\nTable 1: Test accuracies (%) on CIFAR-10/-100 under different noise types and fractions for noise-robust loss and sampleselection approaches. The results for symmetric and asymmetric noise of all baseline methods were taken from Kim et al.\n(2021a). Instance-dependent noise results are reported by our re-implementation based on official codes. The average accuracies\nover three trials are reported. The best results sharing the noisy fraction and method are highlighted in bold.\nthe estimated posterior of the main classifier. However, LS\nwith ACs does not affect the main classifier but effectively\nregularizes the Lipschitzness of intermediate layers. Moreover, as we use multiple classifiers, we obtain more robust\npredictions from the ensemble effect during inference. In Algorithm 1, we denote the parameters of the main classifier\nand ACs as Θ0 and {Θk}K\nk=1, where K is number of AC.\nWe further denote outputs of the k-th classifier as fΘk(·).\nUse of EMA Confidence.\nAs shown in Figure 3, instantaneous confidence suffers from high variance across training epochs and is inaccurate for differentiating regularization power between clean and noisy instances. Incorrect regularization power caused by such instability leads to performance degradation. Hence, we use EMA along the epochs\nto compute confidence and effectively obtain the appropriate regularization power. In SSL, this weight averaging approach has been proposed to mitigate confirmation bias (Liu\net al. 2020). The computation procedure of EMA confidence\nis as follows. (1) To reduce variance and enhance stability, we conduct EMA on output values (Line 3 in Algorithm 1). (2) Because the averaged outputs are over-smooth,\nwhich causes weak regularization for noisy examples and\nstrong regularization for clean examples, we sharpen the\nEMA logits by dividing sharpen temperature τ (Line 4 in Algorithm 1). We observe that regularization powers on clean\nand noisy examples are clearly distinguished and become\nstable after using EMA confidence, as shown in Figure 3.",
        "experiments": "We design experiments to answer the following questions:\n• Can ALASCA improve existing LNL methods, such as\nnoise-robust loss functions and sample-selection methods for both synthetic and real-world datasets? (Section 4.1 & 4.4)\n• How effective is ALACSA in improving the robustness\nof the feature extractor? (Section 4.2)\n• How do the architecture and position of ACs affect the\nperformance and efficiency? & Which component is important to the performance in ALASCA? (Section 4.3)\n4.1\nExperimental Setup and Results on CIFAR\nSetup.\nWe inject uniform randomness into a fraction of\nlabels for symmetric noise and flip labels to specific classes\nfor asymmetric noise by following Kim et al. (2021a). To set\nup instance-dependent noise, we follow the noise generation\nof Cheng et al. (2020). We use the architectures of backbone network and hyperparameter settings for all baseline\nexperiments following Kim et al. (2021a). We set β, τ, and\nλ as 0.7, 1/3, and 2.0, respectively. The detailed experimental setup is described in Appendix C.1. and Appendix C.2.To\nverify the superiority of our method, we combine ALASCA\nwith various existing LNL methods (noise-robust loss functions and sample-selection methods) and identify that ours\nconsistently improves the generalization in the presence of\nnoisy data. Furthermore, we perform additional experiments\nincorporating semi-supervised approaches with ALASCA.\nAppendix D.1 provides detailed description and results for\nthe SSL approaches.\nNoise-Robust Loss Functions.\nNoise-robust loss functions aim to achieve high performances for unseen clean data\n0\n20\n40\n60\n80\n100\n120\nEpoch.\n0.4\n0.5\n0.6\n0.7\n0.8\nF1-Score\nCIFAR-10 w/o ALASCA\nCIFAR-10 w/ ALASCA\nCIFAR-100 w/o ALASCA\nCIFAR-100 w/ ALASCA\n(a) Co-teaching (F1-Score)\nCRUST (S20)\nStandard (S50)Co-teaching (S80)\nELR (A40)\nMethods (Noise)\n20\n30\n40\n50\n60\n70\n80\n90\nAccuracy\nSetup 1\nSetup 2\nSetup 3\nw/o ALASCA\nw/ ALASCA\n(b) Hyperparameter Selection\nFigure 4: (a) Comparison of F1-scores of Co-teaching with and without ALASCA on CIFAR-10 and CIFAR-100 under 80%\nof symmetric noise. (b) Comparison of test accuracies (%) along different hyperparameter settings for various LNL methods.\nThrough the results, we verify that ALASCA enhances the robustness of feature extractors.\ndespite the presence of noisy labels in the training data. We\ncombine our proposed method with three loss functions: (1)\nstandard CE (Standard); (2) generalized CE (GCE; Zhang\nand Sabuncu 2018), which can be seen as a generalization\nof the mean absolute error and standard CE; (3) symmetric\nCE (SCE; Wang et al. 2019), which is the weighted sum of\nCE and reverse version of CE; and (4) early learning regularization (ELR; Liu et al. 2020) which uses a regularization\nterm that incorporates target probabilities from the model\noutput. We observe that ALASCA improves generalization\nwhen applied to the noise-robust loss function of Table 1.\nSample-Selection Methods.\nSample-selection methods,\nwhich select clean sample candidates from the training\ndataset, are a popular direction in LNL. We combine\nALASCA with the following sample-selection approaches:\n(1) Co-teaching (Han et al. 2018), which utilizes two networks, extracts subsets of examples with small losses from\neach network, and trains each network with subsets of examples filtered by another network; (2) CRUST (Mirzasoleiman, Cao, and Leskovec 2020), which selects a subset\nof small weight gradient instances; and (3) FINE (Kim et al.\n2021a), which selects instances whose penultimate vector\nhighly correlates with the class-representative eigenvector.\nTable 1 shows the performance increase of ALASCA with\ndifferent sample-selection methods on various label noise.\n4.2\nQuality of Feature Extractors\nMany LNL methods use posterior information with undesired bias from the corrupted networks under noisy labels,\nwhich can lead to sub-optimal performances. However, if\nthe feature extractor is robustly trained under label noise, we\ncan employ unbiased posterior information and achieve better generalization. To validate the effectiveness of ALASCA\nin terms of improving the robustness of the feature extractor and mitigating undesirable biases, we conduct exploratory experiments: (1) comparison of quality for subsets\nof sample-selection approaches and (2) robustness to hyperparameter selection of existing LNL methods.\nQuality of Sample Selection.\nTo verify that our proposed\nmethod robustly trains the feature extractor on label noise,\nwe compute the F1-score for all training epochs to evaluate\nnoise sample filtering in sample-selection methods for various symmetric and asymmetric label noise. We compare the\nquality of sample selection with and without ALASCA on\nthe Co-teaching. In Figure 4, the F1-scores of sample selection with ALASCA are consistently higher on various label\nnoise than the baseline. Although baseline approaches use\ndifferent criteria to filter noisy instances (Co-teaching with\nloss values), we observe that ALASCA improves the quality of all subsets and is effective in training robust feature\nextractors.\nRobust to Hyperparameter Selection.\nExisting LNL\nmethods have large performance differences depending on\ntheir hyperparameters, and, if the hyperparameter value is\nimproperly selected, the performance is provably lower than\nthat of standard training. However, the value of the optimal\nhyperparameters depends on the network architecture and\ndataset. We combine ALASCA and existing LNL methods\nwith various hyperparameter settings: (1) Standard with different weight decay factors; (2) ELR with different regularization coefficients; (3) Co-teaching along different warmup\nepochs; and (4) CRUST with different coreset sizes. The detailed experiment setup and results are in Appendix C.3 and\nFigure 4b, respectively. While the baseline performances\nvary depending on the hyperparameters, performances with\nALASCA are robust even with different hyperparameters.\n4.3\nAblation Studies\nTo obtain further intuition on ALASCA, we conduct an ablation study on each component of our method. We design\nthree experiments: (1) compare existing methods in terms\nof performance and efficiency; (2) investigate performance\ntendency along the structure of the AC; and (3) component\nanalysis to understand the influence of each component.\nEfficiency of ALASCA.\nWe compare our implicit regularization framework with the following regularizationbased approaches: (1) HAR (Cao et al. 2021), which explicitly and adaptively regularizes the Jacobian matrix norm\nof data points in higher-uncertainty, lower-density regions\nmore heavily and (2) CDR (Xia et al. 2021), which identifies and regularizes non-critical parameters that tend to fit\nnoisy labels and cannot generalize well. These methods are\nsimilar to our proposed method for regularizing the intermediate layer, but are explicit regularization methods that\nMethod\nStandard\nGCE\nSCE\nCo-teaching\nFINE\nDivideMix\nELR+\nALASCA\nA-Coteaching\nA-ELR+\nAccuracy\n68.94\n69.75\n71.02\n70.15\n72.91\n74.76\n74.81\n73.78\n74.20\n74.92\nTable 2: Comparison of test accuracy (%) on Clothing1M dataset. Results for baselines are obtained from Liu et al. (2020) and\nKim et al. (2021a). A-Coteaching and A-ELR+ denote the methods combining ALASCA with Co-teaching and ELR+.\nDataset\nCIFAR-10\nEfficiency\nNoisy Type\nSymm.\nInst.\nMem.\nTime\nStandard\n81.9 ± 0.3\n74.1 ± 2.9\n× 1.0\n× 1.0\nHAR\n88.6 ± 0.1\n67.6 ± 0.7\n× 2.0\n× 5.2\nCDR\n87.0 ± 0.2\n75.8 ± 1.5\n× 1.3\n× 6.5\nALASCA (Different architecture and position of ACs)\nMLP\n88.5 ± 0.3\n76.3 ± 0.3\n× 1.1\n× 1.0\nMLP∗\n89.3 ± 0.1\n80.8 ± 0.3\n× 1.3\n× 1.2\nBottleneck\n89.2 ± 0.1\n77.9 ± 0.4\n× 1.0\n× 1.0\nBottleneck∗\n90.1 ± 0.1\n81.4 ± 0.3\n× 1.1\n× 1.2\nResidual\n89.0 ± 0.2\n77.1 ± 0.3\n× 1.1\n× 1.1\nResidual∗\n89.8 ± 0.2\n77.8 ± 0.5\n× 1.3\n× 1.3\nTable 3: Comparison of accuracy (%), GPU memory, and\ncomputation time. For HAR and CDR, we apply their official code. Without and with * denote attaching ACs at the\nend of only the third and all residual blocks of the backbone.\nare computationally expensive to find noisy data or parameters. Table 3 shows that ALASCA consistently outperforms\ncompeting regularization-based methods for various noise\ntypes and provides efficient training in terms of computational memory and training time.\nEffect of Auxiliary Classifiers.\nWe further compare the\nperformance of ALASCA with different architectures (2 layers MLP, residual block; He et al. 2016, bottleneck; Howard\net al. 2017) and positions of ACs. Table 3 shows how the\nperformance of ALASCA changes according to the architecture and position of ACs. We observe that using several\nACs effectively regularizes the intermediate layer, resulting\nin improved generalization. This result supports our motivations for using ACs to regularize intermediate layers and\nthereby enhance the robustness against label noise. Furthermore, the architecture of ACs does not affect performance\nmuch. Among the three architectures, bottleneck classifiers\nachieve competitive performance with the smallest additional computation costs. From these results, we apply the\nbottleneck block as the AC for all experiments.\nComponent Analysis.\nSince our ALASCA is composed\nof three parts: (1) ALS; (2) ACs; and (3) EMA confidence,\nwe perform a component analysis to understand which component is important for training robust feature extractors.\nOur experiments are conducted on CIFAR-10 under various\nnoise distributions. Table 4 summarizes that each component is indeed effective, as the performance improves with\neach addition of a component. However, the most important factor for high performance is the combination of ALS\nand ACs, which enable effective LR on intermediate layers.\nSymm. 50\nAsym. 40\nInst. 40\nULS\n69.5 ± 1.1\n78.7 ± 0.5\n59.2 ± 0.6\n+ AC\n84.7 ± 0.2\n87.2 ± 0.2\n77.0 ± 1.0\nALS\n82.0 ± 0.7\n85.1 ± 0.2\n74.8 ± 1.3\n+ AC\n86.6 ± 0.2\n90.0 ± 0.3\n78.8 ± 0.6\n+ EMA\n82.9 ± 0.8\n86.9 ± 1.1\n75.1 ± 2.1\nALASCA\n88.0 ± 0.3\n90.3 ± 0.3\n81.4 ± 0.4\nALASCA∗\n89.1 ± 0.4\n90.6 ± 0.2\n82.5 ± 0.9\nTable 4: Component analysis on each component of our proposed methods. ALASCA∗ denotes that result from ensemble of all classifiers during inference phase and the bold\nnumbers indicate the best result.\nMoreover, we verify that the performance of ALASCA improves using an ensemble of predictions from the main and\nACs as we mentioned in Section 3.2.\n4.4\nResults on Real-world Datasets\nClothing1M (Xiao et al. 2015) contains one million clothing images obtained from online shopping websites with\n14 classes and estimated noise level of 38.5% (Song et al.\n2019). We apply ResNet50, which is widely used in previous studies (Liu et al. 2020; Kim et al. 2021b) on the Clothing1M dataset. Table 2 compares ALASCA to the SOTA\nmethods on the Clothing1M dataset. ALASCA achieves a\ncompetitive performance with the SOTA baseline methods\nalthough using only a single network with lower computational costs. Furthermore, we observe that ELR+ with\nALASCA (A-ELR+) realizes a new SOTA performance and\nverify that our proposed method also works well on realworld datasets. Additionally, we apply ALASCA on the\n(mini) WebVision dataset, a famous real-world dataset with\nlabel noise, and obtain similar results to those on Clothing1M. We report the detailed results in Appendix D.4.",
        "conclusion": "In this paper, we provide a theoretical analysis that LS encourages LR, and build upon the resulting insights to propose an effective and practical framework, ALASCA. Based\non the resulting theoretical motivation, we combine ALS,\nAC, and EMA confidence to efficiently enable adaptive LR.\nWe experimentally show that ALASCA enhances the robustness of feature extractors and improves the performance of\nexisting LNL methods on benchmark-simulated and realworld datasets. In future work, we believe that our approach\nwill arise interest in designing a novel regularization strategy\nfor feature extractors.",
        "summary_en": "As deep neural networks can easily overfit noisy labels, robust training in the presence of noisy labels is becoming an important challenge in modern deep learning. While existing methods address this problem in various directions, they still produce unpredictable sub-optimal results since they rely on the posterior information estimated by the feature extractor corrupted by noisy labels. Lipschitz regularization successfully alleviates this problem by training a robust feature extractor, but it requires longer training time and expensive computations. Motivated by this, this paper proposes a simple yet effective method, called ALASCA, which efficiently provides a robust feature extractor under label noise. ALASCA integrates two key ingredients: (1) adaptive label smoothing based on the paper's theoretical analysis that label smoothing implicitly induces Lipschitz regularization, and (2) auxiliary classifiers that enable practical application of intermediate Lipschitz regularization with negligible computations. The paper conducts wide-ranging experiments for ALASCA and combine the proposed method with previous noise-robust methods on several synthetic and real-world datasets. Experimental results show that the framework consistently improves the robustness of feature extractors and the performance of existing baselines with efficiency.",
        "summary_zh": "这篇论文介绍了一种名为ALASCA的方法，旨在解决深度神经网络在存在噪声标签的情况下容易过拟合的问题。现有方法虽然在各个方向上解决了这个问题，但它们依赖于由噪声标签损坏的特征提取器估计的后验信息，仍然会产生不可预测的次优结果。为了解决目前存在问题，论文提出了一种简单而有效的方法，即ALASCA。该方法集成了两个关键要素（1）基于作者对标签平滑隐含诱导 Lipschitz 正则化的理论分析来自适应标签平滑（2）使用辅助分类器，实现了中间Lipschitz正则化的实际应用，并且计算量可以忽略不计。论文进行了广泛的实验，并将其与以前的噪声鲁棒方法结合应用于多个合成和真实数据集。实验结果表明，该框架能持续有效地提高特征提取器的鲁棒性和现有基准的性能。"
    },
    {
        "title": "Denoising after Entropy-Based Debiasing a Robust Training Method for Dataset Bias with Noisy Labels",
        "abstract": "Improperly constructed datasets can result in inaccurate inferences. For instance, models trained on biased datasets perform poorly in terms of generalization (i.e., dataset bias). Recent debiasing techniques have successfully achieved generalization performance by underestimating easy-to-learn samples (i.e., bias-aligned samples) and highlighting difﬁcult-to-learn samples (i.e., bias-conﬂicting samples). However, these techniques may fail owing to noisy labels, because the trained model recognizes noisy labels as difﬁcult-to-learn and thus highlights them. In this study, we ﬁnd that earlier approaches that used the provided labels to quantify difﬁculty could be affected by the small proportion of noisy labels. Furthermore, we ﬁnd that running denoising algorithms before debiasing is ineffective because denoising algorithms reduce the impact of difﬁcult-to-learn samples, including valuable bias-conﬂicting samples. Therefore, we propose an approach called denoising after entropy-based debiasing, i.e., DENEB, which has three main stages. (1) The prejudice model is trained by emphasizing (bias-aligned, clean) samples, which are selected using a Gaussian Mixture Model. (2) Using the per-sample entropy from the output of the prejudice model, the sampling probability of each sample that is proportional to the entropy is computed. (3) The ﬁnal model is trained using existing denoising algorithms with the mini-batches constructed by following the computed sampling probability. Compared to existing debiasing and denoising algorithms, our method achieves better debiasing performance on multiple benchmarks.",
        "introduction": "Deep neural networks (DNNs) have achieved human-like performance in various tasks, such as image classiﬁcation (He\net al. 2016), image generation (Goodfellow et al. 2014), and\nobject detection (He et al. 2017), but require well-organized\ntraining datasets for success. For example, the trained model\nmight have prejudices when its training dataset is biased. In\nreal life, we often encounter dataset bias problems (Bahng\net al. 2020; Nam et al. 2020; Lee et al. 2021; Kim, Lee, and\nChoo 2021). For example, as shown in Figure 1, the dataset\nfor classifying camels in images could be very biased, as\nmost camel images are captured against a desert background\n(i.e., bias-aligned); only a few images are captured against\nother backgrounds, such as forests (i.e., bias-conﬂicting).\nEasy-to-learn\nDifficult-to-learn\nBias\nLabel Flip\nLabel: Horse, Bg: Forest\nLabel: Horse, Bg: Desert\nLabel: Camel, Bg: Forest\nLabel: Camel,  Bg: Desert\nFigure 1: Examples of biased dataset with noisy labels. (1)\nBlue: bias-aligned, clean label samples. (2) Orange: biasconﬂicting, clean labels samples. (3) Green: bias-aligned,\nnoisy labels. (4) Red: bias-conﬂicting, noisy labels. The\ndashed background represents difﬁcult-to-learn samples.\nTherefore, except for (bias-aligned, clean) case, other cases\nare difﬁcult-to-learn. To mitigate dataset bias with noisy labels, training directions for each type differ. For example,\n(bias-aligned, noisy) case must be discarded or cleansed,\nwhile (bias-conﬂicting, clean) cases have to be emphasized.\nThis unintended bias causes the trained model to infer erroneously based on shortcuts (i.e., background). To mitigate\nsuch dataset bias, previous researches have used the fact that\nbias-conﬂicting samples are more difﬁcult-to-learn than biasaligned samples (Nam et al. 2020). Various approaches have\nbeen proposed, such as adjusting the loss function (Bahng\net al. 2020; Nam et al. 2020; Creager, Jacobsen, and Zemel\n2021), feature disentanglement (Lee et al. 2021), creating\nmixed-attribute samples (Kim, Lee, and Choo 2021), or reconstructing balanced datasets (Liu et al. 2021; Ahn, Kim,\nand Yun 2023) to emphasize difﬁcult-to-learn samples.\nIn addition to dataset bias, noisy labels are caused by many\nreasons (Yu et al. 2018; Nicholson, Sheng, and Zhang 2016)\nand are known to degrade training mechanisms. For example,\nin Figure 1, the camel in the bottom row can be labeled\nhorse by human error. Numerous studies have focused on\nalleviating the impact of noisy labels or directly correcting\nthem. Some (Bahri, Jiang, and Gupta 2020; Zhang et al.\n2020; Veit et al. 2017; Ren et al. 2018; Hendrycks et al.\n2018) deal with the problem of noisy labels by assuming\nclean data to set training guidelines (i.e., purifying a given\n𝑓!\nEpoch 𝑡\nGMM\n𝑓!\n…\nEntropy \n𝐻(𝑓(𝑥!))\n𝑓\"\nBatch \nsampler\nLabel-free score\n𝑝(𝑥!,𝑦!)\nEmphasize\n(Algn, Clean)\n(1)\n(2)\n(3)\nAlgn, Clean\nAlgn, Noisy\nConf., Clean\nConf., Noisy\nPer-sample Loss\nFreezing\n𝑓!\nPrejudice Model\nRobust Model\nFreezing\nFigure 2: Overview of DENEB. It is composed of three steps.\n(1) Train by emphasizing (bias-aligned, clean) samples. (2)\nCompute label-free score, i.e., entropy. (3) Train the ﬁnal\nrobust model based on the batch sampler.\ncorrupted dataset by using a model trained on a small clean\ndataset). Recently, various methods have been proposed to\nrelax this strong clean subset assumption by taking advantage\nof the characteristic that noisy labels are more difﬁcult-tolearn than clean samples. For example, such difﬁcult-to-learn\nsamples are guided by regularizers (Liu et al. 2020; Cao et al.\n2019, 2020), giving lower weights (Wang et al. 2019; Zhang\nand Sabuncu 2018), cleansing out (Mirzasoleiman, Cao, and\nLeskovec 2020; Wu et al. 2020; Pleiss et al. 2020; Han et al.\n2018; Yu et al. 2019), or utilizing a semi-supervised learning\nalgorithm by considering them as unlabeled samples (Kim\net al. 2021; Li, Socher, and Hoi 2019).\nAlthough dataset bias and noisy labels can occur simultaneously and independently, few studies (Creager, Jacobsen, and\nZemel 2021)1 have addressed both problems at once. This is\nbecause the fundamental solutions of each problem are exact\nopposites. Difﬁcult-to-learn samples have to be emphasized\nto mitigate dataset bias (Nam et al. 2020; Lee et al. 2021),\nwhile their inﬂuence should be reduced for denoising (Han\net al. 2018; Zhang and Sabuncu 2018). Dataset bias and noisy\nlabels can occur concurrently in the real-world. Therefore,\nboth problems must be handled together.\nContribution. We present a training method that addresses\ndataset bias with noisy labels (DBwNL). We ﬁrst empirically\nanalyze why existing debiasing and denoising algorithms fail\nto achieve their respective objectives. In this regard, we discover two facts. (1) Existing debiasing methods using given\nlabels suffer from problems when the training dataset contains noisy labels because they determine the degree of being\nemphasized based on a given label (e.g., per-sample loss).\n(2) Denoising methods eliminate valuable bias-conﬂicting\nsamples (i.e., samples that should be emphasized for debiasing). This is because their denoising mechanism cleans\nor discards difﬁcult-to-learn samples without considering\nwhether a sample is bias-conﬂicting or noisy label.\nBased on these ﬁndings, we propose an algorithm, coined\nas DENEB, which denoising after entropy-based debiasing\n1EIIL aimed to study dataset bias problem without human supervision. They partially analyzed the impact of noisy labels in their\nsynthetic benchmark only.\n(see Figure 2). The proposed method consists of three stages.\nThe ﬁrst stage trains a prejudice model biased toward (biasaligned, clean) data. To this aim, DENEB uses the Gaussian\nMixture Model (GMM) based on per-sample losses to split\n(bias-aligned, clean) and the others at the beginning of each\nepoch. In the next stage, DENEB measures the entropy for\neach sample from the prejudice model. From the per-sample\nentropy, DENEB calculates the sampling probability of each\nsample in proportion to the entropy. The key intuition of the\nsampling probability is that (bias-aligned, noisy) samples are\npredicted to have low entropy by the prejudice model because it mainly learns (bias-aligned, clean) samples, but the\nexcluded bias-conﬂicting samples will have a large entropy\nprediction. Note that the sampling probability is obtained\nwithout using the given labels as they might be corrupted.\nFinally, DENEB trains the ultimate robust model on the sampled mini-batches, where the sampling probabilities are obtained during Step 2.\nWe demonstrate the efﬁcacy of DENEB on a variety of\nbiased datasets, including Colored MNIST (Nam et al. 2020;\nBahng et al. 2020; Kim, Lee, and Choo 2021; Lee et al.\n2021), Corrupted CIFAR-10 (Nam et al. 2020; Lee et al.\n2021), Biased Action Recognition (BAR) (Nam et al. 2020;\nKim, Lee, and Choo 2021), and Biased Flickr-Faces-HQ (Lee\net al. 2021; Kim, Lee, and Choo 2021), with symmetric noisy\nlabels. Compared to the existing debiasing, denoising, and\nnaive combination of both algorithms, the proposed method\nachieves a successful debiasing performance for all benchmarks. For example, DENEB improves the unbiased test accuracy from 39.24% to 91.81% on a colored MNIST dataset\nwith 1% bias scenario with 10% noisy labels and BAR from\n54.37% to 62.30% compared to the vanilla model.",
        "dataset bias with noisy labels (dbwnl)": "In this section, we deﬁne the dataset bias problem and the\nnoisy label separately. Subsequently, we describe dataset\nbias with noisy labels, which is when dataset bias and noisy\nlabel problems occur in conjunction.\nDataset Bias.\nConsider a dataset D = {(xi, yi)}N\ni=1 in\nwhich each input is xi and its corresponding truth label\nyi = {1, ..., C}. Each sample can be described by a set\nof attributes. For example, the images in Figure 1 can have\nbackground, object, and so on. For convenience of explanation, we look at the top of Figure 1 (Blue and Orange\ncases), without the noisy label case. The objective, i.e., the\ntarget attribute, is to classify the “camel.” When most of the\nsamples have attributes that are strongly correlated with the\ntarget, we call the phenomenon dataset bias and these attributes bias attributes. In Figure 1, the bias-attribute “desert\nbackground,” and the target attribute “camel” are highly correlated, i.e., almost “camel” images are captured against “desert\nbackground.” We call samples whose bias attribute is highly\ncorrelated (weakly correlated) with the target attribute biasaligned (bias-conﬂicting) samples. This dataset bias problem\nis quite harmful when the bias attribute is easier to learn than\nthe target attributes, because the model loses the motivation\nto learn the target attribute given its sufﬁciently low loss.\nWe focus on the case where the bias attribute is easier to\nlearn than the target. For convenience, we denote the set of\nbias-conﬂicting and bias-aligned samples by Dc and Da as\nthey are clearly distinct, but both sets need not be strictly\nseparable. Note that the portion of bias-conﬂicting sample is\ncalled the bias conﬂict ratio α, which is deﬁned as:\nα = |Dc|\n|D| .\nNoisy Labels. Collected labels may be corrupted. If a person labels the image x, the provided label can be corrupted\ni.e., ygiven ̸= y, even though the true label is y. We call samples whose labels are ygiven = y and ygive ̸= y clean label\nand noisy label, respectively. As shown in Figure 1, the lower\nrow represents noisy label cases. For example, although the\nimages in Figure 1 of the bottom boxes are “camel”, they\nare labeled as “horse”. We denote the portion of the samples\nwhose labels are ﬂipped as the noise ratio η. For convenience,\nwe focus on the cases where the label corruption occurs symmetrically.\nygiven =\n\u001a˜y ∼ Uniform(C).\nwith probability η\ny\nwith probability 1 − η ,\nDBwNL. DBwNL cases occur sequentially, gathering images x and labeling y. As mentioned above, the biased dataset\nhas a small portion of bias-conﬂicting samples, i.e., α is small.\nTherefore, most of the samples in the given training dataset\nare bias-aligned. Training a robust model on the DBwNL\ndataset emphasizes bias-conﬂicting samples while discarding\nor reducing the impact of the noisy labels.",
        "failure to debias on a dbwnl dataset": "In this section, we brieﬂy summarize existing debiasing methods and demonstrate that they are vulnerable to noisy labels.\nBrief summary of the previous methods. In previous debiasing algorithms, bias-conﬂicting samples are highlighted\nbased on each proposed score. Almost all previous approaches train a biased model fb on the given training dataset,\nand a debiased model fd is trained with emphasis in the following ways:\n• Relative difﬁculty score (LfF (Nam et al. 2020),\nDisen (Lee et al. 2021))\nW(x, ygiven) =\nLCE(fb(x), ygiven)\nLCE(fb(x), ygiven) + LCE(fd(x), ygiven),\n(1)\nwhere LCE denotes conventional cross-entropy loss and\nfb(·) and fd(·) are softmax outputs of biased and debiased\nmodels, respectively.\n• Per-sample accuracy (JTT (Liu et al. 2021))\nDerror-set = {(x, y) s.t. ygiven ̸= arg max\nc\nfb(x)[c]}, (2)\nwhere fb(x)[c] denotes the softmax output of logit c. The\nultimate debiased model is trained on Dtrain composed of\nλup times Derror-set and the other Dcorr-set = D \\ Derror-set.\nERM\nLfF\n5%\n2%\n1%\n0.5%\nAccuracy (%)\n0\n30\n60\n90\nNoise Ratio (%)\n0 5 10\n20\n50\n(a) LfF (Nam et al. 2020)\nERM 5%\nJTT 5%\nAccuracy (%)\n0\n20\n40\n60\n80\n100\nNoise Ratio (%)\n0 1\n5\n10\n(b) JTT (Liu et al. 2021)\nERM\nDisen\n5%\n2%\n1%\n0.5%\nAccuracy (%)\n0\n30\n60\n90\nNoise Ratio (%)\n0 5 10\n20\n50\n(c) Disen (Lee et al. 2021)\nERM\nEnt\n5%\n2%\n1%\n0.5%\nAccuracy (%)\n0\n30\n60\n90\nNoise Ratio (%)\n0 5 10\n20\n50\n(d) Entropy\nFigure 3: Performance when label corruption occurs. In the\ncase of LfF and Disen, it is the unbiased test accuracy of\ncolored MNIST, and JTT is the worst case test performance\nof the waterbird dataset. The triangle-dotted lines are the\nvanilla results, and the circle-solid lines represent the result\nof each algorithm. All algorithms except for entropy case\nperform worse than vanilla as noise ratio η increases.\n3.1\nDebiasing Meets Noisy Labels\nAs in (1) and (2), all previous techniques are based on the\ngiven label ygiven. Here, we refer to methods using (x, ygiven)\nand only (x) respectively as “label-based debiasing” and\n“label-free debiasing.” We observe the ultimate performance\nof previous methods when noisy labels are injected. We used\nthe settings offered by their ofﬁcial repositories234, such as\ndataset, implementation, and hyperparameters except for label ﬂipping. For comparison, we include entropy-based debiasing, which highlights samples with proportion to the\nper-sample entropy score. It does not require the given label, i.e., label-free method. Detail description about entropybased setting is described in Appendix.\nLabel-based debiasing is prone to noisy labels. As shown\nin Figure 3, the performances of the label-based techniques\nare lower than those of the vanilla case; a small noise ratio\noccurs. However, as demonstrated in Figure 3d, the labelfree method performs better than in the vanilla case. This\nis because label-based methods make incorrect emphasis,\nW(x, ygiven) and Derror-set, when ygiven is corrupted.\nWhy do label-based methods suffer side-effects? As in\nFigure 4a 4b, and 4c, the label-based scores of the (conﬂicting, clean) and (aligned, noisy) are entangled. This means\nthat noisy labels are also emphasized when we run the\nlabe-based algorithms. By contrast, the label-free method,\ni.e., Entropy in Figure 4d, shows that the bias-conﬂicting\nand bias-aligned samples are easily distinguished regardless\n2https://github.com/alinlab/LfF\n3https://github.com/anniesch/jtt\n4https://github.com/kakaoenterprise/Learning-DebiasedDisentangled\n(Clean, Bias-aligned)\n(Noisy, Bias-aligned)\n(Clean, Bias-conﬂicting)\n(Noisy, Bias-conﬂicting)\nDensity\n0\n100\n0\n0.5\n1.0\nNoise 10% Bias 0.5%\nDensity\n0\n100\n0\n0.5\n1.0\nNoise 10% Bias 5%\nWeight\n(a) LfF (Nam et al. 2020)\nDensity\n0\n100\n×1\n× λUp\nNoise 0%\nDensity\n0\n100\n×1\n× λUp\nNoise 10%\nWeight\n(b) JTT (Liu et al. 2021)\nDensity\n0\n100\n0\n0.5\n1.0\nNoise 10% Bias 0.5%\nDensity\n0\n100\n0\n0.5\n1.0\nNoise 10% Bias 5%\nWeight\n(c) Disen (Lee et al. 2021)\nDensity\n0\n10\n0\n1\nNoise 10% Bias 0.5%\nDensity\n0\n10\n0\n0.5\n1.0\nNoise 10% Bias 5%\nWeight\n(d) Entropy\nFigure 4: Score histogram of each methodology. As LfF and\nDisen operate online, we report the weight histogram right\nafter the last epoch of training. Except for the Entropy case,\nLfF, JTT, and Disen shows entangled histograms between\n(noisy, aligned), (clean, conﬂicting), and (noisy, conﬂicting).\nBy contrast, the histogram of entropy case indicates that it is\nclustered not according to label corruption but bias.\nof whether their labels are noisy. In conclusion, a label-free\nmethod is needed to handle DBwNL.",
        "denoising after entropy-based debiasing": "In Section 3, we veriﬁed that the debiasing algorithms do\nnot work properly for DBwNL alone. In this section, we\nanalyze how debiasing algorithms should be combined with\ndenoising algorithms, and ﬁnally propose DENEB.\n4.1\nHow Denoising Algorithms Work in the\nDBwNL\nWe ﬁrst check how the denoising algorithms work in the\nDBwNL dataset by observing two cases.\nObservation Setting. Five denoising methods are used for\nthe analysis: AUM (Pleiss et al. 2020), Co-teaching (Han\net al. 2018), DivideMix (Li, Socher, and Hoi 2019), and\nf-DivideMix (Kim et al. 2021). We measure the number of samples of (noisy, aligned) and (clean, conﬂicting)\nafter running the denoising algorithms. We run two types\nof tests. (1) Without modiﬁcation (\u0017): to check how the\ndenoising algorithms handle bias-conﬂicting samples. (2)\nManually weighted training ( ): we assign ×50 weights to\nbias-conﬂicting samples, i.e., 50 × LCE(x, y), ∀(x, y) ∈ Dc.\nThe second case is unrealistic as we cannot know which sample is bias-conﬂicting, but the result can convey the following\nargument: if we want to protect bias-conﬂicting samples from\nthe discarding by the denoising mechanism, make the biasconﬂicting samples easy-to-learn.\nValuable\nbias-conﬂicting\nsamples\ncan\nbe\ndeemed\nnoisy.\nAs illustrated in Figure 5, all denoising methods\nAUM\nCo-teaching\nDivideMix\nf-DivideMix\nAUM (w)\nCo-teaching (w)\nDivideMix (w)\nf-DivideMix (w)\nInitial\nIntended region\nUnintended region\nNoisy label\n0\n500\n4500\nBias-conﬂicting\n0\n200\n400\n(a) α = 1%, η = 10%\nIntended region\nUnintended region\nNoisy label\n0\n500\n22,000\nBias-conﬂicting\n0\n1000\n(b) α = 5%, η = 50%\nFigure 5: Number of remaining noisy labels and biasconﬂicting samples after denoising is conducted. Star ⋆ mark\nrepresents the number of samples before cleansing, and \u0017 and\n marks indicate with or without weighted training results.\nSince bias-conﬂicting samples is precious for debiasing, biasconﬂicting samples have to be protected. Therefore, the region loses bias-conﬂicting samples (left, blue) is the unintended region. On the other hand, the region ignores noisy labels without losing the bias-conﬂicting samples (right, cyan)\nis the intended behavior.\nsufﬁciently differentiate noisy samples. For example, ⋆\nin Figure 5a represents that the initial number of noisy\nsamples is almost 5, 000, but almost all methods dropped\nto near 0 after denoising (\u0017\nmarks). However, crucial\nbias-conﬂicting samples are also eliminated, i.e., \u0017 marks\nare in the “unintended region.” Therefore, utilizing denoising algorithms before debiasing can discard valuable\nbias-conﬂicting samples. As in Figure 3, because the\nnumber of bias-conﬂicting samples is critical, removing\nthe bias-conﬂicting samples prior to debiasing can cause\nperformance degradation.\nPreventing bias-conﬂicting samples from being discarded. Bias-conﬂicting samples are considered noisy labels\nbecause the trained model thinks that they are difﬁcult-tolearn. As illustrated in Figure 5 using  marks, when we\nsufﬁciently highlight bias-conﬂicting samples, noisy labels\ncan be eliminated by reducing the loss of bias-conﬂicting\nsamples. Therefore, we can conclude that denoising should\nbe performed after highlighting bias-conﬂicting samples.\n4.2\nDesigning the Algorithm for DBwNL\nBased on the previous experimental results, two inferences\ncan be drawn in designing a debiasing algorithm for the\nDBwNL datasets. (1) No label-based: label-based debiasing\nemphasizes noisy labels. (2) Debiasing before denoising: denoising algorithms should be run after debiasing emphasizes\nbias-conﬂicting samples. We summarize our intuitions in\nFigure 6. For the DBwNL dataset, the main consideration is\nwhether to apply debiasing or denoising ﬁrst. If denoising is\napplied ﬁrst, the bias-conﬂicting samples is erased, which is\nburdensome for debiasing (see the lower α cases in Figure 3).\nConversely, if debiasing is conducted ﬁrst, we can choose\nlabel-based or label-free. If a label-based algorithm is selected, the noisy labels are enlarged and a burden is placed on\nthe denoising algorithm (see the higher η cases in Figure 3).\naligned, Clean\nconflicting, Clean\nDeb. (Label-based)\nDeb. (Label-free)\nDeb. (Label-based)\nDeb. (Label-free)\nDen.\nDen.\naligned, Noisy\nconflicting, Noisy\nDen.\nFigure 6: Case study of designing algorithm for DBwNL.\nIn other words, emphasizing the bias-conﬂicting sample and\nproceeding with denoising without emphasizing the noisy\nlabel through the label-free algorithm is the correct order.\n4.3\nDenoising after Entropy-Based Debiasing\nBased on the case study, we propose denoising after entropybased debiasing, DENEB, which is composed of three steps.\nStep 1: Train the prejudice model fp. The key aim while\ntraining the prejudice model fp is that regardless of label corruption, the model should comprehensively learn\nthe bias-aligned samples so that it can identify the biasconﬂicting samples in the next steps. However, it is difﬁcult to detach (bias-aligned, noisy) from the bias-conﬂicting\nsamples. Therefore, DENEB trains the prejudice model on\n(bias-aligned, clean) only. Intuitively speaking, if the model\nis trained using only (bias-aligned,clean) samples, (biasaligned,noisy) samples also can be regarded as easy-to-learn\nthanks to the bias attributes. DENEB ﬁnds (bias-aligned,\nclean) by using the GMM, similar with (Li, Socher, and Hoi\n2019; Kim et al. 2021). Step 1 consists of two sub-steps.\nAt ﬁrst, fp is trained on D with conventional cross-entropy\nloss, until the warm-up epoch ew. After the warm-up phase,\nDENEB splits D and obtains ¯D at the beginning of each\nepoch. To do so, DENEB dynamically ﬁts a GMM on persample losses and obtains ¯D whose probability of GMM\ng(xi, yi) is higher than the threshold pt at the beginning of\neach epoch:\n¯D = {(xi, yi)|g(xi, yi) > pt, where (xi, yi) ∈ D},\n(3)\nNote that, unlike\nDivideMix and\nf-DivideMix,\nDENEB does not use the samples whose g(xi, yi) ≤ pt,\nto deepen bias, i.e., it ignores every-types except for (biasaligned,clean).\nStep 2: Calculate sampling probability.\nBased on the\ntrained prejudice model fp, we extract the entropy score\nfor each sample:\nHτ(x) = −\nC\nX\nc\nfp(x, τ)[c] × log fp(x, τ)[c],\n(4)\nwhere fp(x, τ)[j] is the temperature-scaled softmax for\nclass c with temperature parameter τ, i.e., fp(x, τ)[j] =\nexp(qp(x)[j]/τ)\nP\nc exp(qp(x)[c]/τ) with logit qp(x). Based on Hτ(x), we ﬁnd\nthe sampling probability of each instance as follows:\nP(xi, yi) =\nHτ(xi)\nP\n(xj,yj)∈D Hτ(xj).\n(5)\nThe reason why P(xi, yi) is proportional to the entropy score\nis because fp is sufﬁciently biased and thus the larger entropy\nsamples are the bias-conﬂicting samples (see Figure 4d).\nStep 3: Train the robust model fr. To train the robust\nmodel fr, mini-batches are constructed based on the sampling probability in equation 5. As mini-batches contain sufﬁcient bias-conﬂicting samples, the main purpose of this\nstep is to mitigate the impact of noisy labels. To this end,\nwe inherit previous denoising algorithms by simply modifying the mini-batches. Note that DENEB can utilize any\ngiven denoising algorithm, Aden, but we report based on GCE\nwhich performs better than the others. As analysis of various\ndenoising algorithms is reported in Section 5.",
        "experiment": "The effectiveness of the proposed algorithm is analyzed quantitatively and qualitatively. We compare DENEB to earlier\ndebiasing and denoising techniques in four biased datasets,\ni.e., Colored-MNIST (CMNIST), Corrupted CIFAR-10 (CCIFAR), Biased Action Recognition (BAR), and Biased FFHQ\n(BFFHQ). To test the generalization performance, we report\nthe unbiased test accuracy. Details of the implementation and\ndatasets are given in Appendix.\nDataset\ntrain/valid/test\n#class\nTarget\nBias\nCMNIST\n54K / 6K / 10K\n10\nShape\nColor\nCCIFAR\n45K / 5K / 10K\n10\nObject\nBlur\nBAR\n1,746 / 195 / 654\n6\nAction\nPlace\nBFFHQ\n17,280/1,920/1,000\n2\nGender\nAge\nTable 1: Benchmark Summary\n5.1\nExperimental Settings\nBaselines.\nWe report the performance of the debiasing and denoising algorithms. As debiasing algorithms,\nwe use recent methods that are ofﬁcially available, such\nas LfF, JTT5, EIIL, and Disen. We utilize denoising algorithms GCE, SCE, ELR+, Co-teaching, DivideMix,\nand f-DivideMix. All implementations are reproduced\nfollowing the ofﬁcial codes. The implementation and hyperparameters are reported in Appendix.\nDatasets. We employ four benchmarks: CMNIST, CCIFAR,\nBAR, and BFFHQ. The target and bias attributes are summarized in Table 1. For the CMNIST and CCIFAR datasets,\ntwo pairs of bias-ratio (α, η) = {(1%, 10%), (5%, 50%)}\nare utilized. The other datasets are tested on η = 10%. We\nsummarize in detail the construction recipe in Appendix.\nCMNIST and CCIFAR. CMNIST and CCIFAR datasets\nhave a bias attribute, which is injected manually. The goal of\nCMNIST is to classify the target attribute, digit shape,\nwhen the bias attribute is color. This dataset comes\nfrom (Nam et al. 2020; Bahng et al. 2020; Lee et al. 2021;\nKim, Lee, and Choo 2021). In CCIFAR, the target attribute\n5As (Liu et al. 2021) assume that a balanced validation dataset,\nit is unfair to directly compare with the other algorithms. However,\nsince JTT can be tuned using noisy biased validation dataset, we\nreport the behavior of JTT tuned by using a biased noisy vallidation.\nAlgorithm\nColored MNIST\nCorrupted CIFAR-10\nα = 1%, η = 10%\nα = 5%, η = 50%\nα = 1%, η = 10%\nα = 5%, η = 50%\nVanilla\n39.24 ± 1.91%\n70.13% ± 3.42 %\n25.43% ± 0.84 %\n31.86% ± 0.96 %\nDebiasing\nLfF\n29.87 ± 1.36%\n57.97% ± 1.79 %\n24.51% ± 1.30 %\n29.68% ± 2.63 %\nJTT\n63.24 ± 2.60%\n77.16% ± 1.15 %\n23.75% ± 0.61 %\n24.52% ± 0.98 %\nEIIL\n24.53 ± 0.31%\n42.25% ± 1.43 %\n20.30% ± 1.08 %\n22.66% ± 1.94 %\nDisen\n31.49 ± 5.44%\n69.20% ± 4.13 %\n22.52% ± 0.38 %\n28.35% ± 4.49 %\nDenoising\nGCE\n19.52 ± 1.98%\n73.45% ± 7.62 %\n24.96% ± 1.53 %\n30.72% ± 0.74 %\nSCE\n30.95 ± 2.87%\n62.10% ± 5.02 %\n23.34% ± 1.73 %\n29.87% ± 1.00 %\nELR+\n24.76 ± 0.90%\n49.38% ± 3.74 %\n22.10% ± 0.37 %\n30.84% ± 0.43 %\nAUM\n23.89 ± 2.60%\n49.51% ± 6.62 %\n23.55% ± 1.10 %\n28.06% ± 2.38 %\nCo-teaching\n41.89 ± 1.45%\n76.64% ± 5.52 %\n25.14% ± 0.27 %\n26.84% ± 0.52 %\nDivideMix\n20.48 ± 1.94%\n33.66% ± 2.91 %\n18.86% ± 0.28 %\n22.03% ± 0.59 %\nf-DivideMix\n22.06 ± 1.70%\n39.92% ± 3.26 %\n19.67% ± 0.25 %\n27.60% ± 0.54 %\nDENEB\nDENEB\n91.81 ± 0.84%\n94.55% ± 0.22 %\n26.05% ± 0.54 %\n35.32% ± 1.03 %\nTable 2: Unbiased test accuracy on CMNIST and CCIFAR. Best-performing results are marked in bold. All results are averaged\non three independent runs. DENEB represents i.e., Aden = GCE.\nAlgorithm\nBAR\nBFFHQ\nη = 10%\nη = 10%\nVanilla\n54.37 ± 1.10%\n71.38 ± 0.58%\nLfF\n53.62 ± 1.81%\n54.35 ± 0.91%\nJTT\n55.67 ± 2.16%\n70.18 ± 1.47%\nDisen\n55.80 ± 3.05%\n67.44 ± 2.57%\nGCE\n56.39 ± 0.95%\n68.45 ± 2.98%\nCo-teaching\n54.99 ± 1.28%\n69.28 ± 1.24%\nDivideMix\n52.01 ± 1.51%\n72.20 ± 0.58%\nDENEB\n62.30 ± 0.91%\n75.24 ± 0.68%\nTable 3: Unbiased test accuracy on BAR and BFFHQ. Best\nperforming results are marked in bold. All results are averaged on three independent runs.\nis objective such as {airplane, car,...} with the bias\nattribute corruption like {blur, ...}. We generate CCIFAR following the bias injection mechanism of (Nam et al.\n2020; Lee et al. 2021; Hendrycks and Dietterich 2018).\nBAR and BFFHQ.\nBAR and BFFHQ are consists\nof real-world images. These benchmarks are biased\nwhen selecting samples by seeing the multiple attributes.\nBAR (Nam et al. 2020) aims to classify actions such as\n{racing, climbing,...} with background bias. For example, (Climbing, Rockwall) are bias-aligned samples, while (Climbing, Ice-cliff) are the biasconﬂicting ones. BFFHQ (Kim, Lee, and Choo 2021; Lee\net al. 2021) aims to classify gender when its age is biased.\nFor example, the training dataset is made up of (Female,\nYoung (age ranging from 10 to 29)) and (Male, Old (age\nranging from 40 to 59)) and very few of (Female, Old)\nand (Male, Young) samples.\nImplementation details. For the Colored MNIST, we use\na Simple-ConvNet with three convolutional layers, ReLU\nactivation function (Agarap 2018), batch normalization (Ioffe\nand Szegedy 2015) and dropout (Srivastava et al. 2014).\nResNet-18 (He et al. 2016) pre-trained on ImageNet is used\nas a backbone network for the rest. Based on grid searches,\nwe ﬁnd hyperparameters for all algorithms using 90% and\n10% training and validation split. This implies that validation\ndatasets contain noisy labels and bias-conﬂicting samples.\nThe search space and searched hyperparameters are described\nin Appendix. For all experiments, we report the case where\nDENEB uses GCE as Aden, which achieves the best performance among all the denoising algorithms.\n5.2\nExperimental Results\nCMNIST and CCIFAR. Table 2 presents comparisons of\nthe accuracy of the unbiased test. Among the debiasing baselines, the accuracy-based algorithm, i.e., JTT, is better than\nthe vanilla model in the CMNIST case. However, all debiasing baselines obtain worse performance than the vanilla\nmodel in the CCIFAR case because, as mentioned earlier, the\ndebiasing algorithms highlight noisy labels that should not be\nemphasized. By contrast, denoising algorithms fail to debias,\nas they do not have a module to highlight bias-conﬂicting\nsamples. DENEB achieves the best performance for all injected bias cases. For example, the unbiased test accuracy of\nCMNIST with α = 1% and η = 10% shows that DENEB obtains 52.57% gain compared to the Vanilla model.\nBAR and BFFHQ. The performances of DENEB in realworld image benchmarks is also better than the other baselines. BAR shows 7.92% improvements over vanilla and\n6.5% improvement over Disen, which has the best performance among the debiasing algorithms. DENEB\nalso\nshows 5.91% performance gain over GCE. Similarly, BFFHQ\nshows 3.86% improvement over vanilla, 5.06% over JTT\nand 3.04% over DivideMix. Thus, entropy-based debiases\nwhen trained on a more complex raw image dataset.\nCombination of Debiasing and Denoising.\nIn order to\nstudy how the other debiasing algorithms work with denoising algorithms, i.e., debiasing → denoising similarly\nto DENEB, we report pairwise performance in Figure 7 for\nLfF\nJTT\nDisen\nDENEB\n0\n50\n100\n150\nGCE\nSCE\nELR+\nAUM\nCo-teaching DivideMix\nα=1% η=10%\nLfF\nJTT\nDisen\nDENEB\n0\n50\n100\n150\nGCE\nSCE\nELR+\nAUM\nCo-teaching DivideMix\nα=5% η=50%\nFigure 7: Combination result of Colored MNIST benchmark.\nAll cases are the performances of Debiasing → Denoising,\ni.e., obtain per-sample weights from DENEBand then run\nGCE for DENEB→GCE case.\nCMNIST. As Disen and LfF are an online algorithms, we\nmultiply the per-sample weight at the end of debiasing by\ndenoising loss. Details are provided in Appendix. As shown\nin Figure 7, DENEB performs better than the other debiasing algorithms for all combinations. DENEB has better performance because the side-effects of focusing on the noisy\nsample are minimized when using the label-free entropy.",
        "related work": "Noisy labels. (Ghosh, Kumar, and Sastry 2017) had proposed the mean absolute error (MAE), and (Zhang and\nSabuncu 2018) claim that MAE suffers poor robustness\nwith DNN and suggested another type of cross-entropy (CE)\nloss, called generalized cross-entropy (GCE). The authors\nof (Wang et al. 2019) propose symmetry cross-entropy (SCE)\nloss, which is a combination of conventional CE and reverse\ncross-entropy. Lukasik et al. (Lukasik et al. 2020) use label smoothing techniques for noisy labels. Recently, studies\non the early learning phase have been a topic of extensive\ninterest. These works claim that DNNs memorize difﬁcult-tolearn samples in the later phase and learn common features\nin the early learning phase. Based on this fact, (Liu et al.\n2020) propose the early learning regularizer (ELR) to prohibit memorizing noisy labels. Some works handle noisy\nlabels by detecting and cleansing. To do so, the co-training\nmethod, i.e., teaching each other, is mainly used. In (Han\net al. 2018) and (Yu et al. 2019) utilize loss and disagreement\nare utilized to construct a clean subset. (Pleiss et al. 2020)\nproposes a new metric, area under margin (AUM), to cleanse\nthe dataset. (Li, Socher, and Hoi 2019) look at the noisy label problem as a semi-supervised learning (SSL) approach\nby dividing the training dataset into clean labeled and noisy\nunlabeled sets, and running the SSL algorithm (Berthelot\net al. 2019). FINE (Kim et al. 2021) uses the alignment of\nthe eigenvector to distinguish clean and noisy samples.\nDebiasing with human supervision. (Goyal et al. 2017,\n2020) construct a debiased dataset with the human hand.\n(Alvi, Zisserman, and Nell˚aker 2018; Kim et al. 2019; McDuff et al. 2019; Singh et al. 2020; Li, Li, and Vasconcelos 2018; Li and Vasconcelos 2019) use bias labels to mitigate the impact of bias labels when classifying target labels. EnD (Tartaglione, Barbano, and Grangetto 2021) proposes to entangle the target attribute and disengle the biased\nattributes. Multi-expert approaches (Alvi, Zisserman, and\nNell˚aker 2018; Kim et al. 2019; Teney et al. 2021) use a\nshared feature extrator with multiple FC layers to classify\nmultiple attributes independently. (McDuff et al. 2019; Ramaswamy, Kim, and Russakovsky 2021) use conditional generator to determine if the trained classiﬁer is biased. (Singh\net al. 2020) proposes overlap loss, which is measured based\non the class activation map. (Li and Vasconcelos 2019) employs bias type to detect bias-conﬂicting samples and reconstruct balanced dataset. On the other hand, (Geirhos et al.\n2018; Wang et al. 2018; Lee et al. 2019) using prior knowledge of the bias context to mitigate dataset bias. (Liu et al.\n2021) use bias labels for validation datasets to tune the hyperparameters.\nDebiasing without human supervision. To reduce human\nintervention, (Le Bras et al. 2020; Kim, Lee, and Choo 2021;\nIdrissi et al. 2022) utilize per-sample accuracy. They regard\nthe inaccurate samples as bias-conﬂicting. (Lee et al. 2021;\nNam et al. 2020) use the loss to calculate the weight. In\nthis case, samples with higher loss from the biased model\nare overweighted when training the debiased model. (Creager, Jacobsen, and Zemel 2021; Sohoni et al. 2020) infer the\nbias-conﬂicting labels and use the predicted labels to mitigate dataset bias problem. (Darlow, Jastrzkebski, and Storkey\n2020) generate the samples whose loss becomes large using VAE. (Zhang, Lopez-Paz, and Bottou 2022) propose an\ninitialization point for enlarging the features.",
        "conclusion": "Dataset bias with noisy labels can degrade prior debiasing algorithms. To overcome this issue, we propose\nDENEBcomprising three stpes. First, the prejudice model\nis trained on the clean bias-aligned samples. To do so, we utilize a GMM model to select clean bias-aligned samples. After\ntraining the prejudice model, DENEB compute the entropy\nscore for each sample. This entropy score does not require\nlabels, which can mislead the algorithm into detecting biasconﬂicting samples. Based on the obtained entropy score,\nwe compute a sampling probability proportional to the entropy score. To train the ﬁnal robust model, mini-batches are\nconstructed with sampling probabilities and existing denoising algorithms are run based on the sampled mini-batches.\nThrough extensive experiments across multiple datasets, such\nas Colored MNIST, Corrupted CIFAR-10, BAR, and BFFHQ,\nwe show that DENEBconsistently obtains substantial performance improvements compared to the other algorithms for\nthe debiasing, denoising, or na¨ıvely combined method. For\nfuture work, we plan to adapt this algorithm to other domains\nsuch as NLP, VQA, and so on. We hope that this study opens\nthe door of training a robust model on DBwNL dataset.",
        "summary_en": "Improperly constructed datasets can result in inaccurate inferences. For instance, models trained on biased datasets perform poorly in terms of generalization (i.e., dataset bias). Recent debiasing techniques have successfully achieved generalization performance by underestimating easy-to-learn samples (i.e., bias-aligned samples) and highlighting difficult-to-learn samples (i.e., bias-conflicting samples). However, these techniques may fail owing to noisy labels, because the trained model recognizes noisy labels as difficult-to-learn and thus highlights them. This paper finds that earlier approaches that used the provided labels to quantify difficulty could be affected by the small proportion of noisy labels. Furthermore, the paper finds that running denoising algorithms before debiasing is ineffective because denoising algorithms reduce the impact of difficult-to-learn samples, including valuable bias-conflicting samples. Therefore, the paper proposes an approach called denoising after entropy-based debiasing, i.e., DENEB, which has three main stages. (1) The prejudice model is trained by emphasizing (bias-aligned, clean) samples, which are selected using a Gaussian Mixture Model. (2) Using the per-sample entropy from the output of the prejudice model, the sampling probability of each sample that is proportional to the entropy is computed. (3) The final model is trained using existing denoising algorithms with the mini-batches constructed by following the computed sampling probability. Compared to existing debiasing and denoising algorithms, the method achieves better debiasing performance on multiple benchmarks.",
        "summary_zh": "这篇论文介绍了一种名为DENEB的方法，用于在存在噪声标签的情况下对数据集去偏，旨在解决传统去偏方法会受到噪声标签的影响而失效的问题。该方法包括三个主要阶段：（1）使用高斯混合模型选择出易于学习和困难学习的样本来训练偏见模型；（2）利用偏见模型计算每个样本的熵值，并根据其熵值计算出相应的采样概率；（3）使用现有的去噪算法并结合按照采样概率构建的小批量数据集来训练最终的模型。实验结果表明，相比于已有的去偏和去噪算法，DENEB能够在多个基准测试中取得更好的去偏性能。"
    },
    {
        "title": "A Distillation Approach to Data Efficient Individual Treatment Effect Estimation",
        "abstract": "The potential for using machine learning algorithms as a tool for suggesting optimal interventions has fueled signiﬁcant interest in developing methods for estimating heterogeneous or individual treatment effects (ITEs) from observational data. While several methods for estimating ITEs have been recently suggested, these methods assume no constraints on the availability of data at the time of deployment or test time. This assumption is unrealistic in settings where data acquisition is a signiﬁcant part of the analysis pipeline, meaning data about a test case has to be collected in order to predict the ITE. In this work, we present Data Efﬁcient Individual Treatment Effect Estimation (DEITEE), a method which exploits the idea that adjusting for confounding, and hence collecting information about confounders, is not necessary at test time. DEITEE allows the development of rich models that exploit all variables at train time but identiﬁes a minimal set of variables required to estimate the ITE at test time. Using 77 semi-synthetic datasets with varying data generating processes, we show that DEITEE achieves signiﬁcant reductions in the number of variables required at test time with little to no loss in accuracy. Using real data, we demonstrate the utility of our approach in helping soon-to-be mothers make planning and lifestyle decisions that will impact newborn health.",
        "introduction": "When\ndesigning\na\nlearning\nsystem\nto\nperform\ninterventions—whether through direct action or indirect\nrecommendation—it\nis\nimportant\nto\nmodel\nthe\nintervention’s causal effect on target outcomes rather than\nthe correlation between the two (Pearl 2009). Establishing\na causal relationship between the intervention and the\noutcome is necessary to ensure that the desired outcome\nwill happen with high probability should the intervention be\ncarried out. While conventional causal inference techniques\nfocus on calculating the average effect of an intervention over a population (Rosenbaum and Rubin 1983;\nRosenbaum 2002), more recent methods focus on estimating\npersonalized\nor\nindividual\ntreatment\neffects\n(ITEs; sometimes referred to as conditional average treatment effects) from observational data (Johansson, Shalit,\nand Sontag 2016; Athey, Tibshirani, and Wager 2016;\nShalit, Johansson, and Sontag 2017; Wager and Athey\n2018). These approaches perform what we call ITE discovery, i.e., using observational data to discover the causal\neffect of a treatment for any individual in the population.\nTo calculate individual treatment effects, these methods\nassume that all the variables used to train the ITE discovery model continue to be available for individuals at test\ntime. Unfortunately, there are often signiﬁcant practical constraints limiting the availability of data about new test cases.\nFor example, a physician may need to decide if a treatment will beneﬁt a speciﬁc patient without having all relevant medical test results at her disposal. In this situation,\nthe physician would prefer to identify and conduct the minimal set of necessary medical tests to accurately estimate\nthe treatment effect for this patient. Similar situations arise\nwith social workers, loan ofﬁcers, judges and other decisionmakers; they might need to identify a small set of attributes\nfor an individual in order to accurately estimate the effect of\na decision. We refer to this process as ITE prediction.\nITE prediction and ITE discovery are signiﬁcantly different tasks. For an algorithm to perform reliable ITE discovery, it needs to perform two functions: adjustment for confounding and estimation of heterogeneous effects. Adjustment for confounding accounts for the fact that treatment\nwas not randomly assigned in the observational data, and\nthat people who receive the treatment often have systematically different outcome likelihoods than those who did not.\nFor example, sicker patients who are more likely to die are\nalso more likely to receive aggressive treatments. Heterogeneous effects estimation accounts for the fact that individuals respond differently to the same treatment based on\ntheir characteristics. For example, elderly or frail patients\nmay have a systematically adverse response to an aggressive treatment. To adjust for confounding, researchers could\nappeal to a number of statistical methods that utilize a set\nof variables, confounders, to make the treated and the control populations appear statistically similar. To perform reliable ITE prediction, we only need good estimation of heterogeneous effects (which depend on individual characteristics\nthat are referred to as effect modiﬁers).\nMore formally, confounders affect both treatment likelihood and outcome values, whereas effect modiﬁers interact\nwith treatment status to affect outcomes. Figure 1 is a pictorial depiction of a simple example showing treatment efTreatment effect \nmodified by z = 0\nTreatment effect \nmodified by z = 1\nAverage \ntreatment effect\nFigure 1: ITE varies by effect modiﬁers (z), not by the confounder\nfect varying with the value of the effect modiﬁer, z. The xaxis shows the confounder, while the y-axis shows the value\nof the outcomes. To simplify this didactic example, we assume that z does not affect the outcome for the non-treated\ngroup, but that is not an assumption that is necessary for our\nwork. Note that, while outcomes vary with the confounder,\nthe treatment effect does not; the ITE is independent of the\nconfounders (since the dashed line and the solid blue line\nare parallel) but not the effect modiﬁers. This suggests that\nwhile both confounders and effect modiﬁers are required at\ntraining time, only the latter are required at test time. In situations where confounders are high dimensional while effect\nmodiﬁers are not, requiring the full set of variables at test\ntime would be demanding a number of variables that might\nbe redundant for ITE prediction.\nIn this work, we exploit the difference between the tasks\nat training time (ITE discovery) and test time (ITE prediction) to reduce the number of variables required at test time.\nWe develop an approach similar in spirit to model compression or knowledge distillation methods. Our Data Efﬁcient\nIndividual Treatment Effect Estimator (DEITEE) proceeds\nin two steps. At train time, a base model is tasked with both\nconfounding adjustment and estimation of heterogeneous\ntreatment effects. Next, a lightweight decision tree identiﬁes\nthe variables associated with the most variance in ITE and\nrequires only these variables to be queried during test time.\nIn addition to reducing the number of variables required at\ntest time, DEITEE also:\n1. Allows “early estimation”: the individual can receive an\nITE estimate after each query before answering all queries.\n2. Identiﬁes personalized questions based on the individual’s collected proﬁle, by dynamically following different\npathways in the tree to collect the most informative variables\nfor different individuals.\nTesting DEITEE on 77 semi-simulated datasets with varying data generating processes, we ﬁnd that DEITEE achieves\nlarge reductions in the number of variables required to compute the ITE with little to no loss in accuracy. We ﬁnd that\nthe variables queried tend to be effect modiﬁers even though\nour method provides no guarantees that they would be. Finally, using a dataset of over 89 thousand soon-to-be moms,\nwe show that our method can be used to help them make decisions about their habits and lifestyle choices that are consequential to their newborns’ health.",
        "related work": "Recent work in machine learning and causality has focused\non moving away from average treatment effect estimation to\npersonalized, ITE estimation. Approaches to modeling the\nITE span a large spectrum of statistical tools including the\nuse of Bayesian non-parametric models, random forests, and\ndeep neural networks (Athey, Tibshirani, and Wager 2016;\nAthey and Imbens 2016; Johansson, Shalit, and Sontag\n2016; Shalit, Johansson, and Sontag 2017; Alaa and van der\nSchaar 2018; 2017; Hill 2011). These approaches, however,\nassume that data available at training time will also be readily available at test time, not taking into account the fact that\ndata collection might be a non-trivial part of the pipeline\nat test time. Alaa and van der Schaar discuss the distinction between accounting for confounding, or selection bias,\nand ITE estimation. Their analysis focuses on the relative\nimportance of accounting for selection bias versus response\nsurface estimation (and hence ITE heterogeneity estimation)\nin small and large samples. Previous approaches to ITE estimation should be viewed as complementary to the work\npresented here. In fact, we use these algorithms as a part of\nour suggested approach.\nImportantly, existing work in ITE estimation can be classiﬁed into two: algorithms that model the treatment effect\nonly (e.g., Athey, Tibshirani and Wager 2016; Athey and\nImbens 2015) and those which model counterfactual outcomes (e.g., Hill 2016; Johansson, Shalit and Sontag 2016).\nThe former give an estimate of the difference between the\noutcome under the intervention and the outcome under nonintervention, while the latter give a full estimate of the outcomes under intervention and non-intervention. Estimating\ntreatment effects only is important in situations where the\ndecisions are made based on the difference between the beneﬁt of the treatment and its cost (i.e., return on “investment”)\nor if the outcome under non-treatment is known (e.g., a patient will most likely die if untreated). Our work falls under\nthe category of treatment effect rather than counterfactual\noutcome estimation.\nOur work is different from existing work focusing on recovering causal pathways and causal graphs (Spirtes and\nGlymour 1991). The goal of that line of work is to recover\nthe causal relationships between different variables in the\ndata generating process. This is distinct from our goal, which\nis to identify a small set of variables that are required to accurately predict the ITE with no claims about the causal relationships between the variables. However, we empirically\nshow that the variables collected tend to be effect modiﬁers.\nThe approach we take bears some resemblance to\nthat of the knowledge distillation and model compression\nframework (Lopez-Paz et al. 2016; Bucilua, Caruana, and\nNiculescu-Mizil 2006; Lou, Caruana, and Gehrke 2012).\nModel compression algorithms aim to create a less computationally complex or more intelligible model than the original model. Crucially they assume the task is the same during\ntraining and testing. Our work differs from that in that we acknowledge that the tasks at training and testing are different\nand we exploit that difference to create a more compressed\nmodel requiring fewer features at test time.",
        "preliminaries": "Without loss of generality, we frame our discussion using\nthe Neyman-Rubin framework of potential outcomes (Rubin\n2005). We focus on binary treatments t ∈ {0, 1}. We assume\nthat for a single individual, e.g., a patient, with feature vector x, there exist two potential outcomes Y1 and Y0 but only\none of them is observed. We denote the observed outcome\nby lowercase y. To emphasize that the unobserved or counterfactual outcome is a function of the individual features,\nwe use Yt(x) to refer to the counterfactual outcome for an\nindividual with features x under treatment t. The ITE, τ(x),\nis hence also a function of x and is equal to Y1(x) − Y0(x).\nWe make the classical assumptions of strong ignorability:\nYt\n⊨\nt | x; overlap: 0 < p(t = 1|x) < 1∀x; and consistency: y = Y0 if t = 0 and y = Y1 if t = 1.\nIn addition, we assume that the the counterfactual outcome can be expressed as: Yt(x) = g(x) + 1{t=1} · f(z).\nThroughout the text, we refer to z ⊆ x as effect modiﬁers.\nThis functional decomposition is not a restrictive assumption as g and f can belong to any complicated function class\nand z can include all variables.\nWe assume that a large dataset D = {xi, ti, yi}N\ni=1 is\navailable at training time, but that data for a new test case,\nxj, must be acquired with some non-trivial cost to compute\nan ITE estimate for individual j. We assume that all features\nhave an equal cost at test time, but our approach can be\nextended to incorporate different costs for different features.\nWe distinguish between two tasks\nITE Discovery: The goal is to develop an algorithm\nthat takes D as input, and outputs a function ˆτ : X 7→ R\nsuch that ˆτ(x) ≈ τ(x) for all individuals x ∈ X.\nITE Prediction: For a particular individual we observe a subset of variables z, and we must output a value ˆe\nsuch that ˆe ≈\nR\nPr(x | z)τ(x)dx. ITE Discovery can be a\nuseful sub-goal for this problem, since we might be able to\napproximate ˆe ≈\nR\nPr(x | z)ˆτ(x)dx using D and ˆτ.\nOur goal is data-efﬁcient ITE prediction. That is, what is\na sufﬁcient set of variables z we must observe about an individual, and what should our estimate ˆe be for that person?\nMotivating Insights\nConsider the example of a physician trying to estimate the\neffect of conducting an aggressive surgery. She would only\nchoose to do the surgery if it increases her patient’s life\nexpectancy. What demographic questions and medical tests\nshould she ask/run so that she gets an accurate estimate of\npost-surgery change in life expectancy?\nHeterogeneous treatment estimation\nFrom the deﬁnition of ITE, we have that:\nτ(x) = Y1(x) − Y0(x) = g(x) + f(z) − g(x) = f(z).\nThis reveals that τ is a function of z rather than all of x. In\nscenarios where the z is of a much smaller dimension than\nx, there is a clear advantage to only collecting the effect\nmodiﬁers z. Even if all the features are effect modiﬁers\nthere is an advantage to ordering the variables according to\nthe magnitude of effect modiﬁcation and only collecting the\ntop modiﬁers in budget-constrained test scenarios.\nInsight 1: We only need to collect effect modiﬁers for\nITE prediction.\nPersonalized feature selection\nConsider the case where\nlife expectancy of the patient has the following form:\nf(z) = 1{zv>c } · exp(zlA) + 1{zv≤c } · exp(zlB),\nwhere zv denotes vitals, zlA, zlB denote results of lab test A\nand B respectively and c is some constant. Note, lab test A\nis only relevant for patients for whom zv > c while lab test\nB is only relevant for patients for whom zv ≤ c. The ideal\ndata collection process mimics that hierarchical dependency\nstructure, collecting the vitals ﬁrst and then deciding which\nlab test to conduct next based on their values.\nInsight 2: Individuals may have different effect modiﬁers. We can personalize their queries.\nIdentifying axes of variance\nConsider the situation\nwhere two effect modiﬁers, say, zv, and zlA, are functions\nof another variable x. In that case, querying x, even though\nit might not be an effect modiﬁer, is more efﬁcient than\nquerying zv, and zlA for patients with zv > c. It might\nbe that for some applications, it is important to medically\nunderstand the factors that affect the treatment effect but\nfor the purposes of efﬁcient data collection, which is our\nmain aim, identifying the variables associated with the most\nvariance in the ITE is sufﬁcient.\nInsight 3: Collecting variables that induce the highest\nvariance is sufﬁcient for ITE prediction.\nDirect regularization for ITE discovery does not work\nOne might wonder whether some form of variable regularization can be applied during ITE discovery to ensure\nfeature sparsity and enable data-efﬁcient ITE prediction as\na side-effect. If the regularization penalty leads to excluding\nconfounders in x \\ z (that affect both treatment likelihood\nand the outcome), our estimates will be unnecessarily\nbiased. If it leads to excluding any of the variables in z,\nit would be ignoring an axis of heterogeneity, essentially\nlumping together groups with diverse ITEs.",
        "method: a distillation approach": "The three insights outlined in the previous section inform\nour strategy: we seek to ﬁnd a small set of features with\nwhich the ITE varies and a functional mapping from these\nvariables to the ITE. We start by making 2 unrealistic assumptions – where we have access to τ, and know that the\nnumber of effect modiﬁers is at most K – but relax these\nassumptions later. Our objective function is deﬁned as:\nI∗, θ∗ = arg min\nI,θ\n\u001a 1\nN\nX\ni\n\u0010\nτi − fθ\n\u0000\nxI\ni\n\u0001\u00112\u001b\ns.t. |I| ≤ K, (1)\nwhere I denotes an index set, xI denotes the subset of the\nvector x formed by picking the dimensions, d ∈ I and\nθ parametrizes the mapping from xI to τ. This is essentially an L0 regularization problem which is computationally\nintractable, since it requires optimization over the discrete\nspace of all possible index sets.\nBecause L0 regularization is intractable, we tackle the\nproblem iteratively, only seeking to ﬁnd one relevant feature\nat a time. We opt for an iterative approach because it can be\nstopped at any point, giving us an early estimate of the ITE\nbased on the variables selected so far. In the ﬁrst iteration\nwe ﬁnd the feature associated with the most variance in the\nentire population, which entails solving:\nd∗\n1, θ∗\n1 = arg min\nd,θ\n\u001a 1\nN\nX\ni\n\u0010\nτi − fθ\n\u0000\nxd\ni\n\u0001\u00112\u001b\nwhere d denotes a dimension of x and d∗\nk denotes the optimal dimension picked in the kth iteration. For the kth iteration, the objective is deﬁned as:\nd∗\nk, θ∗\nk =\narg min\nd̸∈{d∗\n1:k−1},θ\n\u001a 1\nN\nX\ni\n\u0010\nτi − fθ\n\u0000\nx\n(d∗\ni 1:k−1,d)\n\u0001\u00112\u001b\n,\nand so forth. While this iterative approach has the advantage\nof allowing early estimation, it makes the assumption that\nthere is a single set of variables that is relevant for the entire\npopulation. To relax that assumption, we redeﬁne the optimization function such that at each iteration it picks the dimension associated with the highest variation and splits the\npopulation into two distinct, less heterogeneous subgroups\nfor which we can repeat the process recursively, optimizing\nthis objective for each group separately.\nTo do so, we introduce a splitting function, hφ(x) which\ngives a partition π, splitting the population into subgroups\nℓ1 and ℓ2. For simplicity, we consider binary splits. Our objective function for the ﬁrst iteration can now be re-written:\nd∗\n1, θ∗\n1, φ∗\n1 = arg min\nd,θ,φ\n\u001a 1\nN\nX\ni\n\u0010\nτi − fθ\n\u0000\nxd\ni ; hφ(xd\ni )\u0001\u00112\u001b\nImportantly, since our objective is to minimize data collection, we require that the same variable that is used for estimation is also used for splitting: fθ and hφ both depend\non the same xd\ni . The objective function for the kth iteration\ncan now be deﬁned separately for each of the ℓj partitions\ncreated in iteration k − 1:\nd∗\nk,j, θ∗\nk,j, φ∗\nk,j = arg min\nd,θ,φ\n\u001a\n1\n#(i : i ∈ ℓj)\nX\ni\n\u0010\nτi − fθ\n\u0000\nx\n(d∗\ni 1:k−1,d)\n; hφ(x\n(d∗\ni 1:k−1,d)\n)\n\u0001\u00112\u001b\nwhere #(i : i ∈ ℓj) denotes the number of samples falling\nin subgroup j of partition induced by hφ(x\n(d∗\ni 1:k−1,d)\n). Note\nthat when hφ is a simple thresholding function and fθ is\nthe mean of the sub-population satisfying the threshold, this\nobjective function is identical to the objective function of a\nsimple decision tree:\nΠ∗, µ∗ =\nX\nj\narg min\nΠ,µ\n\u001a\n1\n#(i : i ∈ ℓj)\nX\ni\n\u0010\nτi − µj(ℓj)\n\u00112\u001b\n(2)\nwhere µj(ℓj) is the mean of leaf j, µ = {µj} for all j and Π\nis a partition, with Π = {ℓj}M\nj , where M is the total number\nof leaves in the tree.\nThe tree can be grown until a pre-speciﬁed number of\nqueries K is achieved or until further queries do not lead\nto further improvements in the accuracy, meaning:\n1\n#(i : i ∈ ℓj)\nX\ni\n\u0010\nτi − µj(ℓj)\n\u00112\n< ϵ\n(3)\nfor some small ϵ for all possible partitions.\nOf course, we never have access to τi. Instead, we assume\nthat at training time, we have access to an algorithm A :\nD → ˜τ(x). Meaning an algorithm that learns a functional\nmapping from the full set of features to the ITE. Using this\nalgorithm, we can train a model to compute an approximate\nestimate of τi for all i in the training data. We refer to this\nmodel as the base model and denote this approximation with\neτi. Replacing τi with eτi, the objective function in 2 can now\nbe rewritten as:\nΠ∗, µ∗ =\nX\nj\nargmin\n\u001a\n1\n#(i : i ∈ ℓj)\nX\ni\n\u0010\neτi − µj(ℓj)\n\u00112\u001b\nAt test time, we need to only query the variables that\ndeﬁne the partition in the order deﬁned by the partition hierarchy. The depth of the partition K can either be deﬁned a\npriori or the partition trees can be allowed to grow until the\nreduction in variance is less than a tolerance parameter ϵ. Alternatively ϵ or K can be acquired through cross-validation\nagainst the base model’s estimates of τ for the validation set.\nWhy trees? The regularization problem expressed in\nequation 1 could have been approximated in a number of\nways. We chose decision trees for several reasons. First, the\ntree could be fully trained at training time, but traversed up\nto some depth K < the maximum depth at test time enabling the end user to stop whenever their budget of queries\nis exhausted. In addition, different pathways are deﬁned\nby different variables, which encodes our intuition that\ndifferent features will be relevant for different individuals.\nFinally, decision trees are easy to train and interpret, which\nadds little to no overhead to the inherently complicated ITE\nestimation process making this approach user-friendly.\nOne limitation of trees is that they consider only binary\nsplits; they are prone to splitting the population using the\nsame feature several times, each time based on a different\nthreshold. To remedy that, we cache the value of the feature the ﬁrst time it is queried and use the cached value to\nevaluate any subsequent splits on the same variable. Other\nlimitations are considered in the conclusions section.\nTable 1: DEITEE leads to large reductions in required features at test time with little to no loss in accuracy\nOracle\nBART\nGRF\nTrt\nPct\nResp\nAlign\nDEITEE\nDEITEE\nBase\nDEITEE\nDEITEE\nDEITEE\nBase\nDEITEE\nDEITEE\nDEITEE\nMech\nTrt\nSurf\nMSE-T\nMVar\nMSE-T\nMSE-T\nMSE-M\nMVar\nMSE-T\nMSE-T\nMSE-M\nMVar\npoly\nlow\nexp\nhigh\n9.97\n5.52\n7.31\n8.97\n1.49\n5.42\n11.12\n11.63\n0.21\n4.30\npoly\nhigh\nexp\nlow\n4.78\n5.51\n3.46\n5.56\n0.39\n5.38\n8.81\n9.53\n0.1\n4.11\nstep\nhigh\nexp\nlow\n4.76\n6.45\n2.53\n4.97\n1.40\n6.16\n7.35\n8.11\n0.22\n4.66\nstep\nhigh\nexp\nhigh\n2.82\n5.91\n1.53\n3.23\n1.55\n5.64\n6.6\n7.17\n0.25\n4.63\npoly\nlow\nexp\nlow\n3.96\n8.03\n1.49\n3.59\n0.46\n5.88\n5.57\n6.21\n0.11\n4.53\npoly\nhigh\nexp\nhigh\n3.63\n5.77\n2.74\n4.4\n0.15\n5.58\n7.54\n8.11\n0.07\n4.38\nstep\nlow\nexp\nlow\n3.27\n5.73\n2.31\n3.99\n0.13\n5.60\n6.80\n7.39\n0.07\n4.70\nstep\nlow\nexp\nhigh\n2.53\n5.29\n1.38\n2.22\n0.09\n5.18\n4.88\n5.19\n0.04\n4.24\nstep\nlow\nstep\nlow\n1.85\n4.63\n1.56\n2.07\n0.23\n4.51\n3.68\n3.75\n0.09\n3.52\npoly\nhigh\nstep\nhigh\n1.48\n4.10\n1.43\n1.87\n0.71\n3.84\n3.15\n3.34\n0.19\n3.35\npoly\nlow\nstep\nhigh\n1.05\n4.08\n1.16\n1.54\n0.14\n4.01\n3.75\n3.84\n0.04\n3.27\nstep\nhigh\nstep\nlow\n0.93\n4.44\n0.61\n1.19\n1.09\n4.26\n3.16\n3.38\n0.18\n3.37\nstep\nhigh\nstep\nhigh\n0.72\n4.39\n0.82\n1.14\n0.09\n4.53\n3.45\n3.63\n0.03\n3.63\npoly\nlow\nstep\nlow\n0.61\n5.14\n0.74\n1.04\n0.25\n3.57\n2.53\n2.65\n0.08\n2.96\npoly\nhigh\nstep\nlow\n0.61\n3.67\n1.14\n1.38\n1.85\n3.66\n2.45\n2.6\n0.29\n3.02\nstep\nlow\nstep\nhigh\n0.61\n4.14\n0.88\n1.11\n1.51\n4.19\n2.42\n2.52\n0.26\n3.49",
        "semi-synthetic experiments": "Setup\nExperiments on real data are ideal in the sense that they\nprovide hard and realistic test-beds for our method. However, since we never observe the true ITE, it is hard to evaluate our method on real data alone. Completely synthetic\nexperiments allow us access to the true ITE at the risk of\nover simplifying the data generating process thus creating\na completely unrealistic test-bed. To strike a balance between the two extremes, we resort to semi-synthetic experiments, where the matrix of features (confounders and/or effect modiﬁers) is extracted from real data while the treatment assignment and response mechanisms are simulated.\nFor our semi-synthetic experiments, we use data generated\nfor the Atlantic Causal Inference Conference Competition\n(Dorie et al. 2017). In this competition, 58 variables were\nextracted from the Collaborative Perinatal Project, a longitudinal study on pregnant women and their children. Of these\nfeatures, 3 are categorical, 5 are binary, 27 are count data,\nand the remaining 23 are continuous. A subset of 4802 of\nthe women in the study was included in the competition. The\ncompetition organizers simulated 77 different experimental\nsetups with varying data generating processes. The data generating processes were varied based on:\n• Overlap: between the treated and control populations.\n• Heterogeneity: how much the treatment varies based on\nfeatures. It is controlled in the setup by controlling the\nnumber of variables that interact with the treatment.\n• Treatment assignment mechanism (Trt mech): the\nfunctional class of the mapping from the features to the\ntreatment (e.g., a polynomial or step function)\n• Response surfaces (Resp Surf): the functional class\nmapping from features and treatment to the ﬁnal outcome.\n• Percent treated (Pct Trt): the percent of the observations receiving the treatment assignment (Low=25%;\nhigh=75%).\n• Alignment (align): A variable included in the treatment\nassignment has a low (25%) or high (75%) chance of also\nbeing included in the response surface.\nFurther details about the simulations can be found in\n(Dorie et al. 2017). We split the data into 2/3 for training\nand validation and 1/3 for testing. For each of the 77 simulated setups, we run DEITEE on one of three base estimates: oracle, Bayesian Additive Regression Trees (BART;\nHill 2011), and Generalized Random Forests (GRF; Athey,\nTibshirani and Wager 2016). For the oracle base model,\nDEITEE directly distills the ground truth ITE, for the latter\ntwo DEITEE distills the training estimates computed using\nthese models. We chose to implement BART because it was\nthe top performing method in the challenge, while GRF is\none of the most widely used heterogeneous treatment effect\nestimation methods. During training time, we do three-fold\ncross validation for each of the base models to pick the optimal hyper-parameters. Details about hyper-parameter tuning\nand software used are in the appendix. We then distill each\nbase method as outlined previously using a decision tree,\nstopping the splitting when the improvement in accuracy\nis less than ϵ = 0.001. We run each experiment 20 times,\neach time randomly picking new simulation parameters and\npresent results averaged over these 20 unique simulations.\nResults\nTable 1 shows the mean squared error (MSE) of the base\nmodels and the distilled model relative to the ground truth\nITE (MSE-T), as well as the MSE of the distilled model relative to the base model (MSE-M), and the mean number of\nqueries (MVar) that DEITEE collected. Poly and exp refer to\nthe polynomial and exponential functions respectively. We\npresent results from the full overlap and high heterogeneity\nsetups in the main text while results for all other simulation\nconditions are presented in the appendix. We chose full overlap because it conforms with our assumptions while high\nheterogeneity is a more challenging setting. The table shows\nthat DEITEE is able to distill the base model without a large\nloss in accuracy. For GRF, we ﬁnd that the distillation procedure required the collection of less than 5 features, compared\nto the full 58 features this constitutes a 91% reduction in required features. Distilling BART led to an 88% reduction\nin required features. We observe that when distilling GRF,\nwhich has lower accuracy than BART, DEITEE collects a\nsmaller number of variables. This implies that DEITEE does\nsome form of early stopping when the base model has a high\nerror. These large reductions in features come without large\nsacriﬁces in accuracy. Comparing MSE-M vs. MSE-T for\nGRFs, we ﬁnd that DEITEE’s MSE-M is less than 0.3 across\nall experiment conditions (in fact, a paired T-test between\nthe base model and DEITEE’s MSE reveals that the difference is statistically indistinguishable from 0 for 10 different\nexperiment conﬁgurations). The drop in accuracy is more\npronounced for BART (DEITEE MSE-M is larger), indicating that there may be better distillation approaches that ﬁt\nthe ITE surface modeled by BART. Still paired T-test shows\nthat the difference is statistically indistinguishable from 0\nfor 8 of the experiment conﬁgurations. Comparing the average performance across simulations, we ﬁnd that the MSE\naveraged across simulations is statistically insigniﬁcant for\nall GRF models and is signiﬁcant only for 7 out of the 77\nconﬁgurations for BART, further pointing to the notion that\nthere might be better models to distill BART. We also ﬁnd\nthat the majority of the MSE relative to the ground truth is\nattributable to the error incurred by the base model. This can\nbe inferred from the fact that the MSE relative to the ground\ntruth of the base model is roughly equal to that of DEITEE\nwhile the MSE of DEITEE relative to the model is negligible.\nFurther inspection of the results show that DEITEE’s\nMSE tends to be higher when the response surface is exponential rather than step or linear functions. This might be attributable to the fact that the base models also have a higher\nMSE when the response surface is exponential but it could\nalso be an additional error introduced by DEITEE. By virtue\nof being a decision tree, it is approximating the smooth exponential function using a piece-wise constant function. In\nsome situations, researchers may opt to ﬁt more ﬂexible\nmodels, e.g., a Generalized additive model (GAM), during\nsplitting or at the leaves.\nWe will focus the remainder of the discussion on the analysis of one of the harder experimental setups: non-linear,\npolynomial treatment assignment mechanism, low percent\ntreated, low alignment and step response surface. Results\nfrom the exponential surface are presented in the appendix.\nIn addition to minimizing feature collection at test time\nwith little to no loss in accuracy, DEITEE is able to personalize the feature selection process, allowing different subpopulations to be asked a different number of questions. Figure 2 is the histogram of the number of features collected\nat test time showing signiﬁcant variability in the number of\nfeatures collected across the test population, thus conforming with insight 2 in the Preliminaries section.\nNext, we inspect DEITEE’s ability to balance the\naccuracy-efﬁciency trade-off, where efﬁciency is measured\nby the number of unique features collected or queried at test\ntime. We compare it to a more naive method of approaching\nFigure 2: Histogram of the number of features collected by\nDEITEE at test time. Number of features collected varies:\nThe majority of individuals require 3 features while few\nharder-to-estimate individuals require more.\nthis problem which relies on simple regularization. Specifically, we train the base model once using all the variables,\ncompute the variable importance1 then retrain the model using only the top k variables. We implement this approach\nfor BART and GRF and refer to these retrained models as\nthe RT-models. Figures 3(a) and 3(b) show the number of\nunique features collected on the x–axis and the corresponding MSE and Median SE on the y–axis respectively. Plotted\nlines show performance of retrained models (RT-GRF and\nRT-BART), and DEITEE-distilled models (DEITEE-GRF\nand DEITEE-BART). Dotted lines show base model accuracy.\nWe ﬁnd that DEITEE-distilled methods have a lower\nmean and median squared errors after the ﬁrst 2-3 queries.\nTo understand the reason behind these gains in accuracy,\nwe inspect the types of features that DEITEE collects and\ncompare them to the retrained models. Figure 3(c) shows\nthe number of unique features collected on the x−axis and\nthe proportion of individuals queried about a variable that\nis not an effect modiﬁer on the y−axis. We see that RTBART tends to favor collecting effect modiﬁers in the ﬁrst\nfew rounds, neglecting to adjust for confounding while RTGRF tends to collect non-effect modiﬁers perhaps prioritizing adjustment for confounding at the expense of estimating\nheterogeneity. While the retrained methods continue asking\nadditional questions which introduce marginal gains in accuracy, DEITEE stops after collecting non-effect modiﬁers\nfrom at most 20% of the population. This is an important\nfeature: In non-simulated data, we would not be able to observe accuracy plots similar to Fig 3(a,b) to pick the k where\nthe error plateaus and subsequently retrain base models using only the k most important variables. In addition, we see\nthat both the retraining method and DEITEE are able to ex1Details regarding variable importance measures are in (Kapelner and Bleich 2016) for BART and (Tibshirani et al. 2018) for\nGRF\nFigure 3: DEITEE models achieve faster initial gains in accuracy, and tend to collect effect modiﬁers.\nploit the efﬁciency-accuracy trade-off; after a small number\nof variables is queried, the mean and median squared errors drop drastically and plateau after 5 or 6 questions. This\nnumber of questions is consistent with the number of variables interacted with the treatment in these experiment settings. At the point where the models’ performances plateau,\nthe MSE of the retrained methods are overall lower than\nthose of the DEITEE-distilled model, however the median\nSE shows that the performance is overall comparable. The\ndifference between the performance as measured through\nthe mean and median errors suggests that the distribution\nover errors is skewed: for the majority of the population,\nDEITEE performs better than retraining but for some “hard”\nsub-populations, DEITEE gives less accurate estimates. Retraining improves estimates for these hard sub-populations\nbut at the cost of collecting many more variables than necessary for the “easy” sub-populations, which DEITEE is able\nto avoid as shown in Figure 2.",
        "real data experiment": "For our real data experiment, we show that DEITEE can enable expecting mothers to plan and make decisions during\ntheir pregnancy based on how they might affect their babies’\nhealth. Speciﬁcally, DEITEE can select the questions needed\nto ask the mother in order to ascertain the effect of different interventions and habits on the baby’s health. In such a\nscenario, collecting all the features (by asking the mother a\nbarrage of questions) is not feasible, especially with vulnerable populations of pregnant women who most need the right\nmedical advice. We explore two interventions: how initialization of perinatal care in the ﬁrst trimester and smoking\naffect the baby’s health. We follow existing literature in using the baby’s birth weight as an indicator of its health and\nwell being (Almond, Chay, and Lee 2005). We use data from\nthe 1989 Linked birth-infant death data which is made publicly available by the Centers for Disease Control (CDC ).\nThe dataset has infant birth weight, as well as parent demographics and mother risk factors for all 4 million babies born\nin the US. We restrict our analysis to the population of singletons born in the state of Massachusetts (N = 91, 065).\nWe further drop any infants who are missing birth weight,\nmother’s smoking status or when she initialized her perinaTable 2: DEITEE achieves large reductions in the features\ncollected about soon-to-be-moms without loss of accuracy\nin estimating the effect of smoking and perinatal care on\ntheir babies’ birth weight.\nBase\nDEITEE\nDEITEE\nDEITEE\nMAE-P\nMAE-P\nMAE-B\nMVar\nSmoking\nBART\n580.20\n580.20\n0.22\n15.42\nGRF\n581.27\n581.38\n8.30\n11.92\nPerinatal care in the ﬁrst trimester\nBART\n587.62\n587.62\n0.26\n16.20\nGRF\n588.03\n588.06\n3.13\n15.70\ntal care (N = 89, 840). We split the data into 2/3 training\nand validation and 1/3 testing. Three-fold cross-validation is\ndone to ﬁnd the best parameters for the base model.\nHere we do not have access to the true ITE so we cannot\ndirectly measure how well DEITEE or the base models do.\nInstead, we compute a proxy for the ITE by matching every mother in the test set with two similar mothers, one of\nwhom belonged to the treatment group while the other does\nnot. To ﬁnd candidate matches, we use data from 1990 and\ndata from 1989 from states other than Massachusetts. We\nidentify the best matches as the ones having the smallest Euclidean distance relative to the features of the main mother.\nThe proxy ITE is then computed as the difference between\nthe birth weight of the baby belonging to the treated mother\nminus the birth weight of the baby of the control mother. We\nare able to match 76.1% for the perinatal care question and\n70.0% for the smoking question.\nWe report MAE because it is in the same units as the treatment effect (change in birth weight in grams). Table 2 shows\nthe mean absolute error of the base model relative to the\nproxy ITE (MAE-P), the MAE of DEITEE relative to the\nbase model (MAE-B) and the mean number of features that\nwere collected for mothers in the test set (Mvar). The results\nconﬁrm our ﬁndings in semi-synthetic experiments: negligible reductions in accuracy, and substantial reduction in the\nnumber of features required at test time.\nFinally, we inspect the trees produced by distilling BART.\nFigure 4: Decision tree stub distilling BART for the smoking\nquestion. Different feature paths are traversed for different\nmothers.\nFigure 4 shows the ﬁrst three questions asked by DEITEE\nupon distilling BART for the smoking question. DEITEE\nchooses to ﬁrst ask women who consume alcohol about their\neducation while for those who do not consume alcohol, it\nasks about age. When the treatment is starting perinatal care\nin the ﬁrst trimester, DEITEE still chooses to ask women\nwho consume alcohol about health risk factors while those\nwho do not are asked about their marital status. Regardless\nof the treatment being studied, DEITEE always asks whether\nor not the mother consumes alcohol during her pregnancy\nsignifying that that most variance in the treatment effect\nhinges on the mother’s alcohol consumption habits.",
        "conclusion": "We presented DEITEE, a distillation method that enables\naccurate ITE estimation while demanding the collection of\nonly a small number of variables at test time. Our approach\nexploits the fact that at training time both confounders and\neffect modiﬁers are required to accurately model the ITE\nwhile at test time only the effect modiﬁers are required\nto predict the ITE. Using 77 semi-synthetic datasets, we\nshowed that DEITEE achieves signiﬁcant reductions in the\nnumber of variables required at test time with little to no loss\nin accuracy. We demonstrated the utility of our approach using real data. There are several speciﬁc areas of future work:\nMore ﬂexible models. While decision trees are appealing\nbecause of their simple and interpretable nature, they can be\noverly simplistic, and struggle to ﬁt smooth response functions. Possible extensions to this work could explore more\nﬂexible function classes or hybrids of trees and more ﬂexible models such as GAMs.\nIdentiﬁcation of Effect modiﬁers. In this work, we were\nconcerned with collecting the smallest number of variables\nto accurately estimate the ITE. In other applications, we\nmight wish to ensure that we only query effect modiﬁers,\neven if querying effect modiﬁers might require more variables. Future work will focus on models which ask for the\nminimal number of effect modiﬁers.",
        "summary_en": "The potential for using machine learning algorithms as a tool for suggesting optimal interventions has fueled significant interest in developing methods for estimating heterogeneous or individual treatment effects (ITEs) from observational data. While several methods for estimating ITEs have been recently suggested, these methods assume no constraints on the availability of data at the time of deployment or test time. This assumption is unrealistic in settings where data acquisition is a significant part of the analysis pipeline, meaning data about a test case has to be collected in order to predict the ITE. This paper presents Data Efficient Individual Treatment Effect Estimation (DEITEE), a method which exploits the idea that adjusting for confounding, and hence collecting information about confounders, is not necessary at test time. DEITEE allows the development of rich models that exploit all variables at train time but identifies a minimal set of variables required to estimate the ITE at test time. Using 77 semi-synthetic datasets with varying data generating processes, the paper shows that DEITEE achieves significant reductions in the number of variables required at test time with little to no loss in accuracy. Using real data, the paper  demonstrates the utility of the approach in helping soon-to-be mothers make planning and lifestyle decisions that will impact newborn health.",
        "summary_zh": "这篇论文介绍了一种数据高效个体治疗效应估计的方法，名为DEITEE，旨在从观察数据中高效地估计个体治疗效果（ITE）。它解决了在部署或测试时数据不足的情况下估计ITE的问题。该方法利用了在测试时不需要调整混杂因素的思想，从而在测试时识别出估计ITE所需的最小变量集。在77个半合成数据集上的实验结果表明，DEITEE在测试时所需变量数量显著减少，并且几乎不损失准确性，并在真实数据上的应用中，展示了该方法在帮助准妈妈做出影响新生儿健康的规划和生活决策方面的效用。"
    },
    {
        "title": "Estimation of Local Average Treatment Effect by Data Combination",
        "abstract": "It is important to estimate the local average treatment effect (LATE) when compliance with a treatment assignment is incomplete. The previously proposed methods for LATE estimation required all relevant variables to be jointly observed in a single dataset; however, it is sometimes difficult or even impossible to collect such data in many real-world problems for technical or privacy reasons. We consider a novel problem setting in which LATE, as a function of covariates, is nonparametrically identified from the combination of separately observed datasets. For estimation, we show that the direct least squares method, which was originally developed for estimating the average treatment effect under complete compliance, is applicable to our setting. However, model selection and hyperparameter tuning for the direct least squares estimator can be unstable in practice since it is defined as a solution to the minimax problem. We then propose a weighted least squares estimator that enables simpler model selection by avoiding the minimax objective formulation. Unlike the inverse probability weighted (IPW) estimator, the proposed estimator directly uses the pre-estimated weight without inversion, avoiding the problems caused by the IPW methods. We demonstrate the effectiveness of our method through experiments using synthetic and real-world datasets.",
        "introduction": "Estimating the causal effects of treatment on an outcome of\ninterest is central to optimal decision making in many realworld problems, such as policymaking, epidemiology, education, and marketing (Skovron and Titiunik 2015; Wood\net al. 2008; Oreopoulos 2006; Varian 2016). However, the\nidentification and estimation of treatment effects usually relies on the untestable assumption referred to as unconfoundedness, namely, independence between the treatment status and potential outcomes (Imbens and Rubin 2015). Violations of unconfoundedness may occur not only in observational studies, but also in randomized controlled trials\n(RCTs) when compliance to the assigned treatment is not\ncomplete. For example, even if a coupon is distributed randomly to measure its effect on sales, the probability of using the coupon is likely to depend on the unobserved nature\nof the individuals. Moreover, noncompliance can also occur\nregardless of the individual’s intentions. In online advertisement placement, the probability of watching the ad depends\non the bidding strategy of other companies because ads that\nare actually displayed are determined through the real-timebidding even if one tries to randomly place the ad.\nIn such cases, it is well known that the local average treatment effect (LATE) can be identified and estimated using the\ntreatment assignment as an instrumental variable (IV) and\nconditions milder than unconfoundedness (Imbens and Angrist 1994; Angrist, Imbens, and Rubin 1996; Fr¨olich 2007).\nLATE is the treatment effect measured for the subpopulation of compliers, individuals who always follow the given\nassignment.\nWe suppose that we cannot observe all relevant variables\nin a single dataset for technical or privacy reasons. For example, in online-to-offline (O2O) marketing, where treatments\nare implemented online and outcomes are observed offline,\nit is often difficult to match the records of the same individuals observed separately online and offline. Additionally, with\nthe global anti-tracking movement gaining momentum, it\nmay become more difficult to combine multiple pieces of information online as well. Although causal inference by data\ncombination has been actively studied (Ridder and Moffitt\n2007; Bareinboim and Pearl 2016; Lee, Correa, and Bareinboim 2020), LATE estimation using multiple datasets has\nnot received much attention despite its practical importance.\nWe extend the problem setting considered in (Yamane et al.\n2018), where two different treatment regimes are available,\nto allow for the existence of noncompliance.\nFor the estimation, we show that the direct estimation\nmethod originally developed for the average treatment effect\n(ATE) under the complete compliance (Yamane et al. 2018)\ncan be applied to the LATE estimation in our setting. However, their method has a practical issue in that model selection and hyperparameter tuning can be unstable owing to its\nminimax objective formulation. We then propose a weighted\nleast squares estimator to avoid the minimax objective, and\nimprove the stability in practice. Unlike the inverse probability weighted (IPW) estimator (Wooldridge 2002, 2007;\nSeaman and White 2013), which is often employed to estimate treatment effects, the proposed estimator directly uses\nthe estimated propensity-score-difference (PSD) as a weight\nwithout inversion. Therefore, our method can also avoid the\ncommon issue in the IPW methods, that is, high variance at\npoints with a propensity score extremely close to zero.\nThe contributions of this study lie in the following three\nparts. First, we show that LATE is identified even when an\noutcome and treatment status cannot be observed simultaneously in a single dataset, and the treatment assignment is\ncompletely missing. Second, we find that the positivity assumption, which is necessary in the standard setting with\none regime, can be omitted in our setting. We show this\nrelaxation of the conditions further facilitates data collection. Third, we develop a novel estimator that enables simpler model selection while maintaining the essence of direct\nestimation as much as possible.",
        "problem setting": "Unlike the standard causal inference studies, we consider\na setting where there are two different assignment regimes\n(Yamane et al. 2018), which we term as the two-regime design (2RD). The concept is quiet general that it only requires\ntwo observational studies, two RCTs or a combination of the\ntwo. We have to be assured that the treatment assignment is\ndone based on different regimes, i.e. different probabilities\n(see Assumption 1.2).\nWe define our problem using the potential outcome framework (Rubin 1974; Imbens and Rubin 2015). Let K ∈\n{0, 1} be a regime indicator, and we use a superscript k =\n0, 1 to specify the regime which the variables come from. Let\nY (k) ∈ Y ⊂ R be an outcome of interest, D(k) ∈ {0, 1} be\na binary treatment indicator, Z(k) ∈ {0, 1} be an assignment\nindicator and X(k) ∈ X ⊂ Rqx be a qx-dimensional vector\nof covariates. D(k)\nz\nis the potential treatment status realized\nonly when Z(k) = z, and Y (k)\nd\nis the potential outcome realized only when D(k) = d, where z, d ∈ {0, 1}. Using this\nnotation, we implicitly assume that Z(k) does not have a direct effect on Y (k), but affects Y (k) indirectly through D(k).\nThis condition, often referred to as the exclusion restriction,\nis necessary for Z(k) to be a valid IV. Let Y1, Y0, D1, D0 and\nX be the potential variables and covariates in the population\nof interest, and we suppose that the iid samples from P(X)\ncan be obtained as test samples.\nBasic Setting\nFirst, we make the following assumption on the relation between the two regimes.\nAssumption 1.\n1. P(Y (1)\n1\n, Y (1)\n0\n, D(1)\n1 , D(1)\n0 , X(1))\n= P(Y (0)\n1\n, Y (0)\n0\n, D(0)\n1 , D(0)\n0 , X(0))\n= P(Y1, Y0, D1, D0, X).\n2. P(Z(1) = 1|X(1) = x) ̸= P(Z(0) = 1|X(0) = x) for\nany x ∈ X.\nBy Assumption 1.1, we suppose that the joint distribution\nof the potential variables and covariates in each assignment\nregime is invariant to each other, and equal to the joint distribution in the population of interest. This means that the\nparticipants in each regime are random draws from the population of interest. We can still identify LATE even if we\nweaken Assumption 1.1 to its conditional version, but the\nK = 1\nK = 0\nP(Y (1)\n1\n, Y (1)\n0\n, D(1)\n1 , D(1)\n0 , X(1))\nP(Y (0)\n1\n, Y (0)\n0\n, D(0)\n1 , D(0)\n0 , X(0))\nP(Y1, Y0, D1, D0, X)\nP(Z(1) = 1|X(1)) 6= P(Z(0) = 1|X(0))\nTreatment Assignment with\nP(Y (1), D(1), X(1))\nP(Y (0), D(0), X(0))\nRegime Assignment\nFigure 1: Overview of the 2RD.\ndirect estimation is no longer possible. See Appendix A in\nthe supplementary material1 for more discussion. Assumption 1.2 indicates that the assignment regimes are different\nto each other for any level of the covariates.\nAdditionally, we make the following assumption necessary for the identification of LATE. Here, A ⊥⊥ B|C means\nA and B are conditionally independent given C.\nAssumption 2. For k = 0, 1,\n1. Y (k)\n1\n, Y (k)\n0\n, D(k)\n1 , D(k)\n0\n⊥⊥ Z(k)|X(k).\n2. D(k)\n=\nZ(k)D(k)\n1\n+ (1 − Z(k))D(k)\n0\nand Y (k)\n=\nD(k)Y (k)\n1\n+ (1 − D(k))Y (k)\n0\n.\n3. P(D(k)\n1\n≥ D(k)\n0 |X(k)) = 1.\n4. P(D(k)\n1\n= 1|X(k)) ̸= P(D(k)\n0\n= 1|X(k)).\nAssumption 2.1 states that Z(k) are randomly assigned\nwithin a subpopulation sharing the same level of the covariates. The mean independence between the potential variables and the treatment assignment conditional on covariates\nis sufficient for identifying LATE, but we maintain Assumption 2.1 as it is typical to assume the full conditional independence in the causal inference literature. Assumption 2.2\nrelates the potential variables to their realized counterparts.\nAssumption 2.3 excludes defiers, those who never follow the\ngiven treatment assignment, from our analysis. Assumption\n2.4 is necessary for Z(k) to be a valid IV of D(k). Figure 1\nillustrates the data generating process under the 2RD.\nAssumption 2 is the direct extension of the standard assumptions used for the LATE estimation (Abadie 2003;\nFr¨olich 2007; Tan 2006; Ogburn, Rotnitzky, and Robins\n2015) to the 2RD. However, we omit the condition referred\nto as positivity 0 < P(Z(k) = 1|X(k)) < 1 as it is\nunnecessary in the 2RD. Therefore, it is possible to set\nP(Z(0) = 1|X(0)) = 0 as long as Assumption 1.2 is satisfied. We term the design with P(Z(1) = 1|X(1)) > 0 for\nany X(1) and P(Z(0) = 1|X(0)) = 0 for any X(0) as the\none-experiment 2RD (1E2RD) since an experiment is conducted only for those with k = 1, and a natural state without\nany intervention is observed for those with k = 0.\nAlthough (Yamane et al. 2018) does not mention anything\non this point, it is of great practical importance since the\n1E2RD is much easier to implement than the general 2RD,\nand extends the plausibility of our setting. In the 1E2RD,\n1https://github.com/kazushino/AAAI22\nwe only have to conduct one experiment or collect one set\nof observational data just like the standard setting for causal\ninference. All we need in addition is a dataset collected from\nthose without any intervention. Such data may be available\nat no additional cost, for example, when there is appropriate\nopen data published by the government.\nHereafter, we abuse the notation by omitting the superscript on the potential variables and covariates unless this\ncauses confusion.\nIdentification\nThe parameter of our interest is LATE as a function of covariates X, defined as\nµ(X) := E[Y1 − Y0|D1 > D0, X].\nIt measures how covariates X affect the average treatment\neffect within the subpopulation of compliers. The following\ntheorem shows that µ(X) is nonparametrically identified in\nan unusual form in our setting.\nTheorem 1. Under Assumption 1 and 2,\nµ(X) = E[Y (1) − Y (0)|X]\nE[D(1) − D(0)|X].\n(1)\nThe proof can be found in Appendix B in the supplementary material. Notably, this form coincides with the identification result of ATE in (Yamane et al. 2018), which means\nthat we can also estimate µ(X) from the same combination\nof datasets proposed in (Yamane et al. 2018) under appropriate assumptions. This is a rather powerful result in practice\nsince Z(k) is not required despite the presence of noncompliance. Moreover, it is obvious that p(k)\nd\n:= P(D(k) = 1) and\nP(X|D(k) = 1) are sufficient to identify the denominator\nof (1) since E[D(k)|X] can be identified as P (X|D(k)=1)p(k)\nd\nP (X)\nby applying the Bayes’ theorem.\nThe point is that we can identify µ(X) as a combination of functions depending only on covariates X in our\nsetting. This property allows us to use a direct estimation\nmethod for µ(X). We can identify LATE as µ(X) =\nE[Y |Z=1,X]−E[Y |Z=0,X]\nE[D|Z=1,X]−E[D|Z=0,X] under the standard assumptions\n(Abadie 2003; Fr¨olich 2007; Tan 2006; Ogburn, Rotnitzky,\nand Robins 2015), but we cannot rewrite it as a combination\nof functions depending only on covariates (see Appendix A).\nTheorem 1 also suggests the usefulness of the 1E2RD.\nWe can simplify (1) when we implement the 1E2RD under\none-sided noncompliance, where individuals assigned to the\ncontrol group never receive treatment, but they have a choice\nif assigned to the treatment group.\nCorollary 1. Assume P(Z(0) = 1|X) = 0 (1E2RD) and\nP(D0 = 1|X) = 0 (one-sided noncompliance) in addition\nto Assumption 1 and 2. Then, E[Y (0)|X] = E[Y0|X] and\nµ(X) = E[Y (1) − Y (0)|X]\nE[D(1)|X]\n.\nThis corollary shows that we can reduce the number of\nnecessary datasets in the 1E2RD under one-sided noncompliance. This fact not only facilitates the data collection,\nbut also benefits the estimation since the denominator is\nnow just a propensity score, thus estimable accurately by,\nfor example, the Positive-Unlabeled (PU) learning (Elkan\nand Noto 2008; du Plessis, Niu, and Sugiyama 2014, 2015;\nKiryo et al. 2017) with the logistic loss.\nAlthough one-sided noncompliance is often associated\nwith RCTs, some observational studies also fit with the\nframework (Fr¨olich and Melly 2013; Kennedy 2020). In the\ncase of one-sided noncompliance, we can also consider our\nproblem as the estimation of the average treatment effect on\nthe treated (ATT) since LATE is equal to ATT under onesided noncompliance and the other standard assumptions\n(Fr¨olich and Melly 2013; Donald, Hsu, and Lieli 2014).\nData Collection Scheme\nWe assume that the joint samples of (Y (k), D(k), Z(k), X)\nare not available. By Theorem 1, the following separate\ndatasets and the estimate of p(k)\nd\nare sufficient for estimating µ(X):\n{x(k)\ndi }\nn(k)\nd\ni=1\niid\n∼ P (X|D(k) = 1),\n{(y(k)\ni\n, x(k)\ni\n)}n(k)\ni=1\niid\n∼ P (Y (k), X),\nfor k = 0, 1. This setting is much easier to apply to realworld situations than the standard setting where the joint\nsamples are required for every individual.\nWe can interpret this data collection scheme in two ways.\nFirst, it can be regarded as a version of the repeated crosssectional (RCS) design (Moretti 2004; Athey and Imbens\n2006; G¨uell and Hu 2006; Ridder and Moffitt 2007; Lebo\nand Weber 2015) with k = 0, 1 representing the time points\nbefore and after the assignment regime switches, respectively. The 1E2RD is also possible in this case by setting\nP(Z(0) = 1|X(0)) = 0. This means collecting data for\nk = 0 at some point before an experiment is conducted.\nGenerally, although panel data are advantageous for statistical analyses because they follow the same individuals over\nmultiple time periods, RCS data have some advantages over\npanel data. RCS data are easier and cheaper to collect in\nmany cases. Consequently, they are often more representative of the population of interest and larger in sample size\nthan panel data.\nHowever, there is a concern regarding the validity of Assumption 1.1 when we use RCS data since the potential variables may change over time. We need some sort of side information to be confident about the use of RCS data in our\nsetting as we cannot directly test whether the potential variables remain unchanged.\nThe second interpretation is to collect data with k = 0, 1\nat the same time by randomly splitting the population of interest. If an experimenter is able to surely perform random\nsplitting, this approach is favorable since we do not have to\nworry about the validity of Assumption 1.1. The implementation of random splitting is easy in the case of RCTs.\nOne specific example of our setting is O2O marketing.\nTo measure the effect of an online ads on sales in physical\nstores, we usually cannot link the data of those who watched\nthe ads with that of those who shopped at the stores. However, it is relatively easy to separately collect data from those\nwho watch the ads and the purchasers. In this case, data can\nbe collected by either RCS design or random splitting.\nAnother example is when estimating the effect of a treatment that takes a certain period of time to take effect. For instance, it is desirable to use panel data to estimate the effect\nof job training on future earnings. However, individuals may\ngradually drop out of the survey, and the probability of the\nattrition can depend on unobserved variables (Hirano et al.\n2001; Nevo 2003). Therefore, it is easier to collect RCS data\nthan to construct balanced panel data. Because we do not\nneed to observe the outcome and treatment status simultaneously, we are more likely to be able to collect even larger and\nmore representative data than with a normal RCS design.",
        "related works": "There have been many proposals for the estimation of µ(X)\nfrom the joint samples (Little and Yau 1998; Hirano et al.\n2000; Abadie 2003; Tan 2006; Okui et al. 2012; Ogburn,\nRotnitzky, and Robins 2015; Wang et al. 2021). These include estimation via the parametric specification of the local average response function E[Yd|D1 > D0, X] (Abadie\n2003), doubly robust estimators (Okui et al. 2012; Ogburn,\nRotnitzky, and Robins 2015) and estimation with a binary\noutcome (Wang et al. 2021), to name a few. However, the estimation of µ(X) by data combination has rarely been considered in the literature.\nThe 2RD has a similar structure to that of the instrumented difference-in-differences (IDID) (Ye et al. 2021).\nThe main differences between them are: the conditional independence assumption in IDID is strictly milder than our\nAssumption 2.1, but IDID requires Z, and the direct estimation is not possible. Which setting is more plausible and\npractical depends on an actual situation.\nOur problem setting is also closely related to the twosample IV (TSIV) estimation (Angrist and Krueger 1992; Inoue and Solon 2010; Pacini and Windmeijer 2016; Choi, Gu,\nand Shen 2018; Buchinsky, Li, and Liao 2021), where the\noutcome, IVs, and covariates are not jointly observed in a\nsingle dataset. Furthermore, the idea of using moments separately estimated from different datasets can be dated back to\nat least (Klevmarken 1982). Although estimands in the TSIV\nestimation are not limited to causal parameters, there have\nbeen some studies on the causal inference in the setting related to TSIV. They include ATE estimation from samples of\n(Y, Z, X) and (D, Z, X) with the existence of unmeasured\nconfounders (Sun and Miao 2022) and causal inference using samples from heterogeneous populations (Zhao et al.\n2019; Shu and Tan 2020). Our setting differs from theirs in\nthat we have to observe D only when D = 1, and we do\nnot need Z at all. Particularly, it is of great practical benefit\nsince samples of those who do not receive treatment are often rather difficult to observe. Although our setting requires\ntwo regimes, it rather opens up the possibility of LATE estimation in the real world since the 1E2RD is possible.\nAnother approach for the causal inference from separately\nobserved samples is the partial identification (Manski 2003;\nTamer 2010; Molinari 2020), that is, deriving bounds for\nthe treatment effects rather than imposing strong assumptions sufficient for the point identification. In (Fan, Sherman,\nand Shum 2014, 2016), the moment inequality proposed in\n(Cambanis, Simons, and Stout 1976) was applied to derive\nthe sharp bounds for ATE when only samples of (Y, D) and\n(D, X) are available. The difficulty of applying their approach to the estimation of a treatment effect conditional on\ncovariates is that it requires conditional distribution functions or conditional quantile functions, which are usually\ndifficult to estimate accurately. Moreover, it is sometimes\ndifficult to derive informative bounds for treatment effects\nwithout imposing strong assumptions depending on the data\ngenerating process (Molinari 2020).",
        "existing methods": "Although the LATE estimation in our specific setting has\nnot been studied before, some existing methods can be applied. We discuss the advantages and disadvantages of these\nmethods especially in terms of accuracy and model selection. Since the true value of treatment effects is not observable by the fundamental problem of causal inference (Holland 1986), model selection and hyperparameter tuning are\nsubstantial issues in practice (Rolling and Yang 2014; Alaa\nand van der Schaar 2018; Saito and Yasui 2020).\nSeparate Estimation\nA naive estimation method for µ(X) is to separately estimate the components and combine them as\nbµsep(x) =\nbE[Y (1)|X = x] − bE[Y (0)|X = x]\nbE[D(1)|X = x] − bE[D(0)|X = x]\n,\n(2)\nwhere a hat denotes an estimator. E[D(k)|X] can be estimated by the PU learning (Elkan and Noto 2008; du Plessis,\nNiu, and Sugiyama 2014, 2015; Kiryo et al. 2017) with the\nlogistic loss by using {x(k)\ndi }\nn(k)\nd\ni=1 as positive data, and the\ncovariates in {(y(1)\ni\n, x(1)\ni )}n(1)\ni=1 and {(y(0)\ni\n, x(0)\ni )}n(0)\ni=1 as unlabeled data. Separate estimation is easy to implement, but\nusually does not provide a good estimate since it has four\npossible sources of error.\nOne advantage of the separate estimation is that the model\nselection can also be naively performed by choosing the\nbest model for each component. We can easily calculate\nthe model selection criteria, such as the mean squared error (MSE) for each component from the separately observed\ndatasets. However, the resulting bµsep may perform poorly\nbecause it does not necessarily minimize the model selection\ncriterion in terms of the true µ (Rolling and Yang 2014).\nWe can alleviate the drawback of the separate estimation by directly estimating the numerator and denominator in (2). Let T := KD(1) − (1 − K)D(0) and U :=\nKY (1)−(1−K)Y (0) be the auxiliary variables for the notational and computational convenience, and assume P(K =\n1|X) = 0.5 without loss of generality. Then, we can rewrite\nthe expression in Theorem 1 as µ(X) = ν(X)/π(X),\nwhere ν(X) := E[U|X] and π(X) := E[T|X]. To estimate ν(X) and π(X), we can construct combined datasets\n{(ti, xti, rti)}nt\ni=1 :=\n( \n(−1)1−k, x(k)\ndi , bp(k)\nd\n(n(1)\nd\n+ n(0)\nd\n)\n2n(k)\nd\n!)n(k)\nd\n,1\ni=1,k=0\n{(ui, xui, rui)}nu\ni=1 :=\n( \n(−1)1−ky(k)\ni\n, x(k)\ni\n, n(1) + n(0)\n2n(k)\n!)n(k),1\ni=1,k=0\nfrom {x(k)\ndi }\nn(k)\nd\ni=1 and {(y(k)\ni\n, x(k)\ni\n)}n(k)\ni=1 , respectively, where\nnt := n(1)\nd\n+ n(0)\nd\nand nu := n(1) + n(0).\nWe can approximate the expectation of a product of any\nfunction f of X and T or U by the simple sample average:\n1\nnt\nnt\nX\ni=1\nrtitif(xti),\n1\nnu\nnu\nX\ni=1\nruiuif(xui).\nAn estimator of ν(X) can be obtained by any regression\nmethod using the above combined dataset, while we need\na little twist to obtain an estimator of the π(X). See Appendix C in the supplementary material for the direct estimation methods for the PSD.\nDirect Least Squares Estimation\nWe can apply the direct least squares (DLS) method (Yamane et al. 2018) originally proposed for ATE estimation\nunder complete compliance. Motivated by the drawbacks of\nthe separate estimation, the DLS directly estimates µ(X),\nwhich is advantageous not only in performance but also\nin computational efficiency. The following theorem corresponds to Theorem 1 in (Yamane et al. 2018).\nTheorem 2. Assume ν ∈ L2, where L2 := {f : X 7→\nR|E[f(X)2] < ∞} in addition to Assumption 1 and 2. Furthermore, define Hf(X) := π(X)f(X) − ν(X). Then,\nµ(X) = argmin\nf∈L2 E\n\u0002\nHf(X)2\u0003\n.\n(3)\nTheorem 2 immediately follows from Theorem 1. In\nwhat follows, we show that the minimizer of the problem (3) can be estimated without going through the estimation of ν and π. Since (Hf(X) − g(X))2 ≥ 0 for any\nsquare integrable function g ∈ L2, we have Hf(X)2 ≥\n2Hf(X)g(X)−g(X)2 by expanding the square. The equality holds at g(X) = Hf(X) for any f, which maximizes\n2Hf(X)g(X) − g(X)2 with respect to g. Hence,\nµ(X) = argmin\nf∈L2\nmax\ng∈L2 J(f, g),\n(4)\nwhere J(f, g) := E\n\u0002\n2Hf(X)g(X) − g(X)2\u0003\n. Rewriting\nJ(f, g) yields\nJ(f, g) = 2E[Tf(X)g(X)] − 2E[Ug(X)] − E[g(X)2]. (5)\nWhile the objective functional in (3) requires the conditional\nmean estimators, this form can be estimated based on the\nsample averages as\nbJ(f, g) = 2\nnt\nnt\nX\ni=1\nrtitif(xti)g(xti) − 2\nnu\nnu\nX\ni=1\nruiuig(xui)\n− 1\nnu\nnu\nX\ni=1\nruig(xui)2.\nWe can implement the DLS estimation of µ with an arbitrary\nregularization term Ω in practice:\nbµdls(x) = argmin\nf∈F\nmax\ng∈G\nbJ(f, g) + Ω(f, g),\nwhere F and G are model classes for f and g, respectively.\nAlthough any model can be trained by optimizing the\nmodel parameters to minimize the above objective functional, a practically useful choice is a linear-in-parameter\nmodel. Set F = {fα : x 7→ α⊤ϕ(x)|α ∈ Rqf } and\nG = {gβ : x 7→ β⊤ψ(x)|β ∈ Rqg}, where ϕ and ψ are\nqf- and qg-dimensional basis functions, respectively. Also,\ndefine ϕi := ϕ(xi) and ψ(xi) := ψ(xi). Using the ℓ2regularizer, we have\nbJ(f, g) + Ω(f, g) = 2α⊤Aβ − 2β⊤b − β⊤Cβ\n+ λfα⊤α + λgβ⊤β,\nwhere\nA := 1\nnt\nnt\nX\ni=1\nrtitiϕtiψ⊤\nti,\nb := 1\nnu\nnu\nX\ni=1\nruiuiψui,\nC := 1\nnu\nnu\nX\ni=1\nruiψuiψ⊤\nui,\nand λf and λg are some positive constants. The advantage\nof this formulation with the linear-in-parameter models and\nℓ2-regularizer is that we have an analytical solution. The solution to the inner maximization is given by\nbβ = (C + λgIqg)−1(A⊤α − b),\nwhere Iqg denotes the qg × qg identity matrix. We can then\nobtain the DLS estimator of α by substituting bβ and solving\nthe outer minimization:\nbαdls =\nn\nA(C + λgIqg )−1A⊤ + λf Iqf\no−1 A(C + λgIqg )−1b.\nFor the model selection, we can choose a model that\nminimizes bJ( bf, bg) evaluated on a validation set. However,\nas pointed out in (Yamane et al. 2018), one cannot tell if\nbJ( bf, bg) is small because bf is a good solution to the outer\nminimization, or bg is a poor solution to the inner maximization. For this reason, bµdls(x) := bα⊤\ndlsϕ(x) can be unstable,\nand one cannot be confident about which model is the best.\nThe increased dimensionality of the search space because\nof the need to simultaneously select models for f and g can\nalso make the model selection based on bJ( bf, bg) challenging.",
        "proposed method": "We propose a novel estimator that enables simpler model\nselection than the DLS estimation while maintaining the\nessence of direct estimation as much as possible. We avoid\nthe minimax formulation of the objective functional as in (4)\nby estimating µ as a solution to the weighted least squares\nproblem derived from the original problem (3). It can be constructed based on the sample averages from the separately\nobserved samples like the DLS estimator, except we need to\nestimate π(X) as a weight. We term the proposed estimator as the directly weighted least squares (DWLS) estimator\nsince the pre-estimated PSD directly appears without inversion in the objective unlike the IPW estimators (Wooldridge\n2002, 2007; Seaman and White 2013).\nConsider the following weighted least squares problem:\nmin\nf∈L2 E\n\u0014w(X)\nπ(X) Hf(X)2\n\u0015\n=: Q0(f|w),\n(6)\nShape\nn\nqx\nDWLS\nIWLS\nSEP\nDLS\n1\n0.65 ± 1.54\n5.04 ± 16.5\n1.80 ± 2.38\n0.15 ± 0.20\n10K\n5\n0.40 ± 0.91\n1.30 ± 3.21\n3.46 ± 2.71\n0.93 ± 1.17\nhcon\n10\n0.65 ± 1.00\n4.96 ± 14.4\n4.18 ± 3.81\n1.81 ± 2.82\n1\n0.22 ± 0.46\n53.5 ± 372\n1.04 ± 0.93\n0.08 ± 0.13\n50K\n5\n0.06 ± 0.07\n0.35 ± 0.88\n2.13 ± 2.00\n0.41 ± 0.67\n10\n0.10 ± 0.11\n1.22 ± 6.07\n3.52 ± 1.92\n0.41 ± 0.52\n1\n0.09 ± 0.15\n0.51 ± 1.25\n0.25 ± 0.30\n0.39 ± 0.30\n10K\n5\n0.14 ± 0.08\n1.32 ± 6.72\n0.58 ± 0.41\n0.88 ± 0.52\nhlin\n10\n0.61 ± 0.27\n4.73 ± 22.4\n1.39 ± 0.65\n1.53 ± 0.75\n1\n0.02 ± 0.03\n6.79 ± 53.1\n0.11 ± 0.11\n0.36 ± 0.33\n50K\n5\n0.04 ± 0.03\n0.12 ± 0.16\n0.36 ± 0.24\n1.04 ± 0.53\n10\n0.32 ± 0.10\n11.9 ± 112\n0.76 ± 0.26\n1.61 ± 0.89\n1\n0.19 ± 0.26\n0.72 ± 1.82\n0.30 ± 0.31\n0.32 ± 0.29\n10K\n5\n0.29 ± 0.23\n12.1 ± 113\n0.58 ± 0.40\n0.97 ± 0.56\nhlog\n10\n0.72 ± 0.31\n2.76 ± 7.65\n1.11 ± 0.49\n1.40 ± 0.72\n1\n0.06 ± 0.08\n0.30 ± 0.88\n0.22 ± 0.16\n0.32 ± 0.30\n50K\n5\n0.10 ± 0.04\n0.18 ± 0.13\n0.56 ± 0.31\n0.75 ± 0.62\n10\n0.30 ± 0.10\n55.3 ± 550\n0.86 ± 0.28\n1.20 ± 0.99\nTable 1: The mean and standard deviation of the MSE over 100 trials. The results are multiplied by 100 (constant), 10 (linear)\nand 10 (logistic), respectively. The bold face denotes the best and comparative results according to the two-sided Wilcoxon\nsigned-rank test at the significance level of 5%.\nwhere w(X) is some weight depending on X. Rewriting the\nobjective yields\nQ0(f|w) = E[T w(X)f(X)2] − 2E [Uw(X)f(X)] + E[S],\n(7)\nwhere S :=\nw(X)\nπ(X) ν(X)2 is a constant and can therefore\nbe safely ignored in the optimization. Choosing w = π\nclearly reduces the problem (6) to (3). Therefore, we can\nplug-in the pre-estimated π(X) as a weight in practice to\nfind a minimizer of the problem (3). However, π(X) may\nnot be the proper weight since it takes a negative value when\nP(Z(1) = 1|X) < P(Z(0) = 1|X). We can estimate\nQ(f|bπ) := Q0(f|bπ) − E[S] based on the sample averages\nusing the separately observed samples as\nb\nQ(f|bπ) =\n1\nnt\nnt\nX\ni=1\nrtitibπ(xti)f(xti)2 −\n2\nnu\nnu\nX\ni=1\nruiuibπ(xui)f(xui).\nUsing the linear-in-parameter model for f and the ℓ2regularizer as in the previous section, we have\nbQ(f|bπ) + Ω(f) = α⊤ e\nAα − 2α⊤eb + λfα⊤α,\nwhere\ne\nA :=\n1\nnt\nnt\nX\ni=1\nrtitibπ(xti)ϕtiϕ⊤\nti, eb :=\n1\nnu\nnu\nX\ni=1\nruiuibπ(xui)ϕui.\nThe solution to minf∈F bQ(f|bπ) + Ω(f) can be obtained as\nbµdwls(x) := bα⊤\ndwlsϕ(x), where bαdwls = ( e\nA + λfIqf )−1eb.\nAlthough the DWLS objective is no longer the MSE,\nbQ( bf|bπ) is still a sufficient measure for model selection because bµdwls minimizing this objective is also a good estimator in terms of the true MSE as long as bπ is sufficiently accurate. Since the DWLS involves only a single minimization,\nthe model selection is easier than in the DLS estimation.\nRemark on the objective formulation\nWe can consider\nthe following least squares problem instead of (6):\nmin\nf∈L2 E\n\u0014 π(X)\nw(X) (f(X) − µ(X))2\n\u0015\n=: Q′\n0(f|w).\n(8)\nThis can also be evaluated without ν:\nQ′\n0(f|w) = E\n\u0014Tf(X)2\nw(X)\n\u0015\n− 2E\n\u0014Uf(X)\nw(X)\n\u0015\n+ π(X)\nw(X)µ(X)2.\nWe refer to the estimator based on this objective as the\ninverse weighted least squares (IWLS) estimator. This estimator tends to be imprecise when the PSD is extremely\nclose to zero, which is a common issue among the IPW estimators (Wooldridge 2002, 2007; Seaman and White 2013).\nAlthough the performance of such estimators can be improved by trimming small probabilities, determining the optimal threshold is nontrivial (Lee, Lessler, and Stuart 2011).\nSelecting the threshold is more complicated in the case\nof the IWLS as the PSD can be negative. At points where\nthe PSD is close to zero, the absolute value of the weights\nincreases while the sign errors are more likely to occur. As\na result, a small estimation error in the PSD tends to have\na large impact on the IWLS. On the other hand, the impact\nof the estimation error of the PSD on the DWLS estimator\nis limited since the absolute value of the weights is small\nwhen a sign error occurs in the PSD estimation. The weight\nin the proposed method is confined to [−0.5, 0.5], whereas\nthe weight in the IWLS is not bounded at all.\nAlthough the unweighted subsampling method has been\nrecently studied to circumvents weighting samples with the\ninverse probability (Wang et al. 2020), it cannot be directly\napplied in this case because it requires the denominator to\nbe a sampling probability.\n0.2\n0.4\na. DWLS\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0.2\n0.4\nb. IWLS\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0.2\n0.4\nc. SEP\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n0.2\n0.4\nd. DLS\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFigure 2: Relation between the squared error (Y-axis) and\nthe PSD (X-axis) for linear µ(X), qx = 5 and n = 10000.\nEach point is coloured according to the spatial density.",
        "performance evaluation": "We test the performance of the proposed method with synthetic and real-world datasets. The details of the datasets and\nother setups can be found in Appendix D in the supplementary material. We denote the separate estimation as SEP.\nSynthetic Experiment\nWe generated synthetic data with the different shapes of µ,\nthe covariates’ dimension qx, and the size of training samples. The mean and standard deviation of the MSE over\n100 trials are summarized in Table 1. We conducted the\nWilcoxon signed-rank test since the IWLS was highly unstable, which made it difficult to use the t-test.\nAlthough we did not trim the PSD for the DWLS estimator, its performance was sufficiently stable to outperform the\nothers in almost all the cases. The DWLS had larger errors\nthan the DLS only when µ was constant and qx = 1. This\nmay be because the benefit of direct estimation outweighed\nthe difficulties of hyperparameter tuning of the DLS since\nthe one-dimensional constant µ is the easiest to learn. This\nresult supports the effectiveness of our approach: avoiding\nthe minimax formulation but without the inverse PSD.\nOn the other hand, the IWLS estimator often worked terribly poorly even with the weight trimming. The IWLS might\nbe affected by the small PSD most severely since it uses the\ninverse PSD both in the estimation and hyperparameter tuning. The performance of the DLS was not as poor, but it had\na larger MSE than SEP in many cases, indicating that hyperparameter tuning did not work as well as the DWLS.\nFigure 2 shows the relation between the squared error\nof each estimator and the PSD for the linear µ(X) when\nqx = 5 and n = 10000. The area with a dense plot is colored\nin light. The squared error of the DWLS shown in Figure 2a\nis kept very small except it is slightly larger when the PSD\nis close to zero. This result demonstrates the robustness of\nour method against the near-zero PSD, whereas the perforRMSE\nMean\nMax\nMin\nDR\n1778\n1837\n1648\nDWLS\n252 ± 141\n1528\n1838\n1013\nIWLS\n222 ± 145\n1559\n1924\n977\nSEP\n280 ± 119\n1538\n2200\n919\nDLS\n946 ± 663\n937\n7966\n-1814\nTable 2: The results for the real data analysis.\nmance of the other estimators is affected more severely by\nthe small PSD. We can observe the instability of the IWLS\nagain in Figure 2b, which shows the sporadic high errors\nover the entire PSD. The squared error of the SEP displays\nthe similar pattern. In Figure 2d, the light-colored area is at\na high position, indicating that the hyperparameter tuning of\nthe DLS does not work well.\nReal Data Analysis\nWe used the dataset of the National Job Training Partnership\nAct (JTPA) study. This is one of the largest RCT dataset\nfor job training evaluations in the US with approximately\n20000 participants, and it has been used in the several previous studies on causal inference (Bloom et al. 1997; Abadie,\nAngrist, and Imbens 2002; Donald, Hsu, and Lieli 2014).\nWe used the doubly robust (DR) estimator (Ogburn, Rotnitzky, and Robins 2015) trained with all joint samples as\nthe pseudo true LATE.\nThe results are summarized in Table 2. We report the\nmean, max, and min of bµ(x) in addition to the root mean\nsquared error (RMSE) to see the behavior of each estimator.\nThe IWLS had the smallest RMSE, but the difference from\nthe DWLS was not statistically significant. The mean, max,\nand min of the DWLS and IWLS also indicate they have\nsimilar performance. The stability of the IWLS in contrast\nto the synthetic experiment is because there were no samples with a small PSD. The relatively good performance of\nthe SEP should be because of the same reason. The performance of the DLS was highly unstable, reflecting the difficulty of the model selection based on the DLS objective.",
        "conclusion": "We proposed a novel problem setting for LATE estimation\nby data combination. Our setting is plausible enough to apply to many real-world problems. The leading examples include O2O marketing and when there is non-random attrition in panel data. We developed a practically useful method\nfor estimating LATE from the separately observed datasets.\nOur method overcomes the issue of the existing method in\nmodel selection by avoiding the minimax formulation of the\nobjective. Furthermore, our method can also avoid the common problem of the IPW methods since the PSD directly\nappears in our estimator without inversion. Our method displayed the promising performance in the experiments on the\nsynthetic and real-world datasets. However, there are sometimes concerns on the homogeneity of the populations. Extending the proposed method to account for heterogeneous\nsamples and time-varying potential variables is an important\nfuture direction to further increase its usefulness in practice.",
        "summary_en": "It is important to estimate the local average treatment effect (LATE) when compliance with a treatment assignment is incomplete. The previously proposed methods for LATE estimation required all relevant variables to be jointly observed in a single dataset; however, it is sometimes difficult or even impossible to collect such data in many real-world problems for technical or privacy reasons. This paper considers a novel problem setting in which LATE, as a function of covariates, is nonparametrically identified from the combination of separately observed datasets. For estimation, the paper shows that the direct least squares method, which was originally developed for estimating the average treatment effect under complete compliance, is applicable to the setting. However, model selection and hyperparameter tuning for the direct least squares estimator can be unstable in practice since it is defined as a solution to the minimax problem. The paper then proposes a weighted least squares estimator that enables simpler model selection by avoiding the minimax objective formulation. Unlike the inverse probability weighted (IPW) estimator, the proposed estimator directly uses the pre-estimated weight without inversion, avoiding the problems caused by the IPW methods. The paper demonstrates the effectiveness of the method through experiments using synthetic and real-world datasets.",
        "summary_zh": "这篇论文研究了在治疗分配的遵从性不完全的情况下估计局部平均治疗效果（LATE）的问题。针对现有方法需要在单个数据集中联合观察所有相关变量的限制，作者提出了一种新的方法。他们考虑了一个新颖的问题设置，即LATE作为协变量的函数从分别观察到的数据集的组合中被非参数地识别出来。为了估计LATE，作者表明直接最小二乘法方法适用于这种情况。然而，直接最小二乘估计器的模型选择和超参数调整在实践中可能不稳定。因此，他们提出了一种加权最小二乘估计器，通过避免极小化目标函数，简化了模型选择。与逆概率加权（IPW）估计器不同，这种方法直接使用预估的权重，而不进行反演，避免了IPW方法引起的问题。作者通过使用合成和真实世界数据集进行的实验验证了他们方法的有效性。"
    },
    {
        "title": "Learning Instrumental Variable from Data Fusion for Treatment Effect Estimation",
        "abstract": "The advent of the big data era brought new opportunities and challenges to draw treatment effect in data fusion, that is, a mixed dataset collected from multiple sources (each source with an independent treatment assignment mechanism). Due to possibly omitted source labels and unmeasured confounders, traditional methods cannot estimate individual treatment assignment probability and infer treatment effect effectively. Therefore, we propose to reconstruct the source label and model it as a Group Instrumental Variable (GIV) to implement IV-based Regression for treatment effect estimation. In this paper, we conceptualize this line of thought and develop a unified framework (Meta-EM) to (1) map the raw data into a representation space to construct Linear Mixed Models for the assigned treatment variable; (2) estimate the distribution differences and model the GIV for the different treatment assignment mechanisms; and (3) adopt an alternating training strategy to iteratively optimize the representations and the joint distribution to model GIV for IV regression. Empirical results demonstrate the advantages of our Meta-EM compared with state-of-the-art methods. The project page with the code and the Supplementary materials is available at https://github.com/causal-machinelearning-lab/meta-em.",
        "introduction": "Estimating the causal effects of treatment/exposure on the\noutcome of interest from the observation dataset is crucial\nfor explanatory analysis and decision-making (Pearl 2009;\nKuang et al. 2020b; Li et al. 2020; Zhang et al. 2021; Tian\net al. 2022). In the presence of unmeasured confounders, assuming a fixed additive noise model (ANM), state-of-the-art\n(SOTA) approaches use an instrumental variable (IV) to implement a two-stage regression to reduce endogenous confounding bias in treatment effect estimation (Hartford et al.\n2017; Lin et al. 2019; Muandet et al. 2020; Wu et al. 2022a).\nThese methods are reliable when the pre-defined IV is a\nX: Covariate\nZ: Instrument (Interviewer)\nT: Treatment (Offer)\nY: Outcome (Development)\nY\nT\nZ\nX\nY\nT\nX\nY\nT\nX\nY\nT\nX\nY\nT\nX\n1f\u0001\n...f\nZ\nf\u0001\nY\nT\nX\nSource 1\n...\nSource K\nMultiple Treatment Assignment Mechanisms\n...\n...\nK\nf\u0001\n: Treatment Assignment Mechanisms\n(Interviewer's Prejudices)\n1,...,\n(\nK Z )\nf\u0001\nFigure 1: The causal diagram for mixed datasets from multiple sources, each source with an independent treatment assignment mechanism. Blue nodes denote observable variables, and gray nodes indicate latent variables. The arrows\nwith different colors define different causal effects. The bidirected arrows encode unmeasured confounders.\nvalid IV that only affects the outcome through its strong association with treatment options, called exclusion assumption. Under these assumptions, Angrist et al. (1996); Newey\nand Powell (2003) verify that causal effects can be identified by exogenous IVs. In instrumental variable literature,\nresearchers usually implement Randomized Controlled Trials (RCTs) to obtain exogenous IVs, such as Oregon health\ninsurance experiment (Finkelstein et al. 2012) and effects of\nmilitary service on lifetime earnings (Angrist 1990), which\nare too expensive to be universally available.\nWith the advent of the big data era, a variety of observation databases collected from different sources have been\nestablished, which may contain the same treatment effect\nmechanism (from treatment to outcome) but different treatment assignment mechanisms (from covariates to treatment)\n(Bareinboim and Pearl 2016; H¨unermund and Bareinboim\n2019). For instance, as shown in Fig. 1, in the study of treatment effect of individual offers (treatment T) on enterprise\ndevelopment (outcome Y ), different human resources (HR)\ninterviewers (instrument Z) may assign different offer decisions to the same individual (covariate X) based on different evaluation strategies (assignment fθZ). In this case, candidates will be randomly assigned to different interviewers,\neach with different prejudices or opinions, to decide whether\nto give an offer or not (Pager and Karafin 2009). Here, the\nomitted interviewer label (source label) can serve as a latent\nmulti-valued IV, which only affects the outcome through its\nstrong association with offer decisions (Kuang et al. 2020c;\nRothenh¨ausler et al. 2021). Such heterogeneous assignment\nmechanism is common and widespread in real applications,\nsuch as the assessment rules in university admissions or academic title evaluation (Harris et al. 2022), and the environments in Generalized Causal Dantzig (Long et al. 2022).\nNevertheless, due to data privacy and missing data, interviewers’ information is rare in public datasets. Besides, the\nsource label is not always available in some scenarios. For\nexample, people tend to consult an expert consultant, and the\nconsultant’s emotional state could be a latent IV that cannot\nbe accessed. A large amount of literature for Summary/Selection IVs has attempted to resolve this problem (Burgess,\nSmall, and Thompson 2017; Kuang et al. 2020c; Hartford\net al. 2021; Yuan et al. 2022). Two main limitations of\nthese methods are that they require expert knowledge to provide well-predefined IV candidates, and lack metrics to test\nthe validity of IV variables learned by unsupervised methods. Moreover, to obtain valid Summary IVs, these methods\nassume that at least half of pre-defined IV candidates are\nvalid strong IVs so that they can synthesize an IV through\na weighted average (Burgess and Thompson 2013; Davies\net al. 2015; Burgess, Small, and Thompson 2017).\nSince summary IVs require half of the IV candidates to be\nvalid, which rarely happens in practice, the estimation might\nbe unreliable. Therefore, it is highly demanded to model\nlatent IVs and implement a data-driven approach to automatically obtain valid IVs directly from the observed variables {X, T, Y }, without pre-defined hand-made IV candidates. Fortunately, the advent of the big data era brought\nnew opportunities to reconstruct IVs from multiple sources\ndata (each source with an independent treatment assignment\nmechanism). In the offer case (Fig. 1), the interviewers generate multiple causal relations between the covariates and\nthe treatment, and it can serve as a latent multi-valued IV.\nMotivation: Thus, we propose to separate the observational\ndata into multiple groups to reconstruct the source label and\nthen explicitly model the group indicator as a Group Instrumental Variable (GIV) to implement IV-based Regression.\nIn this paper, we aim to recover latent IV and estimate\nthe individual treatment effect (ITE) from mixed observational datasets in the presence of unmeasured confounders.\nDue to possibly omitted source labels and unmeasured confounders, traditional methods cannot estimate individual\ntreatment assignment probability and infer treatment effect\neffectively (Wu et al. 2022b; Kuang et al. 2017, 2020a).\nTherefore, we propose to reconstruct the source label and\nmodel it as a Group Instrumental Variable (GIV) to implement IV-based Regression for treatment effect estimation.\nIn this paper, we conceptualize this line of thought and develop a unified framework (Meta-EM1) to (1) map the raw\ndata into a representation space to construct Linear Mixed\nModels for the assigned treatment variable; (2) estimate the\ndistribution differences and model the GIV for the different treatment assignment mechanisms using ExpectationMaximization algorithm (EM); and (3) adopt an alternating\ntraining strategy to iteratively optimize the representations\nand the joint distribution to model GIV for IV regression.\nEmpirical results demonstrate the advantages of the GIV\ncompared with SOTA methods.\nThe contribution of our paper is three-fold:\n• We propose a Meta-EM algorithm to reconstruct the\nsource label as GIV directly from the observed variables,\ni.e., no available IV candidates for learning, which is beyond the capability of existing Summary IV methods.\nGIV (source label) is effective when there are identifiable differences in mechanisms across groups.\n• Meta-EM algorithm uses a shared representation block to\nlearn a nonlinear representation space to EM algorithm,\nwhich relaxes the underlying linear regression assumption. Theoretically, Meta-EM can obtain an asymptotic\nsource label as GIV for ITE estimation.\n• We empirically demonstrate that the Meta-EM algorithm\nreconstructs the source label as GIV from the observed\nvariables for accurate treatment effect estimation and\ngains SOTA performance compared with existing summary IV methods.",
        "related work": "Instrumental Variable Methods\nThe sufficient identification results for causal effect under the additive noise assumption in instrumental variable\nregression were developed by (Imbens and Angrist 1994;\nNewey and Powell 2003). For semi-parametric and nonparametric estimation, there are four main research lines about\nIV methods, including: (1) The two-Stage Least Squares,\nPoly2SLS and NN2SLS; (2) The Kernel-based Methods,\nKernel IV (Singh, Sahani, and Gretton 2019) and DualIV\n(Muandet et al. 2020) map X to a reproducing kernel Hilbert\nspace (RKHS); (3) The Deep Methods, DeepIV (Hartford\net al. 2017), OneSIV (Lin et al. 2019) and DFIV (Xu et al.\n2021) adopts deep neural nets and fit a mixture density network; (4) The Adversarial GMM, AGMM (Dikkala et al.\n2020) and DeepGMM (Bennett et al. 2019) construct a\nstructural function and select moment conditions via adversarial training.\nThe above methods are reliable only if the pre-defined IVs\nare valid and strongly correlated with the treatment. In practice, such valid IVs are hardly satisfied due to the untestable\nexclusion assumption. In this paper, we reconstruct a GIV\nand plug it into IV methods to predict the treatment effect.\nSummary IV Synthesis\nA growing number of works have been proposed to synthesize a Summary IV by combining existing IV candidates. In\n1Meta means “learn nonlinear mappings to learn EM”.\nMendelian Randomization (MR), IV candidates are merged\ninto a summary IV by unweighted/weighted allele scores\n(UAS/WAS) (Burgess, Small, and Thompson 2017; Kuang\net al. 2020c), UAS takes the average of IV candidates while\nWAS weights each IV candidate based on the associations\nwith the treatment. Besides, ModeIV (Hartford et al. 2021)\nadopts the tightest cluster center of estimation points as IV\nto approximate causal effects. Assuming that all IV candidates are independent of the unmeasured confounders, AutoIV (Yuan et al. 2022) generates IV representation. Existing Summary IV Methods require a high-quality IV candidates’ set with at least half valid IVs, which is unrealistic\nin practice due to cost issues and lack of expert knowledge.\nUnder a more practical setting, we model latent IVs and implement a data-driven approach to automatically reconstruct\nvalid Group IVs directly from the observed variables, without hand-made IV candidates.",
        "problem setup and assumptions": "In this paper, we aim to learn latent IV and estimate the\nindividual treatment effect (ITE) from mixed datasets in\nthe presence of unmeasured confounders. As shown in Fig.\n1, a mixed dataset D\n= {D1, D2, · · · , DK} collected\nfrom K sources Dk\n=\n{xi, ti, yi, ϵi\n|\nfθk}nk\ni=1, k\n=\n1, 2, · · · , K, each source with an independent treatment assignment mechanism2 fθk, the size of samples from source\nk is nk and the total sample size is n = PK\nk=1 nk. For unit i\nfrom source zi = k, we observe confounders xi ∈ X where\nX ⊂ RmX with dimension mX, a treatment variable ti ∈ T\nfrom mechanism fθk where T ⊂ R, and a outcome variable\nyi ∈ Y where Y ⊂ R. In data fusion, due to data privacy and\nmissing data, the source label zi and some key confounders\nmay be unrecorded in observational data. We incorporate the\nunobserved confounders into the term ϵi.\nWithout interactions between unmeasured confounders\nand treatment, we can represent the effect of infinitely many\nunmeasured causes as an additive noise {ϵT , ϵY } regardless\nof how they interact among themselves. The sufficient identification results for causal effect under the additive noise\nassumption in instrumental variable regression were developed by (Angrist et al. 1996; Newey and Powell 2003).\nAssumption 0.1 Additive Noise Assumption: Similar to\n(Hartford et al. 2017; Singh, Sahani, and Gretton 2019; Xu\net al. 2021), we assume that the mixed data is generated by:\nT = fθZ(X) + ϵT , Y = g(T, X) + ϵY ,\n(1)\nDefinition 0.2 Individual Treatment Effect (ITE):\nτ = g(t, X) − g(0, X), g(t, X) = E[Y | do(T = t), X].\nDefinition 0.3 An Instrument Variable Z is an exogenous\nvariable that only affects the outcome through its strong association with the treatment. Besides, an valid instrument\nvariable satisfies the following three assumptions:\nRelevance: interviewers Z assign treatments T to each unit,\n2In causal inference, we assume the causal effect of treatment\non the outcome is invariant across sources. If the treatment effect\nvaries across sources, then we will not identify which treatment\neffect mechanism the individual’s outcome came from in testing.\ni.e., P(T | Z) ̸= P(T).\nExclusion: interviewers Z does not directly affect the outcome Y , i.e., Z ⊥ Y | T, X, ϵ.\nUnconfounded: the offer-seekers will be randomly assigned\nto different interviewers, so Z is independent of all confounders, including X and ϵ, i.e., Z ⊥ X, ϵ.\nWith the advent of the big data era, a variety of observation databases collected from different sources have\nbeen established, which may contain the same treatment effect mechanism (from treatment to outcome) but different\ntreatment assignment mechanisms (from covariates to treatment). Such heterogeneous assignment mechanism is common and widespread in real applications(Harris et al. 2022;\nLong et al. 2022). Plausible settings include: the socioeconomic status influences treatment but not outcomes, and\nadmissions assessment rules affect students’ SAT scores but\ndo not determine their success in college. In addition, there\nare many subtle factors that are easily overlooked in realworld applications that may be latent assignment variables,\nsuch as weather, holiday, mood, dresses, travel style, lunch,\netc. All of them may only affect the treatment choice without\ndirectly changing the outcome, but they are often ignored.\nIn the presence of such assignment variables, we propose\nto separate the observational data into multiple groups to reconstruct the assignment variables and then explicitly model\nthe group indicator as a Group Instrumental Variable (GIV)\nto implement IV-based Regression.",
        "algorithm": "In this section, we propose a Meta-EM algorithm to automatically identify the latent source label Z, inducing the different treatment assignment mechanisms, as group indicator\nto separate data into multiple groups. Specifically, the overall Meta-EM architecture (Fig. 2) of our model consists of\nthe following components: (1) Meta-EM uses a Shared Network Layer to map the covariates X to non-linear representations R, and then uses latent variable Z (obtained from EM\nalgorithm) to regress the treatment variables and optimize\nthe representation; (2) Meta-EM estimates the distribution\ndifferences across sources and models latent variable as a\nGIV for the different treatment assignment mechanisms using Expectation-Maximization algorithm (EM); (3) MetaEM adopts an alternating training strategy to iteratively optimize the Representations and the joint distribution for\nGIV Reconstruction. Theoretically, Meta-EM achieve an\nasymptotic IV and accurately predict ITE by plugging GIV\ninto downstream IV-based methods.\nRepresentation Learning Step\nLet zi = k denotes the latent source label (k = 1, 2, · · · , K)\nfor unit i, and source number K is a hyper-parameter. To\nconstruct Linear Mixed Models for the assigned treatment\nvariable T, we use a representation function fR maps the\ncovariates X ∈ RmX into a representation R ∈ RmR. Consider the following representation model (Fig. 2(a)):\nti = fθzi (xi) + ϵi = α′\nzifR(xi) + ϵi = α′\nziri + ϵi,\n(2)\nwhere fR is a shared representation block which can be\nlearned from polynomial functions, kernel functions or a\nFigure 2: Overview of Meta-EM Architecture.\nneural network, ri is a (non-)linear representation of xi, and\nαzi is the corresponding coefficients for Linear Mixed Models. Then we formulate a linear (non-)gaussian mixed model:\nti = PK\nk=1 1zi=k (α′\nkri + ϵi),\n(3)\nwhere\n1zi=k\ndenote\nthe\nindicator\nfunction.\nSpecifically, in polynomial from, we expect to obtain ti\n=\nPK\nk=1 1zi=k\nnPmR\nj=1\n\u0002\nαkj(ξkj,1x1\nij + ξkj,2x2\nij + · · · )\n\u0003\n+ ϵi\no\n,\nwhere ξkj,d denotes the corresponding expectation coefficient of the d-th power of j-th variable xij.\nWe design two prediction networks fT and fX to regress\ntreatment T and covariates X, and adopt an alternating training strategy to optimize the representations iteratively (Fig.\n2(b)), and the superscript (s) denotes the s-th iteration:\nL\n=\nPn\ni\n\u0010\nfT (z(s)\ni\n, r(s)\ni ) − ti\n\u00112\n+ λ Pn\ni\n\u0010\nfX(r(s)\ni ) − xi\n\u00112\n=\nPn\ni\n\u0012\nα′\nz(s)\ni r(s)\ni\n− ti\n\u00132\n+ λ Pn\ni\n\u0010\nfX(r(s)\ni ) − xi\n\u00112,\n(4)\nIn the term fT (z(s)\ni\n, r(s)\ni ), z(s)\ni\nis a latent function indicator\nto activate the corresponding linear coefficients αzi for treatment regression, and the representation ri is shared in all\nsources. The second term is a regularization term to ensure\nthat the representation contains as much information as possible from the original data. Besides, λ is a trade-off parameter to control the relative importance of treatment regression\nand covariate regression. We let λ = 1/mX, representing\nthat we adopt mean square of L2 norm (λ(fX(ri) − xi)2 =\nP\nj(fj − xij)2/mX, and j = 1, · · · , mX) in covariates regression and treat it as important as treatment regression. By\nminimizing L, our model can map the raw data into a representation space to construct Linear Mixed Models for the\nassigned treatment variable.\nDistribution Learning Step\nBased on the traditional Expectation-Maximization (EM) algorithm with group number K (Fig. 2(c)), we seek to find\nthe Maximum Likelihood Estimate (MLE) of the marginal\nlikelihood by iteratively applying the Expectation step and\nMaximization step. Consider the following log-likelihood\nfunction for Gaussian Mixture with θ = {π, µ, Σ}:\nlog Pr(CT R | θ) = log Pr(CT R | π, µ, Σ)\n=\nPn\ni=1\nPK\nk=1 log (πkPr(ci | µk, Σk))1zi=k,\n(5)\nwhere CT R denotes the concatenation of T and R, ci =\n{ti, ri}, and πk = Pr(zi = k), k = 1, 2, · · · , K. µk and\nΣk are the mean vector and covariance matrix of samples\n{ci}i:zi=k for group k. Pr(ci | µk, Σk) is the density of ci\nconditional on zi = k:\nPr(ci | µk, Σk) = (2π)− mR\n2 |Σj|− 1\n2 e− 1\n2 (ci−µk)′Σ−1\nk (ci−µk).\nInitialization.\nPr(R(s)) and Pr(ϵ) should be fixed\namong all groups since the population does not change according to treatment assignment (Fig. 1), i.e., Pr(R|Z =\ni) = Pr(R|Z = j) for any groups Z = i and Z =\nj. Therefore, we can use E[R] and Cov(R, R) to initialize the distribution parameters θ[0] = {π[0], µ[0], Σ[0]} =\n{{π[0]\nk , µ[0]\nk , Σ[0]\nk }k=1,2,··· ,K} with π[0]\nk = 1/K:\nµ[0]\nk = {µ[0]\nk (T), E[R]}, Σ[0]\nk =\n\"\nσ[0]\nk (T, T)\nσ[0]\nk (T, R)T\nσ[0]\nk (T, R)\nCov(R, R)\n#\n,\n(6)\nwhere {µ[0]\nk (T), σ[0]\nk (T, T), σ[0]\nk (T, R)} are the random initialization of the mean of T, the covariance of T, and the\ncovariance matrix of T and R in the group Z = k, respectively. The superscript [h] denotes h-th iteration\nExpectation Step.\nIn the expectation step of the v-th iteration, given the observation data C(s)\nT R and current parameter\nestimation θ[h] = {π[h], µ[h], Σ[h]}, we calculate the log expectation of likelihood function Eq. (5):\nQ(θ[h]) = Pn\ni=1\nPK\nk=1 ˆγik log (πkPr(ci | µk, Σk))1zi=k,\n(7)\nwhere ˆγik is the conditional probability distribution that the\ni-th unit comes from the k-th group given θ[h]:\nˆγik = Pr(zi = k | θ[h]) =\nπ[h]\nk P r(ci|µ[h]\nk ,Σ[h]\nk )\nPK\nj=1 π[h]\nj\nP r(ci|µ[h]\nj\n,Σ[h]\nj\n).\n(8)\nMaximization Step.\nIn the maximization step of the hth iteration, given the observational data C(s)\nT R and the current parameter estimation θ[h]\n=\n{π[h], µ[h], Σ[h]}, we\nmaximize the expectation of the log-likelihood function\nQ({π[h], µ[h], Σ[h]}) (Eq. (7)) to obtain the parameter estimation θ[h+1] of next iteration:\nθ[h+1] = argmaxθQ({π[h], µ[h], Σ[h]}).\n(9)\nThe solution is: for any k = 1, 2, · · · , K,\nµ[h+1]\nk\n=\nPn\ni=1 ˆγikci\nPn\ni=1 ˆγik ,\n(10)\nΣ[h+1]\nk\n=\nPn\ni=1 ˆγik\nh\nci−µ[h+1]\nk\ni2\nPn\ni=1 ˆγik\n,\n(11)\nπ[h+1]\nk\n=\nPn\ni=1 ˆγik\nn\n.\n(12)\nThen, the EM algorithm would obtain the convergent parameters θ∗ = {π∗, µ∗, Σ∗} by iteratively applying the Expectation and Maximization steps. We can sample/identify\nthe sub-group indicator Z(s+1) from the estimated distribution parameters {π∗, µ∗, Σ∗}:\nγ(s+1)\nik\n= Pr(zi = k) =\nπ∗\nkP r(c(s)\ni\n|µ∗\nk,Σ∗\nk)\nPK\nj=1 π∗\nj P r(c(s)\ni\n|µ∗\nj ,Σ∗\nj ),\nz(s+1)\ni\n∼ Disc(γ(s+1)\ni1\n, γ(s+1)\ni2\n, · · · , γ(s+1)\niK\n),\nwhere Disc(·) denotes discrete distribution with {γik}K\nk=1.\nOptimization and MMD\nSpecifically, our Meta-EM is composed of two phases: in\nrepresentation learning phase, we fix GIV z to optimize representation network and corresponding coefficients (fR and\nα) by minimizing objective in Eq. (4); in distribution learning phase, we use representation R to re-devide group and\nupdate GIV z using EM algorithm. As shown in Fig 2, the\nMeta-EM algorithm would learn an optimal representation\nR∗ to learn γ(∗)\nik and z(∗)\ni\nby iteratively applying the representation learning step and distribution learning step:\nγ(∗)\nik = Pr(zi = k) =\nπ∗\nkP r(c(∗)\ni\n|µ∗\nk,Σ∗\nk)\nPK\nj=1 π∗\nj P r(c(∗)\ni\n|µ∗\nj ,Σ∗\nj ),\nz(∗)\ni\n∼ Disc(γ(∗)\ni1 , γ(∗)\ni2 , · · · , γ(∗)\niK ).\nSuppose each coordinate in the coefficient vector αk in Eq.\n(3) is nonzero for all K = k. As (mR, n) → ∞, ˆγik converges to 1zi=k with the rate o(exp(−(mR + M))) for each\nk, where mR is the dimension of the representations. For\ntheorems and proofs, see the Supplementary material.\nAs shown in Fig. 1, Pr(X) should be fixed among\nall groups since the population does not change according to treatment assignment, which means the instrumental variable should be independent of all confounders (Unconfounded Assumption of IV), i.e., Pr(X|Z = i) =\nPr(X|Z = j) for any groups Z = i and Z = j. To implement an end-to-end algorithm, we use Maximum Mean Discrepancy (MMD) to measure the correlation between discrete variable Z and observed confounder X: MMD =\n∥E[R|Z = i] − E[X|Z = j]∥2\n2. Furthermore, we can automatically select the most appropriate group number K∗\nby the minimum correlation:\nMMDK =\n2\nn(n−1)\nPK\ni=1\nPK\nj=i+1 || ¯XZ=i − ¯XZ=j||2\n2,\nK∗ = argminKMMDK, K = {2, 3, ...}.\nwhere ¯XZ=i denotes the mean of the covariates X in the i-th\nsub-group according to the EM algorithm.",
        "experiments": "Baselines\nIV Generation\nIn this paper, we adopt Meta-EM with\nMMD to find the most appropriate group number K and take\nthe cluster results as GIVEM. We compare our algorithm\nMeta-EM with the Summary IV methods: (1) NoneIV uses\na full-zeros vector as IV; (2) UAS (Davies et al. 2015) takes\nthe average of IV candidates as IV; (3) WAS (Burgess, Dudbridge, and Thompson 2016) weights each candidate based\non the associations as IV; (4) ModeIV (Hartford et al. 2021)\ntakes the tightest center of estimation points as IV; (5) AutoIV(Yuan et al. 2022) learns a disentangled representation\nas IV. Besides, we adopt Meta-KM3 to generate GIVKM\nand use the superscript ∗ to represent the priori of the number of groups, i.e., GIVKM∗. TrueIV denotes the known\nground-truth source label.\n3Meta-KM is the K-means replacement of Meta-EM.\nIV Regression\nTo evaluate the performance of Meta-EM\nfor IV generation, we plug synthetic IVs, obtained from\nMeta-EM and other IV generation baselines, into IV regression methods (as listed in Related Work) for ITE estimation.\nExperiments on Synthetic Datasets\nSimilar to DeepIV (Hartford et al. 2017), DFIV (Xu et al.\n2021), DeepGMM (Bennett et al. 2019) and AutoIV (Yuan\net al. 2022), due to lack of the prior of latent outcome\nfunction and instrumental variable in existing real-world\ndatasets, we evaluate and compare our algorithm Meta-EM\nwith the above baselines on the synthetic and semi-synthetic\ndata. To simulate real-world data as much as possible, we adjust the difficulty of the simulation and expand experiments\nto various non-linear scenarios (Fig. 3), increase the number\nof sub-groups and the dimension of covariates (Table 4 in\nSupplementary material).\nDatasets\nWe generate the synthetic datasets as follows:\n• The confounders {X, ϵ}:\nX, ϵ ∼ N(0, ΣmX+1), ΣmX+1 =\n\u0014\nImX\nσ\nσ\n1\n\u0015\n,\n(13)\nwhere mX is the dimensions of observed confounders X,\nImX denotes mX order identity matrix, and σ denotes the\ncovariance between confounders X and unmeasured confounder ϵ. In this paper, we let σ = 0.1.\n• The treatments T collected from multiple sources Z:\nT = PK\nz=1 1[Z=z] [PmX\ni=1 wzi[Xi + fX(Xi)] + fz(ϵ)] + δT, (14)\nZ ∼ Pr(Z = z) = 1/K, wzi ∼ Unif(−1, 1), z = 1, · · · , K, (15)\nwhere Xi, i = {1, · · · , mX} denotes the i-th variable in X,\nδT ∼ N(0, 0.1), Unif means we draw wzi from the parameterized uniform distribution, and fz(ϵ) = 0.2ϵ. The mixed\ndata derives from K different sources, meaning that there\nare K independent potential treatment assignment models.\nZ is the indicator of the potential treatment assignment\nmodel, which can be regarded as an instrumental variable.\nTo simulate real-world data as much as possible, we design 5 different treatment functions fX(·) to discuss the\nperformance of Meta-EM algorithm: (1) linear scenario,\nfX(Xi) = Xi; (2) poly scenario, fX(Xi) = X2\ni ; (3) sin scenario, fX(Xi) = sin(Xi); (4) sigmoid scenario, fX(Xi) =\n1/(1 + exp(−Xi)); (5) abs scenario, fX(Xi) = abs(Xi).\n• The latent outcome function Y :\nY =\n−\n1.5T + 0.9T 2 + Pm\ni=1\nXi\nm + |X1X2|\n−\nsin(10 + X2X3) + 2ϵ + δY .\n(16)\nwhere ϵ is an unmeasured confounder and δY ∼ N(0, 0.1).\nFor synthetic datasets, we sample 3,000 units and perform\n10 independent replications to report mean squared error\n(MSE) and standard deviations of the individual treatment\neffect estimation over the testing data (3000 units) that we\nintervene the treatment as T = do(t). To verify the effectiveness of GIVEM in different scenarios with different dimensions of covariates mX and different group numbers K, we\nuse Data-K-mX to denote the different scenarios. In this\npaper, we set the representation dimension as mR = mX.\nPoly2SLS\nNN2SLS\nKernelIV\nDualIV(1)\nDeepIV\nOneSIV\nDFIV(1)\nDeepGMM\nAGMM\nNoneIV\n0.33(0.04)\n1.90(1.25)\n0.35(0.07)\n1.92(0.48)\n0.37(0.01)\n0.31(0.03)\n1.33(0.14)\n0.33(0.07)\n0.21(0.05)\nUAS\n0.33(0.04)\n2.30(1.46)\n0.35(0.07)\n0.98(0.34)\n0.37(0.02)\n0.31(0.03)\n1.30(0.10)\n0.32(0.04)\n0.21(0.05)\nWAS\n0.31(0.04)\n1.59(0.92)\n0.36(0.05)\n2.16(0.46)\n0.37(0.02)\n0.34(0.03)\n1.29(0.12)\n0.32(0.06)\n0.23(0.04)\nModeIV\n0.33(0.04)\n2.25(1.30)\n0.35(0.08)\n1.90(0.56)\n0.37(0.02)\n0.31(0.02)\n1.29(0.12)\n0.31(0.07)\n0.20(0.04)\nAutoIV\n> 100(2)\n2.10(1.01)\n0.35(0.07)\n0.79(0.32)\n0.37(0.02)\n0.31(0.03)\n1.29(0.11)\n0.31(0.09)\n0.21(0.05)\nGIVKM\n0.27(0.13)\n0.65(0.36)\n0.22(0.04)\n1.47(0.26)\n0.28(0.01)\n0.23(0.02)\n1.25(0.11)\n0.14(0.01)\n0.12(0.02)\nGIVKM*\n0.19(0.09)\n0.37(0.22)\n0.21(0.03)\n1.60(0.36)\n0.26(0.04)\n0.22(0.03)\n1.24(0.10)\n0.14(0.04)\n0.10(0.01)\nGIVEM\n0.05(0.00)\n0.12(0.01)\n0.11(0.02)\n1.99(0.41)\n0.08(0.00)\n0.12(0.01)\n0.79(0.08)\n0.08(0.01)\n0.06(0.00)\nTrueIV\n0.05(0.00)\n0.08(0.01)\n0.11(0.02)\n1.93(0.39)\n0.08(0.00)\n0.11(0.01)\n0.79(0.06)\n0.06(0.01)\n0.06(0.01)\n- (1) DualIV and DFIV don’t perform well on GIV, because they require continuous IVs rather than discrete IVs. (2) ”> 100” means ”MSE > 100”.\nTable 1: The Mean Squared Error mean(std) on Linear Experiments (Linear-3-3)\nPoly(T, X)\nSin(T, X)\nSigmoid(T, X)\nAbs(T, X)\nSort( g(T, X) )\nSort( g(T, X) )\nSort( g(T, X) )\nSort( g(T, X) )\n—— GT \n—— ModeIV\n—— AutoIV\n—— GIVKM\n—— GIVEM\n—— TrueIV\nFigure 3: Treatment Effect Estimation (sorted by Ground-Truth g(T, X)) in Non-linear Scenario Data-3-3.\nLinear\nPoly\nSin\nSigmoid\nAbs\nEM\n86.1%\n82.9%\n87.2%\n89.4%\n85.2%\nMeta-EM\n86.1%\n92.7%\n90.3%\n94.7%\n92.3%\nFun-EM\n86.1%\n98.1%\n96.3%\n98.4%\n96.5%\nTable 2: Ablation Experiments for Meta-EM in Data-2-5\nThe Results of Individual Treatment Effect Estimation\nAs shown in Table 1 (The top-2 is highlighted in bold for\nall tables), following observations are identified from the results: (1) Without valid IV candidates, Summary IVs are\nnot reliable and fail to synthesize a valid IV, and plugging\nthem into the IV methods can hardly improve the estimation\nperformance, which is close to the NoneIV; (2) DualIV and\nDFIV do not perform well on GIV and fail to estimate treatment effect, even with TrueIV, because they require continuous IVs rather than discrete IVs. (3) Through clustering,\nwe reconstruct the latent exogenous IV that generates different treatment mechanisms, GIVs (with Meta-KM or MetaEM) bring higher accuracy on individual treatment effect estimation by comparing with Summary IV methods in various IV-based methods except for DualIV; (4) By estimating\nthe latent differentiated covariate-treatment distribution parameters across groups and reconstructing the source label,\nGIVEM significantly improves the performance of clustering methods compared with GIVKM and achieves SOTA\nperformance for individual treatment effect estimation, even\ncomparable with TrueIV. Empirically, this demonstrates that\nour Meta-EM successfully reconstructs the GIV, and it converges to the TrueIV, i.e., source label.\nThen, to verify the effectiveness of GIV in non-linear\ncases, we design 4 different non-linear treatment functions\nfX(·) to evaluate the treatment effect estimation performance of Meta-EM algorithm. We select the SOTA IV-based\nmethod (AGMM) to evaluate GIV. We plot the estimated\nvalue of effect function with T=do(t) and sort it by GroundTruth (GT) for different synthetic scenarios. The results\n(Fig. 3) show GIVs (with Meta-KM or Meta-EM) achieve\nSOTA performance, especially GIVEM achieves comparable results with TrueIV and estimated outcome curves from\nGIVEM approximate the true curve. For the detailed results\nof non-linear cases, see the Supplementary material.\nThe Ablation Study for Reconstruction Accuracy of GIV\nTo demonstrate that Meta-EM can automatically find the\nproper group number and implement end-to-end train for\nIV generation, we plot MMD line for each group number\nin different synthetic settings (Linear-K-mX). As shown in\nFig. 4, Meta-EM always find the proper group number (redline) automatically, but Meta-KM fails to do it. Besides, as\nan ablation experiment, we compare the accuracy of MetaKM and Meta-EM for GIV reconstruction on data fusion\nwith different group numbers. As shown in Fig. 5, MetaEM algorithm successfully reconstructs the GIV, and the average reconstruction accuracy has reached 77% under various group numbers, especially exceeding 90% accuracy on\nTwo Groups setting. In contrast, the identification accuracy\nof Meta-KM is basically below 60%.\nMeta-EM algorithm uses a shared representation block\nto learn a nonlinear representation space to EM algorithm,\nwhich relaxes the underlying linear regression assumption.\nTo verify it, in the ablation experiments (Table 2), we compare the accuracy of EM, Meta-EM and Fun-EM, where\nFun-EM implements EM algorithm with known non-linear\nfunctions fX(X) (Eq. (14)). The results show that Meta-EM\nimproves the reconstruction accuracy by 6.3% than EM algorithm, bus still below the GT Fun-EM.\nFigure 4: MMD for Selection of Group Number with Different Synthetic Setting (Data-K-mX).\nIHDP Dataset\nPM-CMR Dataset\nPoly2SLS\nKernelIV\nDeepIV\nAGMM\nPoly2SLS\nKernelIV\nDeepIV\nAGMM\nNoneIV\n0.24(0.13)\n0.46(0.24)\n0.58(0.24)\n0.14(0.06)\n0.18(0.04)\n0.35(0.20)\n0.41(0.16)\n0.13(0.06)\nUAS\n0.24(0.133)\n0.46(0.24)\n0.57(0.24)\n0.14(0.06)\n0.18(0.04)\n0.35(0.20)\n0.40(0.16)\n0.13(0.06)\nWAS\n0.24(0.13)\n0.45(0.24)\n0.57(0.23)\n0.14(0.06)\n0.18(0.04)\n0.37(0.21)\n0.42(0.16)\n0.16(0.08)\nModeIV\n0.24(0.13)\n0.46(0.25)\n0.57(0.24)\n0.15(0.06)\n0.18(0.04)\n0.36(0.20)\n0.41(0.15)\n0.13(0.07)\nAutoIV\n> 100\n0.46(0.24)\n0.58(0.25)\n0.14(0.07)\n0.18(0.04)\n0.35(0.20)\n0.41(0.18)\n0.13(0.06)\nGIVKM\n0.05(0.03)\n0.35(0.18)\n0.50(0.20)\n0.11(0.05)\n0.09(0.04)\n0.33(0.20)\n0.38(0.16)\n0.12(0.05)\nGIVEM\n0.03(0.01)\n0.20(0.17)\n0.48(0.23)\n0.09(0.03)\n0.05(0.01)\n0.31(0.21)\n0.34(0.18)\n0.08(0.04)\nTrueIV\n0.03(0.01)\n0.15(0.06)\n0.46(0.17)\n0.09(0.03)\n0.03(0.01)\n0.14(0.07)\n0.14(0.05)\n0.05(0.02)\nTable 3: The Mean Squared Error mean(std) on IHDP & PM-CMR Dataset\nGIVKM (Acc: 60.50%)\nGIVEM (Acc: 93.17%)\nTrueIV(Acc: 100%)\nGIVKM (Acc: 43.00%)\nGIVEM (Acc: 86.60%)\nTrueIV(Acc: 100%)\nGIVKM (Acc: 29.13%)\nGIVEM (Acc: 77.57%)\nTrueIV(Acc: 100%)\n(a) Two Groups\n(b) Three Groups\n(c) Five Groups\nFigure 5: GIV Reconstruction Accuracy of Meta-EM.\nExperiments on Real-World Datasets\nReal-World Datasets\nSimilar to previous methods(Nie\net al. 2020; Hartford et al. 2017; Bica, Jordon, and van der\nSchaar 2020; Schwab et al. 2020), we perform experiments\non two real-world datasets IHDP4 (Shalit, Johansson, and\nSontag 2017) & PM-CMR5 (Wyatt et al. 2020), as the true\neffect function is rarely available for real-world data. Then\nwe use the continuous variables from IHDP & PM-CMR to\nreplace the covariates X in Eq. (14)&(16) to generate treatment T and outcome Y , respectively. Both two datasets are\nrandomly split into training (63%), validation (27%), and\ntesting (10%). We perform 10 replications to report the mean\nsquared error (MSE) and its standard deviations (std) of the\n4IHDP: https://www.fredjo.com/\n5PM-CMR:https://pasteur.epa.gov/uploads/10.23719/1506014/\nSES PM25 CMR data.zip\ntreatment effect function estimation. We select four SOTA\nIV-based methods to evaluate the performance of GIV.\nThe Results of Individual Treatment Effect Estimation\nBy estimating the latent differentiated covariate-treatment\ndistribution parameters across groups, Meta-EM reconstructs the latent IV and the reconstruction accuracy reaches\n93.47% and 82.62% on IHDP and PM-CMR, however, KMeans is only 64.29% and 46.09%. This demonstrates MetaEM can automatically find the optimal IV, but K-Means\ncannot. In Table 3, comparing the two optimal combinations (AutoIV in Poly2SLS & UAS in AGMM) in effect\nestimation in Table 3, Meta-EM further reduced the errors\nby 0.131(↓73%) and 0.043(↓33%), which well eliminated\nthe unmeasured confounding bias. Besides, GIVEM shows\nconsistent and robust performance, always maintaining the\nperformance of top-2 and almost achieving the same effect\nas TrueIV on IHDP & PM-CMR Datasets. Compared with\nGIVEM, the performance of GIVKM exceeds most baselines in downstream tasks, but it is still inferior to GIVEM\nand TrueIV. This means that GIVEM can reconstruct the latent group IV with the data distribution in the real scene and\nobtain asymptotically unbiased causal effect estimation.",
        "conclusion": "In this paper, by estimating the differentiated covariatetreatment distribution across groups, we propose a novel\nMeta-EM, a tool for reconstructing latent Group IVs and\npredicting treatment effect function from data fusion. To the\nbest of our knowledge, using representation learning to reconstruct Group Instrumental Variables by Meta-EM algorithm in data fusion is the first work for IV generation without expert knowledge. Theoretically and empirically, we address a vital problem in causal inference: how to learn valid\nIVs from observational data for ITE.",
        "summary_en": "The advent of the big data era brought new opportunities and challenges to draw treatment effect in data fusion, that is, a mixed dataset collected from multiple sources (each source with an independent treatment assignment mechanism). Due to possibly omitted source labels and unmeasured confounders, traditional methods cannot estimate individual treatment assignment probability and infer treatment effect effectively. Therefore, this paper proposes to reconstruct the source label and model it as a Group Instrumental Variable (GIV) to implement IV-based Regression for treatment effect estimation. The paper conceptualizes this line of thought and develop a unified framework (Meta-EM) to (1) map the raw data into a representation space to construct Linear Mixed Models for the assigned treatment variable; (2) estimate the distribution differences and model the GIV for the different treatment assignment mechanisms; and (3) adopt an alternating training strategy to iteratively optimize the representations and the joint distribution to model GIV for IV regression. Empirical results demonstrate the advantages of the Meta-EM compared with state-of-the-art methods.",
        "summary_zh": "这篇论文研究了在大数据时代，如何综合多个来源的数据集来估计治疗效果。由于数据可能存在遗漏的源标签和未测量的混杂因素，传统方法无法估计个体治疗分配概率并有效推断治疗效果。因此，作者提出了一种新思路，通过重新构建来源标签并将其建模为群体工具变量（GIV），实现了基于IV回归的治疗效果估计。他们开发了一个名为Meta-EM的统一框架，该框架通过映射原始数据、估计分布差异和采用交替训练策略来优化模型，实现了对治疗效果的有效估计。实验结果显示，Meta-EM相比于现有方法具有明显优势。"
    },
    {
        "title": "Treatment Effect Estimation with Data-Driven Variable Decomposition",
        "abstract": "One fundamental problem in causal inference is the treatment effect estimation in observational studies when variables are confounded. Control for confounding effect is generally handled by propensity score. But it treats all observed variables as confounders and ignores the adjustment variables, which have no inﬂuence on treatment but are predictive of the outcome. Recently, it has been demonstrated that the adjustment variables are effective in reducing the variance of estimated treatment effect. However, how to automatically separate the confounders and adjustment variables in observational studies is still an open problem, especially in the scenarios of high dimensional variables, which are common in big data era. In this paper, we propose a Data-Driven Variable Decomposition (D2VD) algorithm, which can 1) automatically separate confounders and adjustment variables with a data driven approach, and 2) simultaneously estimate treatment effect in observational studies with high dimensional variables. Under standard assumptions, we show experimentally that our D2VD algorithm can automatically separate the variables precisely, and estimate treatment effect more accurately and with tighter conﬁdence intervals than the state-of-the-art methods on both synthetic data and real online advertising dataset.",
        "introduction": "Causal inference, which refers to the process of drawing\na conclusion about a causal connection based on the conditions of the occurrence of an effect (Holland 1986), is\na powerful statistical modeling tool for explanatory analysis. The gold standard approaches for causal inference are\nrandomized experiments, for example, A/B testing (Lewis\nand Reiley 2009), where different treatments are randomly\nassigned to units 1. However, the fully randomized experiments are usually extremely expensive (Kohavi and Longbotham 2011) or sometimes even infeasible (Bottou et al.\n2013) in many scenarios. Hence it is highly demanding to\ndevelop automatic statistical approaches to infer treatment\neffect in observational studies.\nIn literature, (Rosenbaum and Rubin 1983) proposed a\nstatistical framework for treatment effect estimation based\nConfounders\nTreatment\nܶ\nOutcome\nܻ\nAdjustment\nVariables\nܼ\nܺ\nVariables\nܷ\nAutomated\nVariables\nDecomposition\nTreatment Effect\nEstimation\nFigure 1: Our causal diagram. We separate all observed variables U into three different sets: (1) Confounders X, which\nare associated with the treatment T and may be causally related to the outcome Y , (2) Adjustment Variables Z, which\nare causally related to outcome Y , but independent with\ntreatment T, and (3) Irrelevant Variables I (Omitted), which\nare independent with both treatment and outcome.\non propensity score adjustment. Such framework has been\nwidely used in observational causal study, including matching, stratiﬁcation, inverse weighting and regression on\npropensity score (Austin 2011; Lunceford and Davidian\n2004; Kuang et al. 2016). The inverse propensity weighting is a most commonly used method and has been part\nof a large family of causal models known as marginal\nstructural model (Hern´an, Brumback, and Robins 2000;\n2002). With combination of inverse propensity weighting\nand regression, (Bang and Robins 2005) proposed a doubly\nrobust estimator. These methods have been widely used in\nvarious ﬁelds, including economics (Stuart 2010), epidemiology (Funk et al. 2011), health care (Reis et al. 2015), social\nscience (Lechner 1999) and advertising (Sun et al. 2015).\nThe essence of these methods is to eliminate the confounding impact of confounders so that the precision of\ntreatment effect estimation can be signiﬁcantly improved.\nHowever, most of these works treat all observed variables\nas confounders when estimating propensity score. Eventually, in the scenarios of high dimensional variables, some of\nthem are not confounders but are predictive of the outcome,\nwhich are denoted by adjustment variables Z as shown in\nour causal diagram in Fig. 1. Ignoring the adjustment variables will make the estimated treatment effect imprecise and\nwith inﬂated variance.\nRecently, some researchers have investigated the importance of the adjustment variables. (Brookhart et al. 2006;\nVanderWeele and Shpitser 2011) have advocated that the adjustment variables should be included in the causal inference\nmodel. And (Sauer et al. 2013) suggested that conditioning\non such adjustment variables is unnecessary to remove bias\nbut can reduce variance in treatment effect estimation. In\nrandomized experiments setting, (Bloniarz et al. 2016) have\nproved that adjusting for the adjustment variables by lasso\ncan reduce the variance of estimated treatment effect.\nAll these methods in observational studies assume that the\ncausal structure, i.e. whether a variable is the cause of the\ntreatment or outcome, is known a priori. However, the causal\nstructure cannot be well deﬁned by prior knowledge in most\ncases, especially in the scenarios of high dimensional variables in the big data era. How to automatically separate confounders and adjustment variables in observational studies\nis still an open problem.\nTo address this problem, we propose a Data-Driven Variable Decomposition (D2VD) algorithm to jointly optimize\nconfounders separation and Average Treatment Effect (ATE)\nestimation. More speciﬁcally, we propose a regularized integrated regression model, where a combined orthogonality and sparsity regularizer is constructed to simultaneously\n1) separate the confounders and adjustment variables with a\ndata driven approach, 2) eliminate irrelevant variables which\nare neither confounders nor adjustment variables to avoid\noverﬁtting, and 3) estimate the ATE in observational studies.\nDuring estimating the ATE, the separated confounders can\neffectively eliminate their confounding impact on treatment,\nwhile the adjustment variables can signiﬁcantly reduce the\nvariances of estimated ATE through outcome adjustment.\nThis enables us to estimate the true ATE more accurately\nand with tighter conﬁdence intervals than baseline methods.\nThe main contributions in this paper are as follows:\n• We study a new problem of automatically separating confounders and adjustment variables, which is critical for\nthe precision and conﬁdence intervals of ATE estimation\nin observational studies.\n• We propose a novel data-driven variables decomposition algorithm, where a regularized integrated regression\nmodel is presented to enable confounder separation and\nATE estimation simultaneously.\n• The advantages of D2VD algorithm are demonstrated in\nboth synthetic and real world data. It can also be straightforwardly applied into other causal inference studies, such\nas social marketing, health care and public policy.",
        "adjusted ate estimator": "In this section, we ﬁrst give the notations and assumptions\nfor the ATE estimation in observational studies, then propose a new adjusted ATE estimator by utilizing the adjustment variables for reducing the variance of estimated ATE.\nNotations and Assumptions\nAs described in our causal diagram in Fig.1, we deﬁne a\ntreatment as a random variable T and a potential outcome as\nY (t) which corresponds to a speciﬁc treatment T = t. In this\npaper, we only consider binary treatment, that is t ∈ {0, 1}.\nWe deﬁne the units which received the treatment, that is T =\n1, as treated units and the others with T = 0 as control units.\nThen for each unit indexed by i = 1, 2, · · · , m, we observe\na treatment Ti, an outcome Y obs\ni\nand a vector of variables\nUi. Our observed outcome Y obs\ni\nof unit i can be denoted by:\nY obs\ni\n= Yi(Ti) = Ti · Yi(1) + (1 − Ti) · Yi(0),\n(1)\nIn observational studies, there are three standard assumptions (Rosenbaum and Rubin 1983) for ATE estimation.\nAssumption 1: Stable Unit Treatment Value. The distribution of potential outcome for one unit is assumed to be\nunaffected by the particular treatment assignment of another\nunit, when given the observed variables.\nAssumption 2: Unconfoundedness. The distribution of\ntreatment is independent of potential outcome when given\nthe observed variables. Formally, T⊥\n\u0002\nY (0), Y (1)\n\u0003\n|U.\nAssumption 3: Overlap. Every unit has a nonzero probability to receive either treatment status when given the observed variables. Formally, 0 < p(T = 1|U) < 1.\nAdjusted ATE Estimator\nThe important goal of causal inference in observational studies is to evaluate the ATE on outcome Y . The ATE represents the mean (average) difference between the potential\noutcome of units under treated and control status. Formally,\nthe ATE is deﬁned as:\nATE = E\n\u0004\nY (1) − Y (0)\n\u0005\n,\n(2)\nwhere Y (1) and Y (0) represent the potential outcome of\nunit with treatment status as treated T = 1 and control\nT = 0, respectively. E(·) refers to the expectation function.\nThe Eq. (2) is infeasible, because for each unit, we can\nonly observe one potential outcome corresponding to its\ntreatment status, treated or control. This is called “the counterfactual problem” (Chan et al. 2010).\nOne can address this counterfactual problem by approximating the unobserved potential outcome. The simplest approach is to directly compare the average outcome between\nthe treated and control units. However, in observational studies, comparing two samples directly is likely to have bias if\nthe treatment assignment is not random, as confounding impact is not taken into account (Chan et al. 2010).\nTo unbiasedly evaluate the ATE in observational studies,\none have to control the impact of confounders. Under the assumptions (1,2,3), (Rosenbaum and Rubin 1983) introduced\nthe propensity score to summarize the information required\nto control the confounders. The propensity score, denoted\nby e(U), was deﬁned as the probability with treated status\n(T = 1) of a unit given all variables U. Actually, only confounders X are associated with the treatment, therefore\ne(U) = p(T = 1|U) = p(T = 1|X) = e(X).\n(3)\nBased on the propensity score, (Rosenbaum 1987) proposed\nthe transformed outcome Y ⋆ to address the counterfactual\nproblem in Eq. (2) with Inverse Propensity Weighting (IPW)\nestimator \u0002\nATEIP W , see also (Athey and Imbens 2016). The\ntransformed outcome Y ⋆ is deﬁned as\nY ⋆ = Y obs ·\nT −e(U)\ne(U)·(1−e(U)) = Y obs ·\nT −e(X)\ne(X)·(1−e(X)),\n(4)\nand the IPW estimator is deﬁned as\n\u0002\nATEIP W = \u0006E(Y ⋆) = \u0006E\n\u0007\nY obs ·\nT −e(X)\ne(X)·(1−e(X))\n\b\n.\n(5)\nHowever, most previous approaches based on propensity\nscore usually treat all observed variables as confounders\nwhen estimating the propensity score. This will make the\nestimated treatment effect imprecise and with inﬂated variance because some variables could be non-confounders and\nhave direct impact on outcome.\nTherefore, based on our causal diagram as shown in\nFig. 1, we propose to separate all observed variables U into\nthree sets, the confounders X, the adjustment variables Z and\nirrelevant variables I (Omitted in Fig.1). And then, we propose a new adjusted estimator by incorporating adjustment\nvariables to reduce the variance of estimated ATE under following assumption.\nAssumption 4: Separateness. The observed variables U\ncan be decomposed into three sets, that is U = (X, Z, I),\nwhere X are confounders, Z are adjustment variables and I\nare irrelevant variables.\nWith assumption 4, we introduce our adjusted transformed outcome Y + based on Y ⋆ with the deﬁnition as\nY + =\n\u0002\nY obs − φ(Z)\n\u0003\n·\nT −e(X)\ne(X)·(1−e(X)),\n(6)\nwhere φ(Z) helps to reduce the variance among Y , which\nare associated with Z.\nThen we propose the adjusted estimator \u0002\nATEadj as\n\u0002\nATEadj = \u0006E(Y +) = \u0006E\n\u0007\u0002\nY obs − φ(Z)\n\u0003\n·\nT −e(X)\ne(X)·(1−e(X))\n\b\n.\n(7)\nAnd our adjusted estimator has following properties.\n• Firstly, under assumptions 1-4, our adjusted estimator\n\u0002\nATEadj is unbiased, that is\nE(Y +|X) = E(Y (1) − Y (0)|X).\n(8)\nThis property is obvious with the Pearl’s back-door criterion (Pearl 2009). Since the conditioning set X blocks all\nback door paths linking treatment T and outcome Y , while\nnot contains any descendants of T in our causal diagram.\n• Secondly, the asymptotic variance of our adjusted estimator \u0002\nATEadj is no greater than IPW estimator \u0002\nATEIP W.\nComparing the Eq. (5) and (7), we know that the IPW estimator only considered the confounders X when provided\nall variables U, while our estimator utilized the adjustment\nvariables Z to make adjustments on outcome Y for reducing variance. The similar adjustments have been proved can\nreduce the variance of ATE estimation in randomized experiments by (Bloniarz et al. 2016).",
        "automated variables decomposition": "D2VD Algorithm\nWith Eq. (8), we can get E(Y +) = E(Y (1) − Y (0)), and\nobtain the estimated ATE by regressing our adjusted transformed outcome Y + against the variables U and minimizing\nthe following objective function\nminimize\n∥Y + − h(U)∥2.\n(9)\nThen we can estimate the ATE by \u0006E(h(U)).\nIn practice, we specify φ(Z) and h(U) as linear functions\nwith coefﬁcient vector α and γ, that is\nφ(Z) = Zα,\nh(U) = Uγ,\n(10)\nand adopt linear-logistic regression to evaluate the propensity score e(X) with coefﬁcient vector β:\ne(X) = p(T = 1|X) = 1/(1 + exp(−Xβ)).\n(11)\nIn the speciﬁcations of Eq. (10, 11), we have assumed the\nknowledge of the variables decomposition U = (X, Z, I).\nNevertheless, we don’t know the exact separation in practice. Hence we use the full set of observed variables U to replace X and Z instead, and propose a data-driven approach\nto automatically separate confounders and adjustment variables. We update our objective function in Eq. (9) as:\nminimize\n∥(Y obs − Uα) ⊙ W(β) − Uγ∥2\n2,\n(12)\ns.t.\nm\n\u0002\ni=1\nlog(1 + exp((1 − 2Ti) · Uiβ)) < τ,\n∥α∥1 ≤ λ, ∥β∥1 ≤ δ, ∥γ∥1 ≤ η, ∥α ⊙ β∥2\n2 = 0.\nwhere W(β) :=\nT −e(U)\ne(U)·(1−e(U)) and \tm\ni=1 log(1 + exp((1 −\n2Ti) · Uiβ)) represents the loss function when estimating\nthe propensity score. ⊙ refers to Hadamard product. With\nthe formula ∥α ⊙ β∥2\n2 = 0, the coefﬁcient vector α is optimized for separating the adjustment variables Z and β is for\nseparating confounders X form variables U.\nIn particular, we employ an orthogonal regularizer on α\nand β to ensure the separation of confounders and adjustment variables. In addition, we add L1 penalties on α, β and\nγ to eliminate irrelevant variables I to further reduce variance and address the sparseness problem of variables.\nThese lead to the following optimization problem, which\nis to minimize J (α, β, γ).\nJ (α, β, γ)\n=\nf(α, β, γ) + g(α, β, γ),\n(13)\nf(α, β, γ)\n=\n∥(Y obs − Uα) ⊙ W(β) − Uγ∥2\n2 + μ∥α ⊙ β∥2\n2\n+ τ \u0003m\ni=1 log(1 + exp((1 − 2Ti) · Uiβ)),\ng(α, β, γ)\n=\nλ∥α∥1 + δ∥β∥1 + η∥γ∥1.\nWith the operator splitting property of proximal gradient algorithm (Parikh and Boyd 2013), we can get the optimized\nparameter (i.e., α(t+1)) at the tth iteration by proximal operator proxκ(t)g of function g(·) with the step size κ(t):\nα(t+1) = proxκ(t)g\n\u0007\nα(t) − κ(t) ∂f(·)\n∂α\n\b\n(14)\nwhere ∂f(·)\n∂α\nrefers to the gradient of function f(·) on the\nvariable α and\nproxκ(t)g(x) =\n⎧\n⎨\n⎩\nxi − κ(t) · λ\nxi ≥ κ(t) · λ\n0\n|xi| ≤ κ(t) · λ\nxi + κ · λ\nxi ≤ −κ(t) · λ\n(15)\nThe λ in Eq. (15) is the coefﬁcient of parameter α in function g(·). If the optimized parameter is β, then it should be δ\nand it should be η for optimizing parameter γ.\nAlgorithm 1 Data-Driven Variable Decomposition (D2VD)\nRequire: Initialization J (0) = J (α(0), β(0), γ(0)).\nEnsure: J (0) ≥ 0, J (t+1) < J (t)\n1: for t = 0, 1, 2, · · · do\n2:\nCalculate ∂f(·)\n∂α , ∂f(·)\n∂β\nand ∂f(·)\n∂γ\n3:\nα(t+1) = OPTIMIZATION(α, t)\n4:\nβ(t+1) = OPTIMIZATION(β, t)\n5:\nγ(t+1) = OPTIMIZATION(γ, t)\n6:\nJ (t+1) = J (α(t+1), β(t+1), γ(t+1))\n7: end for\nWith the proximal gradient algorithm, we can minimize\nthe objective function in Eq. (13). That is, starting from\nsome random initialization on α, β, γ, we solve each them\nalternatively with the other two parameters as ﬁxed and step\nby step until convergence. Our Data-Driven Variable Decomposition algorithm is described in Algorithm 1.\nDuring each iteration in Algorithm 1, we update the parameters α, β and γ with OPTIMIZATION as described in\nAlgorithm 2, where the function ˆfκ(·) is deﬁned as:\nˆfκ(x, y) = f(y) + (x − y) ∂f(·)\n∂x\nT + (1/(2κ))∥x − y∥2\n2.\n(16)\nAnd the gradients of the function f(α, β, γ) with the respect\nto the variables (α, β, γ) are:\n∂f(·)\n∂α\n=\n−2(W(β) · 1T ⊙ U)T · R + 2μα ⊙ β ⊙ β,\n∂f(·)\n∂β\n=\n2\n\u0007\n(Y − Uα) · 1T ⊙ ∂W (β)\n∂β\n\bT\n· R\n+\nUT (T − exp(Uβ)) + 2μα ⊙ β ⊙ α,\n∂f(·)\n∂γ\n=\n−2UT · R.\nwhere\nR =\n\n\n(Y − Uα) ⊙ W(β) − Uγ\n\u000b\nand\n∂W (β)\n∂β\n=\n(2T − 1) ⊙ exp\n\n\n(1 − 2T) ⊙ Uβ\n\u000b\n⊙ (1 − 2T) · 1T ⊙ U.\nWith the optimized parameters ˆα, ˆβ and ˆγ by Algorithm 1,\nwe can separate the confounders as X = {Ui : ˆβi ̸= 0},\nadjustment variables as Z = {Ui : ˆαi ̸= 0} and estimate the\nATE as \u0002\nATED2V D = E(Uˆγ).\nComplexity Analysis\nThe complexity of our D2VD algorithm is dominated by the\nstep of calculating the gradients of function f(α, β, γ) with\nrespect to the variables. The complexity of ∂f(·)\n∂α , ∂f(·)\n∂β\nand\n∂f(·)\n∂γ\nare all O(mn), where m is the sample size and n is\nthe dimension of all observed variables. With considering\nthat only constant time operations is involved in the for-loop\nand while-loop in our algorithms, therefore, the complexity\nof our D2VD algorithm is O(mn).\nParameters Tuning\nThe main challenge of parameters tuning for ATE estimation\nin observational studies is that there is no ground truth about\nthe true ATE in practice.\nAlgorithm 2 OPTIMIZATION(o, t)\n1: Set κ = 1\n2: while 1 do\n3:\nLet o(t+1) = proxκg\n\u0007\no(t) − κ ∂f(·)\n∂o\n\b\n4:\nbreak if f(o(t+1)) ≤ ˆfκ(o(t+1), o(t))\n5:\nUpdate κ = 1\n2κ\n6: end while\n7: return ot+1\nTo address this challenge, we employed the matching\nmethod to evaluate the ATE and set it as “approximal ground\ntruth” like (Athey and Imbens 2016) did. Speciﬁcally, for\neach unit i, ﬁnd its closest match among the units with opposite treatment status:\nmatch(i) = arg minj:Tj=1−Ti ∥Ui − Uj∥2\n2.\n(17)\nWe drop unit i if match(i) > ϵ, that makes the matching\napproximate to exactly matching. We can estimate ATE with\nthe matching estimator by comparing the average outcome\nbetween the matched treated and control units sets, and set\nit as “approximal ground truth”, denoted by ATEmatching.\nWith the “approximal ground truth”, we can tune parameters of our algorithm with cross validation.",
        "experiments": "We apply our algorithm on the synthetic dataset and real online advertising dataset to estimate the ATE.\nBaseline Estimators\nWe implement the following baseline estimators to evaluate\nthe ATE for comparison.\n• Direct Estimator \u0002\nATEdir: It evaluates the ATE by directly\ncomparing the average outcome between the treated and\ncontrol units. It ignores the confounding effect of confounders on treatment.\n• IPW Estimator \u0002\nATEIP W (Rosenbaum and Rubin 1983):\nIt evaluates the ATE via reweighting observations with inverse of propensity score. It treats all variables as confounders and ignores the adjustment variables.\n• Doubly Robust Estimator\n\u0002\nATEDR (Bang and Robins\n2005): It evaluates the ATE by combination of IPW and\nregression methods. It ignores the separation of confounders and adjustment variables.\n• Non-Separation Estimator \u0002\nATED2V D(−): It’s a weakened\nversion of our D2V D estimator. It has no variables separation step by setting coefﬁcient μ = 0 in Eq. (13).\nIn this paper, we implemented \u0002\nATEIP W and \u0002\nATEDR with\nlasso regression for variables selection. The difference between \u0002\nATEDR and \u0002\nATED2V D(−) is that the former estimates\nATE sequentially but the latter does with joint optimization.\nExperiments on Synthetic Data\nDataset\nTo generate the synthetic dataset, we set the sample size m = {1000, 5000} and the dimension of observed\nvariables n = {50, 100, 200}. We ﬁrst generate the variables\nTable 1: Results on synthetic dataset: the true ATE is 1. The Bias refers to the absolute error between the true and estimated\nATE, that is Bias = | \u0002\nATE − ATE|. SD, MAE and RMSE represent the standard deviations, mean absolute errors and root\nmean square errors of \u0002\nATE after 50 times independently experiments, respectively.\nn\nn = 50\nn = 100\nn = 200\nT/m\nEstimator\nBias\nSD\nMAE\nRMSE\nBias\nSD\nMAE\nRMSE\nBias\nSD\nMAE\nRMSE\n\u0002\nATEdir\n0.418\n0.409\n0.479\n0.582\n0.302\n0.490\n0.472\n0.571\n0.405\n0.628\n0.574\n0.720\n\u0002\nATEIP W + lasso\n0.078\n0.310\n0.252\n0.317\n0.097\n0.356\n0.295\n0.366\n0.073\n0.328\n0.267\n0.320\nT = Tlogit\n\u0002\nATEDR + lasso\n0.060\n0.181\n0.152\n0.189\n0.067\n0.190\n0.155\n0.199\n0.081\n0.181\n0.169\n0.190\nm = 1000\n\u0002\nATED2V D(−)\n0.053\n0.138\n0.124\n0.146\n0.064\n0.130\n0.117\n0.144\n0.018\n0.170\n0.128\n0.162\n\u0002\nATED2V D\n0.045\n0.108\n0.091\n0.116\n0.019\n0.114\n0.093\n0.115\n0.067\n0.144\n0.130\n0.152\n\u0002\nATEdir\n0.418\n0.170\n0.418\n0.451\n0.659\n0.181\n0.659\n0.681\n0.523\n0.412\n0.555\n0.653\n\u0002\nATEIP W + lasso\n0.036\n0.201\n0.163\n0.202\n0.034\n0.222\n0.194\n0.213\n0.032\n0.341\n0.274\n0.325\nT = Tlogit\n\u0002\nATEDR + lasso\n0.051\n0.079\n0.071\n0.094\n0.106\n0.075\n0.114\n0.127\n0.055\n0.084\n0.086\n0.096\nm = 5000\n\u0002\nATED2V D(−)\n0.112\n0.080\n0.118\n0.137\n0.114\n0.102\n0.121\n0.150\n0.164\n0.076\n0.164\n0.179\n\u0002\nATED2V D\n0.033\n0.072\n0.061\n0.078\n0.023\n0.073\n0.061\n0.073\n0.042\n0.068\n0.062\n0.076\n\u0002\nATEdir\n0.664\n0.387\n0.670\n0.766\n0.273\n0.445\n0.436\n0.518\n0.380\n0.766\n0.691\n0.848\n\u0002\nATEIP W + lasso\n0.266\n0.279\n0.319\n0.384\n0.298\n0.295\n0.328\n0.417\n0.191\n0.482\n0.403\n0.514\nT = Tmissp\n\u0002\nATEDR + lasso\n0.138\n0.187\n0.174\n0.231\n0.253\n0.197\n0.269\n0.320\n0.050\n0.218\n0.170\n0.222\nm = 1000\n\u0002\nATED2V D(−)\n0.269\n0.162\n0.270\n0.313\n0.129\n0.162\n0.170\n0.206\n0.175\n0.207\n0.236\n0.269\n\u0002\nATED2V D\n0.066\n0.113\n0.102\n0.129\n0.019\n0.119\n0.101\n0.120\n0.059\n0.177\n0.149\n0.184\n\u0002\nATEdir\n0.446\n0.180\n0.446\n0.480\n0.587\n0.323\n0.587\n0.662\n0.778\n0.246\n0.778\n0.812\n\u0002\nATEIP W + lasso\n0.148\n0.133\n0.161\n0.198\n0.172\n0.167\n0.199\n0.239\n0.142\n0.224\n0.206\n0.263\nT = Tmissp\n\u0002\nATEDR + lasso\n0.119\n0.073\n0.123\n0.139\n0.100\n0.067\n0.107\n0.120\n0.127\n0.079\n0.127\n0.148\nm = 5000\n\u0002\nATED2V D(−)\n0.112\n0.070\n0.119\n0.132\n0.058\n0.067\n0.069\n0.086\n0.068\n0.055\n0.073\n0.086\n\u0002\nATED2V D\n0.033\n0.055\n0.052\n0.063\n0.039\n0.068\n0.066\n0.075\n0.032\n0.047\n0.049\n0.055\nU = (X, Z, I) = (x1, · · · , xnx, z1, · · · , znz, i1, · · · , ini)\nwith independent gaussian distributions as\nx1, · · · , xnx, z1, · · · , znz, i1, · · · , ini\niid\n∼\nN(0, 1),\nwhere nx, nz and ni represent the dimension of confounders\nX, adjustment variables Z and irrelevant variables I, respectively. And nx = 0.2 ∗ n, nz = 0.2 ∗ n, ni = 0.6 ∗ n.\nTo test the robustness of all estimators, we generate the\nbinary treatment variable T from a logistic function (Tlogit)\nand a misspeciﬁed function (Tmissp) as\nTlogit ∼ Bernoulli(1/(1 + exp(− \tnx\ni=1 xi))) and\nTmissp = 1 if \tnx\ni=1 xi > 0.5, Tmissp = 0 otherwise.\nThe outcome Y is generated as\nY = \tnx\nj= nx\n2 xj · ωj + \tnz\nj=1 zk · ρk + T + N(0, 2),\nIn this dataset, the features (x nx\n2 , x nx\n2 +1, · · · , xnx) are correlated to the treatment and outcome, simulating a confounding effect. The true treatment effect in this dataset is 1.\nATE Estimation\nTo evaluate the performance of our proposed method, we carry out the experiments 50 times independently. Based on our estimated ATE, we calculate the\nBias, SD, MAE and RMSE, and report the results in Tab.1,\nwhere the smaller Bias, SD, MAE and RMSE are better.\nFrom Tab.1, we have following observations.\nFirst, the direct estimator is failed (with large Bias) under different settings because it did not consider the confounding effect. Second, the IPW estimator can unbiasedly\n(with small Bias) estimate the ATE when T = Tlogit, but\nwith a big Bias when propensity score model is misspeciﬁed by setting T = Tmissp. With combination of IPW and\nregression models, DR estimator can get better performance\nthan IPW estimator, especially when T = Tmissp. Third,\nour D2V D(−) estimator, which has no variables separation\nstep, can get the similar results with DR estimator. But with\nconsidering the separation between confounders and adjustment variables, our D2V D estimator can improve the accuracy (smaller Bias) and reduce the variance (smaller SD)\nfor ATE estimation from D2V D(−), DR and other baseline\nestimators under different settings.\nVariables Decomposition\nAs we described before, with\nthe optimized ˆα and ˆβ, our algorithm can separate the confounders as X = {Ui : ˆβi ̸= 0} and adjustment variables\nas Z = {Ui : ˆαi ̸= 0}. To demonstrate the performance\nof automated variables decomposition of our algorithm, we\ncarry out the experiments 50 times independently and record\nthe true positive rate (TPR) and true negative rate (TNR) in\nTab. 3. The formulations of TPR and TNR for separated confounders X are deﬁned as\nTPR = #{ ˆβi̸=0,βi̸=0}\n#{βi̸=0}\n, TNR = #{ ˆβi=0,βi=0}\n#{βi=0}\n.\n(18)\nIn the same way, we calculate the TPR and TNR for separated adjustment variables Z via Eq. (18) by using α.\nTab. 3 shows that our D2VD algorithm can separate the\nconfounders X more precisely when T = Tlogit, comparing\nwith T = Tmissp. This is because of the logistic assumption\nof treatment assignment in our algorithm is correct. Even if\nsetting T = Tmissp, our algorithm can still precisely separate the confounders and adjustment variables. This enables\nus to estimate the ATE more accurately and with tighter conﬁdence intervals than the state-of-the-art methods.\nTable 2: The top ranked features by their absolute ATE estimated with our D2VD estimator \u0002\nATED2V D, comparing with the\nbaseline estimator \u0002\nATEIP W and \u0002\nATEDR. The ATEmatching is the “approximal ground truth” by matching method, “n/a”\nmeans that we cannot obtain the ATE from matching method since the number of matching samples are not sufﬁcient.\nNo.\nFeatures\n\u0002\nATED2V D (SD)\n\u0002\nATEIP W (SD)\n\u0002\nATEDR (SD)\nATEmatching\n1\nNo. friends (> 166)\n0.295 (0.018)\n0.240 (0.026)\n0.297(0.021)\n0.276\n2\nAge (> 33)\n-0.284 (0.014)\n-0.235 (0.029)\n-0.302(0.068)\n-0.263\n3\nShare Album to Strangers\n0.229 (0.030)\n0.236 (0.030)\n-0.034(0.021)\nn/a\n4\nWith Online Payment\n0.226 (0.019)\n0.260 (0.029)\n0.244(0.028)\nn/a\n5\nWith High-Deﬁnition Head Portrait\n0.218 (0.028)\n0.203 (0.032)\n0.237(0.046)\nn/a\n6\nWith WeChat Album\n0.191 (0.014)\n0.237 (0.021)\n0.097(0.050)\nn/a\n7\nWith Delicacy Plugin\n0.124 (0.038)\n-0.253 (0.037)\n0.067(0.051)\n0.099\n8\nDevice (iOS)\n0.100 (0.024)\n0.206 (0.012)\n0.060(0.021)\n0.085\n9\nAdd friends by Drift Bottle\n-0.098 (0.012)\n0.016 (0.019)\n-0.115(0.015)\n-0.032\n10\nGender (Male)\n-0.073 (0.017)\n-0.240 (0.029)\n0.065(0.055)\n-0.097\nTable 3: Separation results of confounders X and adjustment\nvariables Z. The closer to 1 for TPR and TNR is better.\nT = Tlogit\nn = 50\nn = 100\nn = 200\nm\nTPR\nTNR\nTPR\nTNR\nTPR\nTNR\nm = 1000\nX\n1.000\n0.917\n0.977\n0.948\n0.966\n0.906\nZ\n1.000\n0.973\n1.000\n0.983\n1.000\n0.984\nm = 5000\nX\n1.000\n0.923\n1.000\n0.887\n0.994\n0.989\nZ\n1.000\n0.975\n1.000\n0.987\n1.000\n0.994\nT = Tmissp\nm = 1000\nX\n1.000\n0.844\n0.997\n0.866\n0.867\n0.977\nZ\n1.000\n0.982\n1.000\n0.987\n1.000\n0.983\nm = 5000\nX\n1.000\n0.843\n1.000\n0.837\n0.998\n0.965\nZ\n1.000\n0.986\n1.000\n0.990\n1.000\n0.994\nExperiments on Real World data\nDataset\nThe real online advertising dataset we used is collected during Sep. 2015 from Tencent WeChat App2. In\nWeChat, each user can share posts to his/her friends and receive posts from friends as like in the Twitter and Facebook.\nThe advertisers can push advertisements to users, by merging them into list of the user’s wallposts. There are two types\nof feedback on the advertisements: “Like” and “Dislike”.\nThe online advertising campaign is about LONGCHAMP\nhandbags for young ladies3. This campaign contains 14,891\nuser feedbacks with Like and 93,108 Dislikes. For each user,\nwe have 56 features including (1) demographic attributes,\nsuch as age, gender, (2) number of friends, (3) device (iOS\nor Android), and (4) the user settings on WeChat, for example, whether allowing strangers to see his/her album (“Share\nAlbum to Strangers”) and whether installing the online payment service (“With Online Payment”).\nExperimental Settings\nIn our experiments, we set the\nfeedback of users about the advertisement as outcome Y .\nSpeciﬁcally, we set the outcome Yi = 1 when the user i likes\nthe advertisement, and Yi = 0 if user i dislikes it. And we\nalternatively set one of the features as the treatment T and\nall other features as the variables U. So that we can evaluate\nthe ATE of each feature.\nDuring the parameters tuning, we set the matching thresh2http://www.wechat.com/en/\n3http://en.longchamp.com/en/womens-bags\nold ϵ = 5, which make the matching estimator is close to the\nexactly matching. The hyper-parameters of λ, δ, τ, η and μ\nset as 30, 50, 90, 70 and 30 by using grid search.\nATE Estimation\nFor each user feature, we employ our\nD2VD algorithm to estimate its ATE on the outcome.\nTab. 2 shows the top ranked features by their absolute\nATE estimated with our D2VD estimator, comparing with\nbaseline estimators and the “approximal ground truth”\nATEmatching. Note that the ATEmatching has very rigorous requirements on the sample size with exactly matching.\nFor some user features, we do not have a sufﬁcient number\nof samples thus we cannot derive their ATEmatching.\nFrom Tab. 2, we have following observations.\nO1. Our D2VD estimator evaluate the ATE more accurately than baseline estimators. With separated confounders,\nthe ATE estimated by our D2VD estimator is closer to the\n“approximate ground truth” ATEmatching. While the IPW\nand DR estimators, which treat all variables as confounders,\ngenerate huge error in estimating ATE for some features,\neven make wrong estimation of the ATE polarity (positive\nof negative), such as feature WithDelicacyPlugin for IPW\nestimator and feature Gender for DR estimator.\nO2. Our D2VD estimator can reduce the variance of estimated ATE from baseline estimators. With regression on\nseparated adjustment variables, our estimator obtain smaller\nSD than IPW and DR estimators, where IPW estimator ignores the adjustment variables and DR estimator makes regression on all variables, ignoring the variables separation.\nO3. Younger ladies are with higher probability to like\nthe advertisement about LONGCHAMP handbags. The ATE\nof Age(> 33) is −0.284 and Gender(Male) is −0.073,\nwhich indicate that the younger ladies have higher probability to like the advertisement. This is consistent with our\nintuition since the LONGCHAMP advertisement is mainly\ndesigned for young ladies as their potential customers.\nVariables Decomposition\nTab. 4 shows the separation results between confounders and adjusted variables when we\nset feature “Add friends by Shake” as the treatment. Shake4\nis a two way function where both people using this function at the same time can see each other and make friends\n4https://rumorscity.com/2014/07/25/how-to-add-friends-onwechat-7-ways/\nTable 4: Confounders and adjusted variables when we set\nfeature “Add friends by Shake” as treatment.\nConfounders\nAdjustment Variables\nAdd friends by Drift Bottle\nNo. friends\nAdd friends by People Nearby\nAge\nAdd friends by QQ Contacts\nWith WeChat Album\nWithout Friends Conﬁrmation Plugin\nDevice\non WeChat. In Tab. 4, the confounders are many other\nways for adding friends on WeChat, indicating the separated\nconfounders have signiﬁcant causal association with treatment. While the adjustment variables, for example, the “No.\nfriends” and “Age”, are not associated with treatment but\nhave signiﬁcant effect on outcome, as shown in Tab. 2, they\nare the top ranked features.\nThe results demonstrate that our proposed D2V D algorithm can precisely separate the confounders and adjustment\nvariables in practical. With the separated confounders, our\nestimator can obtain an accurate ATE, and reduce the variance of estimated ATE by the adjustment variables.",
        "conclusion": "In this paper, we focus on how to evaluate the average\ntreatment effect in a more precisely way with tighter conﬁdence intervals in observational studies. We argued that\nmost previous causal methods based on propensity score\nis deﬁcient because they usually treat all variables as confounders. Based on our causal diagram, we proposed to separate the confounders and adjustment variables from all observed variables. And we proposed a Data-Driven Variable\nDecomposition (D2VD) algorithm to jointly optimize the\nvariables decomposition and ATE estimation. Experimental\nresults on synthetic data and real world data verify the practical usefulness of our model and the effectiveness of our\nD2VD algorithm for ATE estimation in observational study.",
        "summary_en": "One fundamental problem in causal inference is the treatment effect estimation in observational studies when variables are confounded. Control for confounding effect is generally handled by propensity score. But it treats all observed variables as confounders and ignores the adjustment variables, which have no influence on treatment but are predictive of the outcome. Recently, it has been demonstrated that the adjustment variables are effective in reducing the variance of the estimated treatment effect. However, how to automatically separate the confounders and adjustment variables in observational studies is still an open problem, especially in the scenarios of high dimensional variables, which are common in big data era. This paper proposes a Data-Driven Variable Decomposition (D$^2$VD) algorithm, which can 1) automatically separate confounders and adjustment variables with a data driven approach, and 2) simultaneously estimate treatment effect in observational studies with high dimensional variables. Under standard assumptions, the paper shows experimentally that the proposed D$^2$VD algorithm can automatically separate the variables precisely, and estimate treatment effect more accurately and with tighter confidence intervals than the state-of-the-art methods on both synthetic data and real online advertising dataset.",
        "summary_zh": "这篇论文介绍了一种数据驱动变量分解算法，用于在观察性研究中自动分离混杂因素和调整变量，并同时估计治疗效果。该方法可以：（1）用数据驱动的方法自动分离混杂因素和调整变量；（2）在具有高维变量的观察性研究中同时估计治疗效果。在标准假设条件下，本文通过实验证明，该算法可以自动精确地分离变量，并在合成数据和真实在线广告数据集上比最先进的方法更准确地估计治疗效果，且置信区间更小。"
    },
    {
        "title": "Treatment Effect Estimation with Disentangled Latent Factors",
        "abstract": "Much research has been devoted to the problem of estimating treatment effects from observational data; however, most methods assume that the observed variables only contain confounders, i.e., variables that affect both the treatment and the outcome. Unfortunately, this assumption is frequently violated in real-world applications, since some variables only affect the treatment but not the outcome, and vice versa. Moreover, in many cases only the proxy variables of the underlying confounding factors can be observed. In this work, we ﬁrst show the importance of differentiating confounding factors from instrumental and risk factors for both average and conditional average treatment effect estimation, and then we propose a variational inference approach to simultaneously infer latent factors from the observed variables, disentangle the factors into three disjoint sets corresponding to the instrumental, confounding, and risk factors, and use the disentangled factors for treatment effect estimation. Experimental results demonstrate the effectiveness of the proposed method on a wide range of synthetic, benchmark, and real-world datasets.",
        "introduction": "Estimating the effect of a treatment on an outcome is a fundamental problem faced by many researchers and has a wide\nrange of applications across diverse disciplines. In social\neconomy, policy makers need to determine whether a job\ntraining program will improve the employment perspective\nof the workers (Athey and Imbens 2016). In online advertisement, companies need to predict whether an advertisement campaign could persuade a potential buyer into buying\nthe product (Rzepakowski and Jaroszewicz 2011).\nTo estimate treatment effect from observational data, the\ntreatment assignment mechanism needs to be independent\nof the possible outcomes when conditioned on the observed\nvariables, i.e., the unconfoundedness assumption (Rosenbaum and Rubin 1983) needs to be satisﬁed. With this assumption, treatment effects can be estimated from observational data by adjusting on the confounding variables which\naffects both the treatment assignment and the outcome. The\ntreatment effect estimation may be biased if not all the confounders are considered in the estimation (Pearl 2009).\nFrom a theoretical perspective, practitioners are tempted\nto include as many variables as possible to ensure the satisfaction of the unconfoundedness assumption. This is because confounders can be difﬁcult to measure in the realworld and practitioners need to include noisy proxy variables to ensure unconfoundedness. For example, the socioeconomic status of patients confounds treatment and prognosis, but cannot be included in the electronic medical\nrecords due to privacy concerns. It is often the case that such\nunmeasured confounders can be inferred from noisy proxy\nvariables which are easier to measure. For instance, the zip\ncodes and job types of patients can be used as proxies to\ninfer their socio-economic statuses (Sauer et al. 2013).\nFrom a practical perspective, the inﬂated number of variables included for confounding adjustment reduces the efﬁciency of treatment effect estimation. Moreover, it has\nbeen previously shown that including unnecessary covariates is suboptimal when the treatment effect is estimated\nnon-parametrically (Hahn 1998; Abadie and Imbens 2006;\nH¨aggstr¨om 2017). In a high dimensional scenario, eventually many included variables will not be confounders and\nshould be excluded from the set of adjustment variables.\nMost existing treatment estimation algorithms treat the\ngiven variables “as is”, and leave the task of choosing confounding variables to the user. It is clear that the users are\nleft with a dilemma: on the one hand including more variables than necessary produces inefﬁcient and inaccurate estimators; on the other hand restricting the number of adjustment variables may exclude confounders themselves or\nproxy variables of the confounders and thus increases the\nbias of the estimated treatment effects. With only a handful\nof variables, the problem can be avoided by consulting domain experts. However, a data-driven approach is required\nin the big data era to deal with the dilemma.\nIn this work, we propose a data-driven approach for simultaneously inferencing latent factors from proxy variables\nand disentangling the latent factors into three disjoint sets as\nillustrated in Figure 1: the instrumental factors zt which only\naffect the treatment but not the outcome, the risk factors zy\nwhich only affect the outcome but not the treatment, and the\nconfounding factors zc that affect both the treatment and the\noutcome. Since our method builds upon the recent advancement of the research on variational autoencoder (Kingma\nand Welling 2014), we name our method Treatment Effect\n𝐱\n𝒛𝑡\n𝑦\n𝑡\n𝒛𝑦\n𝒛𝑐\nFigure 1: Model diagram for the proposed Treatment Effect\nwith Disentangled Autoencoder (TEDVAE). t is the treatment, y is the outcome. x is the “as-is” observed variables\nwhich may contain non-confounders and noisy proxy variables. zt are factors that affect only the treatment, zy are\nfactors that affect only the outcome, and zc are confounding\nfactors that affect both treatment and outcome.\nby Disentangled Variational AutoEncoder (TEDVAE). Our\nmain contributions are:\n• We address an important problem in treatment effect estimation from observational data, where the observed variable may contain confounders, proxies of confounders\nand non-confounding variables.\n• We propose a data-driven algorithm, TEDVAE, to simultaneously infer latent factors from proxy variables and\ndisentangle confounding factors from the others for a\nmore efﬁcient and accurate treatment effect estimation.\n• We validate the effectiveness of the proposed TEDVAE\nalgorithm on a wide range of synthetic datasets, treatment\neffect estimation benchmarks and real-world datasets.\nThe rest of this paper is organized as follows. In Section 2, we discuss related works. The details of TEDVAE\nis presented in Section 3. In Section 4, we discuss the evaluation metrics, datasets and experiment results. Finally, we\nconclude the paper in Section 5.",
        "related work": "Treatment effect estimation has steadily drawn the attentions\nof researchers from the statistics and machine learning communities. During the past decade, several tree based methods (Su et al. 2009; Athey and Imbens 2016; Zhang et al.\n2017, 2018) have been proposed to address the problem by\ndesigning a treatment effect speciﬁc splitting criterion for\nrecursive partitioning. Ensemble algorithms and meta algorithms (K¨unzel et al. 2019; Wager and Athey 2018) have\nalso been explored. For example, Causal Forest(Wager and\nAthey 2018) builds ensembles using the Causal Tree (Athey\nand Imbens 2016) as base learners. X-Learner (K¨unzel et al.\n2019) is a meta algorithm that can utilize off-the-shelf machine learning algorithms for treatment effect estimation.\nDeep learning based heterogeneous treatment effect estimation methods have attracted increasingly research interest in recent years (Shalit, Johansson, and Sontag 2017;\nAlaa and Schaar 2018; Louizos et al. 2017; Hassanpour and\nGreiner 2018; Yao et al. 2018; Yoon, Jordan, and van der\nSchaar 2018). Counterfactual Regression Net (Shalit, Johansson, and Sontag 2017) and several other methods (Yao\net al. 2018; Hassanpour and Greiner 2018) have been proposed to reduce the discrepancy between the treated and untreated groups of samples by learning a representation such\nthat the two groups are as close to each other as possible.\nHowever, their designs do not separate the covariates that\nonly contribute to the treatment assignment from those only\ncontribute to the outcomes. Furthermore, these methods are\nnot able to infer latent covariates from proxies.\nVariable decomposition (Kun et al. 2017; H¨aggstr¨om\n2017) has been previously investigated for average treatment\neffect estimation. Our method has several major differences\nfrom the above methods: (i) our method is capable of estimating the individual level heterogeneous treatment effects,\nwhere existing ones only focus on the population level average treatment effect; (ii) we are able to identify the nonlinear relationships between the latent factors and their proxies, whereas their approach only models linear relationships.\nRecently, a deep representation learning based method, DRCFR (Hassanpour and Greiner 2020) is proposed for treatment effect estimation.\nAnother work closely related to ours is the Causal Effect\nVariational Autoencoder (CEVAE) (Louizos et al. 2017),\nwhich also utilizes variational autoencoder to learn confounders from observed variables. However, CEVAE does\nnot consider the existence of non-confounders, and is not\nable to learn the separated sets of instrumental and risk factors. As demonstrated by the experiments, disentangling the\nfactors signiﬁcantly improves the performance.",
        "method": "Preliminaries\nLet ti ∈ {0, 1} denote a binary treatment where ti = 0\nindicates the i-th individual receives no treatment (control)\nand ti = 1 indicates the individual receives the treatment\n(treated). We use yi(1) to denote the potential outcome of i\nif it were treated, and yi(0) to denote the potential outcome\nif it were not treated. Noting that only one of the potential\noutcomes can be realized, and the observed outcome is yi =\n(1 − ti)yi(0) + tiyi(1). Additionally, let xi ∈ Rd denote\nthe “as is” set of covariates for the i-th individual. When the\ncontext is clear, we omit the subscript i in the notations.\nThroughout the paper, we assume that the following three\nfundamental assumptions for treatment effect estimations\n(Rosenbaum and Rubin 1983) are satisﬁed:\nAssumption 1. (SUTVA) The Stable Unit Treatment Value\nAssumption requires that the potential outcomes for one unit\n(individual) is unaffected by the treatment of others.\nAssumption 2. (Unconfoundedness) The distribution of\ntreatment is independent of the potential outcome when conditioning on the observed variables: t ⊥⊥ (y(0), y(1))|x.\nAssumption 3. (Overlap) Every unit has a non-zero probability to receive either treatment or control when given the\nobserved variables, i.e., 0 < P(t = 1|x) < 1.\nThe ﬁrst goal of treatment effect estimation is estimating the average treatment effect (ATE) which is deﬁned as:\nATE = E[y(1)−y(0)] = E[y|do(t = 1)]−E[y|do(t = 0)],\nwhere do(t = 1) denote an manipulation on t by removing\nall its incoming edges and setting t = 1 (Pearl 2009).\nThe treatment effect for an individual i is deﬁned as\nτi = yi(1) − yi(0). Due to the counterfactual problem,\nwe never observe yi(1) and yi(0) simultaneously and thus\nτi is not observed for any individual. Instead, we estimate\nthe conditional average treatment effect (τ(x)), deﬁned as\nτ(x) := E[τ|x] = E[y|x, do(t = 1)] − E[y|x, do(t = 0)].\nTreatment Effect Estimation from Latent Factors\nIn this work, we propose the TEDVAE model (Figure 1)\nfor estimating the treatment effects, where the observed pretreatment variables x can be viewed as generated from three\ndisjoint sets of latent factors z = (zt, zc, zy). Here zt are instrumental factors that only affect the treatment but not the\noutcome, zy are risk factors which only affect the outcome\nbut not the treatment, and zc are confounding factors that\naffect both the treatment and the outcome.\nOn the one hand, the proposed TEDVAE model in Figure 1 provides two important beneﬁts. The ﬁrst one is that\nby explicitly modelling for the instrumental factors and adjustment factors, it accounts for the fact that not all variables\nin the observed variables set x are confounders. The second\nbeneﬁt is that it allows for the possibility of learning unobserved confounders that from their proxy variables.\nOn the other hand, our model diagram does not pose any\nrestriction other than the three standard assumptions discussed in Section 3.1. To see this, consider the case where\nevery variable in x itself is a confounder, i.e., x = xc, then\nthe generating mechanism in the TEDVAE model becomes\nzc = x with zt = zy = ∅ and the model in Figure 1 becomes identical to the widely used diagram for treatment\neffect estimation (Figure 2 in (Imbens 2019)).\nWith our model, the estimation of treatment effect is immediate using the back-door criterion (Pearl 2009):\nTheorem 1. The effect of t on y can be identiﬁed if we recover the confounding factors zc from the data.\nProof. From Figure 1 we know that zt, zc are the parents of\nthe treatment t, following (3.13) in Pearl we have,\nP(y|do(t)) =\nX\nzt\nX\nzc\nP(y|t, zt, zc)P(zt)P(zc).\n(1)\nUtilizing the fact that y ⊥⊥ zt|t, zc, we have\nP(y|do(t)) = P\nzt\nP(zt) P\nzc\nP(y|t, zc)P(zc|t, zc, zt). (2)\nFurthermore, since zc is not a descendant of t, by Markov\nproperty we have t ⊥⊥ zc|zc, zt. Therefore\nP(y|do(t)) =\nX\nzt\nP(zt)\nX\nzc\nP(y|t, zc)P(zc|zc, zt).\n(3)\nNote that P\nzt\nP(zt)P(zc|zt, zc) = P(zc), which gives us\nP(y|do(t)) =\nX\nzc\nP(y|t, zc)P(zc).\nFor the estimation of the conditional average treatment\neffect, our result follows from Theorem 3.4.1 in (Pearl 2009)\nas shown in the following theorem:\nTheorem 2. The conditional average treatment effect of t\non y conditioned on x can be identiﬁed if we recover the\nconfounding factors zc and risk factors zy .\nProof. Let Gt denote the causal structure obtained by removing all incoming edges of t in Figure 1, Gt denote the\nstructure by deleting all outgoing edges of t.\nNoting that y ⊥⊥ zt|t, zy, zc in Gt, using the three rules\nof do-calculus we can remove zt from the conditioning\nset and obtain P(y|do(t), x) = P(y|do(t), zt, zc, zy) =\nP(y|do(t), zy, zc). with Rule 1. Furthermore, using Rule\n2 with (y ⊥⊥\nt|zc, zy) in Gt yields P(y|do(t), x)\n=\nP(y|do(t), zy, zc) = P(y|t, zy, zc).\nAn implication of Theorem 1 and 2 is that they are not\nrestricted to binary treatment. In other words, our proposed\nmethod can be used for estimating treatment effect of a continuous treatment variable, while most of the existing estimators are not able to do so. However, due to the lack of\ndatasets with continuous treatment variables for evaluating\nthis, we focus on the case of binary treatment variable and\nleave the continuous treatment case for future work.\nTheorems 1 and 2 suggest that disentangling the confounding factors allows us to exclude unnecessary factors\nwhen estimating ATE and CATE. However, keen readers\nmay wonder since we already assumed unconfoundedness,\ndoesn’t straightforwardly adjusting for x sufﬁce?\nTheoretically, it has been shown that both the bias (Abadie\nand Imbens 2006) and the variance (Hahn 1998) of treatment effect estimation will increase if variables unrelated to\nthe outcome is included during the estimation. Therefore, it\nis crucial to differentiate the instrumental, confounding and\nrisk factors and only use the appropriate factors during treatment effect estimation. In the next section, we propose our\ndata-driven approach to learn and disentangle the latent factors using a variational autoencoder.\nLearning of the Disentangled Latent Factors\nIn the above discussion, we have seen that removing unnecessary factors is crucial for efﬁcient and accurate treatment effect estimation. We have assumed that the mechanism which generates the observed variables x from the\nlatent factors z and the decomposition of latent factors\nz = (zt, zc, zy) are available. However, in practice both the\nmechanism and the decomposition are not known. Therefore, the practical approach would be to utilize the complete\nset of available variables during the modelling to ensure the\nsatisfaction of the unconfoundedness assumption, and utilize a data-driven approach to simultaneously learn and disentangle the latent factors into disjoint subsets.\nTo this end, our goal is to learn the posterior distribution\np(z|x) for the set of latent factors with z = (zt, zy, zc)\nas illustrated in Figure 1, where zt, zc, zy are independent\nof each other and correspond the instrumental factors, confounding factors, and risk factors, respectively. Because exact inference would be intractable, we employ neural netp(t|zt,zc)\np(zc)\np(y|t=1,zc,zy)\np(y|t=0,zc,zy)\np(x|zt,zc,zy)\n...\n...\np(zy)\np(zt)\n...\n(a) Generative Model.\nq(zy|x)\nq(zt|x)\n...\n...\np(x)\n...\nq(zc|x)\nq(t|zt,zc)\nq(y|t, zy,zc)\n(b) Inference Model.\nFigure 2: Overall architecture of the model network and the inference network for the Treatment Effect Disentangling Variational AutoEncoder (TEDVAE). White nodes correspond to parametrized deterministic neural network transitions, gray nodes\ncorrespond to drawing samples from the respective distribution and white circles correspond to switching paths according to\nthe treatment t. Dashed arrows in the inference model represent the two auxiliary classiﬁers qωt(t|zt, zc) and qωy(y|zy, zc).\nwork based variational inference to approximate the posterior pθ(x|zt, zc, zy). Speciﬁcally, we utilize three separate\nencoders qφt(zt|x), qφc(zc|x), and qφy(zy|x) that serve as\nvariational posteriors over the latent factors. These latent\nfactors are then used by a single decoder pθ(x|zt, zc, zy) for\nthe reconstruction of x. Following standard VAE design, the\nprior distributions p(zt), p(zc), p(zy) are chosen as Gaussian distributions (Kingma and Welling 2014).\nSpeciﬁcally, the factors and the generative models for x\nand t are described as:\np(zt) =\nDzt\nY\nj=1\nN(ztj|0, 1);\np(zc) =\nDzc\nY\nj=1\nN(zcj|0, 1);\np(zy) =\nDzy\nY\nj=1\nN(zyj|0, 1);\np(t|zt, zc) = Bern(σ(f1(zt, zc))\np(x|zt, zc, zy) =\nd\nY\nj=1\np(xj|zt, zc, zy),\n(4)\nwith p(xj|zt, zc, zy) being the suitable distribution for the jth observed variable, f1 is a function parametrized by neural\nnetwork, and σ(·) being the logistic function, Dzt, Dzc, and\nDzy are the parameters that determine the dimensions of instrumental, confounding, and risk factors to infer from x.\nFor continuous outcome variable y, we parametrize it\nas using a Gaussian distribution with its mean and variance given by a pair of disjoint neural networks that deﬁnes p(y|t = 1, zc, zy) and p(y|t = 0, zc, zy). This pair\nof disjoint networks allows for highly imbalanced treatment.\nSpeciﬁcally, for continuous y we parametrize it as:\np(y|t, zc, zy) = N(µ = ˆµ, σ2 = ˆσ2),\nˆµ = (tf2(zc, zy) + (1 − t)f3(zc, zy)),\nˆσ2 = (tf4(zc, zy) + (1 − t)f5(zc, zy)),\n(5)\nwhere f2, f3, f4, f5 are neural networks parametrized by\ntheir own parameters. The distribution for the binary outcome case can be similarly parametrized with a Bernoulli\ndistribution.\nIn the inference model, the variational approximations of\nthe posteriors are deﬁned as:\nqφt(zt|x) =\nDzt\nY\nj=1\nN(µ = ˆµt, σ2 = ˆσ2\nt );\nqφc(zc|x) =\nDzc\nY\nj=1\nN(µ = ˆµc, σ2 = ˆσ2\nc);\nqφy(zy|x) =\nDzy\nY\nj=1\nN(µ = ˆµy, σ2 = ˆσ2\ny)\n(6)\nwhere ˆµt, ˆµc, ˆµy and ˆσ2\nt , ˆσ2\nc, ˆσ2\ny are the means and variances\nof the Gaussian distributions parametrized by neural networks similarly to the ˆµ and ˆσ in the generative model.\nGiven the training samples, the parameters can be optimized by maximizing the evidence lower bound (ELBO):\nLELBO(x, y, t) =Eqφcqφtqφy [log pθ(x|zt, zc, zy)]\n− DKL(qφt(zt|x)||pθt(zt))\n− DKL(qφc(zc|x)||pθc(zc))\n− DKL(qφy(zy|x)||pθy(zy)).\n(7)\nTo encourage the disentanglement of the latent factors and\nensure that the treatment t can be predicted from zt and zc,\nand the outcome y can be predicted from zy and zc, we add\ntwo auxiliary classiﬁers to the variational lower bound. Finally, the objective of TEDVAE can be expressed as\nLTEDVAE =LELBO(x, y, t)\n+ αtEqφtqφc[log qωt(t|zt, zc)]\n+ αyEqφy qφc[log qωy(y|t, zc, zy)],\n(8)\nwhere αt and αy are the weights for the auxiliary objectives.\nFor predicting the CATEs of new subjects given their\nobserved covariates x, we use the encoders q(zy|x) and\nq(zc|x) to sample the posteriors of the confounding and risk\nfactors for l times, and average over the predicted outcome\ny using the auxiliary classiﬁer qωy(y|t, zc, zy).\nAn important difference between TEDVAE and CEVAE\nlies in their inference models. During inference, CEVAE depends on t, x and y for inferencing z; in other words, CEVAE needs to estimate p(t|x) and p(y|t, x), inference z as\np(z|t, y, x), and ﬁnally predict the CATE as ˆτ(x) = E[y|t =\n1, z]−E[t|y = 0, z]. The estimations of p(t|x) and p(y|t, x)\nare unnecessary since we assume that t and y are generated\nby the latent factors and inferencing the latents should only\ndepend on x as in TEDVAE. As we later show in the experiments, this difference is crucial even when no instrumental\nor risk factors are present in the data.",
        "experiments": "We empirically compare TEDVAE with traditional and neural network based treatment effect estimators. For traditional methods, we compare with tailor designed methods\nincluding Squared t-statistic Tree (t-stats) (Su et al. 2009)\nand Causal Tree (CT) (Athey and Imbens 2016); ensemble methods including Causal Random Forest (CRF) (Wager and Athey 2018), Bayesian Additive Regression Trees\n(BART) (Hill 2011), and meta algorithm X-Learner (K¨unzel\net al. 2019) using Random Forest (Breiman et al. 1984)\nas base learner (X-RF). For deep learning based methods,\nwe compare with representation learning based methods including Counterfactual Regression Net (CFR) (Shalit, Johansson, and Sontag 2017), Similarity Preserved Individual Treatment Effect (SITE) (Yao et al. 2018), and with\na deep learning variable decomposition method for Counterfactual Regression (DR-CFR) (Hassanpour and Greiner\n2020). We also compare with generative methods including\nCausal Effect Variational Autoencoder (CEVAE) (Louizos\net al. 2017) and GANITE (Yoon, Jordan, and van der Schaar\n2018). Parameters for the compared methods are tuned\nby cross-validated grid search on the value ranges recommended in the code repository. The code is available at\nhttps://github.com/WeijiaZhang24/TEDVAE.\nEvaluation Criteria\nFor evaluating the performance of CATE estimation, we\nuse the Precision in Estimation of Heterogeneous Effect\n(PEHE) (Hill 2011; Shalit, Johansson, and Sontag 2017;\nLouizos et al. 2017; Dorie et al. 2019) which measures\nthe root mean squared distance between the estimated and\nthe true CATE when ground truth is available: ϵPEHE =\nq\n1\nN\nPN\ni=1(ˆτ(xi) − τ(xi))2 , where τ(x) is the ground\ntruth CATE for subjects with observed variables xi.\nFor evaluating the performance of the average treatment\neffect (ATE) estimation, the ground truth ATE can be calculated by averaging the differences of the outcomes in\nthe treated and control groups if randomized controlled trials data is available. Then, when comparing the ground\ntruth ATE with the estimated ATE obtained from a nonrandomized sample (observational sample or created via bi0.000\n0.025\n0.050\n0.075\n0.25\n0.50\n0.75\n−2\n−4\n−6\n0\n0.0\n2.5\n5.0\n7.5\n10\n−2\n−4\n−6\n0.0\n2.5\n5.0\n7.5\n10\n0.050\n0.075\n0.100\n0.25\n0.50\n0.75\nProportion of Treated\n0.2\n0.4\n0.6\n0.0\n2.5\n5.0\n7.5\n10\nSize of outcome\nTEDVAE CEVAE\n0.1\n0.2\n0.3\n0.4\n0.5\n1\n2\n3\n4\n5\nSize of CATE\nFigure 3: Comparison of CEVAE and TEDVAE under different settings using the synthetic datasets. Rows: the results for data generating procedure satisﬁes the assumption\nof the TEDVAE model and the CEVAE model, respectively.\nColumns: (Left) Varying the proportional of treated samples;\n(Middle) Varying the size of the outcome; (Right) Varying\nthe size of the CATE. (Figures are best viewed in colour.)\nased sampling) of the dataset, the performances can then be\nevaluated using the mean absolute error in ATE (Hill 2011;\nShalit, Johansson, and Sontag 2017; Louizos et al. 2017;\nYao et al. 2018) for evaluation: ϵATE = |ˆτ − 1\nN\nN\nP\ni=1\n[tiyi −\n(1 − ti)yi]|, where ˆτ is the estimated ATE, ti and yi are the\ntreatments and outcomes from the randomized data. For both\nϵATE and ϵPEHE, we use superscripts tr and te to denote their\nvalues on the training and test sets, respectively.\nSynthetic Datasets\nWe ﬁrst conduct experiments using synthetic datasets to investigate TEDVAE’s capability of inferring the latent factors\nand estimating the treatment effects. Due to the page limit,\nwe only provide an outline of the synthetic dataset and provide the detailed settings in the supplementary materials.\nThe ﬁrst setting of synthetic datasets studies the beneﬁt of\ndisentangling the confounding factors from instrumental and\nrisk factors, and are generated using the structure depicted in\nFigure 1. We illustrate the results in the ﬁrst row of Figure 3.\nIt can be seen that when the instrumental and risk factors exist in the data, the beneﬁt of disentanglement is signﬁcance\nas demonstrated by the PEHE curves between TEDVAE\nand CEVAE. When the proportions of the treated samples\nvaries, the performances of CEVAE ﬂuctuates severely and\nthe error remains high even when the dataset is balanced;\nhowever, the PEHEs of TEDVAE are stable even when the\ndataset is highly imbalanced, and are always stays signiﬁcantly lower than CEVAE. When the scales of outcome and\nCATE change, TEDVAE also performs consistently and signiﬁcantly better than CEVAE.\nThe second setting for the synthetic datasets are designed\nto study how TEDVAE performs when the instrumental and\nrisk factors are absent, and follow the same data generating\nprocedure as in the CEVAE (Louizos et al. 2017). We illustrate the results of this synthetic dataset in the second row of\nFigure 3. Therefore, it is reasonable to expect that CEVAE\nwould perform better than TEDVAE since the instrumental\nfactors zt and risk factors zy do not exist. However, from the\n5-5-5\n5-5-10\n5-10-5\n6-10-10\n10-5-5\n10-10-5\n10-5-1\n0\n10-10-10\nǁŝƚŚ\u0003zƚ\u0003\n\u0003\u0003ǁŝƚŚŽƵƚ\u0003zƚ\n(a)\n5-5-5\n5-5-10\n5-10-5\n5-10-10\n10-5-5\n10-5-10\n10-10-5\n10-10-10\nǁŝƚŚ\u0003zc\u0003 \u0003\u0003\u0003ǁŝƚŚŽƵƚ\u0003zĐ\n(b)\n5-5-5\n5-5-10\n5-10-5\n5-10-10\n10-5-5\n10-5-10\n10-10-5\n10-10-10\nǁŝƚŚ\u0003zǇ \nǁŝƚŚŽƵƚ\u0003zǇ\n(c)\nFigure 4: Radar charts for TEDVAE’s capability in identifying the latent factors. Each vertex on the polygons is identiﬁed with the latent factors’ dimension sequence of the associated synthetic dataset. For example, 5-5-5 indicates that\nthe dataset is generated using 5 dimensions each for the instrumental, confounding, and risk factors.\nsecond row of Figure 3 we can see that TEDVAE either performs better than CEVAE, or performs as well as CEVAE\nusing a wide range of parameters under this setting. This\nis possibly due to the differences in predicting for previous\nunseen samples between TEDVAE and CEVAE, where CEVAE needs to follow a complicated procedure of inferencing\np(t|x) and p(y|t, x) ﬁrst and then inferencing the latents as\np(z|t, y, x), whereas in TEDVAE this is not needed. These\nresults suggests that the TEDVAE model is able to effectively learn the latent factors and estimate the CATE even\nwhen the instrumental and risk factors are absent. It also indicates that the TEDVAE algorithm is robust to the selection\nof the latent dimensionality parameters.\nNext, we investigate whether TEDVAE is capable of recovering the latent factors of zt, zc, and zy that are used\nto generate the observed covariates x. To do so, we compare the performances of TEDVAE when setting the Dzt,\nDzc and Dzy parameters to 10 against itself when setting\none of the latent dimensionality parameter of TEDVAE to\n0, i.e., setting Dzt = 0 and forcing TEDVAE to ignore the\nexistence of zt. If TEDVAE is indeed capable of recovering\nthe latent factors, then its performances with non-zero latent dimensionality parameters should be better than its performance when ignoring the existence of any of the latent\nfactors. Figure 4 illustrates the capability of TEDVAE for\nidentifying the latent factors using radar chart. Taking the\nFigure 4(a) as example, the zt and ¬zt polygons correspond\nto the performances of TEDVAE when setting the dimension\nparameter Dzt = 5 (identify zt) and Dzt = 0 (ignore zt).\nFrom the ﬁgures we can clearly see that the performances of\nTEDVAE are signiﬁcantly better when the latent dimensionality parameters are set to non-zero, than setting any of the\nlatent dimensionality to 0.\nBenchmarks and Real-world Datasets\nIn this section, we use two benchmark datasets for treatment\neffect estimation to compare TEDVAE with the baselines.\nBenchmark I: 2016 Atlantic Causal Inference Challenge\nThe 2016 Atlantic Causal Inference Challenge (ACIC2016)\n(Dorie et al. 2019) contains 77 different settings of benchmark datasets that are designed to test causal inference algorithms under a diverse range of real-world scenarios. The\nMethods\nϵtr\nPEHE\nϵte\nPEHE\nCT\n4.81±0.18\n4.96±0.21\nt-stats\n5.18±0.18\n5.44±0.20\nCF\n2.16±0.17\n2.18±0.19\nBART\n2.13±0.18\n2.17±0.15\nX-RF\n1.86±0.15\n1.89±0.16\nCFR\n2.05±0.18\n2.18±0.20\nSITE\n2.32±0.19\n2.41±0.23\nDR-CFR\n2.44±0.20\n2.56±0.21\nGANITE\n2.78±0.56\n2.84± 0.61\nCEVAE\n3.12±0.28\n3.28±0.35\nTEDVAE\n1.75±0.14\n1.77±0.17\nTable 1: Means and standard deviations of the PEHE metrics (smaller is better) for training and test sets on the 77\nbenchmark datasets from ACIC2016. The bolded values indicate the best performers (Wilcoxon signed rank tests at\np = 0.05).\ndataset contains 4802 observations and 58 variables. The\noutcome and treatment variables are generated using different data generating procedures for the 77 settings, providing benchmarks for a wide range of treatment effect estimation scenarios. This dataset can be accessed at https:\n//github.com/vdorie/aciccomp/tree/master/2016.\nWe report the average PEHE metrics across 77 settings\nwhere each setting is repeated for 10 replications. For TEDVAE, the parameters are selected using the average of the\nﬁrst ﬁve settings, instead of tuning separately for the 77 settings. This approach has two beneﬁts: ﬁrstly and most importantly, if an algorithm performs well using the same parameters across all 77 settings, it indicates that the algorithm\nis not sensitive to the choice of parameters and thus would\nbe easier for practitioners to use in real-world scenarios; the\nsecond beneﬁt is to save computation costs, as conducting\nparameter tuning across a large amount of datasets can be\ncomputationally overwhelming for practitioners. As a result,\nwe set the latent dimensionality parameters as Dzy = 5,\nDzt = 15, Dzc = 15 and set the weight for auxiliary\nlosses as αt = αy = 100. For all the parametrized neural\nnetworks, we use 5 hidden layers and 100 hidden neurons\nin each layer, with ELU activation. with a 60%/30%/10%\ntrain/validation/test splitting proportions.\nThe results on the ACIC2016 datasets are reported in Table 1. We can see that TEDVAE performs signiﬁcantly better\nthan the compared methods. These results show that, without\ntuning parameters individually for each setting, TEDVAE\nachieves state-of-the-art performances across diverse range\nof data generating procedures, which empirically demonstrates that TEDVAE is effective for treatment effect estimation across different settings.\nBenchmark II: Infant Health Development Program\nThe Infant Health and Development Program (IHDP) is a\nrandomized controlled study designed to evaluate the effect of home visit from specialist doctors on the cognitive\ntest scores of premature infants. The datasets is ﬁrst used\nfor benchmarking treatment effect estimation algorithms in\nSetting A\nSetting B\nMethods\nϵtr\nPEHE\nϵte\nPEHE\nϵtr\nPEHE\nϵte\nPEHE\nCT\n1.48±0.12 1.56±0.13 5.46±0.08 5.73±0.09\nt-stats\n1.78±0.09 1.91±0.12 5.40±0.08 5.71±0.09\nCF\n1.01±0.08 1.09±0.16 3.86±0.05 3.91±0.07\nBART\n0.87±0.07 0.88±0.07 2.78±0.03 2.91±0.04\nX-RF\n0.98±0.08 1.09±0.15 3.50±0.04 3.59±0.06\nCFR\n0.67±0.02 0.73±0.04 2.60±0.04 2.76±0.04\nSITE\n0.65±0.07 0.67±0.06 2.65±0.04 2.87±0.05\nDR-CFR\n0.62±0.15 0.65±0.18 2.73±0.04 2.93±0.05\nGANITE 1.84±0.34 1.90±0.40 3.68±0.38 3.84±0.52\nCEVAE\n0.95±0.12 1.04±0.14 2.90±0.10 3.24±0.12\nTEDVAE 0.59±0.11 0.60±0.14 2.10±0.09 2.22±0.08\nTable 2: Means and standard deviations of the PEHE metric\n(smaller is better) on IHDP. The bolded values indicate the\nbest performers (Wilcoxon signed rank tests (p = 0.05).\n(Hill 2011), where selection bias is induced by removing a\nnon-random subset of the treated subjects to create an observational dataset, and the outcomes are simulated using\nthe original covariates and treatments. It contains 747 subjects and 25 variables that describe both the characteristics\nof the infants and the characteristics of their mothers. We\nuse the same procedure as described in (Hill 2011) which\nincludes two settings of this benchmark: ‘Setting A” and\n“Setting B”, where the outcomes follow linear relationship\nwith the variables in “Setting A” and exponential relationship in “Setting B”. The datasets can be accessed at https:\n//github.com/vdorie/npci. The reported performances are averaged over 100 replications with a training/validation/test\nsplits proportions of 60%/30%/10%.\nSince evaluating treatment effect estimation is difﬁcult in\nreal-world scenarios (Alaa and van der Schaar 2019), a good\ntreatment effect estimation algorithm should perform well\nacross different datasets with minimum requirement for parameter tuning. Therefore, for TEDVAE we use the same\nparameters in the ACIC dataset and do not perform parameter tuning on the IHDP dataset. For the compared traditional\nmethods, we also use the same parameters as selected on the\nACIC benchmark. For the compared deep learning methods,\nwe conduct grid search using the recommended parameter\nranges from the relevant papers.\nFrom Table 2 we can see that TEDVAE achieves the lowest PEHE errors among the compared methods on both Setting A and Setting B of the IHDP benchmark. Wilcoxon\nsigned rank tests (p = 0.05) indicate that TEDVAE is significantly better than the compared methods. Since TEDVAE\nuses the same parameters on the IHDP datasets as in the previous ACIC benchmarks, these results demonstrate that the\nTEDVAE model is suitable for diverse real-world scenarios\nand is robust to the choice of parameters.\nReal-world Dataset: Twins\nIn this section, we use a realworld randomized dataset to compare the methods capability\nof estimating the average treatment effects.\nThe Twins dataset has been previously used for evaluating causal inference in (Louizos et al. 2017; Yao et al. 2018).\nTwins\nMethods\nϵtr\nATE\nϵte\nATE\nCT\n0.034±0.002 0.038±0.007\nt-stats\n0.032±0.003 0.033±0.005\nCF\n0.025±0.001 0.025±0.001\nBART\n0.050±0.002 0.051±0.002\nX-RF\n0.075±0.003 0.074±0.004\nCFR\n0.029±0.002 0.030±0.002\nSITE\n0.031±0.003 0.033±0.003\nDR-CFR\n0.032±0.002 0.034±0.003\nGANITE\n0.016±0.004 0.018±0.005\nCEVAE\n0.046±0.020 0.047±0.021\nTEDVAE\n0.006±0.002 0.006±0.002\nTable 3: Means and standard deviations of ϵATE on the Twins\ndatasets. The bolded values indicate the best performers\n(Wilcoxon signed rank tests (p = 0.05).\nIt consists of samples from twin births in the U.S. between\nthe year of 1989 and 1991 provided in (Almond, Chay, and\nLee 2005). Each subject is described by 40 variables related\nto the parents, the pregnancy and the birth statistics of the\ntwins. The treatment is considered as t = 1 if a sample is\nthe heavier one of the twins, and considered as t = 0 if the\nsample is lighter. The outcome is a binary variable indicating\nthe children’s mortality after a one year follow-up period.\nFollowing the procedure in (Yao et al. 2018), we remove\nthe subjects that are born with weight heavier than 2,000g\nand those with missing values, and introduced selection bias\nby removing a non-random subset of the subjects. The ﬁnal dataset contains 4,813 samples. The data splitting is the\nsame as previous experiments, and the reported results are\naveraged over 100 replications. The ATE estimation performances are illustrated in Table ??. On this dataset, we can\nsee that TEDVAE achieves the best performance with the\nsmallest ϵAT E among all the compared algorithms.\nOverall, the experiments results show that the performances of TEDVAE are signiﬁcantly better than the compared methods on a wide range of synthetic, benchmark,\nand real-world datasets. In addition, the results also indicate\nthat TEDVAE is less sensitive to the choice of parameters\nthan the other deep learning based methods, which makes\nour method attractive for real-world application scenarios.",
        "conclusion": "We propose the TEDVAE algorithm, a state-of-the-art treatment effect estimator which infer and disentangle three\ndisjoints sets of instrumental, confounding and risk factors from the observed variables. Experiments on a wide\nrange of synthetic, benchmark, and real-world datasets have\nshown that TEDVAE signiﬁcantly outperforms compared\nbaselines. For future work, a path worth exploring is extending TEDVAE for treatment effects with non-binary treatment variables. While most of the existing methods are restricted to binary treatments, the generative model of TEDVAE makes it a promising candidate for extension to treatment effect estimation with continuous treatments.",
        "summary_en": "Much research has been devoted to the problem of estimating treatment effects from observational data; however, most methods assume that the observed variables only contain confounders, i.e., variables that affect both the treatment and the outcome. Unfortunately, this assumption is frequently violated in real-world applications, since some variables only affect the treatment but not the outcome, and vice versa. Moreover, in many cases only the proxy variables of the underlying confounding factors can be observed. This paper first shows the importance of differentiating confounding factors from instrumental and risk factors for both average and conditional average treatment effect estimation, and then the paper proposes a variational inference approach to simultaneously infer latent factors from the observed variables, disentangle the factors into three disjoint sets corresponding to the instrumental, confounding, and risk factors, and use the disentangled factors for treatment effect estimation. Experimental results demonstrate the effectiveness of the proposed method on a wide range of synthetic, benchmark, and real-world datasets.",
        "summary_zh": "这篇论文介绍了一种变分推理方法用于治疗效果估计。从观察数据中估算治疗效果的问题已经得到了大量研究，大多数方法都假定观察变量只包含混杂因素，即同时影响治疗和结果的变量。然而这一假设在实际应用中经常不成立。并且，在许多情况下，只能观察到基本混杂因素的替代变量。作者提出的方法可以同时从观测变量中推断出潜在因素，并将这些因素分解为三个不相交的集合，分别对应于工具因素、混杂因素和风险因素，并使用分解后的因素进行治疗效果估计。实验结果证明了该方法在各种合成数据集、基准数据集和真实数据集上的有效性。"
    },
    {
        "title": "MAPDP: Cooperative Multi-Agent Reinforcement Learning to Solve Pickup and Delivery Problems",
        "abstract": "Cooperative Pickup and Delivery Problem (PDP), as a variant of the typical Vehicle Routing Problems (VRP), is an important formulation in many real-world applications, such as on-demand delivery, industrial warehousing, etc. It is of great importance to efﬁciently provide high-quality solutions of cooperative PDP. However, it is not trivial to provide effective solutions directly due to two major challenges: 1) the structural dependency between pickup and delivery pairs require explicit modeling and representation. 2) the cooperation between different vehicles is highly related to solution exploration and is difﬁcult to model. In this paper, we propose a novel multi-agent reinforcement learning-based framework to solve the cooperative PDP (MAPDP). First, we design a paired context embedding to well measure the dependency of different nodes considering their structural limits. Second, we utilize cooperative multi-agent decoders to leverage the decision dependence among different vehicle agents based on a special communication embedding. Third, we design a novel cooperative A2C algorithm to train the integrated model. We conduct extensive experiments on a randomly generated dataset and a real-world dataset. Experiments result shown that the proposed MAPDP outperforms all other baselines by at least 1.64% in all settings, and shows signiﬁcant computation speed during solution inference.",
        "introduction": "Vehicle Routing Problem (VRP) has shown its importance\nin formulating many real-world applications, including express systems, industrial warehousing, and on-demand delivery (Zong et al. 2021). A ﬂeet of vehicles/couriers are\nmanaged to fulﬁll given demands, while the goal is to optimize the routing plan to reduce traveling expenses. In realworld scenarios, a given demand order usually has its own\norigination and a designated delivery destination. Furthermore, routing tasks are usually assigned to multiple vehicles simultaneously, which forms the Pickup and Delivery\nProblem (PDP) (Savelsbergh and Sol 1995). For instance,\non-demand delivery couriers need to pick up the food ﬁrst\nfrom the restaurants and then deliver it to the corresponding\ncustomers, as shown in 1. While in industrial manufacturing, large amounts of materials, productions also need to be\ntransported from speciﬁc factories and warehouses at multiple sites (Li et al. 2021b). Efﬁciently generating high-quality\nsolutions of cooperative PDP can help reduce operating expenses, improve public efﬁciency and thus bring signiﬁcant\nbeneﬁts.\nOwing to the NP-hard nature, the cooperative PDP along\nwith other VRP variants is still difﬁcult to be optimally\nsolved by exact methods (Toth and Vigo 2002; Madsen, Fisher, and Jornsten 1997). Even though numerous\nheuristic-based methods are developed to compute nearoptimal solutions, the solution generation process remains\ntime-consuming and there is still potential to ﬁnd more approximate ones. The recent development of deep reinforcement learning (DRL) offers its effectiveness in solving many\ncombinatorial optimization problems including VRPs, and\nthus brings another perspective to solve PDP (Bello et al.\n2016; Kool, van Hoof, and Welling 2019; Nazari et al. 2018;\nChen and Tian 2019; Lu, Zhang, and Yang 2019a). Beneﬁting from learning a parameterized model instead of relying\non manually constructed rules to search for solutions, DRL\nhas shown its appealing performance in typical routing problems. Besides, by splitting the phases of training and online\ninferring, DRL can generate results with much faster computation. Inspired by both high solution quality and inference speed, it is prospective to well solve cooperative PDP\nby constructing a DRL framework.\nHowever, most existing DRL based methods can only be\napplied to typical VRPs where the demands share a common ﬁnal delivery site, i.e., either to be picked up or to be\ndelivered only. They are less effective when facing cooperative PDP with more detailed constraints and challenges\nfrom the following two aspects. First, it is not trivial to measure and structural constraint of PDP among different nodes.\nCompared to typical VRPs, complicated relations among all\nnodes should be considered. Representation upon the relation between paired pickup and deliveries should differ from\nthat between unpaired ones. How to efﬁciently measure such\nrelations is essential for framework effectiveness. Second, it\nis difﬁcult to coordinate the cooperation within the vehicle\nﬂeet. As all vehicles working simultaneously to accomplish\nthe pickup and delivery tasks, the potential decision space of\none is directly inﬂuenced by others. Optimizing the coordinated routing plans as the global objective is different from\noptimizing the one of a single vehicle, where the dependence\nFigure 1: An application visualization of the cooperative\nPDP in on-demand delivery.\namong them must be well modeled and measured.\nTo tackle the challenges above, we propose a novel endto-end cooperative multi-agent RL (MARL) based framework to solve cooperative PDP (MAPDP). MAPDP learns\nto generate the next node to visit step by step for each\nvehicle agent and ﬁnally outputs a complete routing plan.\nFirst, we develop a paired context embedding to represent\neach pickup or delivery node in each order instance. The\npaired context embedding well models the inter-relation inbetween that will further inﬂuence the potential solution\nspace. Second, we utilize cooperated decoders to leverage\nthe decision dependence among different vehicle agents.\nGiven the shared instance encoding results from former\nmodule designs, each vehicle agent obtains its own partial\naction results based on a special communication embedding.\nSuch multi-agent cooperation is trained and could be executed in online inference via a centralized controlling framework. Third, We also design a cooperative A2C algorithm\nfor the integrated model training, where a joint critic value\nnet estimates the state value.\nTo summarize, the main contributions of our work are\nlisted as follows:\n• To the best of our knowledge, we are the ﬁrst to explore\nthe cooperative PDP with multiple vehicle agents using\nMARL.\n• We design a centralized MARL framework to generate\ncooperative decisions. We design a paired context embedding to well capture the inter-dependency of heterogeneous nodes, and train different agents based on a communication embedding via a specially designed cooperative A2C algorithm.\n• We evaluate the effectiveness of MAPDP on two\ndatasets. It outperforms other baselines by at least 1.64%\nin all experiment settings and shows signiﬁcant computation speed during solution inference.\nThe remainder of this paper is organized as follows. We\nﬁrst introduce the related works in Section 2. We then formulate cooperative PDP in Section 3, and introduce our\nmethodology in Section 4. The experiment results are presented in Section 5. Finally, we conclude our paper in Section 6.",
        "related works": "In this section, we review the existing literature on conventional heuristic methods for solving PDP and learning based\nmethods for routing problems. Besides, we also introduce\nthe recent research in multi-agent RL.\nHeuristics for PDP\nAs an important variant of the typical vehicle routing problem (VRP), PDP considering pickups and deliveries was\nﬁrst studied in (Savelsbergh and Sol 1995), and exact methods were put forward early as solutions (Ruland and Rodin\n1997). However, trying to generate exact optimality suffers from heavy computation given then exponential complexity. As a substitution, more researchers tend to utilize\nheuristics to generate approximate optimal solutions, which\ncould signiﬁcantly increase the efﬁciency with small quality costs. A tabu-embedded simulated annealing algorithm\nwas proposed for solving large-scale PDP with time windows (Ropke and Cordeau 2009). An adaptive large neighborhood search heuristic method was presented in (Ropke\nand Pisinger 2006) to solve PDP, incorporating regret insertion method and six removal strategies.\nEven though the heuristic-based methods could generate\nnear-optimal solutions within a more reasonable time instead, they heavily rely on hand-crafted rules and are greatly\nlimited by human experience. Furthermore, the online inference time of these methods is still not satisfactory when\nfacing high dynamics.\nRL based Methods for Routing Problems\nDue to the potential that taking feedback reward as training signals to action attempts when interacting with outside environments, RL has shown its effectiveness in solving many decision-making problems. Bello et al. ﬁrst propose an RL-based algorithm to solve combinatorial optimization problems, including the famous Traveling Salesman Problem (TSP) (Bello et al. 2016). Such an algorithm\nborrows the idea from the constructive heuristics and utilizes\nan agent to directly generate solutions from scratch in an\nend-to-end manner. Following this idea, Nazari et al. utilize\nRNN structures to further expand its capability on solving\ncapacitated VRP by generating decision sequences (Nazari\net al. 2018). Kool et al. (Kool, van Hoof, and Welling 2019)\nfurther propose a attention based network to fully capture\nthe in-between relationships between different nodes. The\nsuitable neural network design greatly improves the solution\nquality, and more routing variants are further evaluated. Xin\net al. develop a multi-decoder based framework to generate\nﬁne-tuned solutions via a special beam-search.\nDespite the idea of directly generate ﬁnal solution from\nscratch as the output results for VRP, researchers also attempts to augment RL into local solution improvement.\nChen et al. (Chen and Tian 2019) formulates the solution improvement process as keeping rewriting based on the current\nones. Following this idea, Lu et al. (Lu, Zhang, and Yang\n2019b) further combined RL with Operation Research operators to keep updating the current solutions based on multiple in-hand operators.\nHowever, most existing RL based approaches can only\nsolve typical VRPs, while the cooperative PDP problem with\nstructural dependency and vehicle cooperation requires speciﬁc modeling. Even though Li et al. (Li et al. 2021b) proposed a framework to solve dynamic PDP, the agent model\nis only used to dispatch new orders to one of the vehicles,\nwhile the real routing process is processed via enumeration. Such a pure dispatching framework suffers from limits on exploration to global optimization. Li et al. (Li et al.\n2021a) proposed a heterogeneous attention based network\nto solve single-vehicle PDP, but ﬁnding solutions for cooperative PDP remains unsolved. In contrast, our proposed\nMAPDP could well model both pickup-delivery dependency\nand vehicle cooperations.\nMulti-Agent Reinforcement Learning\nIn many practical decision-making problems, cooperation or\ncompetition among different agents requires speciﬁc modeling, thus the multi-agent reinforcement learning (Bus¸oniu,\nBabuˇska, and De Schutter 2010; Lowe et al. 2017) has attracted much attention. Tampuu et al. (Tampuu et al. 2017)\nanalyzed the cooperation and competition among two agents\nin reinforcement learning by carefully designing the reward\nschemes. Lin et al.\n(Lin et al. 2018) designed a multiagent model to manage multiple vehicles operating simultaneously in the city. Lee et al. (Lee and Lee 2019) further improved cooperative models by mixing demonstrations from\nthe centralized policy.\nAs for the variants of VRPs, Zhang et al. (Zhang et al.\n2020) proposed a MARL-based framework to solve VRP\nwith soft time windows for a ﬂeet of vehicles. However,\ntheir multi-agent modeling relies on manually crafted rules\nin which vehicles take turns to make decisions. Furthermore,\nall vehicles share the same policy network, which makes the\nframework a single agent control indeed. In contrast, we develop independent policy networks without any manual rules\nin vehicle coordination and thus could enlarge the exploration space of all vehicle agents.",
        "problem formulation": "In this section, we provide the mathematical formulation of\ncooperative PDP.\nWith N delivery nodes and N pickup nodes are included\nin a given problem instance, the node set including the initial depot v0 where all vehicle starts can be represented as\nV = {v0, v1, ..., vN, vN+1, vN+2, ..., v2N, v2N+1}, and\nv2N+1 is the copy of the depot. For simpliﬁcation, we assume that vi(1 ≤ i ≤ N) has parcels to be picked-up and\nvi(N < i ≤ 2N) is the target to be delivered to. vi and vN+i\nform a corresponding pair. The spatial distances between\ndifferent nodes can be further represented as E = {eij},\nwhere 0 ≤ i ≤ 2N, 0 ≤ j ≤ 2N. Each pickup order has a\ndemand volume represented as di, where d0 = d2N+1 = 0.\nA demand is noted as a positive value for pickups and negative for deliveries, where di > 0 if 1 ≤ i ≤ N, di < 0\nif N + 1 ≤ i ≤ 2N and di = −di+N. All PDP tasks are\nassigned to K vehicles with Ck denoting the individual capacity of the k-th vehicle.\nLet xijk ∈ {0, 1} denote whether the vehicle k travels\ndirectly from node vi to node vj, and Ti as the arrival time\nat node vi. S ⊆ V denotes a consecutive routing sequence\nfrom v0 and ends at v2N+1 and does not include v0 in the\nmiddle, the cooperative PDP can be further formally formulated as follows:\nmin\nK\nX\nk=1\n2N\nX\ni=0\n2N+1\nX\nj=1\neijxijk,\n(1)\ns.t.\nK\nX\nk=1\n2N+1\nX\nj=1\nxijk = 1, ∀i ∈ [0, 2N],\n(2)\nK\nX\nk=1\n2N\nX\ni=0\nxijk = 1, ∀j ∈ [1, 2N + 1],\n(3)\nX\ni∈S′\ndi ≤ Ck, ∀S′ ⊆ S, ∀k ∈ [1, K],\n(4)\n2N+1\nX\nj=1\nxi,jk =\n2N+1\nX\nj=0\nxi+N,jk, ∀k ∈ [1, K], i ∈ [1, N],\n(5)\nTi ≤ Ti+N, ∀i ∈ [1, N].\n(6)\nThe overall objective is to minimize the total traveling distance of all vehicles. Constraint (2) and (3) guarantee that\neach node is visited and only visited once. (4) guarantees\nthat a vehicle never carries parcels out of its capacity limit.\n(5) satisﬁes the structural limit that each parcel should be\ndelivered by the same vehicle as it was picked up, and (6)\nguarantees that a pickup is always the precondition of its\nown delivery.",
        "methodology": "To solve the above formulated cooperative PDP, we take advantage of MARL to explicitly learn the effective cooperation among different vehicles. We develop an end-to-end\nframework, MAPDP, to generate partial solution sequence\ncontinuously by combining current routing actions of different agents together, as shown in Figure 2. Different agent\nnetworks share a common public context encoder to capture\nproblem instance representations and learn their own policy\nvia independent decoders. We further train the entire network by computing a common critic value as an approximation to the cooperation quality.\nMulti-Agent Reinforcement Learning Setting\nThe construction for PDP solutions can be formulated as\na sequence generation process. The sequence is completed\ngradually and can be modeled as a Markov Decision Process\n(MDP). We deﬁne the essential elements within as follows.\n• State: The state of agent k at step t includes the remaining available capacity Ct\nk the current traveling trajectory\nSt\nk. Speciﬁcally, the current location, i.e., the last node\nvisited by agent k is represented as vIt\nk, where It\nk is the\nnode index. Note that vI0\nk = v0 and C0\nk = Ck. In the\ncooperative PDP setting, we assume that all vehicles can\ncommunicate via a centralized control so that all states\nare fully observable.\nFigure 2: The overall model structure. Different agents share the same paired context embedding and context encoder structure,\nwhile learns individual policies using individual decoders.\n• Action: The action at step t for vehicle agent k is to determine a node as its next target, represented as v(k, t).\n• Transition: The transition between adjacent states is to\nreplace every agent to its target node as its current action. Then we update both the trajectory and the remaining capacity of each agent: St+1\nk\n= (St\nk; {vIt\nk}),\nCt+1\nk\n= Ct\nk − dIt\nk, where ; means concatenating the partial solution with the new selected node.\n• Reward: To optimize the overall routing solution quality, all agents share a common objective, which is to minimize the accumulated traveling distance of all agents in\nthe entire episode. In each decision step, the one-step reward rt\nk = −eIt\nk,It+1\nk\nis the negative of the length of the\nnewly established arc. The ﬁnal episode reward R can be\ncomputed as R =\nk=K\nP\nk=1\nT −1\nP\nt=0\nrt\nk, where T is the decision\nstep amount in a complete episode and I0\nk = 0 means\nthat all vehicles start from the depot v0.\nPaired Context Embedding and Context Encoding\nDue to the structural dependency in-between the nodes, it is\ncritical to measure the inter-node relationship between one\nand another and provide effective representations for such a\nTo better capture the structural dependency in-between, we\ndevelop a paired context embedding to better characterize\nthe attributes of different nodes.\nGiven the demand di of node vi and its original 2-D location information Li which could be used to calculate nodewise Euclidean distances directly, we formulate the original node embeddings by concatenating the two features and\nmap them into one dense vector as xi = W x[Li, di] + bx.\nWhen an agent decides whether to adopt a pickup request\nand head for its pickup location vi, it means that the agent\nalso has to visit the corresponding paired delivery vi+N afterwards. Thus, a complete representation on vi should consider the information of its paired delivery vi+N. Meanwhile, the agent is only allowed to visit vi+N when vi is\nalready visited. A policy evaluation upon vi+N does not rely\non vi any more. Motivated by this, we update the representation of pickup nodes by concatenating their paired deliveries, i.e. xi = [xi; xi+N]. We further compute the linear\nprojection of such augmented representations to generate the\nﬁnal paired context embeddings:\nh0\ni =\n\n\n\nW x\n0 xi + bx\n0,\ni = 0,\nW x\np [xi; xi+N] + bx\np,\n1 ≤ i ≤ N,\nW x\nd xi + bx\nd,\nN + 1 ≤ i ≤ 2N,\n(7)\nOther than intuitive context embeddings upon each node\ndirectly, we further generate encoded node embeddings considering the entire graph structure of the given instances. We\nadopt the Transformer Model (Vaswani et al. 2017) based\nencoding structure. The initial paired context embedding\nh0\ni is processed through L attention layers, each of which\nconsists of a multi-head attention layer (MHA), a skipconnection layer (He et al. 2016), a feed-forward layer, and\nbatch normalization (BN) layers (Ioffe and Szegedy 2015).\nFormally, each node embedding is updated in the ℓ-th layer\nas follows,\nˆhi = BN ℓ(hℓ−1\ni\n+ MHAℓ\ni(hℓ−1\n1\n, hℓ−1\n2\n, · · ·hℓ−1\n2N )),\n(8)\nhℓ\ni = BN ℓ( ˆhi + FF ℓ( ˆhi)).\n(9)\nThe core of an attention layer above is the multi-head attention block and can be deﬁned as follows:\nQh\ni , Kh\ni , V h\ni = W h\nQhi, W h\nKhi, W h\nV hi,\n(10)\nAh\ni = softmax(Qh\ni KhT /\np\ndk)V h\nj ,\n(11)\nMHAi = Concat(A1\ni , A2\ni , ..., AH\ni )WO,\n(12)\nwhere h = 1, 2, ..., H and dk = dh/H. H is to amount of\nattention heads, Qh\ni , Kh\ni , V h\ni\nare the query, key and value\nvectors respectively. WO is the projection matrix used to\nproject the ﬁnal MHA output. The ﬁnal embedding of each\nnode hi = hL\ni can thus be obtained from the consecutive\nL attention layers. Furthermore, we also generate a graph\nembedding h =\n1\n2N\n2N\nP\ni=0\nhi as the average of all nodes in the\nproblem instance, which represents the global aggregated information.\nIt is worthy to note that even though different agents\ngenerate their individual policies via different decoders, the\npaired context embedding and context encoding upon all\nnodes are shared. This is because, in a fully cooperative\nPDP scenario, different vehicle agents can share their observations under centralized agent control. Thus the representations of nodes from the environment are consistent and\ncan be learned jointly. In addition, learning a shared context\nencoder also accelerates the training stage signiﬁcantly and\nhelps reduce computational costs.\nCooperative Multi-Agent Decoders\nGiven both node-wise and global representations, each vehicle agent learns its own policy via its individual decoder\nnetwork, as depicted in Figure 2. Each agent decoder selects\nthe next node vIt\nk to visit at step t based on the current observations of all agents. The agents work in a cooperative way,\nand each one of them only visits a part of the overall pickupdelivery pairs within each episode for a complete solution\nconstruction.\nFor better cooperation between different agents, we maintain a communication layer to record the updated states of\ndifferent agents as follows:\nCommt = [hIt\n1; Ct\n1; hIt\n2; Ct\n2; ...; hIt\nK; Ct\nK]\n(13)\nLeveraging the up-to-date communication embedding, each\nagent decoder utilizes an MHA-based structure to evaluate the probability of selecting each node at step t.\nWe ﬁrst generate a special context embedding ht\nk,(c) =\n[h; hIt\nk; Ct\nk; Commt] for agent k to concatenate necessary\ninformation used in decision making, including the static\nglobal representation, the agent’s current state and the states\nof others. The context embedding ht\nk,(c) is further taken as\nthe single query vector and processed via another MHA\nlayer. Finally, the output gt\nk is used to compute the compatibility of choosing a node, and the ﬁnal probabilities are\ncomputed via softmax. Such a decoding process is as follows:\ngt\nk = MHAk,(c)(h1, h2, ..., h2N),\n(14)\nQt\nk, Kt\nk,i = WQ,kgt\nk, WK,khi,\n(15)\nut\nk,i = Dtanh(Qt\nk\nT Kt\nk,i/\np\ndk),\n(16)\npθk,φ(v(k, t)) = softmax(Maskt(ut\nk,i)),\n(17)\nwhere WQ,k and WK,k are the weight matrices of the last\nsingle-head attention, D=10 is the clip rate for better exploration (Bello et al. 2016). Maskt resets all compatibility\nvalue of unfeasible (already visited) nodes to −∞. In addition, θk is the parameter set for the agent decoder k, and φ\nis used. to parameterize the common paired context embedding and context encoder.\nDuring the cooperation of different agents, it is possible\nthat several agents make the same decision to the same node.\nHowever, due to the constraint that each node can only be\nvisited once as shown in equation (2) and (3), we design a\nspecial ﬂeet handler to resolve such a conﬂict. It randomly\nmaintains the action of one agent from all candidates to the\nnode and keeps the others stay at their current location vIt\nk.\nWhen all delivery and pickup nodes are visited and all agents\nreturn to the initial depot v0, the episode ends.\nTraining via Cooperative A2C\nThe Advantage Actor-Critic (A2C) (Konda and Tsitsiklis\n2000) is a well-known policy gradient approach that has\nshown its effectiveness. Leveraging the cooperation between\ndifferent vehicle agents, we design a special cooperative\nA2C to train the proposed MAPDP.\nBesides generating individual policies via individual decoders, we also formulate a centralized critic network\nV π\nω (s) to estimate the state-values. In detail, we compute\na weighted sum based on the output policy pθk,φ(v(k, t)) at\nstep t and the node-wise embeddings hi. Then the weighted\nsum vector of all agents is processed with linear projection\ninto a single vector vc with dc = 128. Finally, we obtain\na single critic value from vc through two dense layers. The\nentire critic network is parameterized by ω.\nWe then compute the loss of both networks and update\nθk, γ, ω as follows:\nAπ(s, a) = r(s, a) + V π\nω (s′) − V π\nω (s)\n(18)\n∇L(θk) = E∇θklogπθk,φAπ(s, a)\n(19)\n∇L(φ) = 1/K\nk=K\nX\nk=1\nE∇θk,φlogπθk,φA(s, a)\n(20)\n∇L(ω) = E∇ω(Aπ(s, a))2\n(21)",
        "experiments and evaluation": "In this section, we conduct extensive experiments on two\ndatasets to answer the following research questions:\n• RQ1: How does our proposed MAPDP perform on cooperative PDP compared to other heuristics and RL based\nmethods?\n• RQ2: How balance is the cooperation between different\nagents when incorporating the ﬂeet handler?\n• RQ3: How effective is the multi-agent formulation and\nthe communication embedding?\n• Random Generated Dataset. We ﬁrst generate a random\ndataset with randomly distributed node locations with demands for efﬁcient performance comparison. The location\nof node vi, L = (xi, yi) , is uniformly sampled from a\n5 × 5 square. Both the x and y coordinates are uniformly\ndistributed in (0,5). The demand volume of a pickup node\ndi is uniformly sampled from (1,10), and the capacity\nlimit of each vehicle is 10.\n• Real-World Dataset. We collect real-world data from\nan online logistic platform providing services in Guangdong, China, including more than 100 thousand order\nModel\nRandom Dataset\n2N = 20, K=2\n2N = 50, K=5\n2N = 100, K=10\nCost\nGap\nTime\nCost\nGap\nTime\nCost\nGap\nTime\nACO (Gambardella, Taillard, and Agazzi 1999)\n34.73\n39.60%\n6min\n79.94\n52.01%\n32min\n136.89\n53.86%\n51min\nTabu Search (Glover 1990)\n29.76\n19.67%\n7min\n64.57\n22.78%\n34min\n112.38\n26.31%\n51min\nOR-Tools (Google 2021)\n25.91\n4.18%\n4min\n54.64\n3.90%\n31min\n94.25\n5.93%\n49min\nRL-VRP (Nazari et al. 2018)\n26.79\n7.72%\n1s\n63.12\n20.02%\n5s\n101.13\n13.67%\n9s\nAM-VRP (Kool, van Hoof, and Welling 2019)\n26.64\n7.12.%\n1s\n67.41\n28.18%\n4s\n105.91\n19.04%\n8s\nMDAM (Xin et al. 2021)\n25.98\n4.46%\n8s\n67.24\n27.86%\n25s\n105.11\n18.14%\n51s\nMAPDP\n24.87\n0.00%\n1s\n52.59\n0.00%\n4s\n88.97\n0.00%\n7s\nModel\nReal-World Dataset\n2N = 20, K=2\n2N = 50, K=5\n2N = 100, K=10\nCost\nGap\nTime\nCost\nGap\nTime\nCost\nGap\nTime\nACO (Gambardella, Taillard, and Agazzi 1999)\n812\n30.13%\n6min\n1205\n35.39%\n34min\n2054\n20.47%\n53min\nTabu Search (Glover 1990)\n834\n33.65%\n6min\n1197\n34.49%\n34min\n2033\n19.24%\n51min\nOR-Tools (Google 2021)\n749\n20.03%\n4min\n1056\n18.65%\n31min\n1811\n6.22%\n50min\nRL-VRP (Nazari et al. 2018)\n714\n14.42%\n1s\n1130\n26.97%\n5s\n1842\n8.04%\n9s\nAM-VRP (Kool, van Hoof, and Welling 2019)\n661\n5.93%\n1s\n942\n5.84%\n4s\n1759\n3.17%\n9s\nMDAM (Xin et al. 2021)\n638\n2.24%\n8s\n941\n5.73%\n25s\n1733\n1.64%\n52s\nMAPDP\n624\n0.00%\n1s\n890\n0.00%\n4s\n1705\n0.00%\n7s\nTable 1: Overall performance comparison. The best result in each column is bolded. The improvement row shows the performance gain of our solution compared to the best baseline.\npairs within a month. Delivery and pickup orders are provided one day in advance so that the platform could provide solutions within a rather static scenario. The capacity\nof each vehicle is 6. We collect historical order instances\nthat were accomplished by a ﬁxed ﬂeet.\nIn the evaluation for both datasets, we construct experiments with three different scales, 2N = 20, 50, 100, and\nﬁx the agent amount with K = 2, 5, 10 accordingly. The\nnetworks are trained via Adam optimizer with L = 3,\ndk = 128, H = 8 and learning rate lr = 0.001. All experiments are conducted using Pytorch 1.7 on 4 2080Ti GPUs.\nPerformance Comparison\nBaselines\nWe ﬁrst compare our DRLPR with the three\nwidely recognized heuristics methods:\n• Ant Colony Optimization(ACO) (Gambardella, Taillard, and Agazzi 1999) constructs number of ant colonies\nare established to model and optimize the objective functions.\n• Tabu Search (Glover 1990) as a classic heuristic involves\nan enormous exploration space and keeps searching local\nsolutions in the neighborhood based on the current one.\n• OR Tools (Google 2021) is Google’s vehicle routing\nproblem solver that utilizes a set of metaheuristics.\nWe also compare the most up-to-date RL-based approaches:\n• RL-VRP (Nazari et al. 2018) proposed an RL-based\nframework using an encoder-decoder framework. The\nmodel utilizes an RNN model and generates the solution\nin a sequential manner.\n• AM-VRP (Kool, van Hoof, and Welling 2019) utilize the\ntransformer structure in both encoder and decoder. The\nagent network is trained via REINFORCE, and the results\nare decoded via a greedy method.\n• MDAM (Xin et al. 2021) updates the encoding continuously via an embedding glimpse layer and further generates solutions based on a set of decoders with an additional training loss considering the KL divergence.\nSince all RL baselines are only able to solve the standard CVRP, we manually add an additional mask for pickupdelivery constraints at the decoder of the three methods to\nguarantee that the output solutions are feasible. As for the\ncooperation among different vehicles, we set a vehicle assignment order so that the vehicles will take turns to be assigned with the decoded nodes.\nOverall Comparison\nThe comparison results of the overall performance are shown in Table ??. We report the total\ndistance, the gap of each method to the best among all, and\nthe time spent during inference in both two datasets with\nthree different customer scales. The best results are bolded.\nIn terms of the solution quality, we demonstrate that\nMAPDP outperforms all other baselines in all experiment\nsettings. The closest baseline is MDAM in the Real-World\ndataset with 2N = 100 but still suffers from a 1.64% performance gap from MAPDP. The state-of-the-art RL-based\nmethods are less effective in the cooperative PDP settings\ndue to 1) the paired dependency are not explicitly represented, and 2) the cooperation between different vehicles\ncan not be modeled. In fact, manually ﬁxing the assignment\norder of different vehicles greatly limits the potential of exploring more solutions, and thus inﬂuences the effectiveness\nof reinforcement learning. The cooperative multi-agent decoding process alternatively accepts more possible solution\nexplorations. Thus, MAPDP shows its effectiveness in the\npractical cooperative PDP scenarios.\n(a) Random Dataset.\n(b) Real-World Dataset.\nFigure 3: Case studies on vehicle cooperation analysis from\ntwo datasets.\nBesides the signiﬁcant performance on solution quality,\nMAPDP also shows fast computation speed in solution generation. Compared to the heuristic baselines, we notice that\nMAPDP is able to infer solutions faster with a maximum\nof 400+ times with 2N = 100. In real-world application\nscenarios, the logistic order requests may change continuously and even rapidly, including previous orders canceled\nand new orders accepted. A fast response-ability is of great\nsigniﬁcance for an online policy-making system to be adaptive to any new changing dynamics.\nCase Studies on Vehicle Cooperation\nTo further investigate the cooperation between different vehicles in solving cooperative PDP, we compute the individual traveling distance of each vehicle with 2N = 50\nin both datasets in two case studies, as shown in Figure 3.\nBesides, since making decisions simultaneously might meet\nconﬂicts with other agents, the ﬂeet handler is used to deal\nwith such conﬂicts and keep the other vehicles halting where\nthey are. We also compute and report the halting ratio =\nhaltingtimes/T of each agent k.\nWe notice that the balance among different agent’s workload is greatly affected by the dataset. In the random dataset\nwhere all nodes distribute randomly and are centered within\nthe unit square, different agent decoder ends up with similar\nworkloads. However, in the real-world dataset where nodes\ndistribute unbalanced and even some locate distantly, agents\nshow signiﬁcant differences. This is because, in the same\ndecision step, some vehicles might be assigned with one of\nthe distant nodes, which greatly increases their total traveling distance. The agent with the longest traveling distance\nwe investigated traveled 212 spatial units, while the one with\nthe shortest trajectory only traveled 103 units. Meanwhile, in\nboth datasets, we demonstrate that the agent that was halted\nthe most ends up with the lowest workload. This is because\nsuch an agent skips the decision step while others continue\nto travel on the instance graph.\nAblation Studies on Multi-Agent Design\nWe further investigate how effective each part of the\nMAPDP design is as shown in Tabel 2. Two additional variants of MAPDP are trained and evaluated following the\nsame evaluation protocols: MAPDP-SP stands for the simpliﬁed model where all agent decoders share the same parameters. Thus all agents become homogeneous to generate\nindividual solutions. MAPDP-NC stands for the multi-agent\nframework without consideration on the communication embedding. It generates context embeddings only based on the\nagent’s own state and the global graph embedding.\nDataset\nModel\n2N=20\n2N=50\n2N=100\nRandom\nMAPDP\n24.87\n52.59\n88.97\nMAPDP-SP\n24.99\n53.61\n89.78\nMAPDP-NC\n26.89\n68.78\n108.12\nReal\nMAPDP\n624\n890\n1705\nMAPDP-SP\n639\n943\n1721\nMAPDP-NC\n731\n1033\n1896\nTable 2: Ablation study on the multi-agent structure design.\nResults show that both MAPDP variants are outperformed\nby the original MAPDP. MAPDP-SP is slightly outperformed and is still superior to many other baselines. This\nshows that the multi-agent modeling along with a comprehensive communication mechanism is the core of performance improvement, while heterogeneous training can further slightly improve its effectiveness based on pure parameter sharing. However, MAPDP-NC is even outperformed\nby many other baselines. This is because, in a fully cooperative scenario, up-to-date communication with other agents\nis critical to effective coordination.",
        "conclusion": "In this paper, we propose a multi-agent reinforcement learning framework to solve cooperative pickup and delivery\nproblems (MAPDP). We design a special paired context embedding to explicitly represent the structural dependency\namong different nodes within the instance graph. We develop special cooperative multi-agent decoders to learn the\nindividual policies of different agents. We also design a cooperative A2C algorithm for the integrated model training,\nwhere a joint critic value net estimates the state value. Extensive experiments demonstrate that MAPDP outperforms\nother baselines in all experiment settings and shows signiﬁcant computation speed during solution inference.",
        "summary_en": "Cooperative Pickup and Delivery Problem (PDP), as a variant of the typical Vehicle Routing Problems (VRP), is an important formulation in many real-world applications, such as on-demand delivery, industrial warehousing, etc. It is of great importance to efficiently provide high-quality solutions of cooperative PDP. However, it is not trivial to provide effective solutions directly due to two major challenges: 1) the structural dependency between pickup and delivery pairs require explicit modeling and representation. 2) the cooperation between different vehicles is highly related to the solution exploration and difficult to model. This paper proposes a novel multi-agent reinforcement learning based framework to solve the cooperative PDP (MAPDP). First, the paper designs a paired context embedding to well measure the dependency of different nodes considering their structural limits. Second, the paper utilize cooperative multi-agent decoders to leverage the decision dependence among different vehicle agents based on a special communication embedding. Third, the paper designs a novel cooperative A2C algorithm to train the integrated model. This paper conducts extensive experiments on a randomly generated dataset and a real-world dataset. Experiments result shown that the proposed MAPDP outperform all other baselines by at least 1.64% in all settings, and shows significant computation speed during solution inference.",
        "summary_zh": "这篇论文介绍了一个基于多代理强化学习的新型框架，来解决合作 PDP（MAPDP）问题。PDP是典型车辆路径问题（VRP）的一种变体，在许多实际应用中非常重要，但是提供有效的解决方案面临两个挑战：（1）取货和送货对之间的结构依赖性需要明确的建模和表示。（2）不同车辆之间的合作与解决方案的探索高度相关，难以建模。为了解决这些问题，作者提出了该框架，设计了配对上下文嵌入来有效衡量不同节点之间的依赖关系，利用合作式多智能体解码器来利用不同车辆智能体之间的决策依赖关系，并设计了一种新颖的合作A2C算法来训练集成模型。在随机生成的数据集和真实世界数据集上进行了广泛实验，结果表明MAPDP在所有情况下至少比其他基准方法提高了1.64%，并且在解决方案推理过程中显示出显著的计算速度。"
    },
    {
        "title": "The Multi-Agent Transportation Problem",
        "abstract": "We introduce the multi-agent transportation (MAT) problem, where agents have to transport containers from their starting positions to their designated goal positions. Movement takes place in a common environment where collisions between agents and between containers must be avoided. In contrast to other frameworks such as multi-agent pathfinding (MAPF) or multi-agent pickup and delivery (MAPD), the agents are allowed to separate from the containers at any time, which can reduce the makespan and also allows for plans in scenarios that are unsolvable otherwise. We present a complexity analysis establishing the problem’s NP-completeness and show how the problem can be reduced to a sequence of SAT problems when optimizing for makespan. A MAT solver is empirically evaluated with regard to varying input characteristics and movement constraints and compared to a MAPD solver that utilizes conflict-based search (CBS).",
        "introduction": "In the multi-agent pathfinding (MAPF) problem (Stern et al.\n2019), the objective is to have a number of agents move inside a given environment from designated start to designated\ngoal positions while avoiding collisions. The environment\nis modeled using an undirected graph in which movement\ntakes place in discrete time steps. A collision occurs whenever two agents occupy the same node at the same point in\ntime or when they use an edge at the same time step.\nA generalization of this problem is the multi-agent pickup\nand delivery (MAPD) problem (Ma et al. 2017; Liu et al.\n2019), which models a warehouse logistics setting where\npackets have to be picked up and delivered. In addition to\nfinding collision free trajectories, transportation tasks have\nto be assigned to agents and the agents have to fulfill them.\nIn a production logistics setting, some of the assumptions of the MAPD setting might not apply. For example,\na transported payload might be so large that it is an obstacle after being delivered to its target position. This happens,\ne.g., when containers are used to deliver large workpieces\nto construction sites. Because of that, it may be necessary\nto temporarily move objects out of the way. In order to deal\nwith such issues, we introduce the multi-agent transportation (MAT) problem. In MAT, a set of agents and a number of objects called containers are given. These containers\nshall be transported to their respective goal positions by the\nagents, whereby neither agents nor containers should collide\nwith each other. In order to solve such a problem, we do not\nrequire a fixed assignment between agents and containers as\nin the MAPD setting.\nOne such scenario is depicted in Figure 1. Here, in order\nto get both containers C1 (yellow) and C2 (blue) to their\nrespective goal positions (colored and marked accordingly),\nthe agent A1 has to move container C1 out of the way first,\nbefore it can bring container C2 to its goal position. Finally,\nit needs to move C1 back to its goal position.\né\né\nC1\nC2\nA1\nC1\nC2\nt = 0\né\né\nC1\nC2\nC1\nC2\nt = 2\né\né\nC1\nC2\nC1\nC2\nt = 6\né\né\nC1\nC2\nC1\nC2\nt = 9\nFigure 1: MAT example with a single agent A1 and two\ncontainers C1 and C2. The black crosses indicate blockades\nthat can never be occupied by any agent or container. We\nshow the state at different points in time when executing the\n(unique optimal) plan.\nMAT is a generalization of both MAPF and (offline)\nMAPD, the quintessential differences being that containers\ncan collide with each other and that agents and containers are free to separate at any time. In so far, the problem\nhas some similarity to the package-exchange robot-routing\nproblem (Ma et al. 2016). However, in that framework, packages are not dropped and picked up again, but exchanged\nacross edges. In this framework all instances can be solved.\nAnother problem that appears similar to MAT is the multiagent collective construction problem (Lam et al. 2020). In\nthis framework, blocks can be dropped and picked up again,\nbut the environment is three-dimensional and there are no\ncollisions between blocks because agents can step over existing blocks.\nThe rest of the paper is structured as follows. In the\nnext section, we will formally define the MAT problem\nand provide some illustrative examples. We will show NPcompleteness of MAT and provide an algorithm for solving\nit by reduction to a series of SAT problems. In doing so, we\nuse similar reductions as have been used for solving MAPF\n(Surynek 2014; Surynek et al. 2016; Bart´ak et al. 2017). We\nempirically evaluate the runtime of our implementation with\nregard to varying input characteristics, such as the number\nof containers or the size of the environment, and movement\nconstraints, such as whether or not the containers block each\nother. Finally, we will compare it with a MAPD solver that\nutilizes conflict-based search (CBS).",
        "problem definition": "The multi-agent transportation problem (MAT) is formally\ndefined as follows. Given\n• an undirected, connected, simple graph (V, E),\n• a set of agents A,\n• a set of containers C,\n• starting positions s0 : A ∪ C → V , and\n• goal positions g : C → V ,\nwe want to find a legal sequence s0, . . . , sk of MAT states\nsuch that sk|C = g. The latter meaning that all of the containers are at their goal position in the final state.\nHere, a state is any function st : A ∪ C → V which is\ninjective on A and injective on C. A sequence of states is\nconsidered legal if for any st and st+1 the following constraints are met.\n1. For any a ∈ A, st(a) = st+1(a) or (st(a), st+1(a)) ∈\nE, i.e. an agent either stays in place or moves along an\nedge.\n2. For any c ∈ C, if st(c) ̸= st+1(c), then there exists an\na ∈ A such that (st(a), st+1(a)) = (st(c), st+1(c)), i.e.\na container can only move together with an agent.\n3. For any a, a′ ∈ A where a ̸= a′, (st(a), st+1(a)) ̸=\n(st+1(a′), st(a′)), i.e. no two agents move along the\nsame edge at the same time.\nOur constraints allow parallel and cyclic movement using\npreviously occupied nodes. Using the terminology of Stern\net al. (2019), we assume vertex and swapping conflicts, but\nno following or cycle conflicts.\nWe call a legal sequence s0, . . . , sk of MAT states a MAT\nplan. The length k of such plan is the plan’s makespan. In\nthe context of this paper, a plan is considered optimal if its\nmakespan is minimal.\nExamples\nAn example instance with two agents and two containers is\nshown in Figure 2. For this instance, there exists a unique\noptimal solution with makespan 5, which we indicate with\narrows for the movement of the agents. If the containers\nwere not allowed to be transported by multiple agents, as\nin MAPD, the minimal makespan would be 7.\né\né\nC1\nC2\nA1\nA2\nC1\nC2\nFigure 2: MAT example instance with a unique optimal solution\nMoreover, there are cases in which collaboration is a\nmust. Consider a setting as depicted in Figure 3, where the\nagents block each other. Here we can have a MAT plan, but\nno MAPD plan. The container must be handed over from\none agent to the other at the node x, the ‘counter’.\nC1\nC1\nA2\nx\nFigure 3: Over-the-counter example",
        "computational complexity": "MAT appears to be computationally harder than MAPF.\nHowever, we will show that MAT is nevertheless NPcomplete.\nIn order to show NP-hardness, one can reduce MAPF with\ncyclic rotations, where the optimizing variant is known to be\nNP-hard (Surynek 2010, Theorem 1), to MAT. This can be\ndone by viewing a MAPF agent as a pair of a MAT agent\nand a MAT container starting at the same position.\nLemma 1. Deciding whether there exists a MAT plan with\nmakespan k is NP-hard.\nProving NP-membership is more involved. We have to\nshow that plans can be polynomially bounded. For that purpose we will reduce MAT to a problem where so-called pebbles have to be moved on an undirected, connected graph,\nwhereby only one pebble can occupy one node at a given\npoint in time. The problem treated in the work of Kornhauser, Miller, and Spirakis (1984), the pebble motion (PM)\nproblem, allows moves that take one pebble from its current\nposition to an adjacent unoccupied position, so-called simple moves. The pebble motion with rotations (PMR) problem (Yu and Rus 2014) additionally allows the pebbles to\nmove simultaneously on cycles of the graph. Such moves\nare called rotations. A PMR instance is given as a triple\n⟨G, S, D⟩, where G = ⟨V, E⟩ is a graph, S is the start configuration, and D is the goal configuration.\nIn contrast to PMR, in MAT we may not be able to rotate\ncontainers on large cycles, because we don’t have enough\nagents that can transport the containers simultaneously. For\nthis reason, we introduce the pebble motion with limited\nrotations (PMLR) problem. A PMLR instance is a PMR\ninstance with an additional parameter k that specifies the\nlargest possible rotation.\nIn order to establish a polynomial bound for this problem,\nwe utilize group theory. We start by introducing the necessary definitions.\nA permutation is a bijective function σ: X → X. We say\nthat a permutation is an m-cycle if it exchanges distinct elements x1, . . . , xm in a cyclic fashion, i.e., σ(xi) = xi+1 for\n1 ≤ i < m, σ(xm) = x1 and σ(y) = y for all y /∈ {xi}m\ni=1.\nSuch cyclic permutation is denoted (x1 x2 · · · xm).\nThe composition of two permutations σ and τ, denoted\nστ, is the function mapping x to τ(σ(x)). ϵ is the identity,\nwhich maps every element to itself, and σ−1 is the inverse\nof σ, i.e., σ−1(y) = x if and only if σ(x) = y. The k-fold\ncomposition of σ with itself is denoted σk.\nWe will also consider the conjugate of σ by τ, denoted στ,\nwhich is defined to be τ −1στ. We will use exponential notation as in the book by Mulholland (2021): σα+β := σασβ.\nGiven a set of permutations T, the permutation group generated by T, G, is the closure of T under composition. Such\nG forms a group under composition with ϵ being the identity element and σ−1 being the inverse of a given element σ.\nThe diameter of G then is the maximum over the number of\ncompositions required to generate each element of G.\nIn our context, two permutation groups are of particular\ninterest. One is Sn, the symmetric group over n elements,\nwhich consists of all permutations. Sn is generated by the\nset of all 2-cycles with diameter O(n2). Another group is\nAn, the alternating group over n elements, which is the set\nof all permutations generated by compositions of cycles of\nodd length, so-called even permutations. A generator of this\ngroup is the set of all 3-cycles. An is a subgroup of Sn,\ndenoted An ≤ Sn, i.e., it contains only permutations from\nSn and is closed under composition and inverse.\nA permutation group G is said to be k-transitive if for\nall pairs of k-tuples (x1, . . . , xk), (y1, . . . , yk), there exists\na permutation σ ∈ G such that σ(xi) = yi, 1 ≤ i ≤ k. In\ncase of 1-transitivity we simply say that G is transitive.\nGiven a PMLR instance I = ⟨G, S, D, k⟩ and a set X of\nnodes of G with |X| = p, where p is the number of pebbles,\nwe can transform S to S′ such that in S′ exactly the nodes in\nX are occupied by some pebble using O(n2) simple moves\n(Kornhauser 1984, Subsection 3.1). Therefore, w.l.o.g., we\ncan assume that in every PMLR state we consider the same\nnodes are occupied. Let G(I) be the permutation group generated by simple moves and rotations with at most k pebbles\nin I. Up to a polynomial factor, all possible configurations of\nI can be obtained with a number of operations that is limited\nby the diameter of G(I).\nLemma 2. Let I = ⟨⟨V, E⟩, S, D, k⟩ be a PMLR instance\nsuch that G(I) is transitive. Then G(I) has a polynomial\ndiameter.\nAs in the work of Kornhauser et al. (1984), we consider\nthe transitive substructures that are formed by the subgraphs\ncontaining only pebbles from S that form a transitive permutation group. The subgraph contains all nodes that are reachable by this set of pebbles. The solutions for these transitive\nsubstructures forms the overall solution, if it exists.\nCorollary 1. Let I = ⟨⟨V, E⟩, S, D, k⟩ be a PMLR instance. Then G(I) has a polynomial diameter.\nProof of Lemma 2. Let n = |V |. We will consider only\ngraphs with n > 7. For graphs with n ≤ 7, the diameter\nis obviously constant.\nIf ⟨V, E⟩ is an n-cycle, or cycle graph, the diameter of\nG(I) is O(n2), if only simple moves are possible. It is O(n)\nif the cycle can be rotated.\nFor the remaining cases, we proceed with a case analysis.\nCase p = n: In this case the graph is entirely filled with pebbles. This rules out simple moves, as they require an empty\nnode. Using transitivity, we can infer that the graph is twoedge connected and that every node is part of some cycle.\nIf we are able to rotate all of the cycles, the lemma immediately follows from (Yu and Rus 2014, Proposition 6). We\nwill therefore assume otherwise.\nWe will show that in this case, (i) one can generate a 3cycle from polynomially many rotations and that (ii) the permutation group is 2-transitive. By (Driscoll and Furst 1983,\nDefinition 2.6, Theorem 3.2), this implies that the diameter\nis polynomial.\nLet us first consider a graph such as in Figure 4, which we\nwill call basic graph. W.l.o.g., we will assume that the left\nFigure 4: A basic two-edge-connected graph (Yu and Rus\n2014, Figure 8)\ncycle is not larger than the right one, i.e., n1 ≤ n3. Further,\nwe assume that one of the ni’s is not equal to 2. The case\nn1 = n2 = n3 = 2 will be taken care of later.\nThe available cyclic permutations corresponding to rotations are α, β and their inverses α−1, β−1, where\nα = (b1 · · · bn2 an1 · · · a1),\nβ = (b1 · · · bn2 cn3 · · · c1).\nFor (i) let the desired 3-cycle be\nσ :=\n\n\n\n\n\nα,\nif n1 = 1\nβ−1 α β α−1,\nif n2 = 1\n(β−1 α β α−1)α−1(ϵ+β2) =: χ\notherwise.\nFor the first case, note that n1 = 1 implies n2 = 2. This\nfollows from the fact that if n2 = 1 the graph would not be\ntwo-edge connected and if n2 > 2 all rotations would be\npossible, violating previous assumptions. For the third case,\nwe obtain the following 3-cycle:\nχ = ((a1 b1)(bn2 cn3))α−1(ϵ+β2)\n= ((a1 a2)(bn2−1 cn3))ϵ+β2\n= (a1 a2)(bn2−1 cn3)(a1 a2)(cn3 cn3−2)\n= (bn2−1 cn3−2 cn3)\nand therefore\nσ =\n\n\n\n(a1 b1 bn2),\nif n1 = 1\n(a1 cn3 b1),\nif n2 = 1\n(bn2−1 cn3−2 cn3)\notherwise.\nFor (ii) we show that for any given elements x1, x2 there\nexists a sequence of moves such that x1 ends up at a1 and\nx2 at b1. Start by moving x1 onto c1, which is always possible because G(I) is transitive. If x2 now lies on the right\ncycle, there exists some k such that βα−1βk gives a configuration as required. Otherwise, there exists some k such that\nαkβα−1 does the job.\nIf we can move any elements x1, x2 onto a1, b1, we can\nalso move a1, b1 to arbitrary positions by using the inverse\noperations. By concatenating such sequences of (inverted)\noperations we can ultimately move arbitrary x1, x2 to arbitrary y1, y2 and G(I) is 2-transitive.\nAs mentioned, (i) and (ii) imply that the diameter is polynomial in this case. If the basic graph is embedded in any\ntransitive graph, then our arguments from above apply to any\ntwo-edge connected graph with n > 7, provided it is not a\ncycle graph (which we covered in the beginning) or a graph\nmade out of basic graphs such that n1 = n2 = n3 = 2.\nIn the latter case, the two-edge connected graph must contain one of the subgraphs as shown in Figure 5 (note that\nadding anything different to the n1 = n2 = n3 = 2 graph\nresults in one of the cases above with one ni ̸= 2). A 3-cycle\nfor each case can now be constructed as follows:\nσ :=\n\n\n\nδ−1 β δ2 α−1 β α2\n= (x2 x4 x7),\n(I)\nζ β−1 ζ−1 β α2\n= (x2 x3 x5),\n(II)\nη β−1 η−1 α−1 β α\n= (x1 x3 x7).\n(III)\n2-transitivity can be established as above, so that also in this\ncase, we have a polynomial diameter.\nCase p < n: If the graph is biconnected and n > 7, then we\nknow by (Kornhauser, Miller, and Spirakis 1984, Theorem\n1a, 1b) that any even permutation can be generated using\npolynomially many moves. This implies that G(I) ≥ Ap.\nAs shown in (Yu and Rus 2014, Theorem 2, last paragraph)\nit follows that G(I) has polynomial diameter.\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nI\nα\nβ\nδ\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nII\nα\nβ\nζ\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nIII\nα\nβ\nη\nFigure 5: All relevant minimal extensions of basic graphs\nwith n1 = n2 = n3 = 2\nIf the graph is not biconnected, then it must be separable.\nFor any subcomponent of G that is transitive w.r.t. simple\nmoves we know by (Kornhauser, Miller, and Spirakis 1984,\nTheorem 1) that any permutation can be generated with\npolynomially many compositions. For any minimal seperable subcomponent containing a rotatable cycle we show that\nalso any permutation can be generated with polynomially\nmany compositions.\nConsider the component as in Figure 6. As there is at least\none empty node, it is possible to arrive at a configuration\nsuch that an empty node is adjacent to the cycle. Now we\ncan move a pebble of choice out of the cycle and back in at a\ndifferent position using linearly many operations. With that,\nany two pebbles can be swapped and therefore any permutation can be generated using polynomially many compositions.\ne\nx4\nx3\nx1\nx2\n1\n2\n3\n4\n4\n4\n4\nFigure 6: Swapping pebbles x1 and x2 (adapted from (Kornhauser, Miller, and Spirakis 1984, Figure 10))\nBy dividing G into partially overlapping components of\nthese two kinds, we can compose any 2-cycle from linearly\nmany permutations of the individual components. It follows\nthat any σ ∈ G(I) = Sp can be generated using polynomially many compositions.\nUsing this lemma, showing NP-membership is straightforward.\nLemma 3. Deciding whether there exists a MAT plan with\nmakespan k is in NP.\nProof. Membership follows if we can show that all MAT\nplans can be polynomially bounded. Let us assume we have\nn nodes in the graph and a agents.\nCase a = n: In this case the agents block each other such\nthat simple moves are not possible. The problem can be reduced to PMR with agents as pebbles such that the result\nfollows from (Yu and Rus 2014, Theorem 7).\nCase a < n: The problem can be reduced to PMLR with\ncontainers as pebbles. Corollary 1 applies and we obtain a\npolynomial upper bound for the movements of the containers. The additional movements of the anonymous agents has\nan overhead of at most O(n) for each move of a container\nsuch that the total number of movements is polynomially\nbounded.\nFrom Lemma 1 and Lemma 3 we obtain the following.\nTheorem 1. Deciding whether there exists a MAT plan with\nmakespan k is NP-complete.",
        "algorithmic solution": "For a candidate makespan T ∈ N, we build a propositional\nformula φ(T) whose satisfying assignments constitute exactly the successful plans with makespan at most T. In order to find the optimal makespan, exponential search is employed, a method that has also been used in SAT planning\n(Rintanen 2014). The exponential search proceeds by first\ncomputing an upper bound for the optimal makespan by\njumping to successively higher candidate makespans. The\ndistance between the tested makespans can be adjusted via\nthe jumping parameter f and an initial lower bound may be\nsupplied, which can be obtained from preprocessing, as described below. Once an upper bound has been determined,\nbinary search for the minimal T s.t. φ(T) is satisfiable is\nperformed. The algorithm is sketched in Algorithm 1.\nAlgorithm 1 Exponential search for the optimal makespan\nprocedure MAKESPAN(lower bound=0, f=2)\nT ← lower bound\nwhile ¬ SAT(φ(T)) do\nlower bound ← T + 1\nT ← max{⌈T · f⌉, 1}\nend while\nupper bound ← T\nreturn binary search(...)\nend procedure\nThe formula can be built using the propositional variables\n{st(o) = v}t∈{0,...,T },o∈A∪C,v∈V analogously to the previously defined MAT states. It is defined in a series of six\naxioms shown in Figure 7. Axiom (1) defines initial and final state. Axiom (2) enforces that st is functional. Here and\nlater, we utilize the AMO (at-most-one) constraint, which\nstates that at most one of a given set of propositional variables can be true. There exist many possible encodings of\nAMO (see the paper by Frisch and Giannaros (2010) for an\n^\nc∈C\nsT (c) = g(c) ∧\n^\no∈A∪C\ns0(o) = s0(o)\n(1)\nT^\nt=0\n^\no∈A∪C\nAMO ({st(o) = v}v∈V )\n(2)\nT^\nt=0\n^\nv∈V\nAMO ({st(a) = v}a∈A) ∧ AMO ({st(c) = v}c∈C)\n(3)\nT −1\n^\nt=0\n^\no∈A∪C\n^\nv∈V\n(st(o) = v → st+1(o) = v ∨\n_\n(v,w)∈E\nst+1(o) = w)\n(4)\nT −1\n^\nt=0\n^\n{v,w}∈E\n^\na,b∈A\na̸=b\n¬(st(a) = v ∧ st+1(a) = w\n∧ st(b) = w ∧ st+1(b) = v)\n(5)\nT −1\n^\nt=0\n^\nc∈C\n^\nv∈V\n^\n(v,w)∈E\n(st(c) = v ∧ st+1(c) = w →\n_\na∈A\nst(a) = v)\n∧\n^\na∈A\n(st(c) = v ∧ st+1(c) = w ∧ st(a) = v\n→ st+1(a) = w)\n(6)\nFigure 7: Axioms\nextensive discussion) such as the sequential encoding from\nthe work by Sinz (2005) that we used, which is of size linear in the number of given variables, but introduces linearly\nmany auxiliary variables. Axiom (3) ensures that st is injective on A and injective on C. Axiom (4) states that objects\neither stay at a node or move to an adjacent node. Axiom (5)\nforbids that two agents use one edge at the same time, and\naxiom (6) finally describes the restrictions on transporting a\ncontainer.\nPreprocessing\nWe employ a preprocessing mechanism similar to the one\nused by Bart´ak et al. (2017), which allows us to determine a\nlower bound on the effective distance d(o, v) of every object\no ∈ A∪C to every node v ∈ V . For an agent a this is just the\ndistance between s0(a) and v in the graph. For a container c,\nbecause it cannot move by itself, d(c, v) is increased by the\nminimum distance between s0(c) and any agent (in case v ̸=\ns0(c)). Using the fact that t < d(o, v) implies st(o) ̸= v, we\ncan shortcut the propositional reasoning with these implied\nunit clauses.\nAdditionally, preprocessing yields the lower bound on the\nmakespan max{d(s0(c), g(c))}c∈C, which can be utilized\nto head start the exponential search. The distances can be\ncomputed in low-order polynomial time using breadth-firstsearch.",
        "experimental results": "In order to evaluate the feasibility of our approach, we performed three sets of experiments. First, we analyzed the\nruntime necessary to solve varying MAT instances in order\nto get an idea about the scalability of our approach. Second, we compared MAT solving with solving simplifications of MAT, namely without container conflicts (assuming\nthat packets are small) and with a fixed association between\ncontainers and agents (after target assignment). Third, we\ncompared our SAT-based implementation with a CBS-based\n(Sharon et al. 2015) MAPD implementation on instances\nwith as many agents as there are containers.\nIn order to run our experiments, we randomly generated\nquadratic grid graphs from the following parameters:\n• g, the side length of the grid,\n• b, the percentage of grid cells that are blockades,\n• a, the number of agents,\n• c, the number of containers.\nAll starting and goal positions are distributed among the free\ncells uniformly at random ensuring only that the start configuration is legal and that each container could reach its destination if we would allow the objects to pass over each other.\nWe generated 10 such solvable problem instances for each\ncombination 4 ≤ g ≤ 12, b ∈ {10, 20} and 1 ≤ a, c ≤ 10,\nwhere we call a given (g, b, a, c)-tuple a parameter set.\nWe used the SAT solver Cryptominisat (Soos, Nohl, and\nCastelluccia 2009), which can be used incrementally allowing us to retain all axioms concerning previous time steps\nwhen jumping to a higher T. All tests presented in this paper were conducted using the solver’s default configuration\nand 4 threads on Intel Xeon Gold 6242 processors. The program was allowed to run for at most 10 minutes in total per\ninstance and to use at most 16 GB of memory. We make the\nsource code and all test results available online1.\nInput Characteristics and Scalability\nIn the following we will analyze the program’s runtime relative to the parameters. We aim at identifying MAT scenarios\nthat are more or less difficult to solve for our program or,\nperhaps, in general.\nThe generated instances are quite diverse, which is also\nreflected in the resulting runtimes which are spread over several orders of magnitude. First of all, the size of the environment, which can vary by factors of up to 9 between g2 = 16\nand g2 = 144, is a bad indicator for the runtime.\nThe runtime and solvability correlate much more strongly\nwith the number of objects. In Figure 8 we show the mean\nruntimes and the percentage of unsolved instances by the\nnumber of agents and the number of containers.\nIt is striking that the most difficult instances including almost all of the unsolved ones are such that there are less\nagents than containers, i.e. a < c. In this case any additional\nagent can reduce the runtime significantly. In case a ≥ c,\nhowever, a further increase of the number of agents seems\nto make the instances slightly more difficult.\n1https://github.com/bachorp/mat\nFigure 8: Effect of the number of agents a and the number\nof containers c on the solvability and the mean (n = 180)\nruntime of the solved instances. The lines are dashed if less\nthan half of the instances were solved and values for which\nthe number of agents equals the number of containers (a =\nc) are indicated with a dot. The black line denotes the mean\nruntime when solving MAPF instances with the respective\nnumber of agents.\nWe also modified our solver to solve MAPF. Note that a\nMAT instance with a = c (runtimes denoted with a fat dot\nin Figure 8) does not necessarily correspond to a MAPF instance (runtimes denoted with a black line in Figure 8). Most\nof the time, agents and containers will start at distinct positions such that there is no clear pairing of agents and containers. Finding the optimal, possibly temporary, pairings in addition to non-colliding paths comes at a cost that can clearly\nbe seen by the big difference between the MAT and MAPF\nruntimes.\nComparison with MAT Variants\nAs mentioned above, the property that containers can collide leads to more possible conflicts and probably to higher\nruntime and makespan of the generated plans. This can be\nseen in the two scatter plots in Figure 9. Container collisions\nare a major source for higher makespan and higher overall\nruntimes. In other words, container collisions make it much\nharder to solve the problem.\nFigure 9: CPU runtime and makespan for non-blocking containers vs. MAT\nAnother source of difficulty might be the fact that there\nis no fixed assignment of agents to containers. As we have\ndemonstrated in the beginning, this can be necessary to find\na solution or allow for a shorter makespan.\nWe compared MAT with a variation, where a container\ncannot be transported by multiple distinct agents. As can\nbe seen in Figure 10, for the CPU time, there is no clear\ntrend. On the other hand, there are apparently a number of\ninstances that allow for a shorter makespan if the agents are\nallowed to cooperate in the transportation of the agents.\nFigure 10: CPU runtime and makespan for MAT with a fixed\nagent per container vs. MAT\nComparison with a CBS-based MAPD Solver\nAlthough we would have loved to compare our MAT implementation with state-of-the-art implementations of a CBSbased MAPD solver, we were not successful in identifying\na solver that exactly fits our setting and that optimizes for\nmakespan.\nWe therefore opted to adapt conflict-based search with\noptimal task assignment (CBS-TA) (H¨onig et al. 2018) to\nour definition of optimizing MAPD. To do so, we (1) enhanced the low level search to search consecutive paths from\nan agent to an container’s start and from that container’s\nstart to its goal and (2) modified the algorithm to optimize\nfor makespan. To find makespan-optimal task assignments,\nwe checked for each possible makespan c whether there exists a task matching consisting only of assignments with a\ncost lesser or equal to c using the Hopcroft-Karp algorithm\n(Hopcroft and Karp 1973). The remainder of the required\nmodifications is straightforward. The theoretical properties\nof CBS-TA, i.e. completeness and optimality under the respective objective function, still apply and can be proven\nanalogously.\nFigure 11: CPU runtime and success rate for CBS-MAPD\nvs. SAT-MAPD\nWe compared this version of CBS-TA, which we call\nCBS-MAPD, with a MAPD variant of MAT, which we call\nSAT-MAPD, where assignments are fixed and containers are\nnon-blocking, which is equivalent to the problem solved by\nCBS-MAPD. As can be seen in Figure 11, CBS-MAPD\nshows much faster runtimes in most cases. This can be attributed to the search-based nature of the approach, which\nworks especially fast on relatively easy instances. However,\noverall SAT-MAPD does not fall behind in terms of success\nrate and even seems to gain the advantage with an increasing\nnumber of agents.",
        "conclusion": "We introduced MAT, the multi-agent transportation problem. It is a generalization of the well-known MAPF and\nMAPD problems. One main difference to MAPD is that\nthe transported payloads, called containers, can collide with\neach other. This addresses the requirement in some applications, e.g. production logistics, that the containers are obstacles once they have been brought to their target position. The\nsecond difference is that payload assignment is not fixed,\ni.e., agents can separate from containers at any point in time.\nThis is necessary to solve instances where containers have\nto be moved out of the way (Figure 1) or the environment\nis very tight (Figure 3). Further, it helps to reduce execution\ncosts (Figure 2).\nAlthough the problem (in its optimizing version) appears\nto be computationally harder than MAPF, it turns out that it\nis still NP-complete. We achieved this result using previous\nresults on pebble motion problems and an analysis of the\ncases with limited rotations employing group theory.\nWe devised a solver for MAT by reducing the problem to a\nsequence of SAT problems similar to what has been done in\nthe context of MAPF. As expected, the additional degrees of\nfreedom of MAT compared to MAPF result in higher runtimes. The runtime is highest when there are significantly\nmore containers than agents.\nBy comparing variants of MAT (by simply changing the\naxioms), we demonstrated the effect that blocking containers\nand the ability of agents to cooperate in the transportation of\nthe containers have on runtime and makespan.\nFinally, we explored whether CBS could potentially be\nmore efficient than our SAT method by comparing the two\napproaches in a setting which is similar to MAPD.\nIn the future, we plan to work on optimizations of the system and to solve some open theoretical problems. Although\nwe intended to solve MAT problems only for a small number\nof agents, scalability needs to be improved. One envisioned\nway is to design a MAT solver based on CBS. Many more\ntechniques and optimizations that have been used for solving\nMAPF should be applicable to MAT as well.\nOn the theoretical side, we have shown that the plan\nlength can be polynomially bounded, but we have not shown\nthat feasibility can be decided in polynomial time. However,\nwe are optimistic that it is possible to come up with a lineartime feasibility test, similar to the cases for the pebble motion problem on trees (Auletta et al. 1999) and PMR (Yu and\nRus 2014).",
        "summary_en": "This paper introduces the multi-agent transportation (MAT) problem, where agents have to transport containers from their starting positions to their designated goal positions. Movement takes place in a common environment where collisions between agents and between containers must be avoided. In contrast to other frameworks such as multi-agent pathfinding (MAPF) or multi-agent pickup and delivery (MAPD), the agents are allowed to separate from the containers at any time, which can reduce the makespan and also allows for plans in scenarios that are unsolvable otherwise. The paper presents a complexity analysis establishing the problem's NP-completeness and show how the problem can be reduced to a sequence of SAT problems when optimizing for makespan. A MAT solver is empirically evaluated with regard to varying input characteristics and movement constraints and compared to a MAPD solver that utilizes conflict-based search (CBS).",
        "summary_zh": "这篇论文研究了多代理运输（MAT）问题，即代理必须将集装箱从起始位置运输到指定的目标位置，并且避免代理之间和集装箱之间发生碰撞。与多代理寻路（MAPF）或多代理取货和交货（MAPD）等其他框架不同，本文允许代理可以随时与集装箱分离，因为这可以减少时间跨度，还能在其他方法无法解决的情况下制定计划。作者对问题的复杂性进行了分析，证明了问题的NP完备性，并展示了在优化总运输时间时如何将该问题简化为一系列 SAT 问题。最后，作者还对MAT求解器进行了经验评估以及，并和利用了基于冲突的搜索 (CBS) 的MAPD求解器进行了比较。"
    },
    {
        "title": "Contrastive Identity-Aware Learning for Multi-Agent Value Decomposition",
        "abstract": "Value Decomposition (VD) aims to deduce the contributions of agents for decentralized policies in the presence of only global rewards, and has recently emerged as a powerful credit assignment paradigm for tackling cooperative Multi-Agent Reinforcement Learning (MARL) problems. One of the main challenges in VD is to promote diverse behaviors among agents, while existing methods directly encourage the diversity of learned agent networks with various strategies. However, we argue that these dedicated designs for agent networks are still limited by the indistinguishable VD network, leading to homogeneous agent behaviors and thus downgrading the cooperation capability. In this paper, we propose a novel Contrastive Identity-Aware learning (CIA) method, explicitly boosting the credit-level distinguishability of the VD network to break the bottleneck of multi-agent diversity. Speciﬁcally, our approach leverages contrastive learning to maximize the mutual information between the temporal credits and identity representations of different agents, encouraging the full expressiveness of credit assignment and further the emergence of individualities. The algorithm implementation of the proposed CIA module is simple yet effective that can be readily incorporated into various VD architectures. Experiments on the SMAC benchmarks and across different VD backbones demonstrate that the proposed method yields results superior to the state-of-the-art counterparts. Our code is available at https://github.com/liushunyu/CIA.",
        "introduction": "Cooperative Multi-agent Reinforcement Learning (MARL)\naims to jointly train multiple agents to achieve one common\ngoal, and has witnessed its unprecedented success in various applications, such as video games (Vinyals et al. 2019;\nJaderberg et al. 2019), trafﬁc light systems (Wu et al. 2020),\nand smart grid control (Yan and Xu 2020; Xu et al. 2020;\nWang et al. 2021b). To alleviate the partial observation constraint and the scalability problem in cooperative MARL, the\nCentralized Training and Decentralized Execution (CTDE)\nframework has attracted increasing attention (Lowe et al.\n2017), where agents are granted access to additional global\ninformation during centralized training and deliver actions\nAgent \nNetwork\nAgent \nNetwork\nValue Decomposition Network \nCredits       Values\nLocal History \nLocal History\nValue\nJoint Value\nValue\nGlobal State \nCredit-Level\nDistinguishability\nAgent-Level Diversity\nCredit\nCredit\nCredit\nAgent\nAgent\nAgent\nUpdate\nUpdate\nOurs\nOthers\n(a) Basic VD Paradigm\n(b) Ours v.s. Others\nFigure 1: (a) Basic VD paradigm. N denotes the complex\nnetwork operator based on different VD methods, where\ncredits can be considered as the agent contributions. (b)\nComparing the credit-level distinguishability mechanism\nwith the agent-level diversity mechanism under the VD\nparadigm. Credit-level distinguishability can promote agentlevel diversity, but not vice versa.\nonly based on their local histories in a decentralized way.\nThe decentralized policies with one shared network significantly reduce the number of learnable parameters and the\nexponentially growing joint-action space to the linear complexity. Nevertheless, the CTDE framework still faces a critical challenge of credit assignment, i.e., how to deduce the\ncontributions of individual agents given only global rewards.\nIn recent years, Value Decomposition (VD) has emerged\nas a promising credit assignment paradigm, which endows\nagents with the ability to learn their own optimal policies\nby decomposing the joint value function according to their\nindividual credits, as depicted in Figure 1(a). The seminal\nwork, VDN (Sunehag et al. 2018), additively factorizes the\nteam value function into individual agent-wise terms, imposing a strong constraint of credit equalization. QMIX (Rashid\net al. 2018) identiﬁes that the full factorization of VDN is\nnot necessary and relaxes this additive constraint to a nonlinear combination, which enables the weighted credits for\nper-agent values. Due to the superior performance of QMIX,\nthere have been many recent efforts to improve its representation capability (Rashid et al. 2020; Wang et al. 2021a).\nHowever, despite the large representation capacity of existing VD methods, it is widely observed that agents often\nlearn similar behaviors inevitably (Mahajan et al. 2019; Hu\net al. 2022). Such similar behaviors easily lead to the local\noptimum of the cooperative policies, which may severely\nimpede the efﬁcient exploration and downgrade the ﬁnal\nperformance (Terry et al. 2020). Consider a football game\nwhere different agents observe similarly. If agents behave\nsimilarly for desired credits, all of them may gather to compete for a ball. However, they should keep distance and adopt\ndiverse tactics. Several studies attribute this similarity between agents to the homogeneous policy networks with parameter sharing (Jiang and Lu 2021; Li et al. 2021). They\npropose to design auxiliary objectives in policy networks\ntowards agent-level diversity, and even introduce agentspeciﬁc networks for each agent while sacriﬁcing the advantage of complete parameter sharing. However, these works\nignore the fact that policy networks are evaluated and improved via the VD network, where the diversity of agent\nbehaviors often depends on the distinguishability of credit\nassignment, as depicted in Figure 1(b). Thus, we argue that\ntheir pursuit of diverse policies is still limited when the VD\nnetwork provides ambiguous credits to demonstrate the contributions of different agents.\nIn this paper, we investigate the multi-agent diversity\nfrom a novel perspective of credit assignment. To evaluate the distinguishability of existing VD methods, we design a random-shufﬂe scheme that eliminates the identity\ninformation of input agent values while preserving the original network architecture. Case studies empirically show\nthat the VD network may assign ambiguous credits to the\nagents, indicating that promoting the diversity of agents will\nbe limited by the identity insensitivity of the VD network.\nTherefore, we introduce a new contrastive identity-aware\nlearning method, termed as CIA, to explicitly encourage\ncredit-level distinguishability. The proposed method leverages gradient-based attribution to represent the credit of\neach agent, and further considers the long-term behaviors of\nagents by adopting the temporal credits from the overall trajectory. To encourage the discriminative assignment of credits and further the emergence of diverse behaviors, we propose to maximize the mutual information between the temporal credits and learnable identity representations of different agents. Moreover, we customize a contrastive learning\nobjective to derive a tractable lower bound for the mutual\ninformation since estimating and maximizing the mutual information of neural networks is often intractable. Our main\ncontributions can be summarized as follows:\n• We identify the ambiguous credit assignment problem in\nVD, a highly important ingredient for multi-agent diversity yet largely overlooked by existing literature.\n• We propose a novel contrastive identity-aware learning (CIA) method to promote diverse behaviors via explicitly encouraging credit-level distinguishability. The\nproposed CIA module imposes no constraints over the\nnetwork architecture, and serves as a plug-and-play module readily applicable to various VD methods.\n• Experiments conducted on the StarCraft II micromanagement benchmark show that CIA yields signiﬁcantly superior performance to state-of-the-art competitors.",
        "related work": "Value Decomposition\naims to extract the individual utility from the global reward for credit assignment, which\nhas become a well-established paradigm for tackling cooperative MARL problems (Zohar, Mannor, and Tennenholtz 2022; Jeon et al. 2022; Fu et al. 2022). To realize\nefﬁcient VD, it is critical to satisfy the Individual-GlobalMax (IGM) principle that the global optimal action should\nbe consistent with the collection of individual optimal actions of agents (Son et al. 2019). Following the IGM principle, VDN (Sunehag et al. 2018) proposes to represent\nthe joint value function as a sum of individual value functions, while QMIX (Rashid et al. 2018) extends this additive VD and imposes a monotonicity constraint, showing the\nstate-of-the-art performance. To further alleviate the risk of\nsuboptimal results, WQMIX (Rashid et al. 2020) improves\nQMIX by a weighted projection that allows more emphasis\nto be placed on underestimated actions.\nHowever, VDN and QMIX still suffer from structural\nconstraints which limit their representation capability for\njoint action-value function classes. Thus, QTRAN (Son\net al. 2019) constructs the soft regularization constraints to\nachieve more general decomposition than VDN and QMIX.\nQPLEX (Wang et al. 2021a), on the other hand, proposes a\nduplex dueling network architecture to enable the complete\nVD function class that satisﬁes the IGM principle. These\nVD methods have shown great potential in the ﬁeld of credit\nassignment for challenging MARL tasks. Other works further adapt VD towards transfer learning (Long et al. 2020;\nWang et al. 2020b; Yang et al. 2022; Yang, Ye, and Wang\n2022) and ad hoc teamwork (Gu et al. 2021; Macke, Mirsky,\nand Stone 2021; Rahman et al. 2021). Despite the success of\nVD, existing methods ignore the distinguishability in credit\nassignment, leading to the homogeneous behaviors among\nmultiple agents. GRE (Zhao et al. 2022) tries to use credit\nentropy regularization to mitigate this problem, which only\nimposes single-step distinguishability on credits.\nAgent Diversity\nhas been widely studied in single-agent\nreinforcement learning problems, which provides an exploration bonus to encourage the diverse behaviors (Eysenbach et al. 2019; Burda et al. 2019). In recent years, due to\nthe promising results achieved by the existing single-agent\nmethods, diversity has also emerged as a popular topic in\nMARL (Lee, Yang, and Lim 2019; Christianos et al. 2021).\nIt encourages the difference between individual agents under the cooperative setting when pursuing diversity in the\ncontext of MARL. RODE (Wang et al. 2021c) realizes a\nlearnable role assignment for agents to achieve diversity. It\nlearns action representations and clusters different actions\ninto several restricted roles, while this role-based method\nis limited by the action space that cannot be decomposed.\nEOI (Jiang and Lu 2021) proposes to construct the intrinsic reward, a predicted probability of agent identity given\nits observation, to promote the emergence of individuality.\nHowever, the identity prediction mechanism of EOI is easy\nto overﬁt if the local observation contains the identity information. In MAVEN (Mahajan et al. 2019), agents condition their behaviors on the shared latent variable controlled\nby a hierarchical policy, where the mutual information between the trajectories and latent variables is maximized to\nlearn a diverse set of such behaviors. Similarly, EITI (Wang\net al. 2020a) leverages mutual information to capture the inﬂuence between the transition dynamics of different agents,\nwhile CDS (Li et al. 2021) designs agent-speciﬁc networks\nfor each agent and encourages diversity by optimizing the\nmutual information between the agent identities and trajectories. These works are all built based on the existing VD\nmethods, especially QMIX, encouraging only the diversity\nof agent networks with various strategies. Thus, they still\nsuffer from limited distinguishability when credit assignment in the VD network is ambiguous.",
        "preliminary": "Dec-POMDP.\nWe consider a fully cooperative multiagent setting under the Decentralized Partially Observable\nMarkov Decision Process (Dec-POMDP), which is deﬁned\nas a tuple ⟨A, S, U, P, r, Ω, O, γ⟩, where A = {ak}K\nk=1\nis the set of K agents and s ∈ S is the global state of\nthe environment. At each time step t, each agent ak ∈ A\nreceives an individual partial observation ok\nt ∈ Ω drawn\nfrom the observation function O(st, ak). Then each agent\nchooses a corresponding action uk\nt ∈ U, forming a joint action ut ∈ UK. This causes a transition to the next state st+1\naccording to the state transition function P(st+1|st, ut) :\nS × UK × S → [0, 1]. All agents share the same reward\nfunction r(st, ut) : S × UK → R and γ ∈ [0, 1) is the\ndiscount factor. Each agent ak has an action-observation\nhistory τ k ∈ T\n≡ (Ω × U)∗ and learns its individual\npolicy πk(uk|τ k) : T × U → [0, 1] to jointly maximize\nthe discounted return Rt = P∞\ni=0 γirt+i. The joint actionobservation history is deﬁne as τ ∈ T K. The joint policy π induces a joint action-value function Qtot\nt (st, ut) =\nEst+1:∞,ut+1:∞[Rt | st, ut, π] that represents the expected\ndiscounted return under the given policy.\nThe CTDE Framework\nhas attracted substantial attention in cooperative MARL to achieve effective policy learning (Yu et al. 2021; Luo et al. 2022), where agents must\nlearn the decentralized policies which based on only local\nobservation at execution time, but they are granted access\nto the global information during centralized training. One of\nthe promising ways to exploit the CTDE framework is VD,\nwhich allows agents to learn their individual utility functions\nby optimizing the joint action-value function for credit assignment. To realize VD, a mixing network with parameters θυ is adopted as an approximator to estimate the joint\naction-value function Qtot. The mixing network is introduced to merge all individual action values into a joint one\nQtot = f(q; θυ), where q = [Qk]K\nk=1 ∈ RK and Qk with\nshared parameters θπ is the utility network of each agent ak.\nThe learnable parameter θ = {θπ, θυ} can be updated by\nminimizing the following Temporal-Difference (TD) loss:\nLT D(θ) = ED\nh\u0000\nytot − Qtot\u00012i\n.\n(1)\nwhere E[·] denotes the expectation function, D is the replay\nbuffer of the transitions, ytot = r + γ ˆQtot is the one-step\ntraget and ˆQtot is the target network (Mnih et al. 2015).",
        "method": "In what follows, we provide case studies to evaluate the\ncredit indistinguishability of existing VD methods. Then we\nfurther detail the proposed contrastive identity-aware learning (CIA) module and ﬁnally summarize the complete optimization algorithm.\nCredit Indistinguishability Analysis\nFirstly, we deﬁne that a credit assignment is ambiguous if\nthe learnable credits are invariant to the agent identities. To\ninvestigate the ambiguous credit assignment problem in VD,\nwe design a training scheme that randomly shufﬂes the order of input values to the mixing network at every training\nepoch. Concretely, we denote Φ as the set of all permutation\nmatrices P ∈ RK×K. At each training epoch, we randomly\nsample a permutation matrix P ∈ Φ and obtain the ambiguous joint-action value Qtot = f(P q) instead of the original\none to calculate the TD loss and update the network parameter. This random-shufﬂe scheme eliminates the identity information of input agent values without changing the original network architecture, providing a comparable baseline\nwith ambiguous credit assignment. Furthermore, we propose\nto use KL-divergence distance to measure the similarity of\n0.0\n0.5\n1.0\n1.5\n2.0\nTotal…Timesteps…(×106)\n0\n20\n40\n60\n80\n100\nTest…Win…Rate…(%)\nQMIX\nQMIX…(RS)\nQMIX…(CIA)\n(a) 2c vs 64zg (Hard)\nQMIX\nQMIX\n(RS)\nQMIX\n(CIA)\nQMIX\nQMIX\n(RS)\nQMIX\n(CIA)\n0.00\n0.04\n1.81\n0.04\n0.00\n1.35\n1.85\n1.38\n0.00\n×10\n2\n0.3\n0.6\n0.9\n1.2\n1.5\n1.8\n(b) KL Matrix (2c vs 64zg)\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nTotal…Timesteps…(×106)\n0\n20\n40\n60\n80\n100\nTest…Win…Rate…(%)\nQMIX\nQMIX…(RS)\nQMIX…(CIA)\n(c) 3s5z vs 3s6z (Super Hard)\nQMIX\nQMIX\n(RS)\nQMIX\n(CIA)\nQMIX\nQMIX\n(RS)\nQMIX\n(CIA)\n0.00\n0.04\n0.64\n0.04\n0.00\n0.68\n0.68\n0.70\n0.00\n×10\n2\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n(d) KL Matrix (3s5z vs 3s6z)\nFigure 2: (Left) Learning curves of QMIX and its variants on\ntwo SMAC scenarios. (Right) KL-divergence distance matrices of the credit distributions of different methods.\nAgent 1 \nUtility Network\nAgent K \nUtility Network\nMixing Network\nDerivative\nDerivative\n(b) Temporal Credit Attribution\nAgent 1\nAgent 2\nAgent K\n(c) Identity Representation\nLearnable \nIdentity Representation\n(d) Contrastive Learning Loss \nIdentity-Wise Loss\n(a) Value Decomposition\nIdentity-Wise\nMutual Information\nTemporal Credits\nIdentity \nRepresentations\nx\nx\nx\nSpecific Credit-Identity Space\nx x\nx\nx x\nx\nx\nx\nx\nx x x\nx\nx\nx x\nx\nx\nx x\nx\nFigure 3: An illustrative diagram of the proposed contrastive identity-aware learning (CIA) method.\ncredit distributions1 of different methods. We calculate the\naverage KL-divergence distance by the trajectories sampled\nby the trained networks of all compared methods separately.\nPlease refer to Appendix B for more details about the experiment settings.\nCase studies are conducted on two challenging tasks\nprovided by SMAC (Samvelyan et al. 2019). We take\nQMIX (Rashid et al. 2018) as an example to study due to its\nstate-of-the-art performance, and introduce its variant under\nthe random-shufﬂe training scheme, namely QMIX (RS).\nMoreover, we also integrate the proposed CIA module with\nQMIX, namely QMIX (CIA), to demonstrate the importance\nof credit-level distinguishability. Experimental results of different methods are shown in Figure 2. Interestingly, even\nwith the ambiguous credit assignment, the random-shufﬂe\nvariant can still achieve the performance on par with those\nobtained by original QMIX. The KL-divergence distance\nalso shows that the credit distribution of QMIX is similar as\nits random-shufﬂe variant, indicating that QMIX are insensitive to the identity of agents. Thus, its learned credits may\nbe ambiguous, which limits the diverse behaviors of agents\nand damages the ﬁnal performance.\nContrastive Identity-Aware Learning\nTo encourage multi-agent diversity, we introduce a novel\ncontrastive learning method to realize the identity-aware distinguishability in credit assignment, as shown in Figure 3.\nTemporal Credit Attribution.\nBefore promoting the distinguishability of credits, one key issue to be considered is:\nhow to deﬁne a universal credit to represent the contributions\nof individual agents in different VD methods. Towards addressing this problem, we consider a gradient-based attribution mechanism to extract the credit assignment information\nin the mixing network. In general, the gradient-based attribution is calculated taking the partial derivatives of the output\n1At each time step t, the credit distribution dt is deﬁned as\nthe normalized gradient-based credits (mentioned in Eq. 2) over\nall agents: dt = softmax\n\u0000\u0002\nx1\nt, x2\nt, · · · , xK\nt\n\u0003\u0001\n.\nwith respect to the input, indicating the relative importance\nof different inputs to a speciﬁc output (Ancona et al. 2018).\nThus, we employ this mechanism to attribute the credits as\nthe partial derivatives between the joint-action value and individual values, as shown in Figure 3(a). Specially, the credit\nof the agent ak at each time step t is formulated as:\nxk\nt = ∂Qtot\nt\n∂Qk\nt\n∈ R.\n(2)\nThis credit attribution mechanism enables us to determine\nthe contribution of each agent value, also known as the value\nsensitivity. That is to say, a larger attribution magnitude\nmeans that the corresponding agent has a signiﬁcant impact\non the ﬁnal result. Furthermore, this credit attribution mechanism can be readily applied to different VD methods, regardless of the heterogeneous mixing network architectures.\nIn sequential decision-making problem, it is insufﬁcient\nto deduce the contribution of an agent only by the behavior\nof a single step. The behavior of an agent is usually affected\nby its ﬁnal goal. As shown in Figure 3(b), we therefore adopt\nthe temporal credit attribution of each agent ak from the\noverall sampled trajectory τ with t ∈ {1, 2, · · · , N} as:\nxk\nτ =\n\u0002\nxk\n1, xk\n2, · · · , xk\nN\n\u0003\n∈ RN,\n(3)\nwhere N is the maximum length of the sampled trajectories.\nIf any trajectories are truncated early due to the game rules,\nzero padding will be applied to align the lengths of all trajectories. We denote Xτ ∈ RK×N as the temporal credits\nof K agents in the trajectory τ. This temporal credit attribution considers the long-term behaviors of agents, leading to\na more stable measure for the agent contributions.\nContrastive\nIdentity-Aware\nDistinguishability.\nAfter\ndeﬁning the credits in a reasonable way, we further attempt\nto promote the distinguishability of the credits, avoiding capacity degradation of the VD model due to the ambiguous\ncredit assignment problem. However, obtaining the solution\nthat all the credits differ from each other is not trivial. It\nis hard to directly constrain the distance between different\nlearnable credits during the optimization process. Thus, we\napproximate the solution by introducing latent identity representations for each agent as intermediate variables, and\ncondition the individual temporal credits on these representations, realizing identity-aware distinguishability.\nSpecially, each agent ak is assigned only one learnable\nrandom-initialized identity representation wk ∈ RN with\nthe same dimension of N as the temporal credits during the\nentire training process. We denote W ∈ RK×N as the identity representations of K agents. On the other hand, it is notable that the temporal credits xk\nτ of the agent are various in\neach sampled trajectory τ. We propose to maximize the mutual information I(x; w) between the temporal credits and\nidentity representations of different agents:\nI(x; w) = Ex,w\n\u0014\nlog p(x | w)\np(x)\n\u0015\n.\n(4)\nHowever, directly optimizing this objective is quite difﬁcult since estimating mutual information is often intractable.\nInspired by contrastive learning (Oord, Li, and Vinyals\n2018), we introduce a contrastive learning loss, the InfoNCE\nloss, to provide a traceable lower bound of the mutual information objective as follows:\nI(x; w) ≥ log(K) − LCL,\n(5)\nwhere LCL is the InfoNCE loss and K is the number of\nagents. Contrastive learning can be considered as learning\na differentiable dictionary look-up task, which contrasts semantically similar and dissimilar query-key pairs. To match\nindividual temporal credits with their corresponding identity\nrepresentations, we deﬁne identity representations as queries\nand temporal credits as keys. For example, given a query wk\nand keys X = {xk′\nτ }K\nk′=1, the goal of contrastive learning is\nto ensure that wk is close with xk\nτ while being irrelevant\nto other keys in X\\{xk\nτ}. Specially, the identity-wise contrastive learning loss is given as follows:\nLCL =\nE\n(wk,{xk′\nτ }K\nk′=1)∼D\n\"\n− log\nexp(g(xk\nτ, wk))\nPK\nk′=1 exp(g(xk′\nτ , wk))\n#\n,\n(6)\nwhere g(xτ, w) = xT\nτ w ∈ R is the function that compute\nthe non-negative similarity score between the identity representation w and the temporal credit xτ. Here we adopt a\nsimple dot-product similarity, but it can be easily replaced\nby other methods. The similarity matrix is denoted as G =\nXτW T ∈ RK×K. As shown in Figure 3(d), this identitywise contrastive learning loss constrains the learned identity\nrepresentations to uniformly distribute on a speciﬁc creditidentity hypersphere without divergence, where the temporal\ncredits distribute around their corresponding identity representation (Wang and Isola 2020). With the contrastive learning loss, CIA directly encourages the identity-aware distinguishability among agent credits, providing a more discriminative credit assignment for multi-agent diversity.\nIntuitively, we can explain the intrinsic mechanism in CIA\nfrom the perspective of a classiﬁcation problem, where each\ninput credits predict their corresponding identity labels. The\nidentity representations are the weights of the classiﬁer. For\nclassiﬁcation, there are two ways to obtain a minimal loss:\nuse some easier inputs or adopt a more complex classiﬁer.\nHere the identity representations only form a simple onelayer linear classiﬁer, which imposes a strong constraint that\nthe input credits must be linearly separable for minimal loss.\nThus, the CIA module successfully endows the VD network\nthe ability to promote credit distinguishability.\nOptimization Algorithm\nTo sum up, training our framework based on the CIA module contains two main parts. The ﬁrst one is the original TD\nloss LT D, which enables each agent to learn its individual\npolicy by optimizing the joint-action value of the mixing\nnetwork. The second one is the proposed contrastive learning loss LCL to facilitate the credit-level distinguishability.\nThus, given these two corresponding loss items, the total loss\nof our framework is formulated as follows:\nLall = LT D + αLCL,\n(7)\nwhere α is the coefﬁcient for trading off loss terms. The\noverall framework is trained in an end-to-end centralized\nmanner. It is notable that CIA does not break any constraints\nof original VD paradigm and still follows IGM principle. To\nmake the proposed CIA clearer to readers, we provide the\npseudocode in Appendix A. We only need to introduce an\nadditional linear layer for learnable identity representations\nas shown in Figure 3(c). The algorithm implementation of\nthe CIA module is simple yet effective that can be seamlessly integrated with various VD architectures.",
        "experiments": "To demonstrate the effectiveness of the proposed CIA\nmethod, we conduct experiments on the didactic game and\nthe StarCraft II micromanagement challenge.\nExperimental Settings\nOur methods are compared with various state-of-theart methods, including (i) Value decomposition methods: QMIX (Rashid et al. 2018), QPLEX (Wang et al.\n2021a), QTRAN (Son et al. 2019), OWQMIX and\nCWQMIX (Rashid et al. 2020). (ii) Diversity-based methods: MAVEN (Mahajan et al. 2019), EOI (Jiang and Lu\n2021), GRE (Zhao et al. 2022), SCDS that is a variant of\nCDS (Li et al. 2021) with complete shared agent network to\nensure comparability. Our CIA implementation uses QMIX\nand QPLEX as the integrated backbones to evaluate its performance, namely QMIX (CIA) and QPLEX (CIA). These\ntwo methods are chosen for their robust performance in different multi-agent tasks, while CIA can also be readily applied to other methods. We adopt the Python MARL framework (PyMARL) (Samvelyan et al. 2019) to implement our\nmethod and all baselines. The detailed hyperparameters are\ngiven in Appendix B, where the common training parameters across different methods are consistent.\nDidactic Game\nWe design a didactic game and call it Turn. Two colored\nagents with the observation of 3 × 3 are initialized in a 5 × 5\nVisual\nRange\neat: +10\neat: +10\nTrapped \nMove: -1\n(a) Game Visualization\n0.0\n0.5\n1.0\n1.5\n2.0\nTotal…Timesteps…(×106)\n50\n0\n50\n100\nAverage…Return\nQMIX\nQMIX…(CIA)\n(b) Learning Curves\n0\n5\n10\nQMIX\nAgent 1\nAgent 2\n0\n20\n40\n60\n80\n100\n0\n10\n20\nQMIX (CIA)\nAgent 1\nAgent 2\nTime Step\nTemporal Credit\n(c) Temporal Credits\nFigure 4: The performance comparison on the didactic game of Turn.\nQMIX (CIA)\nQPLEX (CIA)\nQMIX\nQPLEX\nQTRAN\nOWQMIX\nCWQMIX\nMAVEN\nEOI\nGRE\nSCDS\n0.0\n0.5\n1.0\n1.5\n2.0\nTotal…Timesteps…(×106)\n0\n20\n40\n60\n80\n100\nTest…Win…Rate…(%)\n(a) 10m vs 11m (Easy)\n0.0\n0.5\n1.0\n1.5\n2.0\nTotal…Timesteps…(×106)\n0\n20\n40\n60\n80\n100\nTest…Win…Rate…(%)\n(b) 2c vs 64zg (Hard)\n0.0\n0.5\n1.0\n1.5\n2.0\nTotal…Timesteps…(×106)\n0\n20\n40\n60\n80\n100\nTest…Win…Rate…(%)\n(c) 7sz (Hard)\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nTotal…Timesteps…(×106)\n0\n20\n40\n60\n80\n100\nTest…Win…Rate…(%)\n(d) 6h vs 8z (Super Hard)\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nTotal…Timesteps…(×106)\n0\n20\n40\n60\n80\n100\nTest…Win…Rate…(%)\n(e) corridor (Super Hard)\n0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nTotal…Timesteps…(×106)\n0\n20\n40\n60\n80\n100\nTest…Win…Rate…(%)\n(f) 3s5z vs 3s6z (Super Hard)\nFigure 5: Learning curves of our proposed CIA variants and baselines on the SMAC scenarios. All experimental results are\nillustrated with the mean and the standard deviation of the performance over ﬁve random seeds for a fair comparison. To make\nthe results in ﬁgures clearer for readers, we adopt a 50% conﬁdence interval to plot the error region.\nmap, and the corresponding colored apples are generated alternately in the map. The goal of each agent is to take turns to\neat the exclusive apples (agent 1 ﬁrst), where the one that is\nnot in its round will be trapped by fences and punished when\nmoving. This didactic game requires the VD model to learn\na rotating credit assignment strategy. As shown in Figure 4,\nthe proposed CIA works quite well on improving QMIX. We\nfurther visualize the temporal credits of different methods\nin a sampled trajectory. Obviously, CIA realizes the distinguishable credit assessment, where the temporal credits of\ntwo agents rise alternately. However, the credits of QMIX\nare sometimes ambiguous, which even pay more attention\nto the trapped agent in each round. This crisp example veriﬁes the effectiveness of the proposed CIA. Please refer to\nAppendix B for more details.\nSMAC Benchmark\nThe StarCraft Multi-Agent Challenge (SMAC)2 (Samvelyan\net al. 2019) has become a common-used benchmark for evaluating state-of-the-art MARL methods. SMAC focuses on\n2We use SC2.4.10 version instead of the older SC2.4.6.2.69232.\nPerformance is not comparable across versions.\nmicromanagement challenges where each of the ally entities\nis controlled by an individual learning agent, and enemy entities are controlled by a built-in AI. The goal of the agents\nis to maximize the damage to enemies. Hence, proper tactics such as sneaking attack and drawing ﬁre are required\nduring battles. Learning these diverse interaction behaviors\nunder partial observation is a challenging task. To validate\nthe effectiveness of our methods, we conduct experiments\non 6 SMAC scenarios (Samvelyan et al. 2019) which are\nclassiﬁed into Easy (10m vs 11m), Hard (2c vs 64zg, 7sz)\nand Super Hard (6h vs 8z, corridor, 3s5z vs 3s6z). Only\nthe 10m vs 11m scenario is homogeneous, where the army\nis composed of only a single unit type, while the others are\nheterogeneous.\nThe experimental results on different scenarios are shown\nin Figure 5. Compared with the state-of-the-art baseline methods, our proposed CIA successfully improves\nthe ﬁnal performance. In the easy homogeneous scenarios (10m vs 11m), the strength gap between the agents and\nenemies is small. Thus, several baselines can also achieve\npromising results without complex tactics, while the beneﬁt\nof diversity brought by CIA is not obvious. However, in the\nmore difﬁcult heterogeneous scenario (2c vs 64zg, 7sz), our\nmethod yields better performance than baselines. The results\nsuggest that CIA can be utilized to provide a more discriminative credit assignment, which helps heterogeneous agents\nto explore diverse cooperation strategies. Moreover, this exploration may lead to a little drop in learning efﬁciency, but\nit is worthwhile as a trade-off for achieving non-trivial performance. To further test the potentiality of the proposed\nmethod, we also compare CIA in the super hard heterogeneous scenarios (6h vs 8z, corridor, 3s5z vs 3s6z). In these\nchallenging scenarios, learning distinguishable credits becomes complex due to the different unit types. Furthermore,\nthere is a great disparity in strength between the two teams,\nand it is impossible to beat the enemies in a reckless way.\nThe proposed CIA provides an impressive improvement in\nthe performance over the baselines, showing its robustness\nto diversity promoting. Especially in the 3s5z vs 3s6z scenario, almost all compared baselines cannot learn any effective policy and perform poorly, while our method still maintains the superior performance.\nAblation Study\nTo verify the generalization of the proposed method, we apply the CIA module to two VD methods, including QMIX\nand QPLEX, and present the results in Figure 5. In general,\nthe proposed CIA shows promising performance in enhancing different VD methods. Of special interest is the signiﬁcant improvement in the super hard heterogeneous scenarios (3s5z vs 3s6z), where credit assignment plays an important role. CIA (QMIX) and CIA (QPLEX) both obtain\ngratifying results superior to QMIX and QPLEX, indicating\nthat the CIA module successfully enlarges the credit-level\ndistinguishability. Moreover, it is notable that the improvement of incorporating CIA into different VD methods relies\non the representation capability of the original models. The\ndedicated CIA module focuses on providing a regularization\nterm to guide the optimization of the credit assignment withFigure 6: The performance comparison between the QMIX\nvariants with the CC loss and the CIA loss, respectively.\nout changing the intrinsic VD mechanism.\nMoreover, to further demonstrate the advantage of CIA in\npromoting credit-level distinguishability, we also design a\ncomparable variant that uses a credit classiﬁcation loss (CC\nLoss) instead of the proposed contrastive identity-aware\nlearning loss (CIA Loss). The credit classiﬁcation loss is\nbased on directly predicting the corresponding identity labels given the temporal credits. Figure 6 reports the experimental results of QMIX with different loss items. The dedicated CC loss can also make an improvement to alleviate\nthe ambiguous credit assignment problem. In the simple scenarios (10m vs 11m, 2c vs 64zg), the CIA loss offers close\nperformance compared with CC loss. However, in the difﬁcult scenarios (corridor, 3s5z vs 3s6z), the CIA loss outperforms the CC loss by a wide margin, while the CC loss\nsuffers from a large variance. The results show that the CIA\nloss constraints the temporal credits and the identity representations into a speciﬁc credit-identity space without divergence, providing a more stable and robust optimization\nobjective. However, the learned credits of the CC loss may\ndistribute far away from the decision boundary constructed\nby the identity representations.",
        "conclusion": "In this paper, we investigate the multi-agent diversity problem of the value decomposition (VD) paradigm in the\nMARL setting, observing that existing works on diverse\nagent networks are limited by the indistinguishable credit\nassignment of the VD network. Inspired by the observation, we propose a novel contrastive identity-aware learning (CIA) method to explicitly encourage the VD network\nto distinguish the assigned credits of multiple agents. Technically, we maximize the mutual information of the temporal credits and the identity representations of different agents\nwith a tractable lower bound. We validate CIA over the StarCraft II micromanagement benchmark, and showcase that\nit yields results signiﬁcantly superior to the state-of-the-art\ntechniques in MARL. This simple yet effective CIA method\nfurther motivates us to explore the high-quality representation in future work rather than improving the network architecture. Another challenging direction is the theoretical\nstudy of the ambiguous credit assignment phenomenon. It is\ninteresting to study the relationship between the IGM constraint and the ambiguous credit assignment problem. The\nIGM constraint may limit the distinguishable expressiveness\nof credit assignment in the existing VD architectures.",
        "summary_en": "Value Decomposition (VD) aims to deduce the contributions of agents for decentralized policies in the presence of only global rewards, and has recently emerged as a powerful credit assignment paradigm for tackling cooperative Multi-Agent Reinforcement Learning (MARL) problems. One of the main challenges in VD is to promote diverse behaviors among agents, while existing methods directly encourage the diversity of learned agent networks with various strategies. However, this paper argues that these dedicated designs for agent networks are still limited by the indistinguishable VD network, leading to homogeneous agent behaviors and thus downgrading the cooperation capability. The paper proposes a novel Contrastive Identity-Aware learning (CIA) method, explicitly boosting the credit-level distinguishability of the VD network to break the bottleneck of multi-agent diversity. Specifically, the paper's approach leverages contrastive learning to maximize the mutual information between the temporal credits and identity representations of different agents, encouraging the full expressiveness of credit assignment and further the emergence of individualities. The algorithm implementation of the proposed CIA module is simple yet effective that can be readily incorporated into various VD architectures. Experiments on the SMAC benchmarks and across different VD backbones demonstrate that the proposed method yields results superior to the state-of-the-art counterparts.",
        "summary_zh": "这篇论文介绍了一种名为对比身份感知学习（CIA）的方法，旨在提升价值分解网络的信用级可区分性，以打破多代理多样性的瓶颈。该方法利用对比学习来最大化不同代理的时间信用和身份表征之间的互信息，从而鼓励信用分配的充分表达，并进一步促进个性的出现。在SMAC基准和不同的价值分解主干网上的实验结果表明，该方法优于目前最先进的方法。"
    },
    {
        "title": "Value-Decomposition Multi-Agent Actor-Critics",
        "abstract": "The exploitation of extra state information has been an active research area in multi-agent reinforcement learning (MARL). QMIX represents the joint action-value using a non-negative function approximator and achieves the best performance on the StarCraft II micromanagement testbed, a common MARL benchmark. However, our experiments demonstrate that, in some cases, QMIX performs sub-optimally with the A2C framework, a training paradigm that promotes algorithm training efﬁciency. To obtain a reasonable trade-off between training efﬁciency and algorithm performance, we extend value-decomposition to actor-critic methods that are compatible with A2C and propose a novel actor-critic framework, value-decomposition actor-critic (VDAC). We evaluate VDAC on the StarCraft II micromanagement task and demonstrate that the proposed framework improves median performance over other actor-critic methods. Furthermore, we use a set of ablation experiments to identify the key factors that contribute to the performance of VDAC.",
        "introduction": "Many complex sequential decision making problems that involve multiple agents can be modeled as multi-agent reinforcement learning (MARL) problems, e.g. the coordination of semi-autonomous or fully autonomous vehicles (Hu et al. 2019) and the coordination of machines in a product line (Choo, Adams, and Beling 2017). A fully centralized controller that applies single-agent reinforcement learning will suffer from the exponential growth of the action space with the number of agents in the system. Learning decentralized policies that condition on the local observation history of individual agents is a viable way to attenuate this problem. Furthermore, partial observability and communication constraints, two common obstacles in multi-agent settings, also necessitate the use of decentralized policies. In a laboratory or simulated setting, decentralized policies can be learned in a centralized fashion via enabling communication among agents or granting access to additional global state information. This centralized training and decentralized execution (CTDE) paradigm has attracted the attention of researchers. However, it remains an open research question how to best exploit centralized training. In particular, it is not obvious how to utilize joint action-value or global state value to train decentralized policies. Breakthroughs in Q-learning have been made using joint action-value factorization techniques. Value-decomposition networks (VDN) represent joint action-value as a summation of local action-value conditioned on individual agents’ local observation history (Sunehag et al. 2017). In (Rashid et al. 2018), a more general case of VDN is proposed using a mixing network that approximates a broader class of monotonic functions to represent joint action-values called QMIX. In (Son et al. 2019), a more complex factorization framework three modules, called QTRAN, is introduced and shown to have good performance on a range of cooperative tasks. While QMIX reports the best performance on the StarCraft micromanagement testbed (Samvelyan et al. 2019), we ﬁnd that QMIX, in some StarCraft II compositions, has issues learning good policies that can consistently defeat enemies when using the A2C training paradigm (Mnih et al. 2016), which was originally introduced to enable algorithms to be executed efﬁciently. On the other hand, on-policy actor-critic methods, such as counterfactual multi-agent (COMA) (Foerster et al. 2018), can leverage the A2C framework to improve training efﬁciency at the cost of performance. (Samvelyan et al. 2019) point out that there is a performance gap between the stateof-the-art actor-critic method, COMA, and QMIX on the StarCraft II micromanagement testbed. To bridge the gap between multi-agent Q-learning and multi-agent actor-critic methods, as well as offer a reasonable trade-off between training efﬁciency and algorithm performance, we propose a novel actor-critic framework called value-decomposition actor-critic (VDAC). Let V a, ∀a ∈ {1, . . . , n} denote the local state value that is conditioned on agent a’s local observation, and let Vtot denote the global state-value that is conditioned on the true state of the environment. VDAC takes an actor-critic approach but adds local critics, which share the same network with the actors and estimate the local state values V a. The central critic learns the global state value Vtot. The policy is trained by following a gradient dependent on the central critic. Further, we examines two approaches for calculating Vtot. VDAC is based on three main ideas. First, unlike QMIX, VDAC is compatible with a A2C training framework that enables game experience to be sampled efﬁciently. This is due to the fact that multiple games are rolled out independently during training. Second, similar to QMIX, VDAC enforces the following relationship between local state-values V a and the global state-value Vtot: ∂Vtot ∂V a ≥ 0, ∀a ∈ {1, . . . , n}. (1) This idea is related to difference rewards (Wolpert and Tumer 2002), in which each agent learns from a shaped reward that compares the global reward to the reward received when that agent’s action is replaced with a default action. Difference rewards require that any action that improves an agent’s local reward also improves the global reward, which implies the monotonic relationship between shaped local rewards and the global reward. While COMA (also inspired by difference rewards) focuses on customizing shaped rewards ra from the global reward rtot in a pairwise fashion ra = f(rtot), VDAC represents the global reward by all agents’ shaped rewards rtot = f(r1, ..., rn) . Third, VDAC is trained by following a rather simple policy gradient that is calculated from a temporal-difference (TD) advantage. We theoretically demonstrate that the proposed method is able to converge to a local optimum by following this policy gradient. Despite the fact that TD advantage policy gradients and COMA gradients are both unbiased estimates of a vanilla multi-agent policy gradients, our empirical study favors TD advantage policy gradients over COMA policy gradients. This study strives to answer the following research questions: • Research question 1: Is the TD advantage gradient sufﬁcient to optimize multi-agent actor-critics when compared to a COMA gradient? • Research question 2: Does applying state-value factorization improve the performance of actor-critics? • Research question 3: Does VDAC provide a reasonable trade-off between training efﬁciency and algorithm performance when compared to QMIX? • Research question 4: What are the factors that contribute to the performance of the proposed VDAC?",
        "related work": "MARL has beneﬁted from recent developments in deep reinforcement learning, with the ﬁeld moving away from tabular\nmethods (Bu et al. 2008) to deep neural networks (Foerster\net al. 2018). Our work is related to recent advances in CTDE\ndeep multi-agent reinforcement learning.\nThe degree of training centralization varies in the literature on MARL. Independent Q-learning (IQL) (Tan 1993)\nand its deep neural network counterpart (Tampuu et al. 2017)\ntrain an independent Q-learning model for each agent. Those\nthat attempt to directly learn decentralized policies often\nsuffer from the non-stationarity of the environment induced\nby agents simultaneously learning and exploring. (Foerster\net al. 2017; Usunier et al. 2016) attempt to stabilize learning\nunder the decentralized training paradigm. (Gupta, Egorov,\nand Kochenderfer 2017) propose a training paradigm that\nalternates between centralized training with global rewards\nand decentralized training with shaped rewards.\nCentralized methods, by contrast, naturally avoid the nonstationary problem at the cost of scalability. COMA (Foerster et al. 2016), takes advantage of CTDE, where actors are\nupdated by following policy gradients that are tailored by\ntheir contributions to the system. Multi-agent deep deterministic policy gradient (MADDPG) (Lowe et al. 2017) extends\ndeep deterministic policy gradient (DDPG) (Lillicrap et al.\n2015) to mitigate the issue of high variance gradient estimates exacerbated in multi-agent settings. Based on MADDPG, (Wei et al. 2018) propose multi-agent soft Q-learning\nin continuous action spaces to tackle the issue of relative\novergeneralization. Probabilistic recursive reasoning (Wen\net al. 2019) is a method that uses a probabilistic recursive\nreasoning policy gradient that enables agents to recursively\nreason what others believe about their own beliefs.\nMore recently, value-based methods, which lie between\nthe extremes of IQL and COMA, have shown great success in solving complex multi-agent problems. VDN (Sunehag et al. 2017), which represents joint-action value function as a summation of local action-value function, allows\nfor centralized learning. However, it does not make use\nof extra state information. QMIX (Rashid et al. 2018) utilizes a non-negative mixing network to represent a broader\nclass of value-decomposition functions. Furthermore, additional state information is captured by hypernetworks that\noutput parameters for the mixing network. QTRAN (Son\net al. 2019) is a generalized factorization method that can\nbe applied to environments that are free from structural constraints. Other works, such as CommNet (Foerster et al.\n2016), TarMAC (Das et al. 2019), ATOC (Jiang and Lu\n2018), MAAC (Iqbal and Sha 2019), CCOMA (Su, Adams,\nand Beling 2020) and BiCNet(Peng et al. 2017) exploit interagent communication.\nThe proposed VDAC method is similar to QMIX and\nVDN in that it utilizes value-decomposition. However,\nVDAC is a policy-based method that decomposes global\nstate-values whereas QMIX and VDN, which decompose global action-values, belong to the Q-learning family.\n(Nguyen, Kumar, and Lau 2018) address credit-assignment\nissue, however, under a different MARL setting, CDecPOMDP. COMA, which is also a policy gradient method\ninspired by difference rewards and has been tested on StarCraft II micromanage games, represents the work most\nclosely related to this paper.",
        "background": "Decentralized Partially Observable Markov Decision\nProcesses (Dec-POMDPs): Consider a fully cooperative\nmulti-agent task with n agents. Each agent identiﬁed by\na ∈ A ≡ {1, . . . , n} takes an action ua ∈ U simultaneously\nat every timestep, forming a joint action u ∈ U ≡ U a, ∀a ∈\n{1, . . . , n}. The environment has a true state s ∈ S, a transition probability function P(s′|s, u) : S × U × S → S,\nand a global reward function r(s, u) : S × U → R. In\nthe partial observation setting, each agent draws an observations z ∈ Z from the observation function O(S, A) :\nS × A → Z. Each agent conditions a stochastic policy\nπ(ua|τ a) : T × U → [0, 1] on its observation-action history\nτ a ∈ T ≡ Z × U. Throughout this paper, quantities in bold\nrepresent joint quantities over agents, and bold quantities\nwith the superscript −a denote joint quantities over agents\nother than a given agent a. MARL agents aim to maximize\nthe discounted return Rt = P∞\nl=0 γlrt+l. The joint value\nfunction V π(st) = E[Rt|st = s] is the expected return for\nfollowing the joint policy π from state s. The value-action\nfunction Qπ(s, u) = E[Rt|st = s, u] deﬁnes the expected\nreturn for selecting joint action u in state s and following the\njoint policy π.\nSingle-Agent Policy Gradient Algorithms: Policy gradient methods adjust the parameters θ of the policy in order to maximize the objective J(θ) = Es∼pπ,u∼π[R(s, u)]\nby taking steps in the direction of ∇J(θ). The gradient with respect to the policy parameters is ∇θJ(θ) =\nEπ[∇θ log πθ(a|s)Qπ(s, u)], where pπ is the state transition\nby following policy π, and Qπ(s, u) is an action-value.\nTo reduce variations in gradient estimates, a baseline b is\nintroduced. In actor-critic approaches (Konda and Tsitsiklis 2000), an actor is trained by following gradients that are\ndependent on the critic. This yields the advantage function\nA(st, ut) = Q(st, ut) − b(st), where b(st) is the baseline\n(V (st) or another constant is commonly used as the baseline). TD error rt + γV (st+1) − V (st), which is an unbiased estimate of Q(st, ut), is a common choice for advantage functions. In practice, a TD error that utilizes an n-step\nreturn Pk−1\ni=0 γirt +γkV (st+k)−V (st) yields good performance (Mnih et al. 2016).\nMulti-Agent Policy Gradient (MAPG) Algorithms:\nMulti-agent policy gradient methods are extensions of policy gradient algorithms with a policy πθa(ua|oa), a\n∈\n{1, · · · , n} . Compared with policy gradient methods,\nMAPG faces the issues of high variance gradient estimates\n(Lowe et al. 2017) and credit assignment (Foerster et al.\n2018). Perhaps the simplest multi-agent gradient can be\nwritten as:\n∇θJ = Eπ\n\"X\na\n∇θ log πθ(ua|oa)Qπ(s, u)\n#\n.\n(2)\nMulti-agent policy gradients in the current literature often\ntake advantage of CTDE by using a central critic to obtain\nextra state information s, and avoid using the vanilla multiagent policy gradients (Equation 2) due to high variance. For\ninstance, (Lowe et al. 2017) utilize a central critic to estimate\nQ(s, (a1, . . . , an)) and optimize parameters in actors by following a multi-agent DDPG gradient, which is derived from\nEquation 2:\n∇θaJ = Eπ[∇θaπ(ua|oa)∇uaQua(s, u)|ua=π(oa)].\n(3)\nUnlike most actor-critic frameworks, (Foerster et al. 2018)\nclaim to solve the credit assignment issue by applying the\nfollowing counterfactual policy gradients:\n∇θJ = Eπ\n\" X\na\n∇θ log π(ua|τ a)Aa(s, u)\n#\n,\n(4)\nwhere\nAa(s, u)\n=\nQπ(s, u)\n−\nP\nua πθ(ua|τ a)Qa\nπ(s, (u−a, ua))\nis\nthe\ncounterfactual\nadvantage for agent a. Note that (Foerster et al. 2018)\nargue that the COMA gradients provide agents with tailored\ngradients, thus achieving credit assignment. At the same\ntime, they also prove that COMA is a variance reduction\ntechnique.",
        "methods": "In addition to the previously outlined research questions, our\ngoal in this work is to derive RL algorithms under the following constraints: (1) the learned policies are conditioned\non agents’ local action-observation histories (the environment is modeled as Dec-POMDP), (2) a model of the environment dynamics is unknown (i.e. the proposed framework is task-free and model-free), (3) communication is not\nallowed between agents (i.e. we do not assume a differentiable communication channel such as (Das et al. 2019)),\nand (4) the framework should enable parameter sharing\namong agents (namely, we do not train different models for\neach agent as is done in (Tan 1993)). A method that met\nthe above criteria would constitute a general-purpose multiagent learning algorithm that could be applied to a range of\ncooperative environments, with or without communication\nbetween agents. Hence, the following methods are proposed.\nNaive Central Critic Method\nA naive central critic (naive critic) is proposed to answer the\nﬁrst research question: is a simple policy gradient sufﬁcient\nto optimize multi-agent actor-critic methods. Naive critic’s\ncentral critic shares a similar structure with COMA’s critic.\nIt takes (st, ut−1) as the input and outputs V (s). Actors follow a rather simple policy gradient, a TD advantage policy\ngradient that is common in the RL literature given by:\n∇θJ = Eπ\n\" X\na\n∇θ log π(ua|τ a)\n\u0000\nQ(s, u) − V (s)\n\u0001\n#\n, (5)\nwhere Q(s, u) = r + γV (s′). In the next section, we will\ndemonstrate that policy gradients taking the form of Equation 5 under our proposed actor-critic frameworks are also\nunbiased estimates of the naive multi-agent policy gradients.\nThe pseudo code is listed in Appendix.\nValue Decomposition Actor-Critic\nDifference rewards enable agents to learn from a shaped reward Da = r(s, u) − r(s, (u−a, ca)) that is deﬁned by\na reward change incurred by replacing the original action\nua with a default action ca. Any action taken by agent a\nthat improves Da also improves the global reward r(s, u)\nsince the second term in the difference reward equation does\nnot depend on ua. Therefore, the global reward r(s, u) is\nmonotonically increasing with Da. Inspired by difference rewards, we propose to decompose state value Vtot(s) into local states V a(oa) such that the following relationship holds:\n∂Vtot\n∂V a ≥ 0,\n∀a ∈ {1, . . . , n}.\n(6)\nWith Equation 6 enforced, given that the other agents stay\nat the same local states by taking u−a, any action ua that\nleads agent a to a local state oa with a higher value will also\nimprove the global state value Vtot.\nTwo variants of value-decomposition that satisfy Equation 6, VDAC-sum and VDAC-mix, are studied.\nAlgorithm\nCentral Critic\nValue Decomposition\nPolicy Gradients\nIAC (Foerster et al. 2018)\nNo\nTD advantage\nVDAC-sum\nYes\nLinear\nTD advantage\nVDAC-mix\nYes\nNon-linear\nTD advantage\nNaive Critic\nYes\nTD advantage\nCOMA (Foerster et al. 2018)\nYes\nCOMA advantage\nTable 1: Actor-Critics studied.\nFigure 1: VDAC-sum\nVDAC-sum\nIn VDAC-sum, the total state value Vtot(s)\nis a summation of local state values V a(oa): Vtot(s) =\nP\na V a(oa). This linear representation is sufﬁcient to satisfy Equation 6. VDAC-sum’s structure is shown in Figure\n1. Note that the actor outputs both πθ(oa) and Vθv(oa). This\nis done by sharing non-output layers between distributed\ncritics and actors. In this paper, θv denotes the distributed\ncritics’ parameters and θ denotes the actors’ parameters for\ngenerality. The distributed critic is optimized by minibatch\ngradient descent to minimize the following loss:\nLt(θv) =\n\u0012\nyt − Vtot(st)\n\u00132\n=\n\u0012\nyt −\nX\na\nVθv(oa\nt )\n\u00132\n,\n(7)\nwhere yt = Pk−t−1\ni=t\nγiri + γ(k−t)Vtot(sk) is bootstrapped\nfrom the last state sk, and k is upper-bounded by T.\nThe policy network is trained using the following policy gradient g = Eπ[P\na ∇θ log π(ua|τ a)A(s, u)], where\nA(s, u) = r + γV (s′) − V (s) is a simple TD advantage.\nSimilar to independent actor-critic (IAC), VDAC-sum does\nnot make full use of CTDE in that it does not incorporate\nstate information during training. Furthermore, it can only\nrepresent a limited class of centralized state-value functions.\nVDAC-mix\nTo generalize the representation to a larger\nclass of monotonic functions, we utilize a feed-forward\nneural network that takes input as local state values\nVθ(oa), ∀a ∈ {1, . . . , n} and outputs the global state value\nVtot. To enforce Equation 6, the weights (not including bias)\nof the network are restricted to be non-negative. This allows\nFigure 2: VDAC-vmix\nthe network to approximate any monotonic function arbitrarily well (Dugas et al. 2009).\nThe weights of the mixing network are produced by separate hypernetworks (Ha, Dai, and Le 2016). Following the\npractice in QMIX (Rashid et al. 2018), each hypernetwork\ntakes the state s as an input and generates the weights of\none layer of the mixing network. Each hypernetwork consists of a single linear layer. An absolute activation function\nis utilized in the hypernetwork to ensure that the outputted\nweights are non-negative. The biases are not restricted to\nbeing non-negative. Hence, the hypernetworks that produce\nthe biases do not apply an absolute non-negative function.\nThe ﬁnal bias is produced by a 2-layer hypernetwork with\na ReLU activation function following the ﬁrst layer. Finally,\nthe hypernetwork outputs are reshaped into a matrix of appropriate size. Figure 2 illustrates the mixing network and\nthe hypernetworks.\nThe whole mixing network structure (including hypernetworks) can be seen as a central critic. Unlike critics in\n(Foerster et al. 2018), this critic takes local state values\nV a(oa), ∀a ∈ {1, . . . , n} as additional inputs besides global\nstate s. Similar to VDAC-sum, the distributed critics are optimized by minimizing the following loss:\nLt(θv) =\n\u0012\nyt − Vtot(st)\n\u00132\n=\n\u0012\nyt − fmix(Vθv(o1\nt), . . . , Vθv(on\nt ))\n\u00132\n,\n(8)\nwhere fmix denotes the mixing network. Let θc denote\nparameters in the hypernetworks. The central critic is optimized by minimizing the same loss Lt(θc) = (yt −\nVtot(st)). The policy network is updated by following the\nsame policy gradient in Equation 5. The pseudo code is provided in Appendix.\nConvergence of VDAC frameworks\n(Foerster et al.\n2018) establish the convergence of COMA based on the\nconvergence proof of single-agent actor-critic algorithms\n(Konda and Tsitsiklis 2000; Sutton et al. 2000). In the same\nmanner, we utilize the following lemma to substantiate the\nconvergence of VDACs to a locally optimal policy.\nLemma 1: For a VDAC algorithm with a compatible\nTD(1) critic following a policy gradient\ngk = Eπ\n\" X\na\n∇θk log π(ua|τ a)A(s, u))\n#\n,\nat each iteraction k, lim infk||∇J|| = 0\nw.p.1.\nProof The VDAC gradient is given by:\ng = Eπ\n\" X\na\n∇θ log π(ua|τ a)A(s, u)\n#\n,\n(9)\nA(s, u) = Q(s, u) − Vtot(s). We ﬁrst consider the expected\ndistribution of the baseline Vtot:\ngb = −Eπ\n\" X\na\n∇θ log π(ua|τ a)Vtot(s)\n#\n= −Eπ\n\"\n∇θ log\nY\na\nπ(ua|τ a)Vtot(s)\n#\n,\n(10)\nwhere the distribution Eπ is with respect to the stateaction distribution induced by the joint policy π. Writing the joint policy as a product of independent actors π(u|s) = Q\na π(ua|τ a). The total value does not\ndepend on agent actions and is given by Vtot(s)\n=\nf(V1(o1), . . . , Vn(on)) where f is a non-negative function. This yields a single-agent actor-critic baseline: gb =\n−Eπ[∇θ log π(u|s)Vtot(s)].\nNow let dπ(s) be the discounted ergodic state distribution\nas deﬁned by (Sutton et al. 2000):\ngb = −\nX\ns\ndπ(s) X\nu\n∇θ log π(u|s)Vtot(s)\n= −\nX\ns\ndπ(s)Vtot(s)∇θ\nX\nu\nlog π(u|s)\n= −\nX\ns\ndπ(s)Vtot(s)∇θ1\n= 0\n(11)\nThe remainder of the gradient is given by:\ng = Eπ\n\" X\na\n∇θ log π(ua|τ a)Q(s, u)\n#\n= Eπ\n\"\n∇θ log\nY\na\nπ(ua|τ a)Q(s, u)\n#\n,\n(12)\nwhich yields a standard single-agent actor-critic policy gradient g = Eπ[∇θ log π(u|s)Q(s, u)]. (Konda and Tsitsiklis\n2000) establish that an actor-critic that follows this gradient\nconverges to a local maximum of the expected return Jπ,\nsubject to assumptions included in their paper.\nIn the naive critic framework, Vtot(s) is evaluated by the\ncentral critic and does not depend on agent actions. Hence,\nby following the same proof in Equation 11, we can show\nthat the expectation of naive critic baseline is also 0, thus\nproves naive critic also converges to a locally optimal policy.",
        "experiments": "In this section, we benchmark VDACs against the baseline algorithms listed in Table 1 on a standardized decentralised StarCraft II micromanagement environment, SMAC\n(Samvelyan et al. 2019). SMAC consists of a set of StarCraft\nII micromanagement games that aim to evaluate how well\nindependent agents are able to cooperate to solve complex\ntasks. In each scenario, algorithm-controlled ally units ﬁght\nagainst enemy units controlled by the built-in game AI. An\nepisode terminates when all units of either army have died\nor when the episode reached the pre-deﬁned time limit. A\ngame is counted as a win only if enemy units are eliminated.\nThe goal is to maximize the win rate.\nWe consider the following maps in our experiments:\n2s vs 1sc, 2s3z, 3s5z, 1c3s5z, 8m, and bane vs bane. Note\nthat all algorithms are trained under A2C framework where\n8 episodes are rolled out independently during the training.\nRefer to Appendix for training details and map conﬁguration.\nWe perform the following ablations to answer the corresponding research questions:\nAblation 1\nIs the TD advantage gradient sufﬁcient to optimize multi-agent actor-critics? The comparison between the\nnaive critic and COMA will demonstrate the effectiveness of\nTD advantage policy gradients because the only signiﬁcant\ndifference between those two methods is that the naive critic\nfollows a TD advantage policy gradient whereas COMA follows the COMA gradient (Equation 4).\nAblation 2\nDoes applying state-value factorization improve the performance of actor-critic methods? VDAC-sum\nand IAC, both of which do not have access to extra state information, shares an identical structure. The only difference\nis that VDAC-sum applies a simple state-value factorization\nwhere the global state-value is a summation of local state\nvalues. The comparison between VDAC-sum and IAC will\nreveal the necessity of applying state-value factorization.\nAblation 3\nCompared with QMIX, does VDAC provide\na reasonable trade-off between training efﬁciency and algorithm performance? We train VDAC and QMIX under A2C\ntraining paradigm, which is proposed to promote training efﬁciency, and compare their performance.\nAblation 4\nWhat are the factors that contribute to the performance of the proposed VDAC? We investigate the necessity of non-linear value-decomposition by removing the\nnon-linear activation function in the mixing network. The\nresulting algorithm is called VDAC-mix (linear) and can be\nseen as VDAC-sum with access to extra state information.\n(a) 1c3s5z\n(b) 2s vs 1sc\n(c) 2s3z\n(d) 3s5z\n(e) bane vs bane\n(f) 8m\nFigure 3: Overall results: Win rates on a range of SC mini-games. Black dash line represents heuristic AI’s performance\nOverall Results\nAs suggested in (Samvelyan et al. 2019), our main evaluation metric is the median win percentage of evaluation\nepisodes as a function of environment steps observed over\nthe 200k training steps. Speciﬁcally, the performance of an\nalgorithm is estimated by periodically running a ﬁxed number of evaluation episodes (in our implementation, 32) during the course of training, with any exploratory behaviours\ndisabled. The median performance as well as the 25-75%\npercentiles are obtained by repeating each experiment using\n5 independent training runs. Figure 3 demonstrates the comparison among actor-critics across 6 different maps.\nIn all scenarios, IAC fails to learn a policy that consistently defeats the enemy. In addition, its performance across\ntraining steps is highly unstable due to the non-stationarity\nof the environment and its lack of access to extra state information.\nNoticeably, VDAC-mix consistently achieves the best\nperformance across all tasks. On easy games (i.e, 8m), all algorithms generally perform well. This is due to the fact that a\nsimple strategy implemented by the heuristic AI to attack the\nnearest enemies is sufﬁcient to win. In harder games such as\n3s5z and 2s3z, only VDAC-mix can match or outperform the\nheuristic AI. It is worth noting that VDAC-sum, which cannot access extra state information, matches the naive critic’s\nperformance on most maps.\nAblation 1\nConsistent with (Lowe et al. 2017), the comparison between the naive critic and IAC demonstrates the\nimportance to incorporate extra state information, which is\nFigure 4: 2s vs 1sc (Ablation 1)\nalso revealed by the comparison between COMA and IAC\n(Refer to Figure 3 for comparisons between naive critic and\nCOMA across different maps.). As shown in Figure 3, naive\ncritic outperforms COMA across all tasks. It reveals that\nit is also viable to use a TD advantage policy gradients in\nmulti-agent settings. In addition, COMA’s training is unstable, as can be seen in Figure 4, which might arise dues to\nits inability to predict accurate counterfactual action-value\nQa(s, (u−a, ua)) for un-taken actions.\nFigure 5: 2s vs 1sc (Ablation 2)\nAblation 2\nDespite the similarity in structure of VDACsum and IAC, VDAC-sum’s median win rates at 2 million\ntraining step exceeds IAC’s consistently across all maps (Refer to Figure 3 for comparisons between VDAC-sum and\nIAC across 6 different maps.). It reveals that, by using a simple relationship to enforce equation 6, we can drastically improve multi-agent actor-critic’s performance. Furthermore,\nVDAC-sum matches naive critic on many tasks, as shown in\nFigure 5, demonstrating that actors that are trained without\nextra state information can achieve similar performance to\nnaive critic by simply enforcing equation 6. In addition, it\nis noticeable that, compared with naive critic, VDAC-sum’s\nperformance is more stable across training.\nFigure 6: 2s vs 1sc (Ablation 3)\nAblation 3\nFigure 6 shows that, under the A2C training paradigm, VDAC-mix outperforms QMIX in map\n2s vs 1sc. It is also noticeable that QMIX’s performance is\nunstable across the training steps in map 2s vs 1sc. In easier\ngames, QMIX’s performance can be comparable to VDACmix. In harder games such as 3s5z, VDAC-mix’s median test\nwin rates at 2 million training step outnumber QMIX’s by\n71%. Refer to Appendix for complete comparisons between\nVDACs and QMIX.\nFigure 7: 3s5z (Ablation 4)\nAblation 4\nFinally, we introduced VDAC-mix (linear),\nwhich can be seen as a more general VDAC-sum that has\naccess to extra state information. Consistent with our previous conclusion, the comparison between VDAC-mix (linear)\nand VDAC-sum shows that it is important to incorporate extra state information. In addition, the comparison between\nVDAC-mix and VDAC-mix (linear) shows the necessity\nof assuming the non-linear relationship between the global\nstate value Vtot and local state values V a, ∀a ∈ {1, . . . , n}.\nRefer to Appendix for comparisons between VDACs across\nall maps.",
        "conclusion": "In this paper, we propose a new credit-assignment actorcritic framework that enforces the monotonic relationship between the global state-value and the shaped local\nstate-value. Theoretically, we establish the convergence of\nthe proposed actor-critic method to a local optimal. Empirically, benchmark tests on StarCraft micromanagement\ngames demonstrate that our proposed actor-critic bridges\nthe performance gap between multi-agent actor-critics and\nQ-learning, and our method provides a balanced trade-off\nbetween training efﬁciency and performance. Furthermore,\nwe identify a set of key factors that contribute to the performance of our proposed algorithms via a set of ablation experiments. In future work, we aim to implement our framework in real-world applications such as highway on-ramp\nmerging of semi or full self-driving vehicles.",
        "summary_en": "The exploitation of extra state information has been an active research area in multi-agent reinforcement learning (MARL). QMIX represents the joint action-value using a non-negative function approximator and achieves the best performance on the StarCraft II micromanagement testbed, a common MARL benchmark. However, this paper's experiments demonstrate that, in some cases, QMIX performs sub-optimally with the A2C framework, a training paradigm that promotes algorithm training efficiency. To obtain a reasonable trade-off between training efficiency and algorithm performance, the paper extends value-decomposition to actor-critic methods that are compatible with A2C and proposes a novel actor-critic framework, value-decomposition actor-critic (VDAC). The paper evaluates VDAC on the StarCraft II micromanagement task and demonstrates that the proposed framework improves median performance over other actor-critic methods. Furthermore, the paper uses a set of ablation experiments to identify the key factors that contribute to the performance of VDAC.",
        "summary_zh": "这篇论文介绍了一个VDAC框架。利用额外状态信息一直是多代理强化学习的一个活跃研究领域。QMIX 使用非负函数近似值来表示联合行动值，并在StarCraft II微观管理测试平台上取得了最佳性能。而作者指出，在某些情况下，QMIX在A2C框架下表现不佳，而A2C是一种促进算法训练效率的训练范式。为了在训练效率和算法性能之间取得合理的平衡，作者将价值分解扩展到与A2C兼容的行为批判方法，并提出了VDAC框架。作者在StarCraft II微管理任务上对VDAC进行了评估，结果表明该框架相对于其他A2C方法，提升了中位性能。此外，作者还通过一系列消融实验确定了影响VDAC性能的关键因素。"
    },
    {
        "title": "Self-Supervised Learning for Anomalous Channel Detection in EEG Graphs: Application to Seizure Analysis",
        "abstract": "Electroencephalogram (EEG) signals are effective tools towards seizure analysis where one of the most important challenges is accurate detection of seizure events and brain regions in which seizure happens or initiates. However, all existing machine learning-based algorithms for seizure analysis require access to the labeled seizure data while acquiring labeled data is very labor intensive, expensive, as well as clinicians dependent given the subjective nature of the visual qualitative interpretation of EEG signals. In this paper, we propose to detect seizure channels and clips in a self-supervised manner where no access to the seizure data is needed. The proposed method considers local structural and contextual information embedded in EEG graphs by employing positive and negative sub-graphs. We train our method through minimizing contrastive and generative losses. The employ of local EEG sub-graphs makes the algorithm an appropriate choice when accessing to all EEG channels is impossible due to complications such as skull fractures. We conduct an extensive set of experiments on the largest seizure dataset and demonstrate that our proposed framework outperforms the state-of-the-art methods in the EEG-based seizure study. The proposed method is the only study that requires no access to the seizure data in its training phase, yet establishes a new state-of-the-art to the field, and outperforms all related supervised methods.",
        "introduction": "Epilepsy is one of the most prevalent neurological disorders affecting 65 million people (Johnson 2019). Patients\nwith epilepsy suffer from sudden and unforeseen seizures,\nduring which they are vulnerable to injury, suffocation and\ndeath (Surges et al. 2021). Epileptic seizure detection makes\nit possible to identify more accurately the epileptogenic\nzone (EZ), which is the brain area responsible for initiating seizures (Sharma, Rai, and Tewari 2018), and the surgical resection of EZ renders epilepsy patients seizure-free. As\nepilepsy can start at any age, early detection is crucial to prevent further damage during physiological development and\nto increase life expectancy (Beghi and Giussani 2018).\nScalp electroencephalogram (EEG) has been considered\nas the most common tool to detect seizures (Kuhlmann et al.\n2018). EEG measures the voltage changes between electrodes by the ionic currents flowing within the brain neurons and provides spatial-temporal information of the brain.\nHowever, detecting seizures in EEGs requires a direct examination by experienced EEG readers that requires a huge\namount of time and effort. Moreover, different opinions of\nexperts can cause discrepant diagnostic results (Yan et al.\n2018). Therefore, the development of automated and objective methods for epileptic seizure detection is needed.\nAlthough many studies have proposed deep learning (DL)\nbased models for automated seizure detection (Saab et al.\n2020; Shoeibi et al. 2021; Abdelhameed and Bayoumi 2021;\nThuwajit et al. 2021; Khalkhali et al. 2021; Rashed-AlMahfuz et al. 2021; Mahajan, Somaraj, and Sameer 2021;\nSaichand et al. 2021; Shen et al. 2022; Gao et al. 2022),\nseveral challenges still remain unsolved. First, these studies train their proposed model in a supervised approach –\ni.e. availability of labeled seizure data during training is required. This does not address the challenge from a clinical\nperspective as seizure labels obtaining is difficult and labour\nexpensive during the process of manually seizure marking,\nhence very scarce in real-world applications.\nSecond, these studies apply deep convolutional neural\nnetworks (CNNs) directly to the time-series signals or spectrograms hence ignore the critically important information\nphysical distance-based connectivity and functional-based\nconnectivity between different brain regions. Several studies\nhave recently proposed graph learning techniques to capture\nthe relationships between EEG electrodes (aka EEG nodes)\n(Wang et al. 2020; Wagh and Varatharajah 2020; Saboksayr, Mateos, and Cetin 2021; Tang et al. 2021; Saboksayr,\nMateos, and Cetin 2021; Mathur and Chakka 2022). However, they all fail to take into consideration the local patterns (e.g. local sub-graphs and sub-structures) when learning EEG graphs. Such local information could be effective\nwhen detecting anomalies in EEG graphs. The effectiveness\nof such local information has been recently demonstrated in\napplications such as analyzing social, traffic, citation, and\nfinancial transaction networks (Liu et al. 2021a,b; Zheng\net al. 2021). However, its effectiveness in analyzing EEG\nsignals, that are inherently non-stationary and dynamic, has\nremained unexplored to date.\nThird, as is discussed before, in real-world applications,\nthere is not enough training data from seizure class; while\nthe existence of adequate data from both normal and seizure\nclasses is essential when training supervised algorithms. To\nhandle the imbalance-data issue, some graph-based methods\nemploy graph augmentation (Wagh and Varatharajah 2020;\nTang et al. 2021). However, in EEG graphs, not every augmentation technique is effective as some may corrupt the\nunderlying brain region connectivities. Hence, pinpointing\nappropriate augmentation strategies in EEG graphs that preserve the underlying semantic information is necessary towards proving accurate seizure detection and localization.\nBased on the above observations, we propose a novel approach for the detection of abnormal brain regions and EEG\nchannels, called Contrastive and Generative Self-supervised\nLearning for EEG Graphs (EEG-CGS). Although the effectiveness of contrastive learning methods, as a selfsupervised learning technique, has been demonstrated for\nanomaly detection (Li et al. 2021; Hojjati, Ho, and Armanfard 2022) and graph analysis in general (Zeng and Xie\n2021; Xie et al. 2022), its ability on analysing the EEG\ngraphs has remained unexplored.\nThe main contributions of this paper are as follows:\n• We propose a self-supervised method for identifying abnormal brain regions and EEG channels without having access to the abnormal class data during the training phase. To the best of our knowledge, this is the first\nstudy for unsupervised identification of abnormal EEG\nchannels and regions.\n• We propose to model brain regions and their connectivities using attributed graphs where each EEG channel is\nconsidered as a graph node. Each node is associated with\na feature vector constructed from the corresponding EEG\nsignal. Nodes are connected based on four different metrics including nodes Euclidean distance, randomly connection of nodes, node features correlations, and directed\ntransfer function; the first two are meant to capture the\ngeometry of EEG channels and the last two are for capturing connectivity of brain regions.\n• We propose an effective augmentation approach to create\nthe positive and negative pairs. Augmentations are based\non sub-graphs hence allowing to capture the local structural and contextual information.\n• The proposed self-supervised method is realized through\nemploying contrastive and generative learning techniques. We define a channel-based anomaly score function which is a linear combination of the contrastive and\nreconstruction losses.\n• Performance of the proposed abnormal EEG node and\nregion detection is demonstrated on the largest and most\ncomprehensive EEG seizure dataset TUSZ. We show that\nthe proposed EEG-CGS establishes a new state-of-theart on this dataset. EEG-CGS significantly outperforms\nall existing (supervised) seizure detection techniques.\n• The proposed technique can be considered as a general approach for the detection and localization in other\nbrain disorders. The sub-graph based nature of EEGCGS makes it a suitable choice for applications where\nnot all EEG channels are available or reliable during\ntraining. E.g. in coma outcome prediction application,\nthe scalp of comatose patients is usually uneven with\nfractions so that recording of all EEG channels is rarely\npossible (Armanfard et al. 2018).",
        "proposed method": "The block diagram of the proposed method is shown in Figure 1. The proposed EEG-CGS for node anomaly detection\nconsists of four components namely EEG graph construction, positive and negative pair sampling, contrastive and\ngenerative learning based on graph neural networks (GNN),\nand anomaly score computation. The following sections discuss these components in detail.\nEEG Graph Construction\nAn EEG graph is denoted as G = (A, X) where A = (V, E).\nA ∈ Rn×n is the adjacency matrix which its element on the\nith row and jth column, i.e. aij, is greater than zero if a\nconnectivity exists between EEG electrodes vi and vj. V =\n{v1, ..., vn} is the set of n electrodes (aka nodes), E is the\nset of m edges, X ∈ Rn×d is the feature matrix, d denotes\nthe number of attributes (aka features) representing an EEG\nsignal. The ith row of X, denoted by xi, is the feature vector\nrepresenting the ith EEG channel. For a given EEG clip, we\nbuild four types of EEG graphs:\n• Dist-EEG-Graph endeavors to embed the structure of\nelectrode locations in the graph’s adjacency matrix, using Euclidean distance between electrodes (Tang et al.\n2021). As the electrode locations are fixed in an EEG\nrecording cap, the same adjacency matrix is obtained for\nall EEG clips. More specifically, aij of Dist-EEG-Graph\nis obtained as follows:\naij =\n(\nexp(− ∥vi−vj∥2\nζ2\n)\nif ∥vi − vj∥ ≤ k\n0\nif O.W.\n,\n(1)\nwhere ∥·∥ denotes ℓ2-norm and ζ is a scaling constant.\nThe closer the two electrodes vi and vj are, the closer aij\nis to 1. In all our experiments k is set to 0.9 for all EEG\nclips. Setting aij = 0 for far away electrodes imposes\nsparsity to the graph.\n• Rand-EEG-Graph is built based on the assumption that\nall electrodes are connected and equally contribute in\nbrain activities. This graph is realized through forming\nan adjacency matrix as follows:\naij =\n\u001a\n0.5\nif i ̸= j\n1\nif O.W. .\n(2)\nIn this way, every edge has the chance of being present\nin the graph.\n• Corr-EEG-Graph is meant to capture the functional connectivity between electrodes. It is encoded in the adjacency matrix elements defined as below:\naij =\n(\nxcorr(xi,xj)\n∥xi∥∥xj∥\nif vj ∈ N(vi)\n0\nif O.W.\n,\n(3)\nwhere xcorr(·,·) denotes cross correlation function,\nN(vi) presents the top-3 neighborhood nodes of vi that\nFigure 1: The overall framework of EEG-CGS where d′ = 3, α = 4 and τ1 = 0.4. The positive and negative pairs are shown\nfor a typical target node C3.\nhave the highest normalized correlation. N(vi) is set to\nthe top-3 neighborhood nodes to avoid overly connected\ngraphs. We only keep the top three edges (i.e., m = 3)\nfor each node to avoid overly connected graphs.\n• DTF-EEG-Graph is Directed Transfer Function Graph\n(Blinowska 2011) that is meant to capture the mutual\ninfluence between EEG channels, hence it models functional connectivity of the brain regions. The adjacency\nmatrix for this graph is as follows:\naij =\n(\nxcorr(xi,xj)\n√Pn\nm=1,m̸=i,j ∥xcorr(xi,xm)∥2\nif vj ∈ N(vi)\n0\nif O.W.\n,\n(4)\naij takes a value from 0 to 1, and denotes a ratio between\nthe correlation of the attributes of vi and vj to the correlations of the features of all the remaining nodes and\nxi. As similar to Corr-EEG-Graph, we set N(vi) to the\ntop-3 neighbors of vi.\nPositive and Negative Pair Sampling\nFor every EEG clip in an input mini-batch B, we first construct the four graphs discussed in the EEG Graph Construction section (see this construction block in Figure 1). Then,\nwe create positive and negative sub-graphs for each clip to\nbe used in its following step where the algorithm’s networks\nare trained using reconstruction and contrastive losses.\nWe create two positive and one negative sub-graphs for\nevery node in every constructed EEG graph. To this end,\nwe first select an electrode as target node vt and then select the two positive sub-graphs G+\ns1 and G+\ns2, and one negative sub-graph G−\ns1. Each target node is assigned to its own\nsub-graphs. To form G+\ns1 and G+\ns2, we leverage a random\nwalk with restart (RWR) technique (Tong, Faloutsos, and\nPan 2006) by which sub-graphs are centered at the target\nnode with the fixed size α which controls the radius of the\nsurrounding contexts. In the other words, α determines the\nnumber of nodes within the sub-graphs. To impose generalization to the system, we anonymize the target node in the\npositive sub-graphs by replacing its feature vector with an\nall-zero vector. To form the negative sub-graph G−\ns1, we first\nfind the farthest electrode from the target node and then apply the RWR to the farthest node with a same α value as\nof the one used for the positive sub-graphs. In Figure 1, α\nis set to 4 indicating 4 nodes are considered in the positive\nand negative sub-graphs where the farthest node to the target\nnode (i.e. C3) is T4.\nContrastive and Generative Learning\nAs is shown in Figure 1, we employ GNN autoencoder to\nrealize the unsupervised contrastive and generative learning\nobjectives. The autoencoder consists of two networks: encoder and decoder, respectively denoted as GNNe(·, ·) and\nGNNd(·, ·). Encoder takes a graph as input and embed it to a\nlower dimensional space while decoder takes the encoder’s\noutput as input and endeavors to reconstruct the encoder’s\ninput.\nAssume a sub-graph Gs = (As, Xs), where As ∈ Rα×α\nand Xs ∈ Rα×d respectively denote the sub-graph adjacency matrix and the node attributes. The encoder takes Gs\nas input and embed it to a lower dimensional space as below:\nEs = GNNe(Xs, As) ≜ ReLu( ˆ\nAsXsWe),\n(5)\nwhere Es ∈ Rα×d′, d′ ≪ d, We denotes the encoder\ntrainable parameters, and ReLu is the Rectified Linear Unit.\nˆ\nAs = ˜D\n− 1\ns 2\n˜\nAs ˜D\n− 1\ns 2\n, ˜\nAs = As + I is the adjacency matrix with self-loop, ˜Ds is the diagonal node degree matrix\nof the sub-graph where its ith diagonal element is equal\nto P\nj ˜asij. I is the identity matrix. For more details, see\n(Zhang et al. 2021).\nWe also embed the target node to the lower dimensional\nspace defined by the encoder. Since a single node vi has no\nadjacency matrix, we define its embedding as below:\nei = ReLu(xiWe).\n(6)\nIn summary, for each constructed graph, we feed three\nsub-graphs and one target node to the GNNe to create their\nembeddings in the lower dimensional space, as is illustrated\nin the output of GNNe in Figure 1 where d′ is set to 3.\nContrastive Learning Module:\nTo learn the local topological structure of the graphs, we propose to employ contrastive learning module. In order to create positive and negative pairs to be used for the purpose of contrastive learning,\nwe first take an average over all the α rows of Es to map it\nto a d′ dimensional vector, called rs ∈ R1×d′.\nSince we have formed two positive and one negative subgraphs for each target node, we define two positive and one\nnegative pairs. The positive pairs are P+\n1\n= (et, r+\ns1) and\nP+\n2\n= (et, r+\ns2), and the negative pair is P−\n1\n= (et, r−\ns1).\nEmbedding of the target node vt is denoted by et, r+\ns{1,2}\nindicates the d′ dimensional vector obtained by taking an\naverage of the E+\ns{1,2} = GNNe(G+\ns{1,2}), and r−\ns1 indicates\nthe d′ dimensional vector obtained by taking average of the\nE−\ns1 = GNNe(G−\ns1).\nWe introduce a trainable scoring matrix Ws ∈ Rd′×d′\nwhere the similarity of et to r+\ns1 is obtained as below:\nSim+\nt,1 = σ(etWsr+⊤\ns1 ),\n(7)\nwhere σ(.) is the logistic function that outputs a value between 0 and 1 and (·)⊤ denotes the transpose operator. Similarly, Sim+\nt,2 and Sim−\nt,1 can be obtained by replacing r+\ns1 in\n(7) with r+\ns2 and r−\ns1, respectively.\nFinally, inspired by (Zheng et al. 2021), we define the contrastive loss function, Lcon as below:\nLcon = −\n1\n2n|B|\n2\nX\nq=1\nn\nX\nt=1\n\u0010\nlog(Sim+\nt,q) + log(1 − Sim−\nt,1)\n\u0011\n,\n(8)\nwhere |B| indicates the number of EEG clips in the minibatch B, and n denotes the number of all existing EEG electrodes (e.g., n is 19 in a 10-20 EEG cap recording system).\nGenerative Learning Module:\nThis module is to learn\nthe contextual information hidden in the graph through reconstructing the target node anonymized in the positive subgraphs G+\n1 and G+\n2 , using the other node features and edges\nof the sub-graph. More specifically, we reconstruct the input sub-graph G+\ns{1,2} using its embedded version E+\ns{1,2},\ni.e. ˆG+\ns{1,2} = GNNd(E+\ns{1,2}) while minimizing the below\nreconstruction loss:\nLrec =\n1\n2n|B|\n2\nX\nq=1\nn\nX\nt=1\n∥ˆxt,q − xt∥2 ,\n(9)\nwhere ˆxt,q is the reconstructed vector of the target node xt,\ntaken from the reconstructed graph ˆG+\nsq = GNNd(E+\nsq). We\ndenote the parameters of the decoder as Wd.\nFinally, we define the total loss L through weighted sum\nof the reconstruction and contrastive losses, as below:\nL = λLcon + (1 − λ)Lrec,\n(10)\nwhere λ is the balancing hyperparameter.\nAnomaly Score\nFrom the contrastive and generative modules, for a node\nvi ∈ V, we define the anomaly scoring function as below:\nf(vi) = λfcon(vi) + (1 − λ)frec(vi),\n(11)\nwhere fcon(vi) =\n1\n2\nP2\nq=1 δ(Sim−\ni,1 − Sim+\ni,q) is the contrastive anomaly score, and frec(vi) =\n1\n2\nP2\nq=1 δ(||ˆxi,q −\nxi||2) is the reconstruction anomaly score, and δ is the MinMaxScaler function (Bisong 2019) that scales the scores to\nthe range [0, 1].\nThe anomaly score for all nodes is calculated. If vi is an\nanomaly node from structural point of view, both Sim+\ni,q,\nq ∈ {1, 2} and Sim−\ni,1 would be close to 0.5 as there exists a mismatch between vi and the corresponding node in\nsub-graphs G+\nsq,G−\ns1 used for training the algorithm networks,\nbecause we only use normal clips and nodes in the training phase. Here, the scaling function δ maps 0 to 1, hence\nfcon(vi) would be equal to 1. If vi is an anomaly node from\ncontextual point of view, frec(vi) would be close to 1 as the\ndecoder cannot reconstruct an anomaly since the networks\nare trained on only normal nodes and clips. If vi is a normal\nnode, both of fcon(vi) and frec(vi) would be close to 0. In\nother words, an anomaly (normal) node would result into a\nhigh (low) anomaly score.\nIn the inference phase, a node is considered as an anomaly\nif its corresponding anomaly score is higher than a threshold\nτ1. An EEG clip is considered as seizure-free (i.e. normal) if\nthere are less than τ2 abnormal nodes within its corresponding EEG graph, otherwise it is detected as a seizure clip. The\ninference phase for a seizure clip is illustrated in Figure 1 at\nblock (4).\nData\nPatients\n(% SZ)\nEEG Files\n(% SZ)\nEEG Clips\n(% SZ)\nTrainSupervised\n591 (34.0%)\n4,599 (18.9%)\n38,613 (9.3%)\nTrainEEG-CGS\n493 (0%)\n4,028 (0%)\n35,019 (0%)\nTest\n45 (77.8%)\n900 (25.6%)\n8,848 (14.7%)\nTable 1: Train and test sets of TUSZ used in the supervised\nmethod and EEG-CGS. The percentages of the seizure data\n(SZ) is indicated in parenthesis.",
        "experiments": "We use the public benchmark dataset, Temple University\nHospital EEG Seizure Corpus (TUSZ) v1.5.2 (Shah et al.\n2018), which contains the largest number of seizure categories and patient samples. TUSZ is recorded over years,\nand recorded by different equipment generations from subjects of all ages. Thus, the variance is much more than other\navailable EEG datasets in the seizure study, hence it is the\nmost challenging dataset for seizure detection. The number\nof existing channels is 19 recorded in the standard EEG 1020 system, as is shown in Figure 1. Table 1 summarizes the\nTUSZ dataset used in our experiments. As is shown in the\ntable, in the training phase, we use the same number of normal clips as is used in other supervised methods, and do not\nuse any seizure clips. In the test phase, the same number of\ntest clips, including seizure and normal clips, is used in our\ncomparison supervised methods and our method. To evaluate the model’s ability in seizure location detection, we use\nthe available annotations on focal and generalized seizure\ntypes from 23 distinct patients. Note that compared with\nother types of seizures, focal and generalized types are most\npresent in epilepsy patients.\nThe performance of our proposed method is quantified\nusing five criteria including Receiver Operating Characteristics - Area Under the Curve (ROC-AUC), Precision (Pre.),\nF1 score (F1), Sensitivity (Sen.) and Specificity (Spec.).\nWe compare our proposed EEG-CGS with two streams of\nDL-based methods 1: (1) DL models in the EEG time-series\nand/or spectrograms domain, including EEGNet (Lawhern\net al. 2018), EEG-TL (Khalkhali et al. 2021), Dense-CNN,\nLSTM and CNN-LSTM (Tang et al. 2021); and (2) DL models in the EEG graph domain (Tang et al. 2021). Unlike\nour method, these DL models make use of the seizure data\nand their corresponding labels in the training phase. For a\nfair comparison, we test the methods on the same test data,\nwhilst our method is trained on a much smaller number of\ntraining sample because we do not include any seizure data\nin the training phase.\nWe introduce four variations of our EEG-CGS method\nbased on the type of constructed graph used as the input. In\nthe following, we refer to these variations as EEGd-CGS,\nEEGr-CGS, EEGc-CGS, EEGf-CGS and EEGt-CGS, respectively corresponding to the cases where the employed\ninput graph is Dist-EEG-Graph, Rand-EEG-Graph, CorrEEG-Graph, DTF-EEG-Graph and the case where all the\n1To provide a fair comparison, we do not include algorithms\nthat require training their model on an extra EEG dataset (using\ntransfer learning), in addition to the given TUSZ data.\nGraph Type\nPre\nF1\nSen\nSpec\nEEGd-CGS\n0.846\n0.771\n0.688\n0.945\nEEGr-CGS\n0.877\n0.881\n0.887\n0.979\nEEGc-CGS\n0.861\n0.843\n0.825\n0.953\nEEGf-CGS\n0.844\n0.832\n0.822\n0.967\nEEGt-CGS\n0.901\n0.883\n0.920\n0.989\nTable 2: Synthetic anomalous node detection performance.\nThe best performance for each metric is shown in bold.\nfour graph types are concatenated and fed to the system\nas the input representing the given input EEG clip. In our\nexperiments, following (Tang et al. 2021), we simply use\nthe Fast Fourier Transform coefficients as attributes of EEG\nchannels , although adding other attributes such as Wavelet\ncoefficients might also be useful, which is out of the scope\nof this paper.\nThe hyperparameters d, k, m, d\n′, α, λ, τ1, τ2 of the\nproposed method are respectively set to 6000, 0.9, 3, 256,\n4, 0.6, 0.4, 1. To have a fair comparison, all hyperparameters of the proposed method are fixed over all experiments.\nRegarding the EEGt-CGS case, we also set the hyperparameters to be the same as the values mentioned above; where at\nthe inference phase, we take an average of anomaly scores\ncomputed for all nodes of the four graph types – a node is\npredicted as an anomaly if the average score is greater than\nτ1. Based on the average anomaly scores for all nodes, an\nEEG clip is predicted as seizure if the number of predicted\nanomalous nodes (over all clips) is greater than τ2. Hyperparameters of our comparison methods are set to the values\nsuggested by their authors.\nThe performance of our proposed method is demonstrated\nfor anomalous channel detection, seizure clip detection and\nseizure channel detection.\nDetection of Synthetic Anomalous Channels\nThis section is to verify the performance of the proposed\nmethod for reliable anomalous channel detection. To this\nend, we create a synthetic test set using the normal clips\nused in the training phase. More specifically, we first average every 35 normal clips (with no overlap). We then, with\nprobability of 3%, inject an anomaly node to the average\nclip. A node of the selected average clip will be corrupted\nwith a probability of 0.03%, and no more than one node is\ncorrupted per average clip. A node is corrupted both contextually and structurally. We perform two types of corruptions\nto make vi abnormal: (1) we connect vi to all other nodes in\nthe average clip – i.e. we set aij = 1 for j = 1, . . . , n; (2) we\ncorrupt the attribute vector of vi by replacing its feature vector with the feature vector of the node in the average clip that\nhas the largest Euclidean distance to vi. We then feed the average clips (some have injected anomalies) to the EEG-CGS\nnetworks trained on the pure normal clips and let the trained\nsystem compute the anomaly score for all channels. If the\nanomaly score is greater than the pre-defined threshold τ1,\nthe corresponding channels are marked as anomalies.\n(Liu et al. 2021a) demonstrated that, for node anomaly detection in the test phase, it would be more effective to create\nApproach\nMethod\nF1\nSen\nSpec\nSupervised\nEEGNet\n0.474\n0.299\n0.902\nEEG-TL\n0.420\nNA\nNA\nDense-CNN\n0.404\n0.451\n0.869\nLSTM\n0.365\n0.463\n0.814\nCNN-LSTM\n0.330\n0.363\n0.857\nDist-DCRNN\n0.341\n0.326\n0.932\nCorr-DCRNN\n0.448\n0.457\n0.900\nSelfSupervised\n(ours)\nEEGd-CGS\n0.487\n0.481\n0.932\nEEGr-CGS\n0.496\n0.465\n0.942\nEEGc-CGS\n0.521†\n0.497†\n0.942\nEEGf-CGS\n0.516\n0.474\n0.952†\nEEGt-CGS\n0.534\n0.501\n0.974\nTable 3: Seizure clips detection. The best and the secondbest performances for each metric are denoted in bold and †.\nGraph Type\nPre\nF1\nSen\nSpec\nEEGd-CGS\n0.668\n0.474\n0.367\n0.656\nEEGr-CGS\n0.631\n0.334\n0.327\n0.748\nEEGc-CGS\n0.667\n0.433\n0.424\n0.745\nEEGf-CGS\n0.659\n0.394\n0.378\n0.735\nEEGt-CGS\n0.704\n0.551\n0.432\n0.775\nTable 4: Seizure channels detection performance. The best\nperformance for each metric is shown in bold.\nmultiple contrastive pairs to capture more neighbors of the\ntarget node. (Note that in the training phase, we created only\ntwo contrastive pairs.) As such, following (Liu et al. 2021a),\nwe compute the anomaly score for 80 randomly selected\nsub-graphs that include both close and far nodes around the\ntarget node. The final anomaly score of a target node is then\nthe average of the 80 calculated ones. This strategy makes\nthe final anomaly score more reliable. Our experiments show\nhigh detection performance can be obtained for a wide range\nof contrastive pair numbers.\nNode anomaly detection results for the four variations of\nour EEG-CGS method are reported in Table 2. The results\ndemonstrate the effectiveness of our unsupervised method\nfor anomalous EEG channel detection. Note that this is the\nonly existing study for anomalous EEG channel detection\nwith access to no abnormal data. The reported results, confirm the effectiveness of the reconstructed graphs and that\nincluding all graphs for detection results in a more consistent performance across all metrics.\nDetection of Seizure Clips and Channels\nThe performance of the proposed method along with our\ncomparison methods, for seizure clip detection, are shown in\nTable 3. Note that the proposed method is a self-supervised\n1-class classification technique while all the comparison\nmethods are supervised 2-class classification methods. The\ncompared methods are the current state-of-the-art in the\nseizure detection research field. Our method significantly\noutperforms all the comparison methods in all metrics and\nset a new SOTA in the field though it is trained with only\nnormal data compared to the other methods that are trained\nwith much more data including both normal and abnormal.\nFigure 2: Visualization of EEG graphs before, begining and\nduring a typical seizure (SZ) incident. The top and bottom\nbars in each graph indicate the level of connectivity and node\nstrength, respectively. Darker colors indicate higher values.\nTo show the effectiveness of our method for anomalous\nchannel detection, in a seizure test clip, we mark the channel with an anomaly score greater than τ1 as an anomaly\nand compare them with the ground-truth of seizure channels\nprovided in the TUSZ dataset. The performance is reported\nin Table 4. As it can be seen, our method can effectively\nidentify the anomalous node in a real-world dataset, as well.\nFigure 3 visualizes the anomaly score of the EEG channels for a typical focal seizure clip and a typical generalized\nseizure clip. Subfigure (a) shows that, as is expected, the\nchannels with high anomaly scores (e.g., FP2 to C4 and F8\nto T6) are focused on specific brain regions that also match\nthe ground-truth. For the generalized seizure case shown in\nsubfigure (b), as is expected, the EEG-CGS method correctly tells us that anomalous channels are spread across the\nbrain, not focused on a specific region as opposed to the focal seizure case shown in (a).\nVisualization of Graph’s Nodes Connectivity\nThis section is to visualize the EEG graph level of nodes\nconnectivity (i.e. adjacency matrix) changes when a seizure\nhappens. At each graph, the intensity of an edge between\nnodes vi and vj depends on the value of aij in the graph’s adjacency matrix. This figure also visualizes the level of node\nstrength, where the level of strength at node vi is defined\nas Σn\nj=1,j̸=iaij, i.e. the summation of all connectivities per\nnode – if a node is connected to channels with high adjacency values, a high level of strength is assigned to the node.\nAs is discussed in the EEG Graph Construction section,\nthe adjacency matrix of Dist-EEG and Rand-EEG graphs do\nnot change when a seizure happens since they are defined\nbased on the Euclidean distance of node locations and independent from the node attributes. However, the connectivities (i.e. adjacency matrices) vary in the Corr-EEG and\nDTF-EEG graphs when a seizure happens. These changes\nare noticeable when one compares the node connectivities\nand level of strengths before, at the onset, and during a\nseizure incident. As is shown in Figure 2, before a seizure incident (i.e. in normal brain activity), in the Corr-EEG-Graph,\nall nodes have almost similar level of strengths and every\n(a) Focal seizure channels detection versus Ground-truth.\n(b) Generalized seizure channels detection versus Ground-truth.\nFigure 3: Visualization of seizure hannels detection for (a) Focal- and (b) Generalized seizures. Ground-truth channels are\nshaded in pink. Each EEG graph shows the level of the node connectivity (top bar) and node anomaly score (bottom bar). A\ndarker color indicates higher node connectivity and anomaly score.\n(a) Balancing Factor λ\n(b) Dimension of Each Subgraph\n(c) Dimension of Embedding\nFigure 4: Hyper-parameter sensitivity for synthetic anomalous node detection.\nnode is connected to almost similar number of other nodes.\nThis uniform structure changes when the seizure starts and\ndeepen during a seizure episode. The changes happen to the\nDTF-EEG graph are also visualized in the figure. As it can\nbe seen, this graph also carries important information and its\nvariations help in distinguishing a seizure activity from normal. This figure demonstrates the effectiveness of the proposed constructed graphs in identifying seizure channels and\nclips in the self-supervised way.\nHyperparameters Sensitivity\nIn this section, we investigate the effect of hyperparameters\nλ, α, and d′ on the performance of the proposed method.\nIn Figure 4a, we explore the importance of the contrastive\nand generative components through changing the hyperparameter λ in the loss function L defined in (10), where\nλ ∈ {0, 0.6, 1}. λ = 0 (1) is equivalent to removing the\ncontrastive (generative) component of the proposed method,\nwhile setting λ to a moderate value such as 0.6 allows the\nmethod to include both components when training its networks. We explored the performance of EEG{d,r,c,f}-CGS\nfor these three values. The results show that for all method\nvariations, λ = 0.6 provides a better AUC compared to the\nextreme cases λ = 0 and λ = 1.\nIn Figure 4b, we explore the impact of subgraph size, i.e.\nα by selecting α from the set {1, 2, 4, 6, 8}. We observe that\nmore reliable performance (with high AUC value) can be\nobtained for α = 4, i.e. 4 nodes are included in a sub-graph.\nTo investigate the effect of the embedding space\ndimension d′\non the proposed method, we run our\nmethod variations using different d′ values from set\n{16, 32, 64, 128, 256, 512}. The results shown in Figure 4c\ndemonstrate that the method is not too sensitive to this hyperparameter and a reliable and accurate performance can\nbe obtained for a wide range of d′.",
        "conclusion": "This paper is the first attempt on providing a self-supervised\nmethod for anomalous EEG channel detection. The proposed EEG-CGS method, consisting of contrastive and generative components, is trained only on normal EEG data with\nno-access to the abnormal samples. We propose to construct\nEEG attributed sub-graphs to capture the local structural and\ncontextual information. The performance of the proposed\nmethod is demonstrated on both synthetic and real-world\nanomalies. The proposed method sets a new state-of-the-art\nfor seizure detection on the largest and most difficult publicly available seizure dataset. EEG-CGS can be considered\nas an anomalous channel detection technique that can be employed for the detection of other brain disorders. The employ\nof sub-graphs rather than the whole EEG graph also makes\nthe algorithm appropriate in applications, such as coma outcome prognosis, where access to all EEG channels is not\npossible, e.g. due to skull fractures.",
        "summary_en": "Electroencephalogram (EEG) signals are effective tools towards seizure analysis where one of the most important challenges is accurate detection of seizure events and brain regions in which seizure happens or initiates. However, all existing machine learning-based algorithms for seizure analysis require access to the labeled seizure data while acquiring labeled data is very labor intensive, expensive, as well as clinicians dependent given the subjective nature of the visual qualitative interpretation of EEG signals. This paper proposes to detect seizure channels and clips in a self-supervised manner where no access to the seizure data is needed. The proposed method considers local structural and contextual information embedded in EEG graphs by employing positive and negative sub-graphs. The paper trains the method through minimizing contrastive and generative losses. The employ of local EEG sub-graphs makes the algorithm an appropriate choice when accessing to the all EEG channels is impossible due to complications such as skull fractures. The paper conducts an extensive set of experiments on the largest seizure dataset and demonstrate that the proposed framework outperforms the state-of-the-art methods in the EEG-based seizure study. The proposed method is the only study that requires no access to the seizure data in its training phase, yet establishes a new state-of-the-art to the field, and outperforms all related supervised methods.",
        "summary_zh": "这篇论文介绍了一种自监督方法来检测癫痫通道和片段，无需访问癫痫数据。该方法通过使用正负子图，考虑了嵌入在脑电图中的局部结构和上下文信息，并通过最小化对比损失和生成损失来训练。另外，该方法使用局部脑电图子图，使其成为当由于复杂情况而无法访问所有脑电图通道时的合适选择。作者在最大的癫痫数据集上进行了大量实验，结果表明该方法在基于脑电图的癫痫研究中优于现有方法。这种方法是唯一在训练阶段不需要访问癫痫数据的研究，却在领域内树立了新的技术水平，并超越了所有相关的监督方法。"
    },
    {
        "title": "Defending against Backdoors in Federated Learning with Robust Learning Rate",
        "abstract": "Federated learning (FL) allows a set of agents to collaboratively train a model without sharing their potentially sensitive data. This makes FL suitable for privacy-preserving applications. At the same time, FL is susceptible to adversarial attacks due to decentralized and unvetted data. One important line of attacks against FL is the backdoor attacks. In a backdoor attack, an adversary tries to embed a backdoor functionality to the model during training that can later be activated to cause a desired misclassiﬁcation. To prevent backdoor attacks, we propose a lightweight defense that requires minimal change to the FL protocol. At a high level, our defense is based on carefully adjusting the aggregation server’s learning rate, per dimension and per round, based on the sign information of agents’ updates. We ﬁrst conjecture the necessary steps to carry a successful backdoor attack in FL setting, and then, explicitly formulate the defense based on our conjecture. Through experiments, we provide empirical evidence that supports our conjecture, and we test our defense against backdoor attacks under different settings. We observe that either backdoor is completely eliminated, or its accuracy is signiﬁcantly reduced. Overall, our experiments suggest that our defense signiﬁcantly outperforms some of the recently proposed defenses in the literature. We achieve this by having minimal inﬂuence over the accuracy of the trained models. In addition, we also provide convergence rate analysis for our proposed scheme.",
        "introduction": "Federated learning (FL) (McMahan et al. 2016) has been\nintroduced as a distributed machine learning protocol.\nThrough FL, a set of agents can collaboratively train a model\nwithout sharing their data with each other, or any other third\nparty. This makes FL suitable to settings where data privacy\nis desired. In this regard, FL differs from the traditional distributed learning setting in which data is ﬁrst centralized at\na place, and then distributed to the agents (Dean et al. 2012;\nLi et al. 2014).\nAt the same time, FL has been shown to be susceptible\nto backdoor attacks (Bhagoji et al. 2019; Bagdasaryan et al.\n2020). In a backdoor attack, an adversary disturbs the training process to make the model learn a targeted misclassiﬁcation functionality (Chen et al. 2017; Shafahi et al. 2018; Liu\net al. 2018). In centralized setting, this is typically done by\ndata poisoning. For example, in a classiﬁcation task involving cars and planes, the adversary could label all blue cars\nin the training data as plane in an attempt to make the model\nto classify blue cars as plane at the inference/test phase. In\nFL, since the data is decentralized, it is unlikely that an adversary could access all the training data. Thus, backdoor\nattacks are typically carried through model poisoning in the\nFL context (Bhagoji et al. 2019; Bagdasaryan et al. 2020;\nSun et al. 2019). That is, the adversary tries to construct a\nmalicious update that encodes the backdoor in a way such\nthat, when it is aggregated with other agents’ updates, the\naggregated model exhibits the backdoor.\nIn this work, we study backdoor attacks against deep neural networks in FL setting, and formulate a defense. Our solution is based on carefully adjusting the learning rate of\nthe aggregation server during the training. Through experiments, we illustrate that our defense can deter backdoor attacks signiﬁcantly. Further, this achieved with only minimal\ndegradation in the trained model’s accuracy in both i.i.d. and\nnon-i.i.d. settings. We provide empirical evidence justifying\nthe effectiveness of our defense, and also theoretically analyze its convergence properties. In summary, our work signiﬁcantly outperforms some of the existing defenses in the\nliterature, and succeeds in scenarios where they fail.\nThe rest of the paper is organized as follows. In Section 2,\nwe provide the necessary background information. In Section 3, we explain our defense, and in Section 4, we illustrate\nthe performance of our defense under different experimental settings. In Section 5, we discuss and elaborate upon the\nresults of our experiments, and ﬁnally, in Section 6, we provide a few concluding remarks.",
        "background": "Federated Learning (FL)\nAt a high level, FL is multiround protocol between an aggregation server and a set of\nagents in which agents jointly train a model. Formally, participating agents try to minimize the average of their loss\nfunctions\narg min\nw∈Rd f(w) = 1\nK\nK\nX\nk=1\nfk(w),\nwhere fk is the loss function of kth agent. For example, for\nneural networks, fk is typically empirical risk minimization\nunder a loss function L such as cross-entropy, i.e.,\nfk(w) = 1\nnk\nnk\nX\nj=1\nL(xj, yj; w),\nwith nk being the total number of samples in agent’s dataset\nand (xj, yj) being the jth sample.\nConcretely, FL protocol is executed as follows: at round\nt, server samples a subset of agents St, and sends them wt,\nthe model weights for the current round. Upon receiving\nwt, kth agent initializes his model with the received weight,\nand trains for some number of iterations, e.g., via stochastic gradient descent (SGD), and ends up with weights wk\nt .\nThe agent then computes his update as ∆k\nt = wk\nt − wt, and\nsends it back to the server. Upon receiving the update of every agent in St, server computes the weights for the next\nround by aggregating the updates with an aggregation function g: R|St|×d → Rd and adding the result to wt. That is,\nwt+1 = wt + η · g({∆t}) where {∆t} = ∪k∈St∆k\nt , and\nη is the server’s learning rate. For example, original FL paper (McMahan et al. 2016) and many subsequent papers on\nFL (Bhagoji et al. 2019; Bagdasaryan et al. 2020; Sun et al.\n2019; Bonawitz et al. 2017; Geyer, Klein, and Nabi 2017)\nconsider weighted averaging to aggregate updates. In this\ncontext, this aggregation is referred as Federated Averaging\n(FedAvg), and yields the following update rule,\nwt+1 = wt + η\nP\nk∈St nk · ∆k\nt\nP\nk∈St nk\n.\n(1)\nIn practice, rounds can go on indeﬁnitely, as new agents can\nkeep joining the protocol, or until the model reaches some\ndesired performance metric (e.g., accuracy) on a validation\ndataset maintained by the server.\nBackdoor Attacks and Model Poisoning\nTraining time\nattacks against machine learning models can roughly be\nclassiﬁed into two categories: targeted (Bhagoji et al. 2019;\nBagdasaryan et al. 2020; Chen et al. 2017; Liu et al. 2018),\nand untargeted attacks (Blanchard et al. 2017; Bernstein\net al. 2018). In untargeted attacks, the adversarial task is\nto make the model converge to a sub-optimal minima or to\nmake the model completely diverge. Such attacks have been\nalso referred as convergence attacks, and to some extend,\nthey are easily detectable by observing the model’s accuracy\non a validation data.\nOn the other hand, in targeted attacks, adversary wants\nthe model to misclassify only a set of chosen samples with\nminimally affecting its performance on the main task. Such\ntargeted attacks are also known as backdoor attacks. A\nprominent way of carrying backdoor attacks is through trojans (Chen et al. 2017; Liu et al. 2018). A trojan is a carefully crafted pattern that is leveraged to cause the desired\nmisclassiﬁcation. For example, consider a classiﬁcation task\nover cars and planes and let the adversarial task be making\nthe model classify blue cars as plane. Then, adversary could\ncraft a brand logo, put it on some of the blue car samples in\nthe training dataset, and only mislabel those as plane. Then,\npotentially, model would learn to classify blue cars with the\nbrand logo as plane. At the inference time, adversary can\npresent a blue car sample with the logo to the model to activate the backdoor. Ideally, since the model would behave\ncorrectly on blue cars that do not have the trojan, it would\nnot be possible to detect the backdoor on a clean validation\ndataset.\nIn FL, the training data is decentralized and the aggregation server is only exposed to model updates. Given that,\nbackdoor attacks are typically carried by constructing malicious updates. That is, adversary tries to create an update\nthat encodes the backdoor in a way such that, when it is aggregated with other updates, the aggregated model exhibits\nthe backdoor. This has been referred as model poisoning attack (Bhagoji et al. 2019; Bagdasaryan et al. 2020; Sun et al.\n2019). For example, an adversary could control some of the\nparticipating agents in a FL instance and train their local\nmodels on trojaned datasets to construct malicious updates.\nRobust Aggregation Methods\nSeveral works have explored using techniques from robust statistics to deter attacks in FL. At a high level, these works tried replacing\naveraging with robust estimators1 such as coordinate-wise\nmedian, geometric median, α-trimmed mean, or a variant/combination of such techniques (Yin et al. 2018; Pillutla, Kakade, and Harchaoui 2019; Blanchard et al. 2017;\nMhamdi, Guerraoui, and Rouault 2018). However, to the\nbest of our knowledge, the primary aim of these defenses\nare to deter convergence attacks.\nIn contrast, a recent work (Sun et al. 2019) has shown FedAvg can be made robust against backdoor attacks in some\nsettings when it is coupled with weight-clipping and noise\naddition as introduced in (Geyer, Klein, and Nabi 2017).\nConcretely, server inspects updates, and if the L2 norm of\nan update exceeds a threshold M, server clips the update by\ndividing it with an appropriate scalar. Server then aggregates\nclipped updates and adds Gaussian noise to the aggregation.\nIn this case, the update rule can be written as,\nwt+1 = wt + η\n\u0012P\nk∈St nk ·\n∆k\nt\nmax(1,∥∆k\nt ∥2/M)\nP\nk∈St nk\n+ N(0, σ2M 2)\n\u0013\n. (2)\nAnother recent work (Fung, Yoon, and Beschastnikh\n2020) tries to make FL robust by introducing a per-client\nlearning rate rather than having a single learning rate at the\nserver side, yielding the following update rule,\nwt+1 = wt +\nP\nk∈St αt\nk · nk · ∆k\nt\nP\nk∈St nk\n.\n(3)\nwhere αt\nk ∈ [0, 1] is the kth agent’s learning rate for the tth\nround. The exact details of how learning rates are computed\ncan be found in Algorithm 1 of the respective paper. Though,\nat a high level, the algorithm tries to assign lower learning\nrates to updates whose directions are similar, as given by cosine similarity. The rationale of this defense is that, assuming\nadversary’s agents share the common backdoor task, their\nupdates will be more similar among themselves than honest\n1Informally, a statistical estimator is said to be robust if it cannot be skewed arbitrarily in presence of outliers (Huber et al. 1972).\nupdates. Under this assumption, the algorithm will assign\nlower learning rates to malicious updates, and reduce their\neffects. For example, if there are two identical updates, the\nalgorithm assigns 0 as learning rate to both updates. However, as we observe experimentally in Section 4, their assumption does not hold in some realistic settings for FL.\nThat is, if local data distributions of honest agents exhibit\nsome similarity, algorithm cannot distinguish the adversarial agents and end up assigning everyone either the same, or\nvery similar learning rates throughout the training process.\nFinally in (Bernstein et al. 2018), authors develop a communication efﬁcient, distributed SGD protocol in which\nagents only communicate the signs of their gradients. In this\ncase, server aggregates the received signs and returns the\nsign of aggregation to the agents who locally update their\nmodels using it. We refer their aggregation technique as sign\naggregation, and in FL setting, it yields the following update\nrule,\nwt+1 = wt + η\n\u0000\nsgn\nX\nk∈St\nsgn(∆k\nt )\n\u0001\n,\n(4)\nwhere sgn is the element-wise sign operation. Although authors show their approach is robust against certain adversaries who carry convergence attacks, e.g., by sending random signs, or by negating the signs of their gradients, in\nSection 4, we show that it is susceptible against backdoors\nattacks.",
        "robust learning rate": "Backdoor Task vs Main Task\nLet ∆adv, ∆hon, be the\naggregated updates of adversarial, and honest agents respectively. Ideally, ∆adv should steer the parameters of the\nmodel to wadv, which ideally minimizes the loss on both the\nmain, and the backdoor attack task. At the same time, ∆hon\nwould want to move the model parameters towards whon\nthat only minimizes the loss on main task. Our main conjecture is that, assuming whon and wadv are different points,\n∆adv and ∆hon will most likely differ in the directions they\nspecify at least for some dimensions. As we show next, assuming a bound on the number of adversarial agents, we can\nensure the model moves away from wadv, and moves toward\nwhon, by tuning the server’s learning rate based on sign information of updates.\nRobust learning rate (RLR)\nFollowing the above insight,\nwe construct a defense which we denote as robust learning\nrate (RLR) by extending the approach proposed in (Bernstein et al. 2018). In order to move the model towards a particular direction, for each dimension, we require a sufﬁcient\nnumber of votes, in form of signs of the updates. Concretely,\nwe introduce a hyperparameter called learning threshold θ\nat the server-side. For every dimension where the sum of\nsigns of updates is less than θ, the learning rate is multiplied\nby -1. This is to maximize the loss on that dimension rather\nthan minimizing it. That is, with a learning threshold of θ,\nthe learning rate for the ith dimension is given by,\nηθ,i =\n\u001aη\n\f\fP\nk∈St sgn(∆k\nt,i)\n\f\f ≥ θ,\n−η\notherwise.\n(5)\nFor example, consider FedAvg and let ηθ\ndenote\nthe\nlearning\nrate\nvector\nover\nall\ndimensions,\ni.e.,\n[ηθ,1, ηθ,2, . . . , ηθ,d]⊤.\nThen,\nthe\nupdate\nrule\nwith\na\nrobust learning rate takes the form,\nwt+1 = wt + ηθ ⊙\nP\nk∈St nk · ∆k\nt\nP\nk∈St nk\n,\n(6)\nwhere ⊙ is the element-wise product operation. Note that,\nsince we only adjust the learning rate, the approach is agnostic to the aggregation function. For example, we can trivially\ncombine it with update clipping and noise addition as in (2).\nTo illustrate how this might help to maximize adversary’s\nloss, we consider a simple example where the local training\nconsists of a single epoch of full-batch gradient descent. In\nthis case, update of kth agent is just the negative of his gradients, i.e., ∆k\nt = wk\nt − wt = (wt − ∇fk(wt)) − wt =\n−∇fk(wt). Then, aggregated updates is just the average of\nnegative of agents’ gradients, i.e., −gavg. Therefore, if sum\nof the signs at a dimension i is below θ, that dimension is\nupdated as wt,i = wt,i + η · gavg,i. Otherwise, it is updated\nas wt,i = wt,i − η · gavg,i. So we see that, for dimensions\nwhere the sum of signs is below θ, we are moving towards\nthe direction of gradient, and hence, attempting to maximize\nloss. For other dimensions, we are moving towards the negative of gradient and attempting to minimize the loss as usual.\nTherefore, assuming number of adversarial agents is sufﬁciently below θ, the model would try to move away from\nwadv, and would try to move towards whon.\nConvergence Rate\nWe now turn to deriving the convergence rate for full-batch FedAvg with RLR. Let fk(w) =\nEDk[fk(w, ξk)] be the loss function of kth agent, where Dk\nis its distribution2 and ξk is randomness caused by the local batch variability. We use E to denote expectation in respect to all random variables. Let gk be the gradient of the\nkth agent at the tth rounds, i.e. gt\nk = ∇fk(wk\nt−1, ξt\nk), and\nEDk(gt\nk|ϝt) = ∇fk(wk\nt−1) where ϝt is a ﬁltration generated by all random variables at step t, i.e. a sequence of\nincreasing σ-algebras ϝs ⊆ ϝt for all s < t. Finally,\nfollowing Bernstein et al. (2018), we assume that for all\nt, k ∈ Z each component of the stochastic gradient vector gt\nk has a unimodal distribution that satisﬁes population\nweighted symmetry (Wolfe 1974). In particular, let W be\na random variable symmetric around zero, i.e., Pr(W ≤\n−w) = Pr(W ≥ w) for each w > 0. We now consider a\nfamily of asymmetric distributions which are constructed by\ndistorting an arbitrary symmetric distribution with a scalar\nparameter β > 0 such that Pr(Wβ = 0) = Pr(W = 0)\nand for all w > 0 Pr(Wβ ≤ −w) = 2Pr(W ≥ w)/(1+β)\nand Pr(Wβ ≥ w) = 2βPr(W ≥ w)/(1 + β), or equivalently for all w > 0\nPr(Wβ ≥ w) = βPr(Wβ ≤ −w).\n(7)\nCondition (7) is referred to as population weighted symmetry (Wolfe 1974). For a case of β = 1, (7) reduces to a standard symmetric distribution and corresponds to the assumption 4 of Bernstein et al. (2018). For β ̸= 1 (7) describes\n2Note that Di and Dj are not necessarily identical for two different agents i and j\na class of asymmetric distributions (Rosenbaum and Silber\n2009). As such, (7) allows us to consider a broader class of\ndistributions than distributions which are symmetric around\nthe mean as in the case of Bernstein et al. (2018).\nAssumption 1 Gradient is Lipschitz continuous for each\nagent k = 1, . . . K and L > 0\n||∇fk(x) − ∇fk(y)|| ≤ L||x − y||,\n∀x, y ∈ Rd.\nAssumption 2 Variance for each agent k = 1, . . . , K is\nbounded,\nEDk||∇fk(x, ξt\nk) − ∇fk(x)|| ≤ σ2,\n∀x ∈ Rd, ∀k ∈ Z+\nAssumption 3 Random variables ξt\nk are independent for\nall k, t ∈ Z+.\nTheorem 1 (Convergence Rate) Let for all i, k, t ∈ Z+,\n0 ≤ Pr(1−I|\nP\nk∈St sgn(∆k\nt,i)|≥θ|ϝt) ≤ p0 < 0.25, 0 < ν ≤\n(1 − p0)/L and E||wk\nt || < M, where M > 0 is a universal clipping upper bound. Then under Assumptions 1-3, we\nhave the following convergence rate for our robust learning\nrate scheme\n1\nT\nT −1\nX\nt=0\nE||∇f( ˆwt)||2 ≤ 2\nηT (f( ˆw0) − f ∗) + L2M 2 + Lησ2\nn\n,\nwhere ˆwt = 1/n Pn\nk=1 wk\nt . See Appendix3 for the proof of\nthe theorem.",
        "experiments": "In this section, we ﬁrst illustrate the performance of our defense, and then provide some empirical justiﬁcation for its\neffectiveness via experimental evaluation. Our implementation is done using PyTorch (Paszke et al. 2019), and the code\nis available at https://github.com/TinfoilHat0/DefendingAgainst-Backdoors-with-Robust-Learning-Rate.\nThe general setting of our experiments are as follows: we\nsimulate FL for R rounds among K agents where F fraction of them are corrupt. The backdoor task is to make the\nmodel misclassify instances from a base class as target class\nby using trojan patterns. That is, a model having the backdoor classiﬁes instances from base class with trojan pattern\nas target class (see Figure 1). To do so, we assume an adversary who corrupts the local datasets of corrupt agents by\nadding a trojan pattern to P fraction of base class instances\nand re-labeling them as target class. Other than that, adversary cannot view and modify updates of honest agents, or\ncannot inﬂuence the computation done by honest agents and\nthe aggregation server. At each round, the server uniformly\nsamples C·K agents for training where C ≤ 1. These agents\nlocally train for E epochs with a batch size of B before sending their updates. Upon receiving and aggregating updates,\nwe measure three key performance metrics of the model on\nvalidation data: validation accuracy, base class accuracy and\nbackdoor accuracy. Validation and base class accuracies are\ncomputed on the validation data that comes with the used\ndatasets, and the backdoor accuracy is computed on a poisoned validation data that is constructed by (i) extracting all\n3For the Appendix, please refer to the full version of our paper\nat https://arxiv.org/abs/2007.03767\nbase class instances from the original validation data, and\n(ii) adding them the trojan pattern and re-labeling them as\nthe target class. We measure the performance of the following ﬁve aggregation methods: (i) FedAvg (equation 1),\n(ii) FedAvg with our proposed robust learning rate scheme:\nRLR (equation 6), (iii) coordinate-wise median (comed),\n(iv) FoolsGold (equation 3), and (v) sign aggregation (equation 4). We also measure the performance of these aggregations under the proposed defense in (Sun et al. 2019), i.e.,\ncombining aggregations with weight-clipping and noise addition, to see if these techniques provide any robustness for\neach aggregation under our attack setting. Furthermore, in\nAppendix, we provide results when comed and sign aggregation are combined with RLR.\nWhen there is a L2 clipping threshold M on updates, we\nassume M is public and every agent runs projected gradient descent to minimize their losses under this restriction,\ni.e., an agent ensures his update’s L2 norm is bounded by\nM by monitoring the L2 norm of his model during training\nand clips its weights appropriately. Finally we use the same\nmodel as in (Sun et al. 2019), a 5-layer convolutional neural\nnetwork consisting of about 1.2M parameters with the following architecture: two layers of convolution, followed by a\nlayer of max-pooling, followed by two fully-connected layers with dropout. Hyperparameters used in all experiments\ncan be found in Appendix.\nIID Setting\nWe start with a setting where data is distributed in i.i.d. fashion among agents. Concretely, we\nuse the Fashion MNIST (Xiao, Rasul, and Vollgraf 2017)\ndataset, and give each agent an equal number of samples\n(a)\n(b)\nFigure 1: Samples from trojaned base classes and corresponding target classes. Trojan pattern is a 5-by-5 plus sign\nthat is put to the top-left of objects. For i.i.d. case (a), backdoor task is to make model classify trojaned sandals as\nsneakers. For non-i.i.d. case (b), it is to make model classify\ntrojaned digit 1s as digit 7s. Note that original images are\nin grayscale, these ﬁgures are normalized as they appear in\ntraining/validation dataset. We also repeat the experiments\nwe present here under three additional trojan patterns and\nreport the results in Appendix.\nfrom the training data via uniform sampling. In Figure 2, we\nplot the training curves of FedAvg, and FedAvg with RLR,\nand report the ﬁnal accuracies reached in each setting in Table 1. Results reported in Table 1 shows that, compared to\nbaselines, our proposed RLR scheme provides signiﬁcant\nprotection against the backdoor attacks.\nNon-IID Setting\nWe now move on to a more realistic setting for FL in which data is distributed in non-i.i.d. fashion among agents. For this setting, we use the Federated\nEMNIST dataset from the LEAF benchmark (Caldas et al.\n2018). In this dataset, digits 0-9 are distributed across 3383\nusers and each user has possibly a different distribution over\ndigits. Similar to the i.i.d. case, we plot training curves for\nFedAvg and FedAvg with RLR (Figure 3). Table 1 reports\nthe ﬁnal accuracy results for each setting. The results reported indicate that our defense provides the best protection\nwith minimal degradation on the validation accuracy.\nRemoving Backdoor During Training\nDuring experiments, we observed that, FedAvg with RLR rate performs\nsubstantially better than other methods in terms of preventing the backdoor task, but it also reduces convergence speed.\nTherefore, we wonder if one can start without RLR, and then\nswitch to RLR at some point during the training, e.g., when\nthe model is about to converge, to clean any possible backdoors from the model. Our experiments indicate that this is\nthe case. In the interest of space, we provide results in Appendix, however they suggest that one can start without RLR\nand later switch to RLR when the model is about to converge, and/or a backdoor attack is suspected, to clean the\nmodel of backdoor during training. Overall, this improves\nthe time to convergence when compared to using RLR right\nfrom the beginning.\nAnalyzing Our Defense via Parameter and Feature Attributions\nWe now aim to explain why our defense works\nand provide some empirical justiﬁcation for its effectiveness. First, recall our conjecture from Section 3 where we\nbasically argue that the adversary has to overcome the inﬂuence of honest agents to embed the backdoor to model. More\nconcretely, in our scenario, adversary tries to map the base\nclass instances with trojan pattern to the target class (adversarial mapping) where as honest agents try to map them to\nthe base class (honest mapping). If we had a way to quantify\nthe inﬂuence of agents on the model, regarding the mapping\nof trojaned inputs, we would expect the model to exhibit the\nbackdoor if the inﬂuence of adversary is greater than of the\ntotal inﬂuence of honest agents. Given that, we designed a\nsimple experiment to quantify the inﬂuence of agents, and\ntest this conjecture empirically. In the interest of space, we\ndefer the details of this experiment to Appendix, but we note\nthat it is mainly based on doing parameter attribution on\nthe model to ﬁnd out which parameters are most important\nto adversarial/honest mapping, and then tracking how they\nare updated over the rounds. In Figure 4, we can see that\nwith RLR, honest agents’ inﬂuence overcome the adversarial agents’ for the backdoor task.\nSecond, we do a feature attribution experiment which\nis concerned with discovering features of an input that are\nimportant to a model’s prediction. Particularly, we pick an\narbitrary sample from our poisoned validation set that is\ncorrectly classiﬁed (as base class) by the model when it is\ntrained with FedAvg with RLR, but incorrectly classiﬁed (as\ntarget class) when it is trained FedAvg. Figure 5 illustrates\nthat, resulting feature maps on no attack and with our defense scenario are similar. This shows, our defense successfully prevents the model from focusing on the trojan pattern.\nDistributed Backdoor Attacks\nFinally, we brieﬂy test\nour defense against a recent, novel type of backdoor attack\nintroduced in (Xie et al. 2019). The main idea of this attack\nis to partition the pixels of a trojan between the agents of\nthe adversary, and through that, ensuring the resulting malicious updates to be less different than honest’ updates to\nmake attack more stealthy. For example, if adversary has\nfour agents, the plus pattern can be partitioned accross these\nfour agents such that, each adversarial agent applies only\na vertical/horizontal part of the plus. In case the backdoor\nis successful, the model would still misclassify the samples with the complete plus pattern. We test this attack only\nagainst FedAvg with RLR, as other defenses already fail on\ndefault backdoor attacks, on CIFAR10 dataset (Krizhevsky,\nNair, and Hinton 2009). Table 2 indicates our defense performs well against distributed backdoor attacks too.",
        "discussion": "Our experiments show that our approach signiﬁcantly reduces the effectiveness of trojan pattern backdoor attacks.\nOne can wonder that, how it performs with respect to the socalled semantic backdoors (a.k.a label-ﬂipping) attacks. In\nthese attacks, the adversary simply ﬂips the label of the base\nclass instances to a desired target label without adding a trojan pattern. In FL setting, it has been shown that successfully\ncarrying such attacks require boosting (Bhagoji et al. 2019).\nThat is, after training on a poisoned dataset, adversary has to\nmultiply the resulting update with a large constant to overcome the effect of honest agents. Naturally, this results in adversarial updates having a large norm, and as shown in (Sun\net al. 2019), weight-clipping and noise addition signiﬁcantly\ndeters these attacks. Since our defense is compatible with\nclipping and noise addition, it can also deter such attacks. In\nfact, our experiment show that, trojan backdoors are strictly\nmore powerful than semantic backdoors in FL context as an\nadversary does not need to use boosting with them.\nFinally, we ask if an adversary can devise a clever attack.\nAt a high level, as long as the θ parameter is set appropriately, and adversary’s local loss function differs from the\nhonest against, the scheme will try to move the model from\nthe directions the adversarial update speciﬁes. Adversary\ncould try to make his loss function more in-line with honest agents’ via some modiﬁcation, but then this will likely\nresult in his attack losing effectiveness. We emphasize that\nour approach does not “magically” ﬁnds the adversary, and\nnegates his update by multiplying it with −η, so the adversary cannot by-pass our defense just by negating his loss.\nFigure 2: Training curves for FedAvg and FedAvg with RLR in i.i.d. setting. From left-to-right: (a) FedAvg, (b) FedAvg\nwith RLR, (c) FedAvg under clipping&noise, (d) FedAvg with RLR under clipping&noise. As can be seen, FedAvg is weak\nagainst the attack even with clipping&noise. On the other hand, FedAvg with RLR prevents the backdoor with or without\nclipping&noise. Using clipping and noise addition could be a desirable property in contexts where differential privacy is applied,\nor against attackers who try to make the model diverge by sending arbitrarily large values.\nAggregation\nM\nσ\nBackdoor (%)\nValidation (%)\nBase (%)\nFedAvg-No Attack\n0\n0\n1\n93.5\n98.5\nFedAvg\n0\n0\n100\n93.4\n98.5\nFedAvg\n4\n1e-3\n100\n93.2\n99.1\nFoolsGold\n0\n0\n100\n93.1\n98.9\nFoolsGold\n4\n1e-3\n100\n93.3\n98.5\nComed\n0\n0\n100\n92.8\n99.0\nComed\n4\n1e-3\n99.5\n92.8\n98.4\nSign\n0\n0\n100\n92.9\n98.7\nSign\n4\n1e-3\n99.7\n93.1\n98.6\nFedAvg with RLR\n0\n0\n0\n92.9\n98.3\nFedAvg with RLR\n4\n1e-3\n0.5\n92.2\n97.4\nAggregation\nM\nσ\nBackdoor (%)\nValidation (%)\nBase (%)\nFedAvg*-No Attack\n0\n0\n21.1\n98.6\n99.1\nFedAvg\n0\n0\n99.3\n98.5\n99.0\nFedAvg\n0.5\n1e-3\n99.2\n98.0\n98.7\nFoolsGold\n0\n0\n98.5\n98.9\n99.5\nFoolsGold\n0.5\n1e-3\n99.1\n97.9\n98.6\nComed\n0\n0\n82.3\n96.3\n98.4\nComed\n0.5\n1e-3\n95.2\n95.5\n98.1\nSign\n0\n0\n99.8\n97.6\n98.7\nSign\n0.5\n1e-3\n99.7\n97.8\n98.5\nFedAvg with RLR\n0\n0\n3.4\n94.8\n97.6\nFedAvg with RLR\n0.5\n1e-3\n0.4\n93.2\n97.7\nTable 1: Final backdoor, validation and base class accuracies for different aggregations in i.i.d. (top) and non-i.i.d. (bottom)\nsettings. Lowest backdoor, highest validation and base class accuracies are highlighted in bold. FedAvg-No Attack corresponds\nto our baseline where we use FedAvg with no attackers. See Appendix for additional experiments under different combinations\nof M and σ, and our justiﬁcation for the chosen values.\nFigure 3: Plots for FedAvg and FedAvg with RLR in non-i.i.d. setting. From left-to-right: (a) FedAvg, (b) FedAvg with RLR,\n(c) FedAvg under clipping&noise, (d) FedAvg with RLR under clipping&noise.\nFigure 4: Results of parameter attribution experiments. From left-to-right: (a) FedAvg, (b) FedAvg with RLR in i.i.d. setting,\nand (c) FedAvg, (d) FedAvg with RLR in non-i.i.d. setting. Net inﬂuence is the cumulative sum of differences between the\ninﬂuences of honest agents and the adversarial agents for the mapping of trojaned samples. As can be seen, net inﬂuence is\nloosely correlated with the backdoor loss. With RLR, net inﬂuence is positive, indicating that honest agents’ inﬂuence is greater\nthan adversarial agents. This causes backdoor loss to increase, and hence, preventing the backdoor. On the other hand, without\nRLR, net inﬂuence quickly becomes negative and backdoor loss decreases. This results in a successful backdoor attack.\n(a) IID setting\n(b) Non-IID setting\nFigure 5: Feature maps (FM) for i.i.d. and non-i.i.d. settings\non a trojaned sample given by Gradient SHAP (Lundberg\nand Lee 2017). Leftmost image is the sample input from poisoned validation data, and to its right we present FMs in the\nfollowing order: FM of model trained using FedAvg without any attack, FM of model trained using FedAvg under\nattack, FM of model trained using FedAvg with RLR under\nattack. For no attack case, important pixels are either on or\naround the actual objects. For i.i.d. setting, model predicts\nthe sample correctly as sandals with 100% conﬁdence, and\nfor non-i.i.d., model predicts the digit 1 with 99.2% conﬁdence. For no defense scenario, we can see that model’s\nattention has shifted towards the trojan pattern. This is especially very visible for i.i.d. setting where the model almost\ncompletely focuses on the trojan. In i.i.d. case, model predicts the sample as sneakers with 100% conﬁdence, and in\nnon-i.i.d. case, model predicts the digit as 7 with 91.2% conﬁdence. Finally, we see that with robust learning rate, the\nmodel’s attention has been shifted back to the actual objects\nto some extent. Now, model predicts the sample as sandals\nwith 100% conﬁdence in i.i.d. case, and it predicts the digit\nas 1 with 91.2% conﬁdence in non-i.i.d. case.",
        "conclusion": "In this work, we studied FL from an adversarial perspective,\nand constructed a simple defense mechanism, particularly\nAggregation\nBackdoor (%) Validation (%) Base (%)\nFedAvg-No Attack\n6.6\n79.0\n89.4\nFedAvg\n88.6\n79.4\n87.5\nFedAvg with RLR\n9.0\n77.5\n87.8\nAggregation\nBackdoor (%) Validation (%) Base (%)\nFedAvg-No Attack\n6.3\n76.6\n87.7\nFedAvg\n61.7\n76.6\n78.2\nFedAvg with RLR\n8.5\n71.8\n83.3\nTable 2: Backdoor attack on i.i.d.-partitioned CIFAR10.\nBackdoor task is to classify dogs (base class) with plus pattern as horses (target class). Top table is for regular backdoor\nattack, and bottom table is for distributed backdoor attack\nwhere plus pattern is partitioned to 4 adversarial agents out\nof 40 agents. See Appendix for details.\nagainst backdoor attacks. The key idea behind our defense\nwas adjusting the aggregation server’s learning rate, per dimension and per round, based on the sign information of\nagents’ updates. Through experiments we present above and\nin Appendix, we illustrate that our defense reduces backdoor accuracy substantially with a minimal degradation in\nthe overall validation accuracy. Overall, it outperforms some\nof the recently proposed defenses in the literature. As a ﬁnal comment, we believe the insights behind our defense are\nalso related to training in non-i.i.d. setting, even in the presence of no adversaries. Because, the differences in local distributions can cause updates coming from different agents to\nsteer the model towards different directions over the loss surface. As a future work, we plan to analyze how RLR inﬂuences performance of models trained in different non-i.i.d.\nsettings.",
        "summary_en": "Federated learning (FL) allows a set of agents to collaboratively train a model without sharing their potentially sensitive data. This makes FL suitable for privacy-preserving applications. At the same time, FL is susceptible to adversarial attacks due to decentralized and unvetted data. One important line of attacks against FL is the backdoor attacks. In a backdoor attack, an adversary tries to embed a backdoor functionality to the model during training that can later be activated to cause a desired misclassification. To prevent backdoor attacks, this paper proposes a lightweight defense that requires minimal change to the FL protocol. At a high level, the defense is based on carefully adjusting the aggregation server's learning rate, per dimension and per round, based on the sign information of agents' updates. The paper first conjectures the necessary steps to carry a successful backdoor attack in FL setting, and then, explicitly formulate the defense based on the conjecture. Through experiments,the paper provides empirical evidence that supports the conjecture, and the paper tests the defense against backdoor attacks under different settings. The paper observes that either backdoor is completely eliminated, or its accuracy is significantly reduced. Overall, the paper's experiments suggest that the defense significantly outperforms some of the recently proposed defenses in the literature. The paper achieves this by having minimal influence over the accuracy of the trained models. In addition, the paper also provides convergence rate analysis for the proposed scheme.",
        "summary_zh": "这篇论文介绍了一种防御联邦学习中后门攻击的轻量级方法，该方法通过调整聚合服务器的学习率来实现。在联邦学习中，后门攻击是一种重要的攻击方式，攻击者试图在模型训练过程中嵌入后门功能，以便在以后触发并导致所需的误分类。为了防止后门攻击，作者提出了一种轻量级的防御方法，该方法对联邦学习协议的改变很小。通过实验证据，作者证明了他们的防御方法在消除后门或显著降低后门准确率方面取得了成功。总体而言，这项研究表明了他们的防御方法明显优于文献中一些最近提出的防御方法，并且对训练模型的准确性影响很小。此外，他们还对所提出的方案进行了收敛速率分析。"
    },
    {
        "title": "Delving into the Adversarial Robustness of Federated Learning",
        "abstract": "In Federated Learning (FL), models are as fragile as centrally trained models against adversarial examples. However, the adversarial robustness of federated learning remains largely unexplored. This paper casts light on the challenge of adversarial robustness of federated learning. To facilitate a better understanding of the adversarial vulnerability of the existing FL methods, we conduct comprehensive robustness evaluations on various attacks and adversarial training methods. Moreover, we reveal the negative impacts induced by directly adopting adversarial training in FL, which seriously hurts the test accuracy, especially in non-IID settings. In this work, we propose a novel algorithm called Decision Boundary based Federated Adversarial Training (DBFAT), which consists of two components (local re-weighting and global regularization) to improve both accuracy and robustness of FL systems. Extensive experiments on multiple datasets demonstrate that DBFAT consistently outperforms other baselines under both IID and non-IID settings.",
        "introduction": "Nowadays, end devices are generating massive amounts of\npotentially sensitive user data, raising practical concerns\nover security and privacy. Federated Learning (FL) (McMahan et al. 2017) emerges as a privacy-aware learning\nparadigm that allows multiple clients to collaboratively train\nneural networks without revealing their raw data. Recently,\nFL has attracted increasing attention from different areas, including medical image analysis (Liu et al. 2021a; Chen et al.\n2021b), recommender systems (Liang, Pan, and Ming 2021;\nLiu et al. 2021b), natural language processing (Zhu et al.\n2020; Wang et al. 2021), etc.\nPrior studies have demonstrated that neural networks\nare vulnerable to evasion attacks by adversarial examples (Goodfellow, Shlens, and Szegedy 2014) during inference time. The goal of inference-time adversarial attack (Li\net al. 2021a; Chen et al. 2022c; Zhang et al. 2022b; Chen\net al. 2022b) is to damage the global model by adding a carefully generated imperceptible perturbation on the test examples. As shown in Table 1, federated models are as fragile to\nadversarial examples as centrally trained models (i.e. zero\naccuracy under PGD-40 attack (Madry et al. 2017)). Hence,\nit is also important to consider how to defend against adversarial attacks in federated learning.\nThere are several works that aim to deal with adversarial\nattacks in FL (Zhang et al. 2022c,a), i.e, federated adversarial training (FAT) (Zizzo et al. 2020; Hong et al. 2021; Shah\net al. 2021; Chen, Zhang, and Lyu 2022; Chen et al. 2022a).\n(Zizzo et al. 2020) and (Hong et al. 2021) proposed to conduct adversarial training (AT) on a proportion of clients but\nconduct plain training on other clients. (Shah et al. 2021)\ninvestigated the impact of local training rounds in FAT. Nevertheless, these methods all ignore the issue that the clean\naccuracy of federated adversarial training is very low.\nTo further show the problems of federated adversarial\ntraining, we first begin with the comparison between the\nplainly-trained models and AT-trained (Madry et al. 2017)\nmodels in both the IID (Independent and Identically Distributed) and non-IID FL settings, measured by clean accuracy Acln and robust accuracy Arob, respectively. We show\nthe test accuracy of plain training and adversarial training\n(AT) on CIFAR10 dataset under both IID and non-IID FL\nsettings in Fig. 1 (left sub-figure). We summarize some valuable observations as follows: 1) Compared with the plainlytrained models, AT-trained models achieve a lower accuracy,\nwhich indicates that directly adopting adversarial training in\nFL can hurt Acln; 2) Acln drops heavily for both the plainlytrained models and AT-trained models under non-IID distribution, which is exactly the challenge that typical federated\nlearning with heterogeneous data encountered (Zhao et al.\n2018); 3) The performance of AT-trained models with nonIID data distribution decrease significantly compared with\nIID data distribution. Motivated by these observations, we\nfocus on improving both adversarial robustness and clean\naccuracy of adversarial training in FL, i.e., we aim to increase Acln while keeping Arob as high as possible.\nTo achieve this goal, in this paper, we investigate the impact of decision boundary, which can greatly influence the\nperformance of the model in FAT. Specifically, 1) we apply\nadversarial training with a re-weighting strategy in local update to get a better Arob. Our method takes the limited data\nof each client into account, those samples that are close to/far from the decision boundary are assigned larger/smaller\nweight. 2) Moreover, since the global model in FL has a\nType\nDataset\nMNIST\nFMNIST\nImageNet-12\nCIFAR10\nCIFAR100\nTiny-ImageNet\nCentralized\nAcln\n99.42\n92.47\n78.96\n94.26\n86.93\n57.93\nArob\n0\n0\n0\n0\n0\n0\nFederated\nAcln\n99.01\n88.51\n71.65\n85.81\n81.28\n49.79\nArob\n0\n0\n0\n0\n0\n0\nTable 1: The accuracy (%) is tested under PGD-40 attack (Madry et al. 2017). For MNIST, FMNIST, CIFAR10, ImageNet12, CIFAR100, and Tiny-ImageNet, the perturbation bound is {0.3, 32/255, 0.031, 0.031, 0.031, 0.031}, respectively. Acln and\nArob refer to clean accuracy and robust accuracy.\n0\n100\n200\n300\n400\n500\n#…Rounds\n20\n40\n60\n80\nTest……Accuracy\n60\n65\n70\n75\n0\n10\n20\n30\nRobustness\nClean…Accuracy…(IID)\n10\n20\n30\n40\n50\n60\nClean…Accuracy…(non-IID)\n0\n10\n20\n30\nRobustness\nPlainly-trained,…IID\nPlainly-trained,…non-IID\nAT-trained,…IID\nAT-trained,…non-IID\n…………\nFedAvg\nFed_AT\nFed_ALP\nFed_TRADES\nFed_AVMixup\nDBFAT(ours)\nFigure 1: Left: Test accuracy reduces for plainly trained model and adversarially trained model under non-IID data. Meanwhile,\nadversarial training hurts the performance. Right: Evaluations on CIFAR10 for both accuracy and robustness, including several\nstate-of-the-art defense methods combined with FL. Our method outperforms existing baselines on both metric dimensions.\nmore accurate decision boundary through model aggregation, we take advantage of the logits from the global model\nand introduce a new regularization term to increase Acln.\nThis regularization term aims to alleviate the accuracy reduction across distributed clients.\nWe conclude our major contributions as follows:\n• We conduct systematic studies on the adversarial robustness of FL, and provide valuable observations from extensive experiments.\n• We reveal the negative impacts of adopting adversarial\ntraining in FL, and then propose an effective algorithm\ncalled Decision Boundary based Federated Adversarial\nTraining (DBFAT), which utilized local re-weighting and\nglobal regularization to improve both the accuracy and\nrobustness of FL systems.\n• Extensive experiments on multiple datasets demonstrate\nthat our proposed DBFAT consistently outperforms other\nbaselines under both IID and non-IID settings. We\npresent the performance of our method in Fig. 1 (right\nsub-figure), which indicates the improvement in both robustness and accuracy of adversarial training in FL.",
        "related works": "Federated Learning.\nFollowing the success of DNNs in\nvarious tasks (Li et al. 2019; Li, Sun, and Guo 2019; Huang\net al. 2022b,a; Dong et al. 2021), FL has attracted increasing\nattention. A recent survey has pointed out that existing FL\nsystems are vulnerable to various attacks that aim to either\ncompromise data privacy or system robustness (Lyu et al.\n2022). In particular, robustness attacks can be broadly classified into training-time attacks (data poisoning and model\npoisoning) and inference-time attacks (evasion attacks, i.e.,\nusing adversarial examples to attack the global model during\ninference phase). In FL, the architectural design, distributed\nnature, and data constraints can bring new threats and failures (Kairouz 2021).\nAdversarial Attacks.\nThe white-box attacks have access to the whole details of threat models, including parameters and architectures. Goodfellow et al. (Goodfellow,\nShlens, and Szegedy 2014) introduced the Fast Gradient\nSign Method (FGSM) to generate adversarial examples,\nwhich uses a single-step first-order approximation to perform gradient ascent. Kurakin et al. (Kurakin, Goodfellow,\nand Bengio 2017) iteratively applied FGSM with a small\nstep-size to develop a significantly stronger multi-step variant, called Iterative FGSM (I-FGSM). Based on these findings, more powerful attacks have been proposed in recent\nyears including MIM (Dong et al. 2018), PGD (Madry et al.\n2017), CW (Carlini and Wagner 2017), and AA (Croce and\nHein 2020).\nAdversarial Training.\nAdversarial training has been one\nof the most effective defense strategies against adversarial\nattacks. Madry et al. (Madry et al. 2017) regarded adverType\nIID\nNon-IID\nMethods\nFedAvg\nFedProx\nFedNova\nScaffold\nFedAvg\nFedProx\nFedNova\nScaffold\nPerformance\nAcln\nArob\nAcln\nArob\nAcln\nArob\nAcln\nArob\nAcln\nArob\nAcln\nArob\nAcln\nArob\nAcln\nArob\nPGD-AT\n57.99\n31.95\n58.17\n32.06\n58.45\n31.74\n56.84\n29.26\n46.84\n26.79\n48.03\n27.46\n46.95\n26.54\n42.44\n27.19\nALP\n62.81\n31.84\n62.88\n31.20\n62.91\n31.79\n60.30\n29.58\n56.16\n28.78\n55.79\n29.06\n55.80\n29.18\n48.29\n26.56\nTRADES\n64.94\n32.93\n64.29\n32.97\n64.46\n33.29\n63.14\n33.58\n60.94\n27.06\n61.05\n27.94\n60.34\n28.78\n59.53\n27.78\nMMA\n65.14\n30.29\n63.65\n31.29\n65.27\n29.31\n64.28\n32.98\n59.69\n28.64\n60.17\n28.09\n61.03\n28.47\n61.53\n28.13\nAVMixup\n66.14\n32.27\n65.12\n33.19\n65.14\n33.75\n65.11\n33.24\n61.17\n28.56\n61.47\n28.34\n62.04\n28.12\n61.91\n28.81\nTable 2: An empirical study on the adversarial robustness of FL, measured by various combination of defense methods and FL\nalgorithms. We report the clean accuracy and robust accuracy, respectively. Best results are in bold.\nsarial training as a min-max formulation using empirical\nrisk minimization under PGD attack. Kannan et al. (Kannan,\nKurakin, and Goodfellow 2018) presented adversarial logit\npairing (ALP), a method that encourages logits for pairs of\nexamples to be similar, to improve robust accuracy. To quantify the trade-off between accuracy and robustness, Zhang et\nal. (Zhang et al. 2019) introduced a TRADES loss to achieve\na tight upper bound on the gap between clean and robust error. Based on the margin theory and soft-labeled data augmentation, Ding et al. (Ding et al. 2020) proposed MaxMargin Adversarial (MMA) training and Lee et al. (Lee,\nLee, and Yoon 2020) introduced Adversarial Vertex mixup\n(AVmixup).\nFederated Adversarial Training.\nIn terms of the adversarial robustness, Zizzo et al. (Zizzo et al. 2020) investigated\nthe effectiveness of the federated adversarial training protocol for idealized federated settings, and showed the performance of their models in a traditional centralized setting and a distributed FL scenario. Zhou et al. (Zhou et al.\n2022) decomposed the aggregation error of the central server\ninto bias and variance. However, all these methods sacrificed\nclean accuracy (compared to plainly trained models) to gain\nrobustness. In addition, certified defense (Chen et al. 2021a)\nagainst adversarial examples in FL is another interesting direction, which will be discussed in the future.",
        "adversarial robustness of fl": "In this section, we briefly define the goal of federated adversarial training. Then we conduct a systematic study on some\npopular federated learning algorithms with the combination\nof various adversarial training methods and evaluate their\nrobustness under several attacks. Besides, we further reveal\nthe challenges of adversarial training in non-IID FL.\nProblem Definition\nIn typical federated learning, training data are distributed\nacross all the K clients, and there is a central server managing model aggregations and communications with clients.\nIn general, federated learning attempts to minimize the following optimization:\nmin\nw f(w) =\nK\nX\nk=1\nnk\nn Fk(w).\n(1)\nHere, we denote that the global approximate optimal is a\nsum of local objectives weighted by the local data size nk,\n0\n2\n4\n6\n8\nClass\n0\n1\n2\n3\n4\n5\nNumbers\n1e3\n0\n20\n40\n60\n80\n100\nAccuracy\nplain, Acln\nAT, Acln\nAT, Arob\nFigure 2: Test accuracy on a randomly selected client.\nand n is the total data size of all clients that participate in a\ncommunication round. Moreover, each local objective measures the empirical risk over possibly different data distributions Dk, which can be expressed as:\nFk(w) := Exk∼Dk [fk (w; xk)] .\n(2)\nLet x denote the original image, xadv denote the corresponding adversarial example, and δ denote the perturbation\nadded on the original image, then xadv = x + δ. To generate powerful adversarial examples, we attempt to maximize\nthe loss L(x + δ; w), where L is the loss function for local\nupdate.\nTo improve the robustness of the neural networks, many\nadversarial defense methods have been proposed. Among\nthem, adversarial training (Carlini and Wagner 2017) is one\nof the most prevailing and effective algorithms. Combined\nwith adversarial training, the local objective becomes solving the following min-max optimization problem:\nFk(w) = min Exk∼Dk\n\u0014\nmax\n∥xadv−x∥∞≤δ L(w, xadv, y)\n\u0015\n. (3)\nThe inner maximization problem aims to find effective adversarial examples that achieve a high loss, while the outer\noptimization updates local models to minimize training loss.\nIn this work, we conduct a systematic study on several\nstate-of-the-art FL algorithms including FedAvg (McMahan et al. 2017), FedProx (Li et al. 2018), FedNova (Wang\net al. 2020) and Scaffold (Karimireddy et al. 2020), and\nNormal samples\nAdversarial attack\nClassification error\nDecision boundaries\nFigure 3: Left panel: Decision boundary of plainly trained model. Middle panel: Decision boundary of AT-trained model. Right\npanel: Decision boundary of DBFAT-trained model. We use the dotted line to represent the boundary of the clean model, and\nsolid line to represent the boundary of the robust model. The size of the shape represents the value of the weight. Those samples\nthat are close to/far from boundary are assigned larger/smaller weight. The decision boundary of DBFAT-trained model (see the\nright sub-figure) can achieve a higher Arob and meanwhile maintain Acln.\nexplore their combinations with AT methods to defend\nagainst adversarial attacks. We report detailed results in Table 2, here robustness is averaged over four popular attacks (FGSM (Kurakin, Goodfellow, and Bengio 2017),\nMIM (Dong et al. 2018), PGD (Madry et al. 2017),\nand CW (Carlini and Wagner 2017)). Besides, we implement some prevailing adversarial training methods including PGD AT (Madry et al. 2017) , TRADES (Zhang\net al. 2019), ALP (Kannan, Kurakin, and Goodfellow 2018),\nMMA (Ding et al. 2020) and AVMixup (Lee, Lee, and Yoon\n2020). We observe that there is no federated adversarial\nlearning algorithm that can outperform all the others in all\ncases. Moreover, the clean accuracy drops heavily under\nnon-IID distribution. As such, we are motivated to develop a\nmore effective method. Due to the similar performance of\nthese FL methods observed from Table 2, we design our\nmethod based on FedAvg – a representative algorithm in FL.\nAdversarial Traning with non-IID Data\nFederated learning faces the statistical challenge in realworld scenarios. The IID data makes the stochastic gradient as an unbiased estimate of the full gradient (McMahan\net al. 2017). However, the clients are typically highly heterogeneous with various kinds of non-IID settings, such as\nlabel skewness and feature skewness (Li et al. 2021b). According to previous studies (Wang et al. 2020; Karimireddy\net al. 2020), the non-IID data settings can degrade the effectiveness of the deployed model.\nSimilarly, due to the non-IID data, the performance of\nAT may vary widely across clients. To better understand the\nchallenge of adversarial training with non-IID data, we examine the performance of both clean accuracy and robustness on a randomly selected client and report the results in\nFig. 2. Observed from Fig. 2, we can find that: 1) Acln on\nthe plainly trained model drops from majority classes to minority classes, which is exactly what traditional imbalanced\nlearning attempts to solve; 2) A similar decreasing tendency\nreasonably occurs in Arob. It is obvious that adopting adversarial training in federated learning with non-IID data is\nmore challenging.\nAccording to above observations, we conjecture that ATtrained local models with imbalanced data lead to a more\nbiased decision boundary than plainly trained ones. Since\nadversarial examples need a larger number of epochs to\nachieve near-zero error (Zhang et al. 2021), it becomes\nharder to fit adversarial examples than clean data. However, for the local client itself, imbalanced clean data generates imbalanced adversarial examples, making it more difficult for training and enlarging the accuracy gap, which can\nreduce the performance both in accuracy and robustness.\nCompared with the plainly trained models, the aggregation\nof adversarially trained models can enlarge the accuracy gap,\nwhich results in poor consistency between different clients.\nTo overcome this problem, we propose a novel method to\nutilize local re-weighting and global regularization to improve both the accuracy and robustness of FL systems.",
        "methodology": "The generalization performance of a neural network is\nclosely related to its decision boundary. However, models trained in the federated setting are biased compared\nwith the centrally trained models. This is mainly caused\nby heterogeneous data and objective inconsistency between\nclients (Kairouz 2021). Moreover, a highly skewed data distribution can lead to an extremely biased boundary (Wang\net al. 2020). We tackle this problem in two ways: 1) locally,\nwe take full advantage of the limited data on the distributed\nclient; 2) globally, we utilize the information obtained from\nthe global model to alleviate the biases between clients.\nSubsequently, we propose a simple yet effective approach called Decision Boundary based Federated Adversarial Training (DBFAT), which consists of two components. For local training, we re-weight adversarial examples\nto improve robustness; while for global aggregation, we utilize the global model to regularize the accuracy for a lower\nboundary error Abdy. We show the training process of DBFAT in the supplementary and illustrate an example of the\ndecision boundary of our approach in Fig. 3.\nRe-weighting with Limited Data\nAdversarial examples have the ability to approximately measure the distances from original inputs to a classifier’s decision boundary (Heo et al. 2018), which can be calculated\nby the least number of steps that iterative attack (e.g. PGD\nattack (Madry et al. 2017)) needs in order to find its misclassified adversarial variant. To better utilize limited adversarial examples, we attempt to re-weight the adversarial examples to guide adversarial training. For clean examples that\nare close to the decision boundary, we assign larger weights;\nwhile those examples that are far from the boundary are assigned with smaller weights.\nIn this paper, we use PGD-S to approximately measure\nthe geometric distance to the decision boundary, S denotes\nthe number of maximum iteration. We generate adversarial\nexamples as follows (Madry et al. 2017):\nxadv ← ΠB[x,ϵ]\n\u0000\nxadv + α · sign(∇xadvℓ(xadv, y))\n\u0001\n. (4)\nHere ΠB[x,ϵ] is the projection function that projects the\nadversarial data back into the ϵ-ball centered at natural data,\nα is the steps size, ϵ is perturbation bound.\nWe find the minimum step d, such that after d step of\nPGD, the adversarial variant can be misclassified by the network, i.e., arg maxcf (c)(xadv) ̸= y, where f (c)(xadv) is\nthe logits of the c-th label.\nIn this way, given a mini-batch samples {(xi, yi)}m\ni=1,\nthen the weight list ρ can be formulated as :\nρ ← 1 − {\ndi\nPm\ni=1 di\n}.\n(5)\nRegularization with Global Model\nEarly work (Zhang et al. 2019; Cui et al. 2021) claims that\nthere exists a trade-off between accuracy and robustness,\nstandard adversarial training can hurt accuracy. To achieve\na lower boundary error Abdy, we take advantage of logits\nfrom the global model f glo, which is trained after aggregation. Particularly, in federated learning, the model owns the\ninformation obtained from the averaged parameters on distributed clients.\nLet f loc denote the adversarially trained model at each local client, f glo has the most desirable classifier boundary for\nnatural data. Then we can modify the local objective mentioned in Equation 3 as below:\nmin ℓce(ρ · f loc(xadv), y)\n|\n{z\n}\nfor robustness\n+β · ℓkl(f loc(xadv), f glo(x))\n|\n{z\n}\nfor accuracy regularization\n.\n(6)\nWhere ℓce denotes the cross-entropy loss to improve the\nrobustness, and ℓkl is the KL divergence loss to constrain the\nlogits of global model and local model. Here, ℓkl appears as\nan additional regularization term, which is designed to reduce the boundary error Abdy = Acln − Arob. Additionally,\nρ is the weight calculated by Equation 5, β is the parameter\nto be tuned.",
        "experimental results": "Experimental Setup\nFollowing the previous work of FL (McMahan et al. 2017),\nwe distribute training data among 100 clients in both IID and\nnon-IID fashion. For each communication round, we randomly select 10 clients to average the model parameters. All\nexperiments are conducted with 8 Tesla V100 GPUs. More\ndetails can be referred to the supplemental material.\nDatasets\nIn this section, we show that DBFAT improves the robust generalization and meanwhile maintains\na high accuracy with extensive experiments on benchmark CV datasets, including MNIST (Lecun et al. 1998),\nFashionMNIST (Xiao, Rasul, and Vollgraf 2017) (FMNIST), CIFAR10 (Krizhevsky and Hinton 2009), CIFAR100 (Krizhevsky and Hinton 2009), Tiny-ImageNet (Le\nand Yang 2015), and ImageNet-12 (Deng et al. 2009). The\nImageNet-12 is generated via (Li et al. 2021c), which consists of 12 classes. We resize the original image with size\n224*224*3 to 64*64*3 for fast training.\nData partitioning\nIn the federated learning setup, we\nevaluate all algorithms on two types of non-IID data partitioning: Dirichlet sampled data and Sharding. For Dirichlet sampled data, each local client is allocated with a proportion of the samples of each label according to Dirichlet\ndistribution (Li et al. 2020). Specifically, we follow the setting in (Yurochkin et al. 2019), for each label c, we sample\npc ∼ DirJ(0.5) and allocate pc,j proportion of the whole\ndataset of label c to client j. In this setting, some clients may\nentirely have no examples of a subset of classes. For Sharding (McMahan et al. 2017), each client owns data samples\nof a fixed number of labels. Let K be the number of total clients, and q is the number of labels we assign to each\nclient. We divide the dataset by label into K ∗ q shards, and\nthe amount of samples in each shard is\nn\nK·q. We denote this\ndistribution as shards q, where q controls the level of difficulty. If q is set to a smaller value, then the partition is more\nunbalanced.\nMNIST and FMNIST setup\nWe use a simple CNN with\ntwo convolutional layers, followed by two fully connected\nlayers. Following the setting used in (Goodfellow, Shlens,\nand Szegedy 2014), for MNIST, we set perturbation bound\nϵ = 0.3, and step size α = 0.01, and apply adversarial\nattacks for 20 iterations. For FMNIST, we set perturbation\nbound ϵ = 32/255, and step size α = 0.031, we adversarially train the network for 10 steps and apply adversarial\nattacks for 20 iterations. Due to the simplicity of MNIST and\nFMNIST, we mainly use non-IID data (Sharding), which is\nhard to train.\nCIFAR10, CIFAR100, Tiny-ImageNet and ImageNet-12\nsetup\nWe apply a larger CNN architecture, and follow the\nsetting used in (Madry et al. 2017), i.e., we set the perturbation bound ϵ = 0.031, step size α = 0.007. To evaluate the\nrobustness, we conduct extensive experiments with various\ndata partitioning.\nType\nIID\nNon-IID\nDataset\nMethod\nClean\nFGSM\nMIM\nPGD-20\nCW\nAA\nClean\nFGSM\nMIM\nPGD-20\nCW\nAA\nMNIST\nPlain\n99.01\n28.35\n8.65\n5.29\n3.84\n3.02\n98.45\n11.78\n14.06\n8.44\n9.51\n7.45\nPGD AT\n98.52\n76.01\n60.18\n54.50\n55.23\n50.43\n97.82\n67.58\n52.89\n48.03\n47.43\n43.75\nALP\n98.46\n57.37\n55.61\n48.74\n51.17\n44.25\n97.92\n46.49\n51.01\n46.41\n46.24\n41.95\nTRADES\n97.89\n76.79\n63.29\n58.25\n57.24\n53.72\n92.03\n48.45\n51.56\n47.21\n45.81\n42.36\nAVMixup\n98.63\n61.41\n53.34\n42.33\n46.95\n37.78\n97.47\n56.50\n51.86\n46.28\n44.46\n41.84\nOurs\n98.86\n78.06\n70.97\n68.39\n63.09\n59.39\n97.95\n68.54\n54.18\n50.33\n49.12\n44.32\nFMNIST\nPlain\n88.50\n17.89\n3.55\n2.57\n0.40\n0.17\n84.60\n17.86\n3.25\n2.93\n3.05\n-1.40\nPGD AT\n76.05\n68.53\n65.24\n65.40\n64.26\n60.89\n72.93\n60.11\n54.42\n54.33\n52.19\n49.88\nALP\n75.99\n67.31\n63.66\n63.79\n61.55\n59.19\n75.34\n57.67\n53.37\n55.11\n51.12\n51.04\nTRADES\n78.13\n59.33\n52.65\n52.78\n51.44\n48.78\n74.93\n56.53\n44.01\n44.01\n31.80\n39.61\nAVMixup\n79.34\n61.22\n54.93\n54.67\n49.48\n50.07\n72.06\n56.26\n49.21\n49.72\n47.99\n45.15\nOurs\n81.49\n69.23\n66.22\n66.24\n65.71\n61.49\n76.19\n63.11\n56.45\n58.31\n56.96\n53.91\nCIFAR10\nPlain\n78.80\n6.87\n1.15\n1.06\n1.30\n1.23\n61.10\n7.58\n2.94\n2.67\n2.87\n1.28\nPGD AT\n58.75\n30.62\n27.23\n26.11\n28.47\n22.09\n15.27\n13.27\n13.00\n13.00\n12.99\n8.63\nALP\n63.23\n29.42\n26.75\n28.49\n28.13\n23.97\n32.91\n21.41\n20.26\n20.19\n17.74\n15.83\nTRADES\n68.58\n31.53\n25.92\n25.49\n23.07\n20.89\n46.30\n24.81\n22.20\n22.05\n19.59\n17.85\nAVMixup\n70.28\n29.51\n26.22\n26.34\n24.07\n22.25\n48.23\n25.29\n21.42\n24.25\n20.25\n19.43\nOurs\n72.21\n31.47\n28.57\n29.03\n29.31\n24.25\n52.24\n27.03\n24.12\n27.02\n22.13\n21.20\nTable 3: Accuracy and adversarial robustness on MNIST, FMNIST and CIFAR10 under both IID and non-IID distribution. An\nempirical study of FedAvg combined with several defense methods, more detailed comparisons are reported in the Appendix.\nDataset\nCIFAR100\nTiny-ImageNet\nImageNet-12\nMethod\nClean\nPGD-20\nAA\nSquare\nClean\nPGD-20\nAA\nSquare\nClean\nPGD-20\nAA\nSquare\nPGD AT\n39.32\n16.07\n14.36\n23.44\n26.33\n12.26\n10.26\n13.54\n37.42\n22.61\n18.30\n25.57\nALP\n41.12\n18.46\n14.78\n24.54\n32.78\n14.62\n12.19\n16.48\n54.96\n24.78\n19.57\n27.73\nTRADES\n43.39\n20.05\n16.85\n26.43\n37.81\n15.49\n13.26\n19.38\n58.82\n25.49\n21.81\n28.96\nAVMixup\n46.64\n23.56\n19.46\n29.16\n36.19\n15.28\n13.18\n19.25\n59.63\n25.81\n21.92\n29.28\nOurs\n48.31\n24.47\n22.46\n31.57\n38.24\n16.17\n13.96\n20.26\n61.38\n26.47\n22.08\n30.91\nTable 4: Accuracy and adversarial robustness on CIFAR100, Tiny-ImageNet, and ImageNet-12.\nBaselines\nFor attack methods, we perform five popular\nattacks including FGSM (Kurakin, Goodfellow, and Bengio 2017), MIM (Dong et al. 2018), PGD (Madry et al.\n2017), CW (Carlini and Wagner 2017) and AA (Croce\nand Hein 2020). We further use Square (Andriushchenko\net al. 2020) for black-box attack. To investigate the effectiveness of existing FL algorithms, we implement FedAvg(McMahan et al. 2017), FedProx(Li et al. 2018), FedNova(Wang et al. 2020) and Scaffold(Karimireddy et al.\n2020). To defend against adversarial attacks, we implement\nfour most prevailing methods including PGD AT(Madry\net al. 2017), TRADES (Zhang et al. 2019), ALP (Kannan,\nKurakin, and Goodfellow 2018), MMA (Ding et al. 2020)\nand AVMixup (Lee, Lee, and Yoon 2020). We compare the\nperformance of our DBFAT with various kinds of defense\nmethods combined with FL methods.\nPerformance on large datasets\nIn Table 4, we show the\naccuracy and robustness of each method on large datasets\n(e.g., CIFAR100, Tiny-ImageNet, and ImageNet-12). All results are tested under PGD-20 attack (Madry et al. 2017),\nAutoAttack (Croce and Hein 2020), and Square attack (Andriushchenko et al. 2020) in non-IID settings. From the results reported in Table 4, we can find that our method still\noutperforms other baselines in terms of both clean accuracy\nand robustness. Note that our method can achieve the highest accuracy and robustness of 61.38% and 22.08% under\nAutoAttack, respectively. It thus proves that our method can\nalso be used to improve the accuracy and robustness of the\nmodel on large datasets. We think that the higher clean accuracy is a result of the regularization term introduced in\nEquation 6, while maintaining a high robustness.\nConvergence For Local Training\nTo show the convergence rate of DBFAT, we use the Dirichlet sampled CIFAR10 dataset, where each client owns 500\nsamples from 5 classes. Fig. 4 (left sub-figure) shows the\nimpact of local epoch E during adversarial training. Indeed,\nfor a very small epoch (e.g., E = 2), it has an extremely\nslow convergence rate, which may incur more communications. Besides, a large epoch (e.g., E = 20) also leads to\na slow convergence, as model may overfit to the local data.\nConsidering both the communication cost and convergence\nissues, we set E = 5 in our experiments, which can maintain\na proper communication efficiency and fast convergence.\nEffectiveness of Our Method\nWe verify the effectiveness of our method compared with\nseveral adversarial training techniques on Dirichlet sampled\nCIFAR10. Evaluation of model robustness is averaged under\nfour attacks using the the same setting for a fair comparison\nand all defense methods are combined with FedAvg.\nTo show the differences between DBFAT and above mentioned defense methods, we report the training curves on\n0\n25\n50\n75\n100\n125\n150\n175\n200\nCommunication…rounds\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nTraning……Loss\n1e 2\nepochs=2\nepochs=5\nepochs=10\nepochs=20\n0\n25\n50\n75\n100\n125\n150\n175\n200\nCommunication…rounds\n10\n20\n30\n40\n50\n60\n70\nAccuracy\nPGD_AT\nALP\nTRADES\nAVMixup\nDBFAT\nFigure 4: Left: Convergence rate for different local epochs. Right: Accuracy of FedAvg combined with different AT methods.\nDataset\nCIFAR10\nFMNIST\nMethods\nAcln\nAvg Arob\nAcln\nAvg Arob\nOurs\n52.16\n27.80\n75.89\n59.63\nOurs (w/o re-weighting)\n48.44\n25.89\n72.35\n56.34\nOurs (w/o regularization)\n51.04\n26.84\n73.96\n58.23\nTable 5: Ablation Study by cutting off different modules.\nnon-IID CIFAR10 dataset in the right sub-figure of Fig. 4.\nFig. 4 confirms that our DBFAT achieves the highest clean\naccuracy. We speculate that this benefit is due to the regularization term and re-weighting strategy introduced in Equation 6. It is worth mentioning that in the training curves,\nthe model trained with PGD AT performs very poorly. It indicates that standard AT may not be a suitable choice for\nadversarial robustness in FL, as it only uses cross-entropy\nloss with adversarial examples, but ignores the negative impact on clean accuracy. We further report the results on various datasets under both IID and non-IID settings in Table 3,\nwhich indicates that DBFAT significantly outperforms other\nmethods in terms of both accuracy and robustness.\nAblation Study\nCutting off different modules\nAs part of our ablation\nstudy, we first investigate the contributions of different modules introduced in DBFAT. As shown in Table 5, turning off both the re-weighting strategy and regularization\nterm will lead to poor performance, which demonstrates the\nimportance of both modules. Moreover, cut-offing the reweighting strategy can lead to a more severe degradation.\nWe conjecture this is a reasonable phenomenon. As mentioned in Fig. 1, non-IID data can cause a serious accuracy\nreduction. Our re-weighting strategy can alleviate the bias\nby taking the limited data on each client into account.\nEffects of Regularization\nThe regularization parameter β\nis an important hyperparameter in our proposed method. We\nshow how the regularization parameter affects the performance of our robust classifiers by numerical experiments on\ntwo datasets, MNIST and FMNIST. In Equation 6, β conDataset\nMNIST\nFMNIST\nβ\nAcln\nAvg Arob\nAcln\nAvg Arob\n4\n98.30\n26.64\n81.73\n37.36\n2\n98.14\n34.24\n75.59\n47.83\n1.5\n98.46\n53.22\n74.93\n44.08\n1\n97.32\n47.35\n65.43\n42.33\n0.5\n96.57\n44.09\n61.02\n45.28\nTable 6: Effect of hyper-parameter β. “Avg Arob” refers to\nthe average robustness under four attacks.\ntrols the accuracy obtained from the global model, which\ncontains information from distributed clients. Since directly\ntraining on adversarial examples could hurt the clean accuracy, here we explore the effects of β on both accuracy and\nrobustness. As shown in Table 6, we report the clean accuracy and robustness by varying the value of β. We empirically choose the best β for different datasets. For example,\nfor MNIST, β = 1.5 can achieve better accuracy and robustness. For FMNIST, we let β = 2 for a proper trade-off in\naccuracy and robustness.",
        "conclusion": "In this paper, we investigate an interesting yet not well explored problem in FL: the robustness against adversarial attacks. We first find that directly adopting adversarial training\nin federated learning can hurt accuracy significantly especially in non-IID setting. We then propose a novel and effective adversarial training method called DBFAT, which is\nbased on the decision boundary of federated learning, and\nutilizes local re-weighting and global regularization to improve both accuracy and robustness of FL systems. Comprehensive experiments on various datasets and detailed comparisons with the state-of-the-art adversarial training methods demonstrate that our proposed DBFAT consistently outperforms other baselines under both IID and non-IID settings. This work would potentially benefit researchers who\nare interested in adversarial robustness of FL.",
        "summary_en": "In Federated Learning (FL), models are as fragile as centrally trained models against adversarial examples. However, the adversarial robustness of federated learning remains largely unexplored. This paper casts light on the challenge of adversarial robustness of federated learning. To facilitate a better understanding of the adversarial vulnerability of the existing FL methods, the paper conducts comprehensive robustness evaluations on various attacks and adversarial training methods. Moreover, the paper reveals the negative impacts induced by directly adopting adversarial training in FL, which seriously hurts the test accuracy, especially in non-IID settings. This paper proposes a novel algorithm called Decision Boundary based Federated Adversarial Training (DBFAT), which consists of two components (local re-weighting and global regularization) to improve both accuracy and robustness of FL systems. Extensive experiments on multiple datasets demonstrate that DBFAT consistently outperforms other baselines under both IID and non-IID settings.",
        "summary_zh": "这篇论文研究了联邦学习的对抗鲁棒性问题。在联邦学习中，模型与集中训练的模型一样，在对抗对抗性示例时非常脆弱，但联邦学习的对抗鲁棒性却鲜为人知。为了更好地理解现有联邦学习方法的对抗脆弱性，论文对各种攻击和对抗训练方法进行了全面的鲁棒性评估。作者还揭示了直接采用对抗性训练在联邦学习中带来的负面影响，特别是在非独立同分布的情况下，严重影响了测试准确率。为此，他们提出了一种名为基于决策边界的联邦对抗训练（DBFAT）的新算法，包括本地重加权和全局正则化两个部分，以提高联邦学习系统的准确性和鲁棒性。在多个数据集上进行的大量实验表明，DBFAT在IID和non-IID设置下始终优于其他基准方法。"
    },
    {
        "title": "On the Vulnerability of Backdoor Defenses for Federated Learning",
        "abstract": "Federated Learning (FL) is a popular distributed machine learning paradigm that enables jointly training a global model without sharing clients’ data. However, its repetitive serverclient communication gives room for backdoor attacks with aim to mislead the global model into a targeted misprediction when a specific trigger pattern is presented. In response to such backdoor threats on federated learning, various defense measures have been proposed. In this paper, we study whether the current defense mechanisms truly neutralize the backdoor threats from federated learning in a practical setting by proposing a new federated backdoor attack method for possible countermeasures. Different from traditional training (on triggered data) and rescaling (the malicious client model) based backdoor injection, the proposed backdoor attack framework (1) directly modifies (a small proportion of) local model weights to inject the backdoor trigger via sign flips; (2) jointly optimize the trigger pattern with the client model, thus is more persistent and stealthy for circumventing existing defenses. In a case study, we examine the strength and weaknesses of recent federated backdoor defenses from three major categories and provide suggestions to the practitioners when training federated models in practice.",
        "introduction": "In recent years, Federated Learning (FL) (McMahan et al.\n2017; Zhao et al. 2018) prevails as a new distributed machine learning paradigm, where many clients collaboratively\ntrain a global model without sharing clients’ data. FL techniques have been widely applied to various real-world applications including keyword spotting (Leroy et al. 2019),\nactivity prediction on mobile devices (Hard et al. 2018; Xu\net al. 2021), smart sensing on edge devices (Jiang et al.\n2020), etc. Despite FL’s collaborative training capability,\nit usually deals with heterogeneous (non-i.i.d.) data distribution among clients and its formulation naturally leads\nto repetitive synchronization between the server and the\nclients. This gives room for attacks from potential malicious\nclients. Particularly, backdoor attack (Gu, Dolan-Gavitt, and\nGarg 2019), which aims to mislead the model into a targeted\nmisprediction when a specific trigger pattern is presented\nby stealthy data poisoning, can be easily implemented and\nhard to detect from the server’s perspective. The feasibility of backdoor attacks on plain federated learning has been\nstudied in (Bhagoji et al. 2019; Bagdasaryan et al. 2020;\nXie et al. 2019; Zhang* et al. 2022). Such backdoor attacks can be effectively implemented by replacing the global\nFL model with the attackers’ malicious model through carefully scaling model updates with well-designed triggers, and\nthe attacks can successfully evade many different FL setups (McMahan et al. 2017; Yin et al. 2018).\nThe possible backdoor attacks in federated learning\narouse a large number of interest on possible defenses that\ncould mitigate the backdoor threats. Based on the different defense mechanisms they adopt, the federated backdoor defenses can be classified into three major categories: model-refinement, robust-aggregation, and certifiedrobustness. Model-refinement defenses attempt to refine the\nglobal model to erase the possible backdoor, through methods such as fine-tuning (Wu et al. 2020) or distillation (Lin\net al. 2020; Sturluson et al. 2021). Intuitively, distillation or\npruning-based FL can also be more robust to current federated backdoor attacks as recent studies on backdoor defenses\n(Li et al. 2021; Liu, Dolan-Gavitt, and Garg 2018) have\nshown that such methods are effective in removing backdoor\nfrom general (non-FL) backdoored models. On the other\nhand, different from FedAvg (McMahan et al. 2017) and its\nvariants (Karimireddy et al. 2020; Li et al. 2020; Wang et al.\n2020b) which directly average the participating clients’ parameters, the robust-aggregation defenses exclude the malicious (ambiguous) weights and gradients from suspicious\nclients through anomaly detection, or dynamically re-weight\nclients’ importance based on certain distance metrics (geometric median, etc.). Examples include Krum (Blanchard\net al. 2017), Trimmed Mean (Yin et al. 2018), Bulyan (Guerraoui, Rouault et al. 2018) and Robust Learning Rate (Ozdayi, Kantarcioglu, and Gel 2020). Note that some of the\nrobust-aggregation defenses are originally proposed for defending model poisoning attack (Byzantine robustness) yet\nthey may also be used for defending backdoors. The last\nkind, certified robustness aims at providing certified robustness guarantees that for each test example, i.e., the prediction would not change even some features in local training\ndata of malicious clients have been modified within certain\nconstraint. For example, CRFL (Xie et al. 2021) exploits\nclipping and smoothing on model parameters, which yields a\nsample-wise robustness certification with magnitude-limited\nbackdoor trigger patterns. Provable FL (Cao, Jia, and Gong\n2021) learns multiple global models, each with a random\nclient subset and takes majority voting in prediction, which\nshows provably secure against malicious clients.\nDespite the efforts in backdoor defense in federated learning, there exist many constraints and limitations for defenses\nfrom these three major categories. First, the effectiveness of\nthe model-refinement defenses relies on whether the refinement can fully erase the backdoor. Adversaries may exploit\nthis and design robust backdoor patterns that are persistent,\nstealthy and thus hard to erase. Second, robust aggregation\ndefenses are usually based on i.i.d. assumptions of each participant’s training data, which does not hold for the general federated learning scenario where participant’s data are\nusually non-i.i.d. Existing attacks (Bagdasaryan et al. 2020)\nhave shown that in certain cases, such defense techniques\nmake the attack even more effective. Moreover, to effectively reduce the attack success rate of the possible backdoor attacks, one usually needs to enforce stronger robust\naggregation rules, which can in turn largely hurt the normal\nfederated training progress. Lastly, certified robustness approaches enjoy theoretical robust guarantees, yet also have\nquite strong requirements and limitations such as a large\namount of model training or a strict limit on the magnitude\nof the trigger pattern. Also, certified defenses usually lead to\nrelatively worse empirical model performances.\nTo rigorously evaluate current federated backdoor defenses and examine whether the current mechanisms truly\nneutralize the backdoor threats, we propose a more persistent and stealthy federated backdoor attack for circumventing most existing defenses. Through comprehensive experiments and in-depth case study on several state-of-the-art federated backdoor defenses, we summarize our main contributions and findings as follows:\n• We propose a persistent and stealthy backdoor attack\nfor federated learning. Instead of traditional training (on\ntriggered data) and rescaling (malicious client updates)\nbased backdoor injection, our attack selectively flips the\nsigns of a small proportion of model weights and jointly\noptimizes the trigger pattern with client local training.\n• The proposed attack does not explicitly scale the updated\nweights (gradients) and can be universally applied to various architectures beyond convolutional neural networks,\nwhich is of independent interest to general backdoor attack and defense studies.\n• In a case study, we examine the effectiveness of several\nrecent federated backdoor defenses from three major categories and give practical guidelines for the choice of the\nbackdoor defenses for different settings.",
        "proposed approach": "Federated Learning Setup Suppose we have K participating clients, each of which has its own dataset Di with size\nni and N = P\ni ni. At the t-th federated training round,\nthe server sends the global model θt to a randomly-selected\nsubset of m clients. The clients then perform K steps of local training to obtain θi,K\nt\nbased on the global model θt, and\nsend the updates θi,K\nt\n−θt back to the server. The server aggregates the updates with specific rules to get a new global\nmodel θt+1 for the next round. As the standard FedAvg\n(McMahan et al. 2017), the server adopts sample-weighted\naggregation to average the m received updates:\nθt+1 = θt + 1\nN\nm\nX\ni=1\nni(θi,K\nt\n− θt)\n(2.1)\nVarious variants of FedAvg have also been proposed\n(Karimireddy et al. 2020; Li et al. 2020; Wang et al. 2020b;\nReddi et al. 2020). In this work, we adopt the most commonly used FedAvg (McMahan et al. 2017) method as our\nstandard federated learning baseline.\nBackdoor Attacks in FL Assume there exists one or several malicious clients with goal to manipulate local updates\nto inject a backdoor trigger into the global model such that\nwhen the trigger pattern appears in the inference stage, the\nglobal model would give preset target predictions ytarget. In\nthe meantime, the malicious clients do not want to tamper\nwith the model’s normal prediction accuracy on clean tasks\n(to keep stealthy). Therefore, the malicious client has the\nfollowing objectives:\nmin\nθ Ltrain(x, x′, ytarget, θ) := 1\nni\nni\nX\nk=1\nℓ(fθ(xk), yk)\n+λ · ℓ(fθ(x′\nk), ytarget) + α · ||θ − θt||2\n2,\n(2.2)\nwhere x′\nk = (1 − m) ⊙ xk + m ⊙ ∆ is the backdoored\ndata and ∆ denotes the associated trigger pattern, m denotes the trigger location mask, and ⊙ denotes the elementwise product. The first term in (2.2) is the common empirical\nrisk minimization while the second term aims at injecting\nthe backdoor trigger into the model. The third term is usually employed additionally to enhance the attack stealthiness\nby minimizing the distance to the global model (for bypassing anomaly detection-based defenses). The loss function ℓ\nis usually set as the CrossEntropy Loss. λ and α control the\ntrade-off between the three tasks. Most existing backdoor attacks on FL (Bagdasaryan et al. 2020; Bhagoji et al. 2019;\nXie et al. 2019) are based on iteratively training triggered\ndata samples over the loss (2.2) or its variants. Then the\nbackdoored local updates will be rescaled in order to have\nenough influence on the final global model update on the\nserver. Such a process lasts by rounds until the global model\nreaches a high attack success rate.\nThreat Model We suppose that the malicious attacker has\nfull control of their local training processes, such as backdoor data injection, trigger pattern, and local optimization.\nThe scenario is practical since the server can only get the\ntrained model from clients without the information on how\nthe model is trained. Correspondingly, the malicious attacker\nis unable to influence the operations conducted on the central\nserver such as changing the aggregation rules, or tampering\nwith the model updates of other benign clients.\n2.1\nFocused Flip Federated Backdoor Attack\nMost existing backdoor attacks on FL (Bagdasaryan et al.\n2020; Bhagoji et al. 2019) are based on training on triggered data and rescaling the malicious updates to inject the\nbackdoor. There are several major downsides: (1) it requires\nrescaling the updates to dominate the global model update,\nrendering the updates quite different from other clients and\neasier to be defended by clipping or anomaly detection; (2)\nthe dominating malicious updates can also significantly impair the prediction performance of the global model, which\nmakes the backdoored model less attractive to be adopted.\nIn this section, we propose Focused-Flip Federated\nBackdoor Attack (F3BA), in which the malicious clients\nonly compromise a small fraction of the least important\nmodel parameters through focused weight sign manipulation. The goal of such weight sign manipulation is to cause\na strong activation difference in each layer led by the trigger pattern while keeping the modification footprint and influence on model accuracy minimal. A sketch of our proposed attack is illustrated in Figure 1. Let’s denote the current global model as θi,0\nt\n:= {w[1]\nt , w[2]\nt , .., w[L]\nt } and each\nlayer’s output as z[1](·), z[2](·), .., z[L](·). Generally, our attack can be divided into three steps:\nStep 1: Search candidate parameters for manipulation:\nWe only manipulate a small fraction of candidate parameters in the model that are of the least importance to the\nnormal task to make it have a slight impact on the natural accuracy. Specifically, we introduce movement-based importance score to identify candidate parameters for manipulation, which is inspired by the movement pruning (Sanh,\nWolf, and Rush 2020). Specifically, the importance of each\nparameter S[j]\nt\nis related to both its weight and gradient:\nS[j]\nt\n= − ∂Lg\n∂w[j]\nt\n⊙ w[j]\nt , where Lg is the global training loss\nand ⊙ denotes the elementwise product1. We make two major changes on S[j]\nt\nfor our federated backdoor attack:\n• In our federated setting, it is hard to obtain the global loss\nLg since the attack is carried only on the malicious workers. We simply approximate the partial derivative with\nthe model difference − ∂Lg\n∂w[j]\nt\n≈ w[j]\nt −w[j]\nt−1. When t = 0\nwe simply generate a random importance score2 S[j]\n0 ;\n• To handle defense mechanisms with different emphasizes, we extend it into two importance metrics (Directional Criteria and Directionless Criteria) and choose3 the\none that best exploits the weakness of the defense:\nDirectional: S[j]\nt\n= (w[j]\nt\n− w[j]\nt−1) ⊙ w[j]\nt ,\n(2.3)\nDirectionless: S[j]\nt\n= |(w[j]\nt\n− w[j]\nt−1) ⊙ w[j]\nt |.\n(2.4)\nGiven the importance score S[j]\nt , we choose the least important parameters in each layer as candidate parameters. We\ndefine m[j]\ns\nas a mask that selects the s% lowest scores in\nS[j]\nt\nand ignore the others. In practice, setting s = 1% for\nthe model parameters is usually sufficient for our attack.\n1More explanations regarding this movement-based importance\nscore can be found in the Appendix.\n2In practice, since only a subset of clients participate in the\ntraining, the malicious client keeps its last received global model\nuntil it is chosen for training and compute the model difference.\n3A detailed discussion on how to choose the appropriate criteria\ncan be found in the Appendix.\nStep 2: Flip the sign of candidate parameters: Our next\ngoal is to manipulate the parameters to enhance their sensitivity to the trigger by flipping their signs. Take the simple\nCNN model as an example4. We start flipping from the first\nconvolutional layer. For a trigger pattern5 ∆, to maximize\nthe activation in the next layer, we flip w[1]’s signs if they\nare different from the trigger’s signs in the same position:\nw[1] = m[1]\ns ⊙ sign (∆) ⊙ |w[1]| + (1 − m[1]\ns ) ⊙ w[1],\n(2.5)\nwhere m[1]\ns is the candidate parameter mask generated in\nStep 1. Through (2.5), the activation in the next layer is indeed enlarged when the trigger pattern is present. For the\nsubsequent layers, we flip the signs of the candidate parameters similarly. The only difference is that after the sign-flip\nin the previous layer j − 1, we feed a small set of validation\nsamples xv and compute the activation difference of layer\nj − 1 caused by adding the trigger pattern ∆ on xv:\nδ = σ(z[j−1](x′\nv)) − σ(z[j−1](xv)),\nwhere x′\nv = (1 − m) ⊙ xv + m ⊙ ∆\n(2.6)\nσ(·) is the activation function for the network (e.g. ReLU\nfunction) and x′\nv is the backdoor triggered validation samples. Similarly, we can flip the signs of the candidate parameters to maximize δ. This ensures that the last layer’s activation is also maximized when the trigger pattern is presented.\nStep 3: Model training: Although we have maximized the\nnetwork’s activation for the backdoor trigger in Step 2, the\nlocal model training step is still necessary due to: 1) the\nflipped parameters only maximize the activation but have not\nassociated with the target label ytarget, and the training step\nusing (2.2) would bind the trigger to the target label; 2) only\nflipping the signs of the parameter will lead to a quite different model update compared with other benign clients and\na further training step will largely mitigate this issue. Note\nthat since the flipped candidate parameters are the least important to the normal task, our flipping operations will not be\nlargely affected by the later model training step, and thus the\nresulting trigger injection is expected to be more persistent.\nGenerally, Focused Flip greatly boosts training-based\nbackdoor attacks, whereas its time overhead is negligible as\nthe flipping operation does not require backpropagation.\n2.2\nExtensions to Other Network Architectures\nThe flip operation can be similarly extended to other network architectures as the candidate weights selection (Step\n1) and the model training (Step 3) are not relevant to the\nmodel architecture at all. Therefore, we only need to adapt\nthe sign flipping part (Step 2). For CNN, we resize the trigger and flip the sign of the candidate parameters to maximize\nthe convolution layer’s activation. The same strategy applies\nfor any dot product based operation (convolution can be seen\nas a special dot product). Take MLP as an example, assume\nthe first layer’s weight is w[1]. We flip these weights’ signs\nby w[1] = m[1]\ns ⊙sign (x′\nin − xin)⊙|w[1]|+(1−m[1]\ns )⊙w[1],\n4It applies to fully connected layers with simple modifications.\n5If the size of the trigger is not aligned with w[1], we simply\nresize it into the same size as w[1]\nconvolutional layer 𝐰 𝟏\nresized 𝚫 layer#1\n0.1\n-0.3\n0.02\n-0.9\n0.2\n0.6\nmaximize the product, flip\nStep 1.Find the least important \nparameters (yellow weights).\n𝐰[𝟏] layer#1\nStep 2.a. Resize the trigger 𝚫 and flip the \nsigns of convolutional layer (𝐰 𝟏 ).\nStep 2.b. Flip weights of following layers to \nmaximize the activation difference.\nactivation \ndifference 𝜹\nflipped 𝐰[𝟏]\nproduct maximized, no flip\nconvolutional layer 𝐰 𝟐\nresized 𝜹 for layer#1\n𝐰[𝟐] layer#1\nStep 3. Train a backdoored \nlocal models\nCorrect \nClass\nBackdoor\nClass\nTrain for correct class\nTrain for backdoor class\nFigure 1: A sketch of our proposed Focused Flip Federated Backdoor Attack.\n(x′\nin and xin is the flatten input sample with and without trigger, and the non-zero elements of x′\nin − xin only take place\non pixels with the trigger.) The Equation is similar to Equation 2.5 except that we do not need to resize the trigger as in\nCNN. The sign flipping of the rest layers follows the same.\n2.3\nOptimize the Trigger Pattern\nTo further improve the effectiveness of F3BA, we equip the\nattack with trigger pattern optimization6, i.e., instead of fixing the trigger, we optimize the trigger to fit our attack.\nSpecifically, trigger optimization happens in the middle\nof Step 2 and repeats for P iterations: in each iteration, we\nfirst conduct the same focused-flip procedure for w[1]. Then\nwe draw batches of training data xp and generate the corresponding triggered data x′\np using the current trigger ∆. We\nfeed both the clean samples xp and the triggered samples x′\np\nto the first layer and design the trigger optimization loss to\nmaximize the activation difference:\nmax\n∆ Ltrig(xp, ∆) := ∥σ(z[1](xp) − σ(z[1](x′\np))∥2\n2,\n(2.7)\nwhere x′\np = (1 − m) ⊙ xp + m ⊙ ∆.\nIn practice, we optimize Ltrig via simple gradient ascent. It\nis noteworthy that since the pattern ∆ is being optimized\nin each iteration, we need to re-flip the candidate parameters in w[1] to follow such changes. The remaining steps for\nflipping the following layers are the same as before.",
        "evaluating the state-of-the-art federated backdoor defenses": "We evaluate F3BA with trigger optimization on state-ofthe-art federated backdoor defenses (3 model-refinement defenses, 3 robust-aggregation defenses, and 1 certified defense) and compare with the distributed backdoor attack\n(DBA) (Xie et al. 2019) and Neurotoxin (Zhang* et al.\n2022). We test on CIFAR-10 (Krizhevsky and Hinton 2009)\nand Tiny-ImageNet (Le and Yang 2015) with a plain CNN\nand Resnet-18 model under the non-i.i.d. data distributions.\nThe performances of the federated backdoor attacks is measured by two metrics: Attack Success Rate (ASR), i.e., the\nproportion of triggered samples classified as target labels\n6The complete algorithm with the detail of trigger optimization\nis in the Appendix.\nand Natural Accuracy (ACC), i.e., prediction accuracy on\nnatural clean examples. We test the global model after each\nround of aggregation: use the clean test dataset to evaluate\nACC, average all the optimized triggers as a global trigger\nand attached it to the test dataset for ASR evaluation.\nAttack Settings Our goal is to accurately evaluate the robustness of the current backdoor defense capabilities. We\nbelieve that the attack setting adopted in DBA where a certain number of malicious clients is guaranteed to be selected\nfor the global training in each round, is not realistic. Instead,\nwe randomly pick a certain number of clients (benign or malicious). For more details, we set the non i.i.d. data with the\nconcentration parameter h = 1.0 and the total number of\nclients c is 20 with 4 malicious clients. Each selected client\nin F3BA locally trains two epochs as benign clients before\nproposing the model to the server. For F3BA, we choose the\ndirectional criteria by default unless specified.\n3.1\nAttacking Model-Refinement Defenses\nFedDF (Lin et al. 2020) performs ensemble distillation on\nthe server side for model fusion, i.e. distill the next round\nglobal model using the outputs of all the clients’ models on\nthe unlabeled data. Specifically, FedDF ensembles all the\nclient models θi,K\nt\ntogether as the teacher model, and use\nit to distill the next round global model:\nθt+1 = θt − η∇θKL(σ(yensembled), σ(fθt(xunlabeled))),\n(3.1)\nwhere yensembled = 1\nm\nX\ni∈[m]\nfθi,K\nt\n(xunlabeled)\nHere KL stands for Kullback Leibler divergence, σ is the\nsoftmax function, and η is the stepsize. FedDF is regarded\nas a backdoor defense as recent studies (Li et al. 2021) have\nshown that distillation is effective in removing backdoor\nfrom general (non-FL) backdoored models.\nFedRAD (Sturluson et al. 2021) extends FedDF by giving\neach client a median-based score si, which measures the frequency that the client output logits become the median for\nclass predictions. FedRAD normalizes the score to a weight\nsi/ PK\ni=1(si) and use the weight for model aggregation.\nThe distillation part is similar to FedDF. The intuition of\nFedRAD comes from the median-counting histograms for\nprediction from the MNIST dataset, where the malicious\nclients’ prediction logits are less often selected as the median. This suggests that the pseudo-labels constructed by\nmedian logits will be less affected by malicious clients.\nFedMV Pruning (Wu et al. 2020) is a distributed pruning scheme to mitigate backdoor attacks in FL, where each\nclient provides a ranking of all filters in the last convolutional layer based on their averaged activation values on\nlocal test samples. The server averages the received rankings, and prunes the filters in the last convolutional layer\nof the global model with larger averaged rankings. Besides,\nFedMV Pruning erases the outlier weights (far from the average parameter weight) after every few rounds.\nFigure 2: ASR/ACC against Model-Refinement Defenses.\nResults: From Figure 2, the three attacks penetrate all the\nthree model refinement defenses on the CIFAR-10 dataset\nwith closed ACC. While on the Tiny-ImageNet dataset, both\nDBA’s and Neurotoxin’s ASR soon decreases as the training proceeds, suggesting the benign updates eventually overpower the malicious ones and dominate in global model updates. F3BA still evades all three defenses with higher accuracy. Standalone from ensemble distillation, FedMV pruning causes sudden ACC loss in some rounds due to setting some weights with large magnitudes to zero, and these\nweights can be important to the main task.\nDiscussion: From our results, the current model-refinement\ndefenses cannot truly neutralize the backdoor threat from\nmalicious clients. FedDF and FedRAD are designed to overcome data drift, yet their enhanced model robustness cannot fully erase the backdoor from F3BA. On the other hand,\nFedMV pruning cannot precisely target the parameters important for backdoor, and thus damage the performance of\nthe main task when prune the chosen parameters.\n3.2\nAttacking Robust-Aggregation Defenses\nBulyan (Guerraoui, Rouault et al. 2018) is a strong\nByzantine-resilient robust aggregation algorithm originally\ndesigned for model poisoning attacks. It works by ensuring\nthat each coordinate is agreed on by a majority of vectors selected by a Byzantine resilient aggregation rule. It requires\nthat for each aggregation, the total number of clients n satisfy n ≥ 4f + 3, f is the number of malicious clients.\nTo efficiently evade Bulyan, we replace directional criterion (eq. (2.3)) with directionless criteria (eq. (2.4)) to find\ncandidate parameters with small magnitudes and updates.\nRobust LR (Ozdayi, Kantarcioglu, and Gel 2020) works\nby adjusting the servers’ learning rate based on the sign of\nclients’ updates: it requires a sufficient number of votes on\nthe signs of the updates for each dimension to move towards\na particular direction. Specifically, if the sum of signs of updates of dimension k is fewer than a pre-defined threshold β,\nthe learning rate ηβ[k] at dimension k is multiplied by −1:\nηβ[k] =\n(\nη,\n| P\ni∈[m] sgn(θi,K\nt\n[k] − θt[k])| ≥ β\n−η,\nelse\n(3.2)\nTherefore, for dimensions where the sum of signs is below\nthe threshold, Robust LR attempts to maximize the loss. For\nother dimensions, it tries to minimize the loss as usual.\nDeepSight (Rieger et al. 2022) aims to filter malicious\nclients (clusters) and mitigate the backdoor threats: it clusters all clients with different metrics and removes the cluster in which the clients identified as malicious exceeds a\nthreshold. Specifically, 1) it inspects the output probabilities\nof each local model on given random inputs xrand to decide\nwhether its training samples concentrate on a particular class\n(likely backdoors); 2) it applies DBSCAN (Ester et al. 1996)\nto cluster clients and excludes the client cluster if the number\nof potentially malicious clients within exceeds a threshold.\nResults: As Figure 3, Bulyan’s exclusion of anomaly updates largely undermines the evasion of F3BA. By increasing the weight of model-difference-based loss term and applying directionless criteria when flipping parameters, F3BA\nboosts its stealthiness and achieves high ASR. As for Robust\nLR, F3BA exploits the restriction of its voting mechanism\nand easily hacks into it. DeepSight can not defend F3BA under the extremely non-i.i.d data distribution either. In comparison, DBA and Neurotoxin fail on Tiny-Imagenet dataset\nunder all three defenses while maintaining the experiment\nsetting of client numbers and data heterogeneity.\nDiscussion: Overall, Bulyan is a strong defense as F3BA\nneeds to adopt the directionless metric to fully penetrate the\ndefense on both datasets. What’s more, it takes more rounds\nfor F3BA (with directionless metric) to break Bulyan compared with other defenses. One downside is that Bulyan assumes that the server knows the number of malicious clients\nf among n total clients and it satisfies n ≥ 4f + 3. Without\nsuch prior knowledge, one can only guess the true f. For a\ncomprehensive study, we adjust the number of the actual malicious clients from 2 to 8 while keeping the server assume\nthere are at most7 4 malicious clients. As shown in Table 1,\nBulyan indeed protects the FL training when the number of\nmalicious clients is less than 4. When the number reaches\nand goes beyond this cap, Bulyan fails to protect the model\nas the ASR soars, along with a huge loss of ACC as it frequently excludes benign local updates in each round.\nFigure 3: ASR/ACC against Robust Aggregation Defenses.\nRobust LR reverses the updates of dimensions that are possibly compromised by the malicious clients. To understand\nwhy F3BA easily breaks Robust LR, we track the proportion\nof the reversed dimensions during model training in Figure\n4. We observe that F3BA’s majority voting result does not\ndiffer much from that of plain FedAvg while DBA’s reversed\ndimensions are much more than FedAvg. This is easy to understand since F3BA flips a very small fraction of the least\nimportant parameters (thus it would not largely change the\nvoting outcome). The performance of DeepSight largely depends on the clustering result while in the non-i.i.d case such\nresult is unstable, and thus its defense mechanism easily fails\nand reduces to plain FedAvg.\n3.3\nAttacking Certified-robustness\nCRFL (Xie et al. 2021) gives each sample a certificated\nradius RAD that the prediction would not change if (part\nof) local training data is modified with backdoor magnitude\n||∆|| < RAD. It provides robustness guarantee through\n7Bulyan can assume at most 4 malicious clients for a total of 20\nclients to satisfy n ≤ 4f + 3.\nclipping and perturbing in training and parameter-smoothed\nin testing. During training, the server clips the model parameters, then add isotropic Gaussian noise ϵ ∼ N(0, σ2I) for\nparameter smoothing. In testing, CRFL samples M Gaussian noise from the same distribution independently, adds to\nthe tested model, and uses majority voting for prediction.\nClients(f/n)\nRounds\nCIFAR-10\nTiny-Imagenet\nACC\nASR\nACC\nASR\n2/20\n100\n66.30%\n10.36%\n26.31%\n9.98%\n4/20\n100\n63.33%\n61.20%\n25.74%\n92.51%\n6/20\n100\n61.02%\n80.87%\n22.66%\n99.98%\n8/20\n100\n56.76%\n88.89%\n18.96%\n100%\nTable 1: ASR/ACC of F3BA against Bulyan with f malicious clients.\n0.24\n0.22\n0.20\n0.18\n0.16\n0\n20\n40\n60\n80\n100\nRounds\nReversed Updates\nFigure 4: Reversed coordinate proportion for Robust LR.\nWe do not test another ensemble-based certified defense\n(Cao, Jia, and Gong 2021) since its proposed sample-wise\ncertified security requires hundreds of subsampled models\ntrained from the combinations of proposed local models,\nwhich is computationally challenging in practical use.\nResults: To test the defense performance of CRFL, we adjust the variance σ for CRFL and test with backdoor attacks.\nFigure 5 shows that using the same level of noise σ = 0.001,\nF3BA reaches the ASR of nearly 100% while DBA and Neurotoxin fail on Tiny-Imagenet. If we further increase variance to provide larger RAD for F3BA that completely covers\nthe norm of the trigger in each round as in Figure 6, CRFL\ncan defend the F3BA yet with a huge sacrifice on accuracy.\nDiscussion: CRFL provides quantifiable assurance of robustness against backdoor attacks, i.e., a sample-specific robustness radius (RAD). However, in the actual use of CRFL,\nthere is a trade-off between certified robustness and accuracy: the larger noise brings better certified robustness but\nthe apparent loss of accuracy. For defending F3BA in our\nexperiment, the noise that nullifies the F3BA attack in most\nrounds makes the global model lose nearly half of the accuracy on its main task compared to plain FedAvg.\n3.4\nAblation Study on Component Importance\nWe use CRFL to test the components in the proposed attack on the attack effectiveness. We find that F3BA with all\n100\n80\n60\n20\n20\n80\n100\n0\n0\n100\n80\n60\n20\n20\n0\n0\nACC/ASR (%)\nDBA-ASR\nDBA-ACC\nF3BA-ASR\nF3BA-ACC\nNeurotoxin-ASR\nNeurotoxin-ACC\n40\nACC/ASR (%)\n40\n60\n40\n60\n80\n100\n40\nACC/ASR (%)\nFigure 5: ACC/ASR against CRFL with σ = 0.001.\n100\n60\n40\n20\n0\n100\n80\n60\n40\n20\n0\nACC/ASR (%)\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\nASR(σ=1e-3)\nASR(σ=2e-3)\nASR(σ=5e-3)\nACC(σ=1e-3)\nACC(σ=2e-3)\nACC(σ=5e-3)\nRound\nRound\n80\nACC/ASR (%)\nASR(σ=1e-3)\nASR(σ=5e-3)\nASR(σ=1e-2)\nACC(σ=1e-3)\nACC(σ=5e-3)\nACC(σ=1e-2)\nASR(σ=1e-3)\nASR(σ=2e-3)\nASR(σ=5e-3)\nACC(σ=1e-3)\nACC(σ=2e-3)\nACC(σ=5e-3)\nFigure 6: ACC/ASR of F3BA against CRFL with σ.\nof components including training, flipping, and the trigger\noptimization achieves better ASR and can even fully evade\ndefenses not compromised by previous training-based backdoor attacks. While a flip operation with respect to a fixed\ntrigger can already boost attack, an adaptively-optimized\ntrigger further amplify the attack effectiveness and thus improving the ASR to a higher level.\nCIFAR-10\nTrain\nTrain+Flip\nTrain+Flip+TrigOpt\n50 rounds\n86.95%\n88.90%\n99.01%\n100 rounds\n84.45%\n85.58%\n98.79%\nTinyImagenet\nTrain\nTrain+Flip\nTrain+Flip+TrigOpt\n50 rounds\n50.81%\n71.41%\n98.77%\n100 rounds\n65.52%\n67.49%\n99.90%\nTable 2: Effect of different components in F3BA on ASR.",
        "takeaway for practitioners": "From the results in Section 3, there is no panacea for the\nthreat of backdoor attacks. Current federated backdoor defenses, represented by the three categories, all have their\nown Achilles’ heel facing stealthier and more adaptive attacks such as F3BA: model-refinement defenses enhance the\nglobal model’s robustness towards data drift while completely fail to erase the backdoor in malicious updates;\ncertain robust-aggregation (e.g., Bulyan, Robust LR) and\ncertified-robustness (e.g., CRFL) defenses achieve acceptable backdoor defense capabilities in practice when imposing strong intervention mechanisms such as introducing\nlarge random noise or reversing global updates. However,\nsuch strong interventions also inevitably hurt the model’s\nnatural accuracy. Overall, we recommend the practitioners\nto adopt Bulyan or CRFL in the cases where the natural accuracy is already satisfiable or is less important, as they are\nthe most helpful in defending against backdoors.",
        "additional related work": "In this section, we review the most relevant works in general\nFL as well as the backdoor attack and defenses of FL.\nFederated Learning: Federated Learning (Koneˇcn`y et al.\n2016) was proposed for the communication efficiency in\ndistributed settings. FedAvg (McMahan et al. 2017) works\nby averaging local SGD updates, of which the variants have\nalso been proposed such as SCAFFOLD (Karimireddy et al.\n2020), FedProx (Li et al. 2020), FedNova (Wang et al.\n2020b). (Reddi et al. 2020; Wang, Lin, and Chen 2022)\nproposed adaptive federated optimization methods for better adaptivity. Recently, new aggregation strategies such as\nneuron alignment (Singh and Jaggi 2020) or ensemble distillation (Lin et al. 2020) has also been proposed.\nBackdoor Attacks on Federated Learning: (Bagdasaryan\net al. 2020) injects backdoor by predicting the global model\nupdates and replacing them with the one that was embedded with backdoors. (Bhagoji et al. 2019) aims to achieve\nboth global model convergence and targeted poisoning attack by explicitly boosting the malicious updates and alternatively minimizing backdoor objectives and the stealth\nmetric. (Wang et al. 2020a) shows that robustness to backdoors implies model robustness to adversarial examples and\nproposed edge-case backdoors. DBA (Xie et al. 2019) decomposes the trigger pattern into sub-patterns and distributing them for several malicious clients to implant.\nBackdoor Defenses on Federated Learning: Robust\nLearning Rate (Ozdayi, Kantarcioglu, and Gel 2020) flips\nthe signs of some dimensions of global updates. (Wu et al.\n2020) designs a federated pruning method to remove redundant neurons for backdoor defense. (Xie et al. 2021)\nproposed a certified defense that exploits clipping and\nsmoothing for better model smoothness. BAFFLE (Andreina et al. 2021) uses a set of validating clients, refreshed\nin each training round, to determine whether the global updates have been subject to a backdoor injection. Recent\nwork (Rieger et al. 2022) identifies suspicious model updates via clustering-based similarity estimations.",
        "conclusion": "In this paper, we propose F3BA to backdoor federated learning. It does not explicitly scale malicious clients’ local updates but instead flips the weights of some unimportant\nmodel parameters for the main task. With F3BA, we evaluate the current state-of-the-art federated backdoor defenses.\nIn most tests, F3BA is able to evade and reach a high attack success rate. From this we argue that despite providing\nsome robustness, the current stage of backdoor defenses still\nexpose the vulnerability to the advanced backdoor attacks.",
        "summary_en": "Federated Learning (FL) is a popular distributed machine learning paradigm which enables jointly training a global model without sharing clients' data. However, its repetitive server-client communication gives room for possible backdoor attacks which aims to mislead the global model into a targeted misprediction when a specific trigger pattern is presented. In response to such backdoor threats on federated learning, various defense measures have been proposed. This paper studies whether the current defense mechanisms truly neutralize the backdoor threats from federated learning in a practical setting by proposing a new federated backdoor attack framework for possible countermeasures. Different from traditional training (on triggered data) and rescaling (the malicious client model) based backdoor injection, the proposed backdoor attack framework (1) directly modifies (a small proportion of) local model weights to inject the backdoor trigger via sign flips; (2) jointly optimize the trigger pattern with the client model, thus is more persistent and stealthy for circumventing existing defenses. In a case study, the paper examines the strength and weaknesses of several recent federated backdoor defenses from three major categories and provide suggestions to the practitioners when training federated models in practice.",
        "summary_zh": "这篇论文研究了针对联邦学习中后门威胁的防御措施的有效性。论文提出了一种新的联邦后门攻击框架，以评估这些防御措施的实际效果。该攻击框架与传统的后门注入方法不同，采用直接修改局部模型权重的方式注入后门触发器，同时与客户端模型联合优化触发模式，更加持久和隐蔽，以规避现有的防御措施。在案例研究中，作者评估了几种最近提出的联邦后门防御方法的优劣，并为实践者在联邦模型训练时提供了建议。"
    },
    {
        "title": "Poisoning with Cerberus: Stealthy and Colluded Backdoor Attack against Federated Learning",
        "abstract": "Are Federated Learning (FL) systems free from backdoor poisoning with the arsenal of various defense strategies deployed? This is an intriguing problem with significant practical implications regarding the utility of FL services. Despite the recent flourish of poisoning-resilient FL methods, our study shows that carefully tuning the collusion between malicious participants can minimize the trigger-induced bias of the poisoned local model from the poison-free one, which plays the key role in delivering stealthy backdoor attacks and circumventing a wide spectrum of state-of-the-art defense methods in FL. In our work, we instantiate the attack strategy by proposing a distributed backdoor attack method, namely Cerberus Poisoning (CerP). It jointly tunes the backdoor trigger and controls the poisoned model changes on each malicious participant to achieve a stealthy yet successful backdoor attack against a wide spectrum of defensive mechanisms of federated learning techniques. Our extensive study on 3 large-scale benchmark datasets and 13 mainstream defensive mechanisms confirms that Cerberus Poisoning raises a significantly severe threat to the integrity and security of federated learning practices, regardless of the flourish of robust Federated Learning methods.",
        "introduction": "The distributed nature of federated learning makes it vulnerable to backdoor attacks carried out by malicious participants, as unveiled in recent studies (Fung, Yoon, and\nBeschastnikh 2018; Baruch, Baruch, and Goldberg 2019;\nXie et al. 2020; Bagdasaryan et al. 2020; Wang et al. 2020;\nFang et al. 2020; Shejwalkar and Houmansadr 2021). Several malicious participants can collude to embed a welldesigned backdoor trigger into their local training data to\npoison the global aggregated model. The perturbed global\nmodel then misclassifies the input instances embedded with\nthe backdoor trigger as the target label specified by the adversary. However, the perturbed global model performs normally on clean input instances. In security-critical applications, e.g., distributed video surveillance and credit risk assessment, the adversary can compromise several local computing devices of the target federated learning system to\nlaunch such backdoor attacks. The adversary can mislead the\njointly trained global model to misidentify malicious activities with the trigger signal on one hand. On the other hand,\nthe poisoned model works normally on other inputs without\nthe trigger. It is thus difficult for the FL service owners to\nflag the malfunction caused by colluding backdoor attacks.\nThere has been a long line of efforts exploring various defensive mechanisms to prevent distributed poisoning attacks,\nincluding backdoor attacks. These defense methods detect\nanomalies in the submitted local models to mitigate potential poisoning effects (Blanchard et al. 2017; Yin et al. 2018;\nMhamdi, Guerraoui, and Rouault 2018; Pillutla, Kakade,\nand Harchaoui 2019; Shejwalkar and Houmansadr 2021;\nCao et al. 2021), introduce additional random perturbations\nto model parameters to gain certified robustness against the\nbackdoor noise (Geyer, Klein, and Nabi 2017; Sun et al.\n2019; Wei et al. 2021; Xie et al. 2021; Sun et al. 2021),\ndown-weigh the poisoned local model updates sharing similar model parameters to mitigate colluding poisoning (Fu\net al. 2019; Fung, Yoon, and Beschastnikh 2020), and build\na voting-based defense mechanism to filter poisoned local\nmodels (Cao, Jia, and Gong 2021; Andreina et al. 2021).\nAs confirmed in our empirical study, these defensive mechanisms can indeed effectively mitigate the existing backdoor\nattacks. They provide a rich arsenal of robust learning solutions for service providers of FL systems.\nHowever, our study shows that the distributed backdoor\npoisoning threat against federated learning is far from being well addressed. We demonstrate that adversaries can\norganize the collusion of malicious participants to easily\ndodge various state-of-the-art defensive mechanisms while\nsuccessfully launching distributed backdoor attacks. The adversary achieves this goal by exploiting the fundamental assumptions of different defensive mechanisms and adjusting\nthe learning objectives of backdoor attacks accordingly.\nOur contributions can be summarized in the following\nperspectives.\n1) Theoretically, we establish lower and upper bounds\non the trigger-induced local model changes for malicious\nparticipants in general federated learning tasks. Instead\nof requiring the local model parameters of different participants to be IID, we assume the gradients of the local models are bounded and Lipschitz-continuous. This assumption\nholds for most practically deployed machine learning models and is generic enough for both IID and non-IID federated learning scenarios. Our analysis identifies the key factors controlling the local model bias induced by backdoor\ntriggers, thus deciding the feasibility of federated backdoor\nattacks exposed to defense methods.\n2) We propose a stealthy distributed backdoor attack,\nnamely Cerberus Poisoning (CerP) by exploiting the algorithmic principles of current defense methods in federated learning. Despite originating from different perspectives and threat model settings, these defensive mechanisms\nshare the same core assumption: regardless of the data distribution of participants, each poisoned local model trained\nwith poisoned data is biased largely from those trained by\nthe poison-free data. Exploiting the limit of this assumption, CerP casts the distributed backdoor attack as a joint\noptimization process of three learning objectives.\nAutomatic fine-tuning of backdoor triggers. Based on\nthe theoretical analysis, we believe that the injected trigger\nis an important factor in determining the magnitude of variation in the parameter bias of the poisoned local models. In\nCerP, we propose to fine-tune the backdoor trigger to facilitate the learning of the poisoned data and reduce the parameter bias of the poisoned local models.\nControl over local model bias. For each malicious participant, we suppress the model parameter bias between the\npoisoned local model and its poison-free counterpart that\nwould have been derived if no trigger noise was injected.\nWe also explain the theoretical rationality of explicitly suppressing the bias in the poisoned local models.\nDiversity of poisoned local models. To bypass the sybilattack mitigation methods (Fung, Yoon, and Beschastnikh\n2018), we require the poisoned local models submitted by\nmalicious participants to be as dissimilar as possible. We\nenlarge the divergence of the poisoned local models to avoid\nbeing flagged by the similarity-based defenses.\n3) Our comprehensive empirical evaluation shows that\nthe proposed CerP method circumvents all the 13 defense\nmethods on 3 large-scale benchmark datasets (CIFAR100, Fashion-MNIST, and LOAN). Compared to other\nstate-of-the-art distributed backdoor attack methods (Sybil\nattack (Fung, Yoon, and Beschastnikh 2018), LIE (Baruch,\nBaruch, and Goldberg 2019), and DBA (Xie et al. 2020)), the\nexperimental results show that CerP achieves consistently\nhigher attack success rates and maintains the accuracy of the\nmain learning task, no matter which defense strategy is employed. In contrast, none of the 3 distributed backdoor attack\nmethods can neutralize all defenses.",
        "related work": "Backdoor Attacks against Federated Learning. Pioneering studies on backdoor attacks against federated learning\nsystems (Fung, Yoon, and Beschastnikh 2018; Bagdasaryan\net al. 2020) assume that each malicious participant trains\ntheir local models individually, without any collusion between them. Since they use the same backdoor trigger, the\npoisoned local models tend to share similar parameter values\nand are largely deviated from benign local models. These attacks are thus easily mitigated by the Byzantine-robust aggregation methods and sybil-attack mitigation methods like\nFoolsgold (Fung, Yoon, and Beschastnikh 2018).\nMore advanced distributed backdoor threats (Baruch,\nBaruch, and Goldberg 2019; Sun et al. 2019) consider how\nto evade Byzantine-robust aggregation rules. They clip the\nparameters of the poisoned local model according to the parameter range of the benign local models. However, they either assume the parameters of the benign local models are\nIID Gaussian variables so that the bounds on benign parameter values can be estimated (Baruch, Baruch, and Goldberg 2019), or assume that the benign parameter bounds\nare known as prior knowledge (Sun et al. 2019). Neither of\nthese assumptions holds in practice, especially in non-IID\nfederated learning tasks. Therefore, the manually configured\nparameter clipping bounds may be overestimated (downgrading the learning capability), or underestimated (failing\nthe attack task). Besides, some methods require knowing\nthe model parameters committed by the benign participants,\nwhich violates the protocol of federated learning.\nAlternatively, DBA (Xie et al. 2020) manages stealthy\nbackdoor attacks by manually decomposing a global backdoor trigger into different local triggers and assigning separately the local triggers to each malicious participant. The\nmalicious participants learn to fit different local triggers,\nand thus have dissimilar poisoned local models to bypass\nthe sybil-attack mitigation methods. However, how to properly decompose global triggers to guarantee successful backdoor attacks remains an open issue. Manual decomposition\nof global triggers unavoidably introduces artifacts into local triggers. DBA may thus lead to large deviations of the\npoisoned local models from the benign ones. Therefore, this\nmethod fails to attack the popular Byzantine-robust aggregation methods such as Krum and Bulyan.\nByzantine-robust aggregation methods. These methods\n(Blanchard et al. 2017; Yin et al. 2018; Mhamdi, Guerraoui,\nand Rouault 2018; Pillutla, Kakade, and Harchaoui 2019;\nShejwalkar and Houmansadr 2021; Cao et al. 2021) follow\nthe spirit of anomaly detection. The core hypothesis assumes\nthat the parameters of all benign local models stay within\na bounded lp-ball centered on the global model. Therefore,\nthe poisoned local models are considered to be outliers that\nlargely deviate from benign local models. Nevertheless, they\nmay fail to detect distributed backdoor attacks that are dedicated to minimizing the distance between the malicious and\nbenign local models.\nDifferential privacy-based methods. These defense\nmethods (Geyer, Klein, and Nabi 2017; Wei et al. 2021;\nSun et al. 2021; Xie et al. 2021) adopt the core ideas of\ndifferential privacy theory and random smoothing (Cohen,\nRosenfeld, and Kolter 2019). These methods add Gaussian\nor Laplace noises to the parameters of the global model or\nlocal models. The injected perturbation makes the disturbed\nmodels insensitive to backdoor triggers. However, how to\nproperly set the noise magnitude is still open in practice.\nToo strong or too weak noise may either harm the utility of\nthe target model or weaken the capability to defend against\nthe attack.\nOther defense methods like Foolsgold (Fung, Yoon, and\nBeschastnikh 2018) identify poisoning sybils based on the\nsimilarity of local model updates. Nevertheless, the adversary can encourage the diversity of poisoned local models to\nbypass these defense methods. Ensemble FL method (Cao,\nJia, and Gong 2021) trains multiple global models to make\na majority vote on prediction decisions. Similarly, BaFFLe\n(Andreina et al. 2021) is also a voting-based defense, where\nparticipants validate the global model on their local data and\nvote to accept or reject the global model. Since benign participants are agnostic to attacker-designed triggers, it is difficult to recognize and flag backdoor poisoning efforts only\nby inspecting the overall classification performance of the\npoison-free testing data.",
        "algorithm description of cerberus poisoning": "Preliminaries\nFederated Learning. We focus on the setting of Federated\nLearning with partial participation, i.e. Np out of N participants are selected in each training iteration of federated\nlearning. Compared to the full participation setting, the partial participation setting of FL is better adapted to real-world\nmachine learning applications, such as mobile edge computing, where local devices may join or leave the FL service at will. Each participant i hosts a local dataset Di =\nn\n{xi,j ∈ Rm, yi,j}di\nj=1\no\n, where di = |Di| and {xi,j, yi,j}\nrepresents the features and label of each data instance. At\neach training iteration t, we use gt−1 and ht\ni to represent\nthe global model shared with the selected participants and\nthe local model of each participant i respectively. The server\nupdates the global model by aggregating the local model updates with a learning rate η: gt = gt−1+ η\nNp\nNp\nP\ni=1\n(ht\ni − gt−1).\nAdversary’s goal. The goal of backdoor attacks on FL is\ntwofold: 1) The classifier derived by the poisoned federated\ntraining process should produce the expected decision output set by the backdoor trigger. 2) The accuracy of the main\nlearning task on poison-free data should not be perturbed.\nAdversary’s capability. The adversary compromises C of\nthe total N participants (C ≪ N). In a training iteration of\npartial-participation federated learning, each selected malicious participant controlled by the adversary injects backdoor poisoned instances into the local training set. Note that\nthe adversary cannot know or tamper with the global aggregation rules or the local training process of benign participants. The malicious participants can collude by sharing\ntheir poisoned local models with the adversary. The adversary can then share back to each malicious participant the\npoisoned local models submitted by other malicious participants as additional knowledge to guide how to control the\nlocal model changes of the malicious participants.\nThe Objective of Cerberus Poisoning\nWe define CerP as a distributed optimization problem with\nan objective function given in Eq.1. The aggregated global\nmodel is trained to fit both the clean training data and poisoned training data with the trigger. Given a federated training iteration t, the adversary launches the backdoor attack\nto derive the poisoned local models h∗,t\ni\nof the compromised participants via jointly optimizing the learning objective with compromised participants, which gives in Eq.1.\n∆x∗,t, {h∗,t\ni }i∈S = arg min\n∆x,ht\ni(i∈S)\n{\nX\ni∈S\n(\nX\nj∈Dnor\ni\nℓht\ni(xi,j, yi,j)\n+\nX\nj∈Dmal\ni\nℓht\ni(xi,j + ∆x, ˆyi,j)) + α\nX\ni∈S\n\r\rht\ni − hnor,t\ni\n\r\r\nF ro\n+ β\nX\ni,i′∈S\ncs(ht\ni, ht\ni′)}\ns.t.\n\r\r∆x − ∆x0\r\r\n2 ≤ φ\n(1)\nwhere ℓht\ni denotes the classification loss function given the\nlabelled data instance (x, y) and the local classifier model\nht\ni of each participant i. ∥∥F ro is the Frobenius norm of a\nmatrix. We use S to represent the set of compromised participants in the t-th federated training iteration. The attack\nprocess of CerP can be summarized from two perspectives:\nFor a compromised participant i ∈ S, Dmal\ni\nand Dnor\ni\nrepresent the backdoor-poisoned and the poison-free training data hosted by the participant i, respectively. hnor,t\ni\nis\nthe poison-free model trained by the compromised participant i at the iteration t. cs(ht\ni, ht\ni′) is the pairwise cosine\nsimilarity between the local models submitted by a pair of\ncompromised participants i and i′ (i, i′ ∈ S). According\nto the threat model setting, each compromised participant\ncan access the poisoned local models derived by other compromised participants via the adversary. Except that, all of\nthe compromised participants follow the standard federated\nlearning setting. Besides, we consider the backdoor trigger\n∆x as an optimization variable in CerP. ∆x0 is the initial\nbackdoor trigger designed by the adversary before launching the optimization process of Eq.1. The parameter φ in\nEq.1 limits the distance between the finely tuned trigger ∆x\nand the initial trigger ∆x0.\nAt the iteration t, solving the constrained optimization\nproblem in Eq.1 produces the tuned backdoor trigger ∆x∗,t\nand the poisoned local model h∗,t\ni . The finely tuned backdoor trigger ∆x∗,t can be used in the attack. The poisoned\nlocal models h∗,t\ni\nare committed to the central server to generate the poisoned global model for the next iteration. The\nlearning objective of CerP (Eq.1) is four-fold:\nObjective 1. Learning both clean and backdoor poisoned\ntraining data. The classification accuracy of the main learning task and the backdoor attack task is optimized. The main\nlearning task is to train ht\ni on the clean training data hosted\nby the compromised participants (the first term in Eq.1).\nThe backdoor attack task is to make ht\ni fit the backdoor poisoned training data hosted by the compromised participants\n(the second term of Eq.1).\nObjective 2. Trigger fine-tuning for stealthy backdoor\nattacks. The backdoor trigger ∆x is considered as an optimization variable of the attack objective. Intuitively, the\nbackdoor trigger ∆x and the poisoned local models are\njointly tuned to minimize the learning loss of the backdoor\npoisoned training data. This is used to facilitate the poisoned\nlocal model ht\ni to accurately fit the backdoor poisoned data\nwithout inducing large biases in the local model parameters.\nWe establish the following theoretical analysis to explain the\nrationality of backdoor trigger tuning.\nWithout loss of generality, we use a multi-layer neural network H for C-class classification with K fully connected layers as the target model for the federated learning\ntask. The classification function can be written as H(x) =\nσK−1hK−1(σK−2hK−2(σK−3hK−3(···σ0h0(x)))), where\nσk and hk ∈ Rdk×dk+1 (k = 0, 1, 2, ..., K −1 and dK = C)\nare the activation function and the parameter matrix of each\nlayer, respectively. Following (Wang et al. 2020), we define\nan ϵ-adversarial equivalent sample x′ to a backdoor poisoned\nsample x + ∆x.\nDefinition 1. Given a targeted classifier Hnor and a backdoor poisoned classifier Hmal, an ϵ-adversarial equivalent\nx′ is defined as:\nHnor(x′) = Hmal(x + ∆x),\nx′ = x + ∆x + ϵx,\ns.t. ∥ϵx∥2 ≤ ϵ\n(2)\nAssume that the backdoor attack is successfully delivered\nat the training iteration t. Let hnor,t\ni,k\nand hmal,t\ni,k\ndenote the\nparameter matrix of the layer k of the poison-free and poisoned local model of the compromised participant i. Let\ngnor,t−1\nk\nbe the parameter matrix of the layer k of the aggregated global model at the training iteration t − 1. The\ndistance between hmal,t\ni,k\nand gnor,t−1\nk\n(the trigger-induced\nlocal model bias) can be bounded in Theorem 1.\nTheorem 1 Let ℓH(x, y) be the classification risk function\nof a federated learning task. Its gradient with respect to\neach coordinate j of ht\ni,k, ∇ht\ni,k,jℓH(x, y) is bounded by the\nLipschitz constant L, i.e. |∇ht\ni,k,jℓH(x, y)| ≤ L. Thus the\ncoordinate-wise gradient follows a γ-subgaussian distribution with mean µ. X(k) represents the input to the layer\nk of H by feeding a set of input instances X to H. For a\ngiven layer k (1 ≤ k ≤ K), the distance between hmal,t\ni,k\nand\ngnor,t−1\nk\ncan be bounded from above with a probability of\np≥1 − 2dkdk+1mNδe{−n min{√\ndkdk+1L/γ,2dkdk+1L2/γ2}}.\nThe constant Nδ is defined such that Nδ ≤ (1+D/δ)dkdk+1,\nwhere δ is the covering number of the layer k parameter matrix hnor,t\ni,k\n(Vershynin 2011).\n∥hmal,t\ni,k\n− gnor,t−1\nk\n∥F ro ≤ ∥ϵ∥2\np\ndmal\ni\nQk\ns=0 ∥hnor,t\ni,s\n∥∗\nρk\n+2ηt\np\ndkdk+1L + ∥gnor,t\nk\n− gnor,t−1\nk\n∥F ro\n(3)\nwhere ρk is the minimum eigenvalue of X(k). ϵ is the perturbation bound of the adversarial equivalent to the backdoor\npoisoned sample x + ∆x. ∥∥∗ denotes the spectral norm of\nthe parameter matrix. ηt is the learning rate of the federated\ntraining iteration t.\nCorollary 1.1 Inheriting the setting of Theorem 1, if the\nclassification loss ℓH(x, y) is Lc-Lipschitz continuous, the\nupper bound Eq.3 can be further formulated as:\n∥hmal,t\ni,k\n− gnor,t−1\nk\n∥F ro ≤ ∥ϵ∥2\np\ndmal\ni\nQk\ns=0 ∥hnor,t\ni,s\n∥∗\nρk\n+ 2ηk\np\ndkdk+1L + ηtLc∥gnor,t\nk\n− gnor,∗\nk\n∥F ro\n(4)\nwhere gnor,∗\nk\nas the optimal parameters of the layer k derived once converged.\nTheorem 2 Following the same setting in Theorem 1, if the\nclassification loss ℓH(x, y) is Lc-Lipschitz continuous, for a\ngiven layer k (1 ≤ k ≤ K), the distance between hmal,t\ni,k\nand\ngnor,t−1\nk\ncan be bounded from below as:\n∥hmal,t\ni,k\n− gnor,t−1\nk\n∥F ro ≥\nνk∥ϵ∥2\nmax\nxnor\ni,k ,xmal\ni,k\n∥xnor\ni,k − xmal\ni,k ∥2\n− ηtLc∥gnor,t\nk\n− gnor,∗\nk\n∥F ro\n(5)\nwhere νk is the minimum non-zero singular value of the\nproduct of the parametric matrices hnor\ni,k , hnor\ni,k−1 · · · hnor\ni,0 .\nxnor\ni,k and xmal\ni,k are the poison-free and poisoned samples in\nX(k) hosted by the participant i respectively.\nOur unveilings in the analysis can be summarized in two\naspects. First, under the adversary-free scenario, ∥gnor,t\nk\n−\ngnor,t−1\nk\n∥F ro in Eq.3 and ∥gnor,t\nk\n− gnor,∗\nk\n∥F ro in Eq.5 vanish when the federated training is close to convergence.\nBearing this in mind, it is generally impossible to ensure\nthe success of backdoor attacks without causing local model\nchanges on compromised participants, unless ϵ = 0 according to Eq.3 and Eq.5. However, the exception with ϵ = 0\nholds only when Hnor(x+∆x) = ˆy. In this case, the trigger\n∆x should be chosen in a way that a classifier can produce\naccurately the target label of an input instance carrying the\ntrigger, even without poisoning the classifier with the backdoor data. This situation is difficult to meet in practice.\nSecond, according to Eq.3 in Theorem 1, minimizing the\nvalue of the product ∥ϵ∥2\nQk\ns=0 ∥hnor,t\ni,s\n∥∗ is the key to minimize the trigger-induced parameter changes to evade defensive methods. We propose to achieve this goal by adjusting\nthe designated backdoor trigger ∆x via Eq.6 (the second\nterm in Eq.1). It aims at adapting the trigger to minimize\nthe classification loss of the poisoned local model ht\ni on\nthe backdoor poisoned instance (x + ∆x, ˆy). According to\nthe definition of ϵ-adversarial equivalent, Eq.6 directly minimizes the adversarial noise magnitude ∥ϵ∥2 with respect to\nthe local model ht\ni. Consequently, the derived finely tuned\ntrigger ∆x∗,t can reduce both the upper and lower bounds of\nthe trigger-induced model changes, as ∥ϵ∥2 decreases without changing the local model ht\ni (thus Qk\ns=0 ∥hnor,t\ni,s\n∥∗ keeps\nunchanged in the product ∥ϵ∥2\nQk\ns=0 ∥hnor,t\ni,s\n∥∗). Meanwhile, fine-tuning the trigger helps reduce the learning loss\non the backdoor poisoned training data, which alleviates the\ndifficulty of memorizing the trigger-induced feature-label\ncorrelation.\n∆x∗,t = arg min\n∆x\nP\ni∈S,j∈Dmal\ni\nℓht\ni(xi,j + ∆x, ˆyi,j)\ns.t.\n\r\r∆x − ∆x0\r\r\n2 ≤ φ\n(6)\nWithout loss of generality, we adopt L2 distance to measure\nthe magnitude of changes adapted to the finely tuned trigger.\nObjective 3. Deviation regularization on local models.\nTo bypass the defense methods, we control the parameter\nchanges of the poisoned local models at the participant level.\nFor each compromised participant i, we minimize the distance between the poisoned local model ht\ni and the poisonfree local model that could be derived if no trigger noise\nwas injected (noted as hnor,t\ni\n). The distance is measured using the Frobenius norm as ∥ht\ni − hnor,t\ni\n∥F ro. Enforcing the\ndistance regularization (the third term of Eq.1) helps reduce the spectral norm of ht\ni. Assuming hnor,t\ni\nis close to\nconvergence on the poison-free training data, hnor,t\ni\ncan be\nconsidered as a constant as the gradient vanishes. Following\nthe matrix norm inequality, it can be found that minimizing\n∥ht\ni − hnor,t\ni\n∥F ro suppresses the upper bound of the spectral\nnorm of ht\ni:\n∥ht\ni − hnor,t\ni\n∥F ro ≥ ∥ht\ni∥∗ − const\n(7)\nwhere all the factors independent of the poisoned training\ndata are absorbed into const. More intuitively, dragging ht\ni\nand hnor,t\ni\nclose together helps evade from the Byzantinerobust aggregation methods, e.g., Trimmed mean and Krum.\nThe closer the poisoned local model ht\ni stays to the benign\none hnor,t\ni\n, the more difficult it is for these defense methods\nto identify and exclude the compromised participants without raising false alarms.\nThe joint application of the trigger fine-tuning and the\ndeviation regularization term over the poisoned local models makes both ∥ϵ∥2 and the classifier’s spectral norm in\nEq.3 and Eq.5 decrease. They form the core of the proposed CerP attack method: controlling trigger-induced local\nmodel changes to deliver stealthy yet successful federated\nbackdoor attacks.\nObjective 4. Pairwise similarity regularization. We suppress the cosine similarity between the poisoned local models of the compromised participants (the fourth term of\nEq.1). Foolsgold (Fung, Yoon, and Beschastnikh 2018)\nagainst sybil attacks reduces the aggregated weights of participants that repeatedly contribute similar local model updates. We enhance the diversity of the malicious local models to evade similarity-based defense methods.\nOptimization Algorithm\nWe provide the pseudocodes of CerP in Algorithm 1. Multiple malicious participants introduce the backdoor trigger\ndesignated by the adversary into local training data. The malicious participants jointly optimize the CerP’s attack objective by fine-tuning the backdoor trigger and suppressing the\ntrigger-induced local model biases, as defined in Eq.1. The\nfine-tuning trigger stage utilizes only the poisoned training\ndata hosted by the malicious participants. Both the poisoned\nand poison-free local models are aggregated at the central\nserver to produce a poisoned global model.",
        "experimental evaluation": "We evaluate the attack performance of the distributed backdoor attack methods using 3 benchmark datasets of different application scenarios. We implement all the involved\nalgorithms using PyTorch on an Ubuntu workstation with\nNVIDIA 3090 GPUs. Our code can be found at the link 1.\n1https://github.com/xtlyu/CerP\nAlgorithm 1: Cerberus Poisoning\nInput: The global model g and the local model hi of malicious\nparticipant i. The set of malicious participants S.\nOutput: The finely tuned backdoor trigger ∆x∗,t and the backdoor poisoned local model h∗,t\ni .\n1: for t = E → T do\n2:\nThe central server sends the global model gt−1 to all the\nselected participants.\n3:\nfor all participants i ∈ S in parallel do\n4:\nht\ni ← gt−1;\n5:\nThe adversary uses the data instances hosted by the malicious participants based on the model ht\ni to optimize the trigger ∆x∗,t by Eq. 6;\n6:\nPatch x, x ∈ Dmal\ni\nwith finely tuned backdoor trigger\n∆x∗,t;\n7:\nCalculate h∗,t\ni\nby optimizing Eq.1 with Dmal\ni\nand\nDnor\ni\n;\n8:\nSend the poisoned local model h∗,t\ni\nto the server.\n9:\nend for\n10: end for\nDatasets, triggers, models, and hyperparameters. We\nevaluate CerP on 3 large-scale benchmark datasets: the applications of image classification (CIFAR-100 (Krizhevsky,\nHinton et al. 2009) and Fashion-MNIST (Xiao, Rasul, and\nVollgraf 2017)), and the loan/credit risk assessment (LOAN\n(George 2020)). On each dataset, we adopt the setting of\nnon-IID data distribution. The datasets, hyperparameters,\nand model structures are summarized in Table 1.\nBaseline backdoor attacks. To organize a comparative\nstudy, the 3 distributed backdoor attacks (LIE (Baruch,\nBaruch, and Goldberg 2019), Sybil attack (Fung, Yoon, and\nBeschastnikh 2018), and DBA (Xie et al. 2020)) are involved.\nDefense methods. We study the attack performance under\nthe 13 defense methods: Trimmed mean (Yin et al. 2018),\nMedian (Yin et al. 2018), Krum (Blanchard et al. 2017),\nMKrum (Blanchard et al. 2017), Bulyan (Mhamdi, Guerraoui, and Rouault 2018), RFA (Pillutla, Kakade, and Harchaoui 2019), DnC (Shejwalkar and Houmansadr 2021),\nFLTrust (Cao et al. 2021), Foolsgold (Fung, Yoon, and\nBeschastnikh 2018), CRFL (Xie et al. 2021), FedCDP\n(Geyer, Klein, and Nabi 2017), FedLDP (Wei et al. 2021),\nand FL-WBC (Sun et al. 2021).\nAttack settings. To define a partial-participation FL setting, the global server selects 20 out of 100 participants\non CIFAR-100 and Fashion-MNIST and 80 participants on\nLOAN respectively for parameter aggregation in each federated training iteration. During the backdoor attack, in each\nattack iteration, we constraint no more than 4 participants\ninvolved in aggregation as malicious participants. We apply this setting to make our studied attack scenario compatible with the feasibility assumption of Byzantine-robust\nFL algorithms, in order to organize a fair comparison. These\ndefenses are assumed to work in the case where no more\nthan 50% or 25% of the participants are malicious. For the\nLOAN, CIFAR-100, and Fashion-MNIST datasets, the adversary starts to attack from the 10-th, 10-th, and 70-th training iterations of FL, respectively.\nDataset\nInstances\nFeatures\nModel\nBenign lr\n|S|\nN\nPoison lr\nPoison Ratio r\nα\nβ\nCIFAR-100\n60,000\n1024\nResnet-18\n0.01\n4\n100\n0.005\n5/64\n0.0001\n0.0001\nFashion-MNIST\n60,000\n784\n2 conv and 2 fc\n0.1\n4\n100\n0.05\n5/64\n0.0001\n0.0001\nLOAN\n887,380\n53\n3 fc\n0.001\n3\n80\n0.0005\n5/64\n0.0001\n0.0001\nTable 1: Dataset, model structure, and hyperparameter description.\nDefense\nAttack\nSybil\nDBA\nLIE\nCerP\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nKrum\n61.15\n89.03\n69.41\n0.07\n70.42\n0.98\n63.03\n90.76\nMKrum\n65.24\n88.08\n66.01\n88.17\n63.91\n2.57\n64.03\n99.34\nBulyan\n68.69\n87.48\n69.29\n5.59\n66.80\n37.67\n68.11\n99.74\nTrimmed mean\n66.05\n56.72\n64.78\n3.85\n69.77\n9.39\n66.84\n90.24\nMedian\n67.23\n60.25\n65.64\n2.49\n70.23\n10.36\n67.68\n92.25\nRFA\n70.60\n73.39\n70.34\n11.60\n70.86\n6.61\n70.53\n81.80\nFoolsgold\n69.28\n72.73\n69.98\n0.44\n69.97\n0.92\n69.29\n91.48\nFLTrust\n71.20\n77.32\n71.53\n63.55\n71.34\n5.52\n71.30\n94.58\nDnC\n62.64\n92.38\n62.89\n73.36\n70.24\n4.90\n66.19\n97.21\nFedLDP\n70.91\n73.01\n70.89\n73.50\n70.86\n8.94\n71.19\n93.67\nFedCDP\n70.35\n85.16\n70.36\n71.42\n70.14\n66.58\n70.68\n95.52\nCRFL\n70.71\n71.88\n71.14\n73.74\n70.74\n11.18\n70.59\n93.66\nFL-WBC+Median\n67.17\n60.10\n32.26\n0.44\n70.32\n10.31\n67.67\n92.13\nFL-WBC+Trim\n66.22\n56.58\n34.93\n0.72\n69.81\n7.37\n66.94\n89.72\nTable 2: ASR and ACC of different distributed backdoor attacks on CIFAR-100(%).\nEvaluation metrics. We involve two popularly used benchmarks ACC and ASR (Xie et al. 2020) to measure the attack\nperformance of backdoor attack methods. ASR denotes the\nattack success rates, measuring the classification accuracy\nof the derived poisoned global model on the poisoned testing data. In parallel, ACC measures the main task’s classification accuracy on the poisoned-free testing data.\nAttack Performance\nWe compare CerP with 3 state-of-the-art distributed backdoor attacks against 13 defense methods. We show the ACC\nand ASR values of all the backdoor attacks involved in the\ncomparison. We compare the ASR values of different backdoor attacks at the same ACC level. Due to the space limit,\nwe show the results on CIFAR-100 and LOAN in this section. In the following tables, we use the bolded fonts to highlight the highest ASR values obtained among all the attack\nmethods facing various defense methods. As seen in Tables\n2–3, CerP can achieve higher ASR values than other baseline\nattack methods given the same ACC level and against all the\ndeployed defense methods.\nLIE is proved to conceal the robust aggregation methods under the IID data assumption. However, with the nonIID data setting in our study, we can find that LIE cannot\nbypass most defense methods. The ASR of LIE varies significantly across different datasets. Moreover, LIE requires\nknowing the model parameters committed by benign participants, which violates the protocol of FL. The empirical results show that the lack of the context knowledge of benign\nparticipants brings a noticeable drop to the ASR value of\nLIE. For example, LIE can hardly bypass any defense methods except FedCDP on CIFAR-100. The results show the\nlimitations of LIE for general federated learning.\nThe decentralized nature of DBA facilitates defeating both\nthe robust aggregation methods and Foolsgold. However,\nDBA cannot bypass most defense methods on CIFAR-100\nand LOAN. The main reason is that DBA uses local triggers\nmanually separated from the global trigger. Manually splitting the global trigger into chunks can bring unexpected artifacts to the learning of the backdoor poisoned data, which\nleads to large deviations of the poisoned local models and\nmake it easy to be flagged by the Byzantine-robust FL methods. For Sybil attack, malicious participants submit similar local model updates, which makes it mitigated easily by\nFoolsgold. The ASR of Sybil attack against Foolsgold is less\nthan 1% on LOAN. In addition, Sybil attack does not consider suppressing the deviations between the malicious and\nbenign local models. Therefore, it is difficult for Sybil attack\nto bypass robust aggregation methods.\nAttack Effectiveness\nThe finely tuned trigger.\nAccording to the previous theoretical analysis, fine-tuning the trigger is dedicated to reshaping the backdoor trigger to suppress the trigger-induced\nlocal model biases and facilitate the learning of the poisoned data to achieve stealthy backdoor attacks. To illustrate the impact of fine-tuning triggers, we compare the attack performance of our proposed method using only the\noriginal triggers (denoted as CerP-NT) and using the finely\ntuned triggers (CerP). The results in Table 4 show a significant increase in attack performance by simply integrating\nthe trigger tuning module into the CerP attack, compared\nto the CerP-NT attack with the original triggers. For example, CerP can achieve ASR values higher than 90% against\nKrum and Bulyan on LOAN. On the contrary, CerP-NT cannot attack successfully, and the ASR values are only 0%.\nAblation study.\nTo better understand the parameter sensitivity of CerP, we alternately set one of α (CerP-ND) and β\nDefense\nAttack\nSybil\nDBA\nLIE\nCerP\nACC\nASR\nACC\nASR\nACC\nASR\nACC\nASR\nKrum\n90.99\n0\n90.83\n0\n90.78\n0\n91.16\n99.98\nMKrum\n91.17\n0\n91.17\n0\n91.10\n0\n91.16\n99.98\nBulyan\n91.15\n0\n91.12\n0\n91.06\n0\n91.11\n99.98\nTrimmed mean\n91.15\n0\n91.13\n0\n91.09\n0\n91.15\n99.98\nMedian\n91.18\n0\n91.16\n0\n91.10\n0\n91.19\n99.99\nRFA\n92.34\n100\n92.38\n100\n92.06\n98.97\n92.22\n100\nFoolsgold\n90.95\n0\n91.05\n0.85\n90.95\n0\n90.80\n99.97\nFLTrust\n92.37\n100\n92.37\n100\n92.12\n99.98\n92.31\n100\nDnC\n91.38\n99.99\n91.35\n99.98\n91.08\n0\n91.32\n100\nFedLDP\n92.30\n100\n92.53\n100\n92.05\n99.80\n92.26\n100\nFedCDP\n92.53\n100\n92.60\n100\n92.17\n99.98\n92.31\n100\nCRFL\n92.07\n100\n91.83\n100\n91.64\n99.79\n91.66\n100\nFL-WBC+Median\n91.19\n0\n91.16\n0\n91.10\n0\n91.14\n99.99\nFL-WBC+Trim\n91.15\n0\n91.13\n0\n91.10\n0\n91.11\n99.98\nTable 3: ASR and ACC of different distributed backdoor attacks on LOAN(%).\nAttack\nDefense\nKrum\nMKrum\nBulyan\nTrim\nMedian\nFools\nFLtrust\nDnC\nFedLDP\nFL-WBC\n+Median\nFL-WBC\n+Trim\nFashion-MNIST\nCerP-NT\n64.67↓\n70.36↓\n73.01↓\n86.98↓\n95.90↓\n99.55↓\n99.47↓\n99.61↓\n96.72\n82.10↓\n87.04↓\nCerP-ND\n62.46↓\n90.75↓\n85.16↓\n90.67↓\n97.63\n99.83\n99.63↓\n99.72↓\n96.10↓\n91.43↓\n91.33↓\nCerP-NS\n94.75\n90.53↓\n85.75\n91.07↓\n97.63\n99.80↓\n99.65↓\n99.71↓\n96.16↓\n91.53↓\n91.37\nLOAN\nCerP-NT\n0↓\n0↓\n0↓\n0↓\n0↓\n0↓\n100\n99.99↓\n100\n0↓\n0↓\nCerP-ND\n99.98\n99.98\n99.98\n99.98\n99.99\n99.96↓\n100\n100\n100\n99.99\n99.98\nCerP-NS\n99.98\n99.99\n99.98\n99.98\n99.99\n99.96↓\n100\n100\n100\n99.99\n99.98\nTable 4: ASR of CerP-NT, CerP-NS, and CerP-ND.\n1\n3\n6\nRounds for local training\n0\n20\n40\n60\n80\n100\nAttack Success Rate\n(a) CIFAR-100 - I\nKrum\nMKrum\nTrim\nMedian\nBulyan\nRFA\n1\n3\n6\nRounds for local training\n0\n20\n40\n60\n80\n100\nAttack Success Rate\n(b) CIFAR-100 - II\nFoolsgold\nFLTrust\nDnC\nFedLDP\nFedCDP\nCRFL\n1\n3\n6\nRounds for local training\n0\n20\n40\n60\n80\n100\nAttack Success Rate\n(c) Fashion-MNIST - I\nKrum\nMKrum\nTrim\nMedian\nBulyan\nRFA\n1\n3\n6\nRounds for local training\n0\n20\n40\n60\n80\n100\nAttack Success Rate\n(d) Fashion-MNIST -II\nFoolsgold\nFLTrust\nDnC\nFedLDP\nFedCDP\nCRFL\nFigure 1: ASR of CerP on CIFAR-100 and Fashion-MNIST\nwith different number of the poison local training rounds.\n(CerP-NS) to 0, while keeping the other unchanged in Eq.1.\nParameter α controls the regularization strength over the\nmodel deviation. As seen in Table 4, the deviation module is\nimportant to bypass the Byzantine robust aggregation methods. Removing it induces a drop in the ASR values of our\nattack when exposed to multiple defense methods. Removing the regularization of similarity (β = 0) causes CerP’s\nASR against Foolsgold to drop.\nImpact of the poison local training rounds.\nFigure 1\nshows the ASR value changes of CerP on Fashion-MNIST\nand CIFAR-100 when the number of the poison local training rounds increases from 1 to 6. Intuitively, more poison local training rounds can lead to better attack performance. However, when the number of the poison local training rounds is too large, the attack performance of our attack\ndegrades. One possible reason is that too many poison local\ntraining rounds make the malicious local models deviate too\nmuch from the benign local models, resulting in the malicious local model being identified as an abnormal model by\nthe defense methods.",
        "conclusion": "In this work, we establish theoretical and empirical studies\non the feasibility of organizing stealthy yet effective backdoor attacks on FL against defense methods. Our study explicitly unveils the key factors deciding the magnitude of\nmalicious local model biases in general federated learning\ntasks. Instantiating the theoretical discussion, we propose\na unified and highly flexible optimization framework Cerberus Poisoning (CerP) to coordinate effective backdoor attacks even with various defense methods deployed, which\nconducts the fine-tuning of the backdoor triggers and regularizes the trigger-induced local model bias. Substantial experimental results show that the CerP attack demonstrates\nan effective and stealthy backdoor poisoning threat to FL.",
        "summary_en": "Are Federated Learning (FL) systems free from backdoor poisoning with the arsenal of various defense strategies deployed? This is an intriguing problem with significant practical implications regarding the utility of FL services. Despite the recent flourish of poisoning-resilient FL methods, this paper shows that carefully tuning the collusion between malicious participants can minimize the trigger-induced bias of the poisoned local model from the poison-free one, which plays the key role in delivering stealthy backdoor attacks and circumventing a wide spectrum of state-of-the-art defense methods in FL. This paper instantiate the attack strategy by proposing a distributed backdoor attack method, namely Cerberus Poisoning (CerP). It jointly tunes the backdoor trigger and controls the poisoned model changes on each malicious participant to achieve a stealthy yet successful backdoor attack against a wide spectrum of defensive mechanisms of federated learning techniques. The paper extensive studies on 3 large-scale benchmark datasets and 13 mainstream defensive mechanisms confirms that Cerberus Poisoning raises a significantly severe threat to the integrity and security of federated learning practices, regardless of the flourish of robust Federated Learning methods.",
        "summary_zh": "这篇论文研究了针对联邦学习的一种后门攻击方法，名为“Cerberus Poisoning”（CerP）。尽管近年来出现了许多抵御中毒攻击的联邦学习方法，但作者的研究表明，通过精心调整恶意参与者之间的勾结，可以最小化受污染的本地模型与无污染模型之间由触发器引发的偏差，从而实现隐秘的后门攻击，并绕过联邦学习中一系列最新的防御方法。他们的方法在三个大型基准数据集和13种主流防御机制上进行了广泛研究，结果显示Cerberus Poisoning对联邦学习实践的完整性和安全性构成了显著严重的威胁，无论是哪种鲁棒的联邦学习方法都无法阻止。"
    },
    {
        "title": "Heuristic Search for Multi-Objective Probabilistic Planning",
        "abstract": "Heuristic search is a powerful approach that has successfully been applied to a broad class of planning problems, including classical planning, multi-objective planning, and probabilistic planning modelled as a stochastic shortest path (SSP) problem. Here, we extend the reach of heuristic search to a more expressive class of problems, namely multi-objective stochastic shortest paths (MOSSPs), which require computing a coverage set of non-dominated policies. We design new heuristic search algorithms MOLAO* and MOLRTDP, which extend well-known SSP algorithms to the multi-objective case. We further construct a spectrum of domain-independent heuristic functions differing in their ability to take into account the stochastic and multi-objective features of the problem to guide the search. Our experiments demonstrate the beneﬁts of these algorithms and the relative merits of the heuristics.",
        "introduction": "Stochastic shortest path problems (SSPs) are the de facto\nmodel for planning under uncertainty. Solving an SSP involves computing a policy which maps states to actions so\nas to minimise the expected (scalar) cost to reach the goal\nfrom a given initial state. Multi-objective stochastic shortest path problems (MOSSPs) are a useful generalisation of\nSSPs where multiple objectives (e.g. time, fuel, risk) need to\nbe optimised without the user being able to a priori weigh\nthese objectives against each other (Roijers and Whiteson\n2017). In this more complex case, we now aim to compute\na set of non-dominated policies whose vector-valued costs\nrepresent all the possible trade-offs between the objectives.\nThere exist two approaches to solving MOMDPs (Roijers and Whiteson 2017), a special case of MOSSPs: inner\nloop planning which consists in extending SSP solvers by\ngeneralising single objective operators to the multi-objective\n(MO) case, and outer loop planning which consists in solving a scalarised MOSSP multiple times with an SSP solver.\nWe focus on the former. The canonical inner loop method,\nMO Value Iteration (MOVI) (White 1982) and its variants\n(Wiering and de Jong 2007; Barrett and Narayanan 2008;\nRoijers and Whiteson 2017), require enumerating the entire\nstate space of the problem and are unable to scale to the huge\nstate spaces typically found in automated planning.\nTherefore, this paper focuses on heuristic search methods which explore only a small fraction of the state space\nwhen guided with informative heuristics. Heuristic search\nhas been successfully applied to a range of optimal planning settings, including single objective (SO) or constrained\nSSPs (Hansen and Zilberstein 2001; Bonet and Geffner\n2003; Trevizan et al. 2017), and MO deterministic planning (Mandow and P´erez-de-la-Cruz 2010; Khouadjia et al.\n2013; Ulloa et al. 2020). Moreover, a technical report by\nBryce et al. (2007) advocated the need for heuristic search\nand outlined an extension of LAO* (Hansen and Zilberstein\n2001) for ﬁnite horizon problems involving multiple objectives and partial observability. Convex Hull Monte-Carlo\nTree-Search (Painter, Lacerda, and Hawes 2020) extends the\nTrial-Based Heuristic Tree Search framework (Keller and\nHelmert 2013) to the MO setting but applies only to ﬁnitehorizon MOMDPs. Yet to the best of our knowledge there is\nno investigation of heuristic search for MOSSPs.\nThis paper ﬁlls this gap in heuristic search for MOSSPs.\nFirst, we characterise the necessary conditions for MOVI to\nconverge in the general case of MOSSPs. We then extend the\nwell-known SSP heuristic search algorithms LAO* (Hansen\nand Zilberstein 2001) and LRTDP (Bonet and Geffner 2003)\nto the multi-objective case, leading to two new MOSSP algorithms, MOLAO* and MOLRTDP, which we describe\nalong with sufﬁcient conditions for their convergence. We\nalso consider the problem of guiding the search of these algorithms with domain-independent heuristics. A plethora of\ndomain-independent heuristics exist for classical planning,\nbut works on constructing heuristics for (single objective)\nprobabilistic or multi-objective (deterministic) planning are\nmuch more recent (Trevizan, Thi´ebaux, and Haslum 2017;\nKl¨oßner and Hoffmann 2021; Geißer et al. 2022). Building\non these recent works, we investigate a spectrum of heuristics for MOSSPs differing in their ability to account for the\nprobabilistic and multi-objective features of the problem.\nFinally, we conduct an experimental comparison of these\nalgorithms and of the guidance obtained via these heuristics.\nWe observe the superiority of heuristic search over value iteration methods for MOSSPs, and of heuristics that are able\nto account for the tradeoffs between competing objectives.",
        "background": "A multi-objective stochastic shortest path problem (MOSSP)\nis a tuple (S, s0, G, A, P, ⃗C) where: S is a ﬁnite set of states,\none of which is the initial state s0, G ⊆ S is a set of\ngoal states, A is a ﬁnite set of actions, P(s′ | s, a) is the\nprobability of reaching s′ after applying action a in s, and\n⃗C(a) ∈ Rn\n≥0 is the n-dimensional vector representing the\ncost of action a. Two special cases of MOSSPs are stochastic\nshortest path problems (SSPs) and bi-objective SSPs which\nare obtained when n equals 1 and 2, respectively.\nA solution for an SSP is a deterministic policy π, i.e.,\na mapping from states to actions. A policy π is proper or\nclosed w.r.t. s0 if the probability of reaching G when following π from s0 is 1; if this probability of reaching G is less\nthan 1, then π is an improper policy. We denote by Sπ ⊆ S\nthe set of states visited when following a policy π from s0.\nThe expected cost of reaching G when using a proper policy\nπ from a state s ∈ Sπ is given by the policy value function\ndeﬁned as\nV π(s) = C(π(s)) +\nX\ns′∈S\nP(s′|s, π(s))V π(s′)\n(1)\nfor s ∈ Sπ \\G and V π(g) = 0 for g ∈ G. An optimal policy\nfor an SSP is any proper policy π∗ such that V π∗(s0) ≤\nV π′(s0) for all proper policies π′. Although π∗ might not be\nunique, the optimal value function V ∗ is unique and equals\nV π∗ for any π∗.\nStochastic policies are a generalisation of deterministic\npolicies which map states to probability distributions of actions. The deﬁnitions of proper, improper and policy value\nfunction are trivially generalised to stochastic policies. A\nkey result for SSPs is that at least one of its optimal policies\nis deterministic (Bertsekas and Tsitsiklis 1991); thus it sufﬁces to search for deterministic policies when solving SSPs.\nCoverage Sets and Solutions for MOSSPs\nIn the context of MOSSPs, given a policy π, we denote by\n⃗V π : S → Rn\n≥0 the vector value function for π. The function\n⃗V π is computed by replacing V and C by ⃗V and ⃗C in (1),\nrespectively. In order to deﬁne the solution of an MOSSP, we\nneed to ﬁrst deﬁne how to compare two different vectors: a\ncost vector ⃗v dominates ⃗u, denoted as ⃗v ⪯ ⃗u, if ⃗vi ≤ ⃗ui for\ni = 1, . . . , n. A coverage set for a set of vectors V, denoted\nas CS(V), is any set satisfying ∀⃗v ∈ CS(V), ∄⃗u ∈ CS(V)\ns.t. ⃗u ⪯ ⃗v and ⃗u ̸= ⃗v.1 An example of a coverage set is\nthe Pareto coverage set (PCS) which is the largest possible\ncoverage set. For the remainder of the paper we focus on the\nconvex coverage set (CCS) (Barrett and Narayanan 2008;\nRoijers and Whiteson 2017) which is deﬁned as the convex\nhull of the PCS. Details for computing the CCS of a set\nV with a linear program (LP) can be found in (Roijers and\nWhiteson 2017, Sec. 4.1.3). We say that a set of vectors U\ndominates another set of vectors V, denoted by U ⪯ V, if\nfor all ⃗v ∈ V there exists ⃗u ∈ U such that ⃗u ⪯ ⃗v.\n1We will denote sets of vectors or functions which map to sets\nof vectors with bold face, e.g. V, and single vectors or functions\nwhich map to single vectors with vector notation, e.g. ⃗V .\ns0\ng1\ng2\na1\na2\n0.5\n0.5\n0.5\n0.5\nFigure 1: A MOSSP with action costs given by ⃗C(a1) =\n[1, 0] and ⃗C(a2) = [0, 1].\nGiven an MOSSP deﬁne the optimal value function V∗\nby V∗(s) = CCS({⃗V π(s) | π is a proper policy}). Then\nwe deﬁne a solution to the MOSSP to be any set of proper\npolicies Π such that the function ϕ : Π → V∗(s0) with\nϕ(π) = ⃗V π(s0) is a bijection. By choosing CCS as our\ncoverage set operator, we may focus our attention to only\nnon-dominated deterministic policies, where non-dominated\nstochastic policies are implicitly given by the points on the\nsurface of the polyhedron drawn out by the CCS. In this\nway, we avoid having to explicitly compute inﬁnitely many\nnon-dominated stochastic policies.\nTo illustrate this statement, consider the MOSSP in\nFig. 1 with actions a1 and a2 where P(s0|s0, a1)\n=\nP(g1|s0, a1) = P(s0|s0, a2) = P(g2|s0, a2) = 0.5 and\n⃗C(a1) = [1, 0] and ⃗C(a2) = [0, 1]. One solution consists of only two deterministic policies π1(s0) = a1 and\nπ2(s0) = a2 with corresponding expected costs [2, 0] and\n[0, 2]. Notice that there are uncountably many stochastic\npolicies obtained by the convex combinations of π1 and π2,\ni.e., πt(a1|s0) = 1 − t, πt(a2|s0) = t for t ∈ [0, 1]. The\nexpected cost of each πt is [2 − 2t, 2t] and these stochastic\npolicies do not dominate each other. Therefore, if the PCS\nis used instead of the CCS in the deﬁnition of optimal value\nfunction, then V∗(s0) would be {[2 − 2t, 2t] | t ∈ [0, 1]}\nand the corresponding solution would be {πt | t ∈ [0, 1]}.\nThe CCS allows us to compactly represent the potentially\ninﬁnite PCS by storing only the deterministic policies: in\nthis example, the actual solution is {π1, π2} and the optimal\nvalue function is V∗(s0) = {[2, 0], [0, 2]}.\nValue Iteration for SSPs\nWe ﬁnish this section by reviewing how to compute the optimal value function V ∗ for SSPs and extend these results\nto MOSSPs in the next section. The optimal (scalar) value\nfunction V ∗ for an SSP can be computed using the Value\nIteration (VI) algorithm (Bertsekas and Tsitsiklis 1996):\ngiven an initial value function V 0 it computes the sequence\nV 1, . . . , V k where V t+1 is obtained by applying a Bellman\nbackup, that is V t+1(g) = 0 if g ∈ G and, for s ∈ S \\ G,\nV t+1(s) = min\na∈A Qt+1(s, a)\n(2)\nQt+1(s, a) = C(a) +\nX\ns′∈S\nP(s′|s, a)V t(s′).\nVI guarantees that V k converges to V ∗ as k → ∞ under\nthe following conditions: (i) for all s ∈ S, there exists a\nproper policy w.r.t. s; and (ii) all improper policies have\ninﬁnite cost for at least one state. The former condition is\nknown as the reachability assumption and it is equivalent\nAlgorithm 1: MOVI\nData: MOSSP problem P = (S, s0, G, A, P, ⃗C),\ninitial values V(s) for each state s (default to\nV(s) = {⃗0}), and consistency threshold ε.\n1 while maxs∈S res(s) < ε do\n2\nfor s ∈ S do\n3\nif s ∈ G then Vnew(s) ← {⃗0} ;\n4\nelse Vnew(s) ← BellmanBackup(s) ;\n5\nres(s) ← D(V, Vnew)\n6\nV ← Vnew\n7 return V\nto requiring that no dead ends exist, while the latter condition can be seen as preventing cycles with 0 cost. Notice\nthat ﬁnite-horizon Markov Decision Processes (MDPs) and\ninﬁnite-horizon discounted MDPs are special cases of SSPs\nin which all policies are guaranteed to be proper (Bertsekas\nand Tsitsiklis 1996).",
        "value iteration for mossps": "Value Iteration has been extended to the MO setting\nin special cases of SSPs, e.g. inﬁnite-horizon discounted\nMDPs (White 1982). In this section, we present the MO version of Value Iteration for the general MOSSP case. While\nthe changes in the algorithm are minimal, the key contribution of this section is the generalisation of the assumptions\nneeded for convergence. We start by generalising the Bellman backup operation from the SO, i.e. (2), to the MO case.\nFor s ∈ S \\ G we have\nVt+1(s) = CS\n\u0010 [\na∈A\nQt+1(s, a)\n\u0011\n(3)\nQt+1(s, a) = {⃗C(a)} ⊕\n\u0010 M\ns′∈S\nP(s′|s, a)Vt(s′)\n\u0011\n,\nand Vt+1(g) = {⃗0} for g ∈ G where ⊕ denotes the sum of\ntwo sets of vectors V and U deﬁned as {⃗u +⃗v | ⃗u ∈ U,⃗v ∈\nV}, and L is the generalised version of ⊕ to several sets.\nAlg. 1 illustrates the MO version of Value Iteration\n(MOVI) which is very similar to the single-objective VI algorithm with the notable difference that the convergence criterion is generalised to handle sets of vectors. The Hausdorff\ndistance between two sets of vectors U and V is given by\nD(U, V) = max\n\b\nmax\n⃗u∈U min\n⃗v∈V d(⃗u,⃗v), max\n⃗u∈V min\n⃗v∈U d(⃗u,⃗v)\n\t\nfor some choice of metric d such as the Euclidean metric.\nWe use this distance to deﬁne residuals in line 5. As with\nVI, MOVI converges to the optimal value function at the\nlimit (White 1982; Barrett and Narayanan 2008) under certain strong assumptions presented in the next section. We\ncan extract policies with the choice of a scalarising weight\n⃗w from the value function (Barrett and Narayanan 2008).\nAssumptions for the Convergence of MOVI\nSimilarly to the SO version of VI, MOVI requires that the\nreachability assumption holds; however, the assumption that\nAlgorithm 2: MOVI under Assumption 1\nData: MOSSP problem P = (S, s0, G, A, P, ⃗C),\ninitial values V(s) for each state s (default to\nV(s) = {⃗0}), consistency threshold ε and\nupper bound⃗b.\n1 while maxs∈S res(s) < ε do\n2\nfor s ∈ S do\n3\nif s ∈ G then Vnew(s) ← {⃗0} ;\n4\nelse Vnew(s) ← BellmanBackupB(s) ;\n5\nres(s) ← D(V, Vnew)\n6\nV ← Vnew\n7 for s ∈ S do V(s) = V(s) \\ {⃗b} ;\n8 return V\nimproper policies have inﬁnite cost needs to be generalised\nto the MO case. One option is to require that all improper\npolicies cost ⃗∞ which we call the strong improper policy assumption and, if it holds with the reachability assumption, then MOVI (Alg. 1) is sound and complete for\nMOSSPs. The strong assumption is very restrictive because\nit implies that any cycle in an MOSSP must have a cost\ngreater than zero in all dimensions; however, it is common\nfor MOSSPs to have zero-cost cycles in one or more dimensions. For instance, in navigation domains where some actions such as wait, load and unload do not consume fuel and\ncan be used to generate zero-fuel-cost loops.\nAnother possible generalisation for the cost of improper\npolicies is that they cost inﬁnity in at least one dimension.\nThis requirement is not enough as illustrated by the (deterministic) MOSSP in Fig. 2. The only deterministic proper\npolicy is π(s0) = ag with cost [0, 1], and the other deterministic policy is π′(s0) = a1, π′(s1) = a2 which is improper\nand costs [∞, 0]. Since neither [0, 1] or [∞, 0] dominate each\nother, two issues arise: MOVI tries to converge to inﬁnity\nand thus never terminates; and even if it did, the cost of an\nimproper policy would be wrongly added to the CCS.\nThese issues stem from VI and its generalisations not explicitly pruning improper policies. Instead they rely on the\nassumption that improper policies have inﬁnite cost to implicitly prune them. This approach is enough in the SO case\nbecause any proper policy dominates all improper policies\nsince domination is reduced to the ‘≤’ operator; however,\nas shown in this example, the implicit pruning approach for\nimproper policies is not correct for the MO case.\nWe address these issues by providing a version of MOVI\nthat explicitly prunes improper policies and to do so we need\na new assumption:\ns0\ns1\ng\na2\na1\nag\nFigure 2: An MOSSP with action costs given by ⃗C(a1) =\n[1, 0], ⃗C(a2) = [1, 0], ⃗C(ag) = [0, 1].\nAssumption 1 (Weak Improper Policy Assumption). There\nexists a vector ⃗b ∈ Rn\n≥0 such that for every improper policy\nπ and s ∈ S, ⃗V π(s) ̸⪯ ⃗b, and for every proper policy π and\ns ∈ S, ⃗V π(s) ⪯ ⃗b and ⃗V π(s) ̸= ⃗b.\nThe weak assumption allows improper policies to have\nﬁnite cost in some but not all dimensions at the expense of\nknowing an upper bound⃗b on the cost of all proper policies.\nThis upper bound ⃗b lets us infer that a policy π is improper\nwhenever ⃗V π(s) ̸⪯ ⃗b, i.e., that ⃗V π(s) is greater than ⃗b in at\nleast one dimension.\nAlg. 2 outlines the new MOVI where the differences can\nbe found in lines 4 and 7. In line 4, we use a modiﬁed Bellman backup which detects vectors associated to improper\npolicies and assigns them the upper bound ⃗b given by the\nweak improper policy assumption. The modiﬁed backup is\ngiven by\nVt+1(s) = CSB\n\u0012 [\na∈A\nQt+1\nB (s, a)\n\u0013\n(4)\nQt+1\nB (s, a) = {⃗C(a)}⊕B\n\u0010M\ns′∈S\nBP(s′|s, a) ⋆BVt(s′)\n\u0011\nwhere ⋆B and ⊕B are generalisations to sets of vectors of\nc ⋆B ⃗v=\n(\n⃗b\nif ⃗v ̸⪯ ⃗b\nc⃗v else\n⃗u ⊕B ⃗v=\n(\n⃗b\nif ⃗u ⊕ ⃗v ̸⪯ ⃗b\n⃗u ⊕ ⃗v else.\nAlso deﬁne CSB to be the modiﬁed coverage set operator which does not prune away ⃗b if it is in the input set.\nThe modiﬁed backup (4) enforces a cap on value functions\n(i.e., Vt(s) ⪯ {⃗b}) through the operator ⊕B. This guarantees that a ﬁnite ﬁxed point exists for all ⃗V π(s) and, as a\nresult, that Alg. 2 terminates. Once the algorithm converges,\nwe need to explicitly prune the expected cost of improper\npolicies which is done in line 7. By Assumption 1, we have\nthat no proper policy costs ⃗b thus we can safely remove ⃗b\nfrom the solution. Note that the overhead in applying this\nmodiﬁed backup and post-processing pruning is negligible.\nTheorem 3.1 shows that MOVI converges to the correct solution.\nTheorem 3.1. Given an MOSSP in which the reachability\nand weak improper policy assumptions hold for an upper\nbound⃗b, and given a set of vectors V0 such that V0 ⪯ V∗,\nthe sequence V1, . . . , Vk computed by MOVI converges to\nthe MOSSP solution V∗ as k → ∞.\nProof sketch. By the assumption that V0 ⪯ V∗, we have\nthat MOVI will not prune away vectors associated with\nproper policies which contribute to a solution. If V0 ̸⪯ V∗,\ne.g., V0(s) = {⃗b} for all s, then Alg. 2 is not guaranteed\nto ﬁnd the optimal value function since it will incorrectly\nprune proper policies. Otherwise, we have that V1, . . . , Vk\nconverges to V† = V∗ ∪ {⃗b} if the original MOVI in Alg.\n1 does not converge, and V† = V∗ otherwise. For example, V∗ = {[0, 1]} in the example seen in Fig. 2 but\nV† = {[2, 2], [0, 1]} if we choose⃗b = [2, 2].\nThis convergence result follows by noticing that, by definition of the modiﬁed backup in (4), every vector in Vt(s)\nfor all t dominates ⃗b. We may then apply the proof for the\nconvergence of MOVI with convex coverage sets by Barrett and Narayanan (2008) which reduces to the convergence\nof scalarised SSPs using VI in the limit, of which there are\nﬁnitely many since the number of deterministic policies is\nﬁnite. Here, we have for every ⃗w that ⃗w ·⃗b is an upper bound\nfor the problem scalarised by ⃗w. Finally, we have that line 7\nremoves⃗b from V† such that we correctly return V∗.\nNote that the original proof for MOMDPs by Barrett and\nNarayanan (2008) does not directly work for MOSSPs as\nsome of the scalarised SSPs have a solution of inﬁnite expected cost such that VI never converges. The upper bound\n⃗b we introduce solves this issue as achieving this bound is the\nsame as detecting improper policies. The proof remains correct in the original setting applied to discounted MOMDPs\nin which there are no improper policies.\nRelaxing the Reachability Assumption\nThe reachability assumption can also be relaxed by\ntransforming an MOSSP with dead ends into a new\nMOSSP without dead ends. Formally, given an MOSSP\n(S, s0, G, A, P, ⃗C) with dead ends, let A′ = A ∪ {give-up};\nP(sg|give-up, s) = 1 for all s ∈ S and any sg ∈ G;\n⃗C′(a) = [ ⃗C(a) : 0] for all a ∈ A; and ⃗C′(give-up) = [⃗0 : 1].\nThen the MOSSP (S, s0, G, A′, P, ⃗C′) does not have dead\nends (i.e., the reachability assumption holds) since the giveup action is applicable in all states s ∈ S. This transformation is similar to the ﬁnite-penalty transformation for\nSSPs (Trevizan, Teichteil-K¨onigsbuch, and Thi´ebaux 2017);\nhowever it does not require a large penalty constant. Instead,\nthe MOSSP transformation uses an extra cost dimension to\nencode the cost of giving up and the value of the n+1-th dimension of ⃗C′(give-up) is irrelevant as long as it is greater\nthan 0. Since the give-up action can only be applied once,\ndeﬁning the cost of give-up as 1 let us interpret the n+1th dimension of any ⃗V π(s) as the probability of giving up\nwhen following π from s.",
        "heuristic search": "Value iteration gives us one method for solving MOSSPs\nbut is limited by the fact that it requires enumerating over the\nwhole state space. This is impractical for planning problems,\nas their state space grows exponentially with the size of\nthe problem encoding. This motivates the need for heuristic\nsearch algorithms in the vein of LAO∗ and LRTDP for SSPs\n(Hansen and Zilberstein 2001; Bonet and Geffner 2003),\nwhich perform backups on a promising subset of states at\neach iteration. To guide the choice of the states to expand\nand explore at each iteration, they use heuristic estimates of\nstate values initialised when the state is ﬁrst encountered.\nIn this section, we extend the concept of heuristics to the\nmore general MOSSP setting, discuss a range of ways these\nheuristics can be constructed, and provide multi-objective\nversions of LAO∗ and LRTDP.\nHeuristics\nFor the remainder of the paper, we will call a set of vectors\na value. Thus, a heuristic value for a state is a set of vectors\nH(s) ⊂ Rn\n≥0. We implicitly assume that any heuristic value\nH(s) has already been pruned with some coverage set operator. The deﬁnition of an admissible heuristic for MOSSPs\nshould be at most as strong as the deﬁnition for deterministic\nMO search (Mandow and P´erez-de-la-Cruz 2010).\nDeﬁnition 4.1. A heuristic H for an MOSSP is admissible\nif ∀s ∈ S \\G, H(s) ⪯ V∗(s) where V∗ is the optimal value\nfunction, and ∀g ∈ G, H(g) = {⃗0}.\nFor example, if for some MOSSP we have V∗(s) =\n{[0, 2]} for a state s, then H(s) = {[0, 1], [3, 0]} and\nH(s′) = {⃗0} for all other s′ is an admissible heuristic. As in\nthe single-objective case, admissible heuristics ensure that\nthe algorithms below converge to near optimal value functions for ε > 0 with ﬁnitely many iterations and to optimal value functions with possibly inﬁnitely many iterations\nwhen ε = 0.\nDeﬁnition 4.2. A heuristic H for an MOSSP is consistent\nif we have for all s ∈ S, a ∈ A that H(s) ⪯ { ⃗C(a)} ⊕\n(L\ns′∈S P(s′|s, a)H(s′)).\nThe deﬁnition of consistent heuristic is derived and generalised from the deﬁnition of consistent heuristics for deterministic search: h(s) ≤ c(s, a, s′) + h(s′). The main idea is\nthat assuming non-negative costs, state costs increase monotonically which results in no re-expansions of nodes in A∗\nsearch. Similarly, we desire monotonically increasing value\nfunctions for faster convergence.\nWe now turn to the question of constructing domainindependent heuristics satisfying these properties from the\nencoding of a planning domain. This question has only recently been addressed in the deterministic multi-objective\nplanning setting (Geißer et al. 2022): with the exception\nof this latter work, MO heuristic search algorithms have\nbeen evaluated using “ideal-point” heuristics, which apply\na single-objective heuristic hi to each objective in isolation,\nresulting in a single vector Hideal(s) = {[h1(s), . . . hn(s)]}.\nMoreover, informative domain-independent heuristics for\nsingle objective SSPs are also relatively new (Trevizan,\nThi´ebaux, and Haslum 2017; Kl¨oßner and Hoffmann 2021):\na common practice was to resort to classical planning heuristics obtained after determinisation of the SSP (Jimenez,\nColes, and Smith 2006).\nWe consider a spectrum of options when constructing\ndomain-independent heuristics for MOSSPs, which we instantiate using the most promising families of heuristics\nidentiﬁed in (Geißer et al. 2022) for the deterministic case:\ncritical paths and abstraction heuristics. All heuristics are\nconsistent and admissible unless otherwise mentioned.\n• The baseline option is to ignore both the MO and stochastic aspects of the problem, and resort to an ideal-point\nheuristic constructed from the determinised problem.\nIn our experiments below, we apply the classical hmax\nheuristic (Bonet and Geffner 2001) to each objective and\ncall this Hmax\nideal.\n• A more elaborate option is to consider only the stochastic aspects of the problem, resulting in an ideal-point\nSSP heuristic. In our experiments, we apply the recent\nSSP canonical PDB abstraction heuristic by Kl¨oßner and\nHoffmann (2021) to each objective which we call Hpdb2\nideal\nand Hpdb3\nideal for patterns of size 2 and 3, respectively.\n• Alternatively, one might consider only the multiobjective aspects, by applying some of the MO deterministic heuristics (Geißer et al. 2022) to the determinised SSP. The heuristics we consider in our experiments are the MO extension of hmax and canonical PDBs:\nHcomax\nmo\n, Hpdb2\nmo , and Hpdb3\nmo . These extend classical planning heuristics by using MO deterministic search to solve\nsubproblems and combining solutions by taking the maximum of two sets of vectors with the admissibility preserving operator comax (Geißer et al. 2022).\n• The best option is to combine the power of SSP and MO\nheuristics. We do so with a novel heuristic Hpdb\nmossp which\nextends the SSP PDB abstraction heuristic (Kl¨oßner and\nHoffmann 2021) to the MO case by using an MO extension of topological VI (TVI) (Dai et al. 2014) to solve\neach projection and the comax operator to combine the\nresults. Our experiments use Hpdb2\nmossp and Hpdb3\nmossp.\n(i)MOLAO∗\nReaders familiar with the LAO∗ heuristic search algorithm\nand the multi-objective backup can convince themselves that\nan extension of LAO∗ to the multi-objective case can be\nobtained by replacing the backup operator with the multiobjective version. This idea was ﬁrst outlined by Bryce,\nCushing, and Kambhampati (2007) for ﬁnite-horizon MDPs\nwhich is a special case of SSPs without improper policies. In\nthe same vein as (Hansen and Zilberstein 2001), we provide\nMOLAO∗ alongside an ‘improved’ version, iMOLAO∗, in\nAlg. 3 and 4 respectively.\nMOLAO∗ begins in line 1 by lazily assigning an initial\nvalue function V to each state with the heuristic function, as\nopposed to explicitly initialising all initial values at once. Π\nis a dictionary representing our partial solution which maps\nstates to sets of optimal actions corresponding to their current value function. The main while loop of the algorithm\nterminates once there are no nongoal states on the frontier\nas described in line 2, at which point we have achieved a set\nof closed policies.\nThe loop begins with lines 3-5 by removing a nongoal\nstate s from the frontier representing a state on the boundary\nof the partial solution graph, and adding it to the interior set\nI. The nodes of the partial solution graph are partial solution\nstates, and the edges are the probabilistic transitions under\nall partial solution actions. Next in line 6 we add the set of\nyet unexplored successors of s to the frontier: any state s′ ∈\nS \\ I such that ∃a ∈ A, P(s′|s, a) > 0. Then we extract\nthe set Z of all ancestor states of s in the partial solution Π\nusing graph search in line 7. We run MOVI to ε-consistency\non the MOSSP problem restricted to the set of states Z and\nupdate the value functions for the corresponding states in\nline 8. The partial solution is updated in line 9 by extracting\nAlgorithm 3: MOLAO∗\nData: MOSSP problem P = (S, s0, G, A, P, ⃗C),\nheuristic H, and consistency threshold ε\n1 V ← H; Π ← ∅; F ← {s0} ; I ← ∅; N ← {s0}\n2 while (F ∩ N) \\ G ̸= ∅ do\n3\ns ← any element from (F ∩ N) \\ G\n4\nF ← F \\ {s}\n5\nI ← I ∪ {s}\n6\nF ← F ∪ (successors(s) \\ I)\n7\nZ ← ancestorStates(s, Π)\n8\nV|Z ← MOVI(P|Z, V|Z, ε)\n9\nfor s ∈ Z do Π(s) ← getActions(s, V) ;\n10\nN ← solutionGraph(sI, Π)\n11 return V\nAlgorithm 4: IMOLAO∗\nData: MOSSP problem P = (S, s0, G, A, P, ⃗C),\nheuristic H, and consistency threshold ε\n1 V ← H; Π ← ∅; F ← {s0} ; I ← ∅; N ← {s0}\n2 while ((F ∩ N) \\ G ̸= ∅) ∧ (maxs∈N res(s) < ε) do\n3\nF = ∅\n4\nN ← postorderTraversalDFS(sI, Π)\n5\nfor s ∈ N in the computed order do\n6\nV(s) ← BellmanBackup(s)\n7\nΠ(s) = getActions(s, V)\n8\nif s /∈ I then F = F ∪ {s} ;\n9\nI = I ∪ {s}\n10 return V\nthe actions corresponding to the value function with\ngetActions(s, V) = {a ∈ A | Q(s, a) ∩ V(s) ̸= ∅} .\nThis function can be seen as a generalisation of arg min to\nthe MO case where here we select the actions whose Q value\nat s contribute to the current value V(s). Next, we extract\nthe set of states N corresponding to all states reachable from\ns0 by the partial solution Π in line 10.\nThe completeness and correctness of MOLAO∗ follows\nfrom the same reasoning as LAO∗ extended to the MO case.\nSpeciﬁcally, we have that given an admissible heuristic the\nalgorithm achieves ε-consistency upon termination.\nOne potential issue with MOLAO∗ is that we may waste a\nlot of backups while running MOVI to convergence several\ntimes on partial solution graphs which do not end up contributing to the ﬁnal solution. The original authors of LAO∗\nproposed the iLAO∗ algorithm to deal with this. We provide\nthe MO extension, namely iMOLAO∗. The main idea with\niMOLAO∗ is that we only run one set of backups every time\nwe (re-)expand a state instead of running VI to convergence\nin the loop in lines 5 to 9. Backups are also performed asynchronously using DFS postorder traversal of the states in the\npartial solution graph computed in line 4, allowing for faster\nconvergence times.\nTo summarise, the two main changes required to extend\nAlgorithm 5: MOLRTDP\nData: MOSSP problem P = (S, s0, G, A, P, ⃗C),\nheuristic H, and consistency threshold ε\nprocedure MOLRTDP(P, ε, H)\n1\nV ← H\n2\nwhile ¬s0.solved do\n3\nvisited ← ∅\n4\ns ← s0\n5\nwhile ¬s.solved do\n6\nvisited.push(s)\n7\nif s ∈ G then break ;\n8\nV(s) ← BellmanBackup(s)\n9\na ← sampleUnsolvedGreedyAction(s)\n10\ns ← sampleUnsolvedNextState(s, a)\n11\nwhile ¬visited.empty() do\n12\ns ← visited.pop()\n13\nif ¬checkSolved(s) then break ;\n14\nreturn V\nroutine checkSolved(s)\n1\nrv ← true; open ← ∅; closed ← ∅\n2\nif ¬s.solved then open.push(s) ;\n3\nwhile ¬open.empty() do\n4\ns ← open.pop()\n5\nif res(s) > ε then\n6\nrv ← false\n7\ncontinue\n8\nfor a ∈ getActions(s, V) do\n9\nfor s′ ∈ successors(s, a) do\n10\nif ¬s′.solved ∧ s′ /∈ open ∪ closed\nthen open.push(s′) ;\n11\nif rv then for s ∈ closed do s.solved = true ;\n12\nelse\n13\nwhile closed ̸= ∅ do\n14\ns ← closed.pop()\n15\nV(s) ← BellmanBackup(s)\n16\nreturn rv\n(i)LAO∗ to the MO case are (1) replacing the backup operator with the MO version, and (2) storing possibly more than\none greedy action at each state corresponding to incomparable vector Q-values, resulting in a larger set of successors\nassociated to each state. These ideas can be applied to other\nasynchronous VI methods such as Prioritised VI (Wingate\nand Seppi 2005) and Topological VI (Dai et al. 2014).\nMOLRTDP\nLRTDP (Bonet and Geffner 2003) is another heuristic search\nalgorithm for solving SSPs. The idea of LRTDP is to perform random trials using greedy actions based on the current\nvalue function or heuristic for performing backups, and labelling states as solved when the consistency threshold has\nbeen reached in order to gradually narrow down the search\nspace and achieve a convergence criterion. A multi-objective\nextension of LRTDP is not as straightforward as extending\nthe backup operator given that each state has possibly more\nthan one greedy action to account for. The two main changes\nfrom the original LRTDP are (1) the sampling of a random\ngreedy action before sampling successor states in the main\nloop, and (2) deﬁning the descendants of a state by considering successors of all greedy actions (as opposed to just one\nin LRDTP). Alg. 5 outlines the MO extension of LRTDP.\nMOLRTDP consists of repeatedly trialing paths through\nthe state space and performing backups until the initial state\nis marked as solved. Trials are run by randomly sampling a\ngreedy action a from getActions(s, V) at each state s followed by a next state sampled from the probability distribution of a until a goal state is reached as in the ﬁrst inner loop\nfrom lines 5 to 10. The second inner loop from lines 11 to\n13 calls the checkSolved routine in the reverse trial order to\nlabel states as solved by checking whether the residual of all\nits descendant states under greedy actions are less than the\nconvergence criterion.\nThe checkSolved routine begins with inserting s into the\nopen set if it has not been solved, and returns true otherwise due to line 2. The main loop in lines 3 to 10 collects\nthe descendent states under all greedy actions (lines 8-10)\nand checks whether the residual of all the descendent states\nare small (lines 5-7). If true, all descendents are labelled as\nsolved as in line 11. Otherwise, backups are performed in\nthe reverse order of explored states as in lines 12 to 15.\nThe completeness and correctness of MOLRTDP follows\nsimilarly from its single objective ancestor. We note specifically that the labelling feature works similarly in the sense\nthat whenever a state is marked as solved, it is known for\ncertain that all its descendents’ values are ε-consistent and\nremain unchanged when backups are performed elsewhere.",
        "experiments": "In this section we empirically evaluate the different algorithms and heuristics for MOSSPs in several domains. Since\nno benchmark domains for MOSSPs exist, we adapt domains from a variety of sources to capture challenging features of both SSPs and MO deterministic planning. Our\nbenchmark set is introduced in the next section and it is followed by our experiments setup and analysis of the results.\nBenchmarks\nk-d\nExploding\nBlocksworld\nExploding\nBlocksworld\n(ExBw) was ﬁrst introduced by Younes et al. (2005) and\nlater slightly modiﬁed for the IPPC’08 (Bryce and Buffet\n2008). In ExBw, a robot has to pick up and stack blocks on\ntop of each other to get a target tower conﬁguration. Blocks\nhave a chance of detonating and destroying blocks underneath or the table. We consider a version with no dead ends\nusing an action to repair the table (Trevizan et al. 2017).\nWe extend ExBw to contain multi-objective costs. ExBw-2d\nhas two objectives: the number of actions required to stack\nthe blocks and number of times we need to repair the table.\nExBw-3d has an additional action to repair blocks with an\nobjective to minimise the number of block repairs.\nMO Search and Rescue\nSearch and Rescue (SAR-n) was\nﬁrst introduced by Trevizan et al. (2017) as a Constrained\nSSP. The goal is to ﬁnd, board and escort to a safe location any one survivor in an n × n grid as quickly as possible, constrained by a fuel limit. Probabilities are introduced\nwhen modelling fuel consumption and partial observability\nof whether a survivor exists in a given location. The location\nof only one survivor is known for certain. We extend the\nproblem by making fuel consumption as an additional objective instead of a constraint. A solution for a SAR MOSSP\nis a set of policies with different trade-offs between fuel and\ntime.\nMO Triangle Tireworld\nTriangle Tireworld, introduced\nby Little and Thi´ebaux (2007) as a probabilistically interesting problem, consists of a triangular grid of locations. The\ngoal is to travel from an initial to a target location where\neach location has a probability of getting a ﬂat tire. Some\nlocations contain a spare tire which the agent can load into\nthe car for future use. These problems have dead ends and\nthey occur when a tire is ﬂat and no spare is available. We\napply the give-up transformation from Section 3 resulting in\nan MOSSP with two objectives: total number of actions and\nprobability of using the give-up action.\nProbabilistic\nVisitAll\nThe\ndeterministic\nMO\nVisitAll (Geißer et al. 2022) is a TSP problem on a grid, where\nthe agents must collectively visit all locations, and each\nagent’s objective is to minimise its own number of moves.\nThis problem is considered MO interesting because any\nadmissible ideal-point heuristic returns the zero vector\nfor all states since it is possible for a single agent to visit\nall locations while no actions are performed by the other\nagents. We extend this domain by adding a probabilistic\naction move-risky which has probability 0.5 of acting as the\nregular move action and 0.5 of teleporting the agent back to\nits initial location. The cost of the original move action was\nchanged to 2 while the cost of the move-risky action is 1.\nVisitAllTire\nThis domain combines features of both the\nprobabilistic Tireworld and the deterministic MO VisitAll\ndomains into one that is probabilistically and MO interesting. The underlying structure and dynamics is the same as\nthe deterministic MO VisitAll except that the move action\nnow can result in a ﬂat tire with probability 0.5. We also\nadded the actions for loading and changing tires for each\nof the agents. Similarly to Triangle Tireworld, the problems\nin this domain can have dead ends when a ﬂat tire occurs\nand no spare tire is available. Applying the give-up transformation from Section 3 to this domain results in k + 1 cost\nfunctions where k is the number of agents.\nSetup\nWe implemented the MO versions of the VI, TVI, (i)LAO∗\nand LRTDP algorithms and the MO version of the PDB\nabstraction heuristics (Hpdb\nmossp) in C++.2 PDB heuristics are\ncomputed using TVI, ε = 0.001 and ⃗b =\n⃗\n100. We include\nin our experiments the following heuristics for deterministic\n2Code at https://github.com/DillonZChen/cpp-mossp-planner\n0\n5\n10\n15\n20\nSAR-4 (107/120)\nSAR-5 (101/120)\nExBw-2d (56/155)\nExBw-3d (52/155)\nTireworld (4/10)\nPr.VisitAll (19/19)\nVisitAllTire (5/24)\nFigure 3: Boxplot of CCS size of instances across domains\nwhich have been solved at least once. Number of solved and\ntotal instances for each domain is indicated in parentheses.\nMO planning from (Geißer et al. 2022): the ideal-point version of hmax (Hmax\nideal); the MO extension of hmax (Hcomax\nmo\n);\nand the MO canonical PDBs of size 2 and 3 (Hpdb2\nmo\nand\nHpdb3\nmo ). The SO PDB heuristics for SSPs from (Kl¨oßner and\nHoffmann 2021) of size 2 and 3 (Hpdb2\nideal and Hpdb3\nideal) are also\nincluded in our experiments. All problem conﬁgurations are\nrun with a timeout of 1800 seconds, memory limit of 4GB,\nand a single CPU core. The experiments are run on a cluster\nwith Intel Xeon 3.2 GHz CPUs. We used CPLEX version\n22.1 as the LP solver for computing CCS. The consistency\nthreshold is set to ε = 0.001 and we set ⃗b =\n⃗\n100. Each\nexperimental conﬁguration is run 6 times and averages are\ntaken to reduce variance in the data.\nWe also modify the inequality constraint in Alg. 4.3\nfrom Roijers and Whiteson 2017 for computing CCS to\n⃗w · (⃗v − ⃗v′) + x ≤ −λ, ∀⃗v′ ∈ V with λ = 0.01 to deal\nwith inevitable numerical precision errors when solving the\nLP. If the λ term is not added, the function may fail to prune\nunnecessary points such as those corresponding to stochastic\npolicies in the CCS, resulting in slower convergence. However, an overly large λ may return incorrect results by discarding important points in the CCS. One may alternatively\nconsider the term as an MO parameter for trading off solution accuracy and search time, similarly to the ε parameter.\nFig. 3 shows the distribution of CCS sizes for different\ndomains. Recall that the CCS implicitly represents a potentially inﬁnite set of stochastic policies and their value functions. As a result, a small CCS is sufﬁcient to dominate a\nlarge number of solutions, for instance, in Triangle Tireworld, a CCS of size 3 to 4 is enough to dominate all other\npolicies even though the number of deterministic policies for\nthis domain is double exponential in its parameter n.\nResults\nFig. 4 summarises our results by showing the cumulative\ncoverage of different planners and heuristics across all considered problems. The following is a summary of our ﬁndings (we omit the MO in the planner names for simplicity):\nWhat is the best planner and heuristic combination?\nReferring to the overall cumulative coverage plot in Fig. 4a\nand coverage tables in Tab. 1, we notice that LRTDP+Hpdb2\nmossp\nperforms best followed by LRDTP+Hpdb2\nmo , LRTDP+Hpdb3\nmo\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n200\n225\n250\n275\n300\n(a) Cumulative Coverage per Planner+Heuristic\nLRTDP-Hpdb2\nmossp\nLRTDP-Hpdb2\nmo\nLRTDP-Hpdb3\nmo\niLAO∗-Hpdb3\nmossp\niLAO∗-Hpdb3\nmo\nLRTDP-Hpdb3\nmossp\nLRTDP-Hmax\nideal\niLAO∗-Hpdb2\nmossp\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n0\n500\n1000\n1500\n2000\n2500\n(b) Cumulative Coverage per Planner\nLRTDP\niLAO∗\nLAO∗\nTVI\nVI\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n500\n600\n700\n800\n900\n(c) Cumulative Coverage per Heuristic\nHpdb3\nmossp\nHpdb2\nmossp\nHpdb3\nmo\nHpdb2\nmo\nHpdb3\nideal\nHmax\nideal\nHpdb2\nideal\nblind\nHcomax\nmo\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n1000\n1200\n1400\n1600\n1800\n2000\n(d) Cumulative Coverage per PDB type\nHpdb\nmossp\nHpdb\nmo\nHpdb\nideal\nFigure 4: Cumulative coverage of: (a) planners and heuristics combinations (low-performing planners omitted); (b)\nplanners only, i.e., summation across different heuristics;\n(c) heuristics only, i.e. summation across different planners;\n(d) PDB approaches considered, i.e., summation across the\nideal-point, MO, and MOSSP approaches. Notice that the xaxis is the same for all plots but the y-axis is different and\nmight not start at 0.\nand iLAO∗+Hpdb3\nmossp. The ranking of the top 3 planners remain the same if we normalise the coverage by domain.\nWe note that the considered Exbw3 problems had several\nimproper policies resulting in slow convergence of TVI for\nsolving PDBs of size 3. This is due to Assumption 1 and the\nplanner + heuristic\ncoverage\nheuristic\ncoverage\nLRTDP\nHpdb2\nmossp\n307.6 ± 2.7\nHpdb3\nmossp\n893.5 ± 1.9\nLRTDP\nHpdb2\nmo\n302.8 ± 2.1\nHpdb2\nmossp\n893.3 ± 3.2\nLRTDP\nHpdb3\nmo\n291.2 ± 1.3\nHpdb3\nmo\n871.2 ± 1.8\niLAO∗\nHpdb3\nmossp\n276.7 ± 0.7\nHpdb2\nmo\n851.8 ± 2.1\niLAO∗\nHpdb3\nmo\n272.0 ± 0.0\nHpdb3\nideal\n768.7 ± 0.7\nLRTDP\nHpdb3\nmossp\n271.2 ± 1.7\nHmax\nideal\n755.2 ± 0.7\nLRTDP\nHmax\nideal\n263.2 ± 0.7\nHpdb2\nideal\n737.2 ± 1.2\niLAO∗\nHpdb2\nmo\n262.0 ± 0.0\nblind\n572.9 ± 1.4\nplanner\ncoverage\nPDB\ncoverage\nLRTDP\n2297.3±3.0\nHpdb\nmossp\n1786.8±1.6\niLAO∗\n2111.7±0.7\nHpdb\nmo\n1723.0±2.0\nLAO∗\n1529.0±1.2\nHpdb\nideal\n1505.8±1.9\nTVI\n460.0 ± 0.5\nVI\n450.3 ± 0.7\nTable 1: Average marginalised cumulative coverages with\n95% conﬁdence intervals. Higher values are better.\nchosen⃗b = ⃗\n100 for our experiments which that requires the\ncost of improper policies in each PDB to cost at least 100 in\none of its dimensions. Ignoring the Exbw3 domain, the top\n3 conﬁgurations are LRTDP+Hpdb3\nmossp, LRTDP+Hpdb2\nmossp and\niLAO∗+Hpdb3\nmossp and their 95% conﬁdence intervals all overlap. The ranking of the top few planners remain the same if\nthe coverage is normalised.\nWhat planner is best?\nTo answer this question, we look at\nthe cumulative coverage marginalised over heuristics, that is,\nwe sum the coverage of a planner across the different heuristics. The results are shown in Fig. 4b and Tab. 1 and the\nbest three planners in order are LRTDP, iLAO∗ and LAO∗.\nNotice that the difference in coverage between LRTDP and\niLAO∗ is at 185.6 solved instances while the difference between TVI and VI is only 9.7. The ranking between planners\nremains the same when the coverage is normalised per domain. Considering domains individually, LRTDP is the best\nplanner for Exbw and VisitAllTire while for iLAO∗ is the\nbest planner for SAR and Probabilistic VisitAll. For MO\nTireworld, both LRTDP and iLAO∗ are tied in ﬁrst place.\nWhat\nheuristic\nis\nbest?\nThe\ncumulative\ncoverage\nmarginalised over planners for selected heuristics is shown\nin Fig. 4c and Tab. 1. The MOSSP PDB heuristic of size 3\n(Hpdb3\nmossp) has the best performance followed by Hpdb2\nmossp, Hpdb3\nmo\nand Hpdb2\nmo . We note there is a large overlap in the 95% conﬁdence intervals of Hpdb2\nmossp and Hpdb3\nmossp due to the slow computation of Hpdb3\nmossp on Exbw3. The overlap disappears when\nwe remove the Exbw3 results with coverage and conﬁdence\nintervals given by 862.9 ± 3.0 and 807.6 ± 2.9 for Hpdb3\nmossp\nand Hpdb2\nmossp, respectively. We further quantify heuristic accuracy using the directed Hausdorff distance between heuristic\nand optimal values in Tab. 2. We notice that Hcomax\nmo\nachieves\nstrong accuracy relative to other heuristics but has the worst\ncoverage of 504.5 due to its high computation time.\nCrit. Path\nAbstractions\nblind\nHmax\nideal\nHcomax\nmo\nHpdb2\nideal\nHpdb3\nideal\nHpdb2\nmo\nHpdb3\nmo\nHpdb2\nmossp\nHpdb3\nmossp\nSAR-4\n100\n97\n44\n93\n92\n44\n44\n38\n26\nSAR-5\n100\n97\n43\n93\n92\n43\n43\n37\n27\nExBw-2d\n100\n59\n22\n51\n45\n51\n45\n51\n45\nExBw-3d\n100\n58\n22\n52\n44\n52\n44\n52\n44\nTireworld\n100\n100\n68\n100\n100\n68\n68\n57\n13\nPr.VisitAll\n100\n100\n54\n100\n100\n61\n53\n22\n12\nVisitAllTire\n100\n100\n50\n100\n100\n66\n45\n66\n45\nTable 2: The mean relative error (%) of the heuristic\nvalue relative to the optimal value at the initial state\ncomputed with the directed Hausdorff distance divided\nby the norm of the largest vector in the optimal value:\nmax⃗v∈V∗ min⃗u∈H d(⃗v, ⃗u)/ max⃗v∈V∗ ∥⃗v∥. Only solved instances for which all heuristics were computed for the initial\nstate within the time limit were considered. Best value in\neach row is indicated in bold. Lower values are better.\nWhat feature of MOSSPs is more important to capture in\na heuristic?\nTo answer this question, consider the performance of the 3 classes of PDB heuristics: probabilistic-only\nPDBs (Hpdb\nideal), MO only PDBs (Hpdb\nmo ), and MO probabilistic\nPDBs (Hpdb\nmossp). The cumulative coverage marginalised over\nthe different PDB heuristics shown in Fig. 4d and Tab. 1\nhighlights the effectiveness of MO PDBs. Hpdb\nmossp is able to\nsolve 63.8 and 281 more instances than Hpdb\nmo and Hpdb\nideal, respectively. The ranking remains the same when the coverage\nis normalised per domain. Moreover, notice in Tab. 2 that\nHpdb\nmo heuristics are at least as informative as Hpdb\nideal heuristics\non all domains. Lastly, we note that an admissible ideal point\nheuristic is upper bounded by {⃗u} where ⃗ui = min⃗v∈V∗ ⃗vi\nfor i = 1, . . . , n. These results suggest that it is more important for a heuristic to maintain the MO cost structure of\nMOSSPs than the stochastic structure of actions.",
        "conclusion": "In this work we deﬁne a new general class of problems,\nnamely multi-objective stochastic shortest path problems\n(MOSSPs). We adapt MOVI which was originally constructed for solving multi-objective Markov Decision Processes to our MOSSP setting with conditions for convergence. We further design new, more powerful heuristic\nsearch algorithms (i)MOLAO∗ and MOLRTDP for solving MOSSPs. The algorithms are complemented by a range\nof MOSSP heuristics and an extensive set of experiments\non several benchmarks with varying difﬁculty and features.\nThrough our evaluation, we can conclude that MOLRTDP is\nthe best performing MOSSP solver, and abstraction heuristics which consider both the MO and probabilistic aspects of\nMOSSPs the best performing MOSSP heuristic. Our future\nwork includes adding probabilistic LTL constraints as additional multi-objectives in order to compute solutions to MOPLTL SSPs (Baumgartner, Thi´ebaux, and Trevizan 2018)\nthat are robust to the choice of probability thresholds.",
        "summary_en": "Heuristic search is a powerful approach that has successfully been applied to a broad class of planning problems, including classical planning, multi-objective planning, and probabilistic planning modelled as a stochastic shortest path (SSP) problem. This paper extends the reach of heuristic search to a more expressive class of problems, namely multi-objective stochastic shortest paths (MOSSPs), which require computing a coverage set of non-dominated policies. The paper designs new heuristic search algorithms MOLAO* and MOLRTDP, which extend well-known SSP algorithms to the multi-objective case. The paper further constructs a spectrum of domain-independent heuristic functions differing in their ability to take into account the stochastic and multi-objective features of the problem to guide the search. The paper's experiments demonstrate the benefits of these algorithms and the relative merits of the heuristics.",
        "summary_zh": "这篇论文介绍了一种名为多目标随机最短路径（MOSSPs）的问题类型，并提出了两种新的启发式搜索算法。MOSSPs需要计算非支配策略的覆盖集，为此，作者设计了新的启发式搜索算法MOLAO*和MOLRTDP，这些算法将知名的SSP算法扩展到多目标情况。此外，作者还设计了一系列与领域无关的启发函数来引导搜索。最后，实验结果证明了这两种算法的优势以及启发式方法的相对优点。"
    },
    {
        "title": "Progression Heuristics for Planning with Probabilistic LTL Constraints",
        "abstract": "Probabilistic planning subject to multi-objective probabilistic temporal logic (PLTL) constraints models the problem of computing safe and robust behaviours for agents in stochastic environments. We present novel admissible heuristics to guide the search for cost-optimal policies for these problems. These heuristics project and decompose LTL formulae obtained by progression to estimate the probability that an extension of a partial policy satisﬁes the constraints. Their computation with linear programming is integrated with the recent PLTL-dual heuristic search algorithm, enabling more aggressive pruning of regions violating the constraints. Our experiments show that they further widen the scalability gap between heuristic search and veriﬁcation approaches to these planning problems.",
        "introduction": "In safety-critical planning applications, optimising performance is not enough, and utility must be traded-off against\nthe risk of jeopardizing complex mission goals and constraints. For instance, consider a Mars rover that must gather\nscientiﬁc information and send it to earth (Zilberstein et al.\n2001). Safety requirements and constraints include avoiding\ncollision against obstacles, not traversing into unsafe terrains,\nmaintaining safe operational temperature and battery levels,\nand so on. The rover should seek to optimise expected science returns while remaining safe by proactively keeping the\nprobability of violating each of the above constraints within\nacceptable levels.\nSuch planning problems can be modelled as stochastic shortest path problems (SSP) subject to multi-objective\nprobabilistic linear temporal logic (MO-PLTL) constraints\nPr(ψi)∈zi, where the ψi are LTL formulae and zi ⊆ [0, 1]\nare intervals bounding their respective probabilities (Baumgartner, Thi´ebaux, and Trevizan 2018). Optimal solutions to\nMO-PLTL SSPs take the form of stochastic ﬁnite-memory\npolicies, where the probability of the next action to perform\ndepends both on the current state of the environment and on\na mode used to track the truth value of the LTL formulae.\nVariants of MO-PLTL SSPs have been extensively studied\nby the automated veriﬁcation community (Baier and Katoen\n2008), and their resolution is supported by tools such as the\nPRISM model-checker (Kwiatkowska, Norman, and Parker\n2011). However, these approaches build, upfront, the entire\nstate space of the MO-PLTL SSP, i.e. the synchronous product of the modes and environment states (Etessami et al.\n2008; Forejt et al. 2011; Kwiatkowska and Parker 2013). The\nprohibitive size of this construction, polynomial in the huge\nnumber of reachable environment states and in the worst case\ndouble exponential in the size of the formulae, precludes applicability to the factored state spaces found in AI planning.\nRecently, heuristic search has become the state of the art\napproach for solving planning problems modelled as factored\nMO-PLTL SSPs. In particular, Baumgartner et al. (2018)\nintroduced PLTL-dual, an algorithm which builds the state\nspace of the MO-PLTL SSPs on-the-ﬂy from the factored\nrepresentation. PLTL-dual applies linear programming to increasingly larger subsets of the reachable state space, guided\nby admissible heuristics to prune regions that cannot satisfy\nthe constraints or are too costly to form part of an optimal\npolicy. When guided by informative heuristics, PLTL-dual\nonly needs to expand a fraction of the reachable state space to\nﬁnd an optimal policy satisfying the constraints. This yields\nsigniﬁcant scalability improvements over the conventional\napproach implemented in PRISM.\nUnfortunately, effective heuristic guidance can be difﬁcult\nto obtain. Classical planning enjoys a multitude of admissible\nheuristics (Bonet and Geffner 2001; Helmert, Haslum, and\nHoffmann 2007; Helmert and Domshlak 2009; Pommerening et al. 2014). However, it is only recently that the ﬁrst\nSSP heuristics taking into account both probabilities and\ncosts have been proposed (Trevizan, Thi´ebaux, and Haslum\n2017). Moreover, only a couple of heuristics exist for deterministic planning with LTL-like constraints (Baier, Bacchus, and McIlraith 2009; Bienvenu, Fritz, and McIlraith\n2011), and heuristic search for SSPs with probabilistic LTL\nconstraints is in its infancy. Baumgartner et al. started to investigate the latter, and devised projection heuristics based\non a representation of LTL formulae and policy modes in\nterms of non-deterministic B¨uchi automata (NBA) (Vardi\nand Wolper 1994; Babiak et al. 2012). Their experimental\nresults show that the size of the linear programs (LPs) required to represent the projections of NBAs grows unacceptably large, so that the projection heuristic with NBAs larger\nthan 100 states obtains worse results than the trivial heuristic (Pr(ψi) = 1 ∀i). To avoid generating NBAs, they also\nexperimented with modes obtained by formula progression\n(Bacchus and Kabanza 1998) and the trivial heuristic, but\nwere unable to devise heuristics based on progression.\nThis paper presents the ﬁrst admissible heuristics based\non progression for probabilistic LTL. These heuristics overestimate the probability of an LTL formula being satisﬁable\nby completions of a partial policy. We present two heuristic\nestimates. The ﬁrst loosely ties together various projections of\nthe MO-PLTL SSP over subsets of state variables chosen via\na principled examination of the formulae. The second applies\na further relaxation that decomposes the formulae, sums the\nprobabilities of disjuncts, and averages the probabilities of\nconjuncts. We show how to embed the computation of these\nheuristics into the LPs used by PLTL-dual, which makes\noptimisation and heuristic estimation synergic and avoids\nrepeated calls to heuristic estimators. Our results show that\nthe progression heuristics are competitive and further increase\nthe superiority of PLTL-dual over PRISM.\nThe paper starts with background on MO-PLTL SSPs in\nSection 2. Sections 3 and 4 describe the projection and decomposition heuristics, respectively, and Section 5 their integration into PLTL-dual. Section 6 gives experimental results\nand Section 7 concludes with related and future work.",
        "background": "2.1\nMO-PLTL SSPs\nA Stochastic Shortest Path problem (SSP) with MultiObjective Probabilistic Temporal Logic (MO-PLTL) constraints is a tuple ⟨L, S, s0, G, A, P, C, ⃗ψ⟩ where: L is a set\nof atoms, S ⊆ 2L is the ﬁnite set of states; s0 ∈ S is the\ninitial state; G ⊆ S is the non-empty set of goal states; A\nis the ﬁnite set of actions and we write A(s) for the set of\nactions applicable in state s; P(s′|s, a) is the probability of\ntransitioning from s to s′ when action a ∈ A(s) is applied\nin s; C(a) ∈ R∗\n+ is the immediate cost of applying action\na; and ⃗ψ is a vector of k probabilistic LTL constraints. Each\nconstraint is of the form ψi ≡ Pr(ψi) ∈ zi where ψi is a\nlinear temporal logic (LTL) formula over atoms in L, and\nzi ⊆[0, 1] is an interval bounding its probability.\nWe assume that the reader is familiar with Linear Temporal\nLogic (LTL) and refer to (Baier and Katoen 2008) for a detailed account. Brieﬂy, the standard version of LTL speciﬁes\nproperties of inﬁnite sequences of states (or paths). It extends\npropositional logic with the operators Xϕ, which holds if ϕ\nholds at the next position in the sequence, and ψUϕ, which\nspeciﬁes that ψ must hold at every point in the sequence\nuntil ϕ holds. We also use the operator ψRϕ = ¬(¬ψU¬ϕ)\nwhich is required by the transformation to negation normal\nform assumed in Subsection 3.2.\nThe standard semantics of LTL speciﬁes when an inﬁnite\npath q satisﬁes the formula ϕ, which we write q |= ϕ. However, in planning, the sequences we seek are ﬁnite and end\nin a goal state.1 For a ﬁnite path p ending in state last(p),\nwe follow Baumgartner et al. (2018) in using the Inﬁnite\nExtension Semantics (Bacchus and Kabanza 1998; Bauer and\nHaslum 2010), which stipulates that p satisﬁes ϕ iff the inﬁnite sequence that loops in the ﬁnal state of p satisﬁes ϕ, i.e.\n1For SSPs the lengths of the ﬁnite paths to the goal are unbounded, which is why SSPs are said to have an indeﬁnite horizon.\niff p last(p)ω |= ϕ. Our heuristics can trivially be adapted\nto slightly different ﬁnite path semantics, such as f-FOLTL\n(Baier and McIlraith 2006) or LTLf (De Giacomo and Vardi\n2013). Note that we do not require the formulas to be either\nsafe or co-safe, as in e.g. (Lacerda, Parker, and Hawes 2015).\nA solution to an MO-PLTL SSP is a stochastic ﬁnitememory policy, π: S × M × A → [0, 1] where M is a set of\nmode vectors ⃗m which act as memory for the policy, starting\nfrom an initial mode vector ⃗\nm0. Intuitively, each element\nmi of ⃗m is the memory that keeps track of the satisfaction\nof the LTL formula ψi, and is updated upon transitioning\nbetween states. For the purpose of this paper, we assume that\nthis update function is deterministic2 (i.e., a single mode ⃗m′\nmay result from updating mode ⃗m when the state changes\nfrom s to s′). The policy maps the current state s and mode\nvector to a probability distribution over the applicable actions\nA(s). Due to the determinism of the mode update function,\nthe policy corresponds to a Markov chain and its execution\ninduces a probability distribution over the sequences of states\nof the MO-PLTL SSP. A valid policy must reach the goal\nG with probability 1 and satisfy the PLTL constraints, i.e.\nthe probability mass of the sequences satisfying ψi must fall\nwithin zi. An optimal policy is a valid policy with minimal\nexpected cost.\nWe consider policies whose modes are obtained by progression of the LTL formulae (Bacchus and Kabanza 1998).\nAs shown by Baumgartner et al., this does not compromise\noptimality. Formula progression is a technique to track an\nagent’s progress towards satisfying an LTL formula, as the\nsequence of states followed when executing the policy unfolds. Progression looks at the current state s of the sequence\nand at the formula ϕ to satisfy, and returns a new formula\nϕ′ = prog(s, ϕ) that must be satisﬁed by the rest of the sequence: sp |= ϕ iff p |= ϕ′. Fitting with inﬁnite extension\nsemantics, we also deﬁne idle(s, ϕ) which returns true if\nand only if sω |= ϕ. Using progression, the policy modes\nare vectors of LTL formulae ⃗ϕ = (ϕ1, . . . , ϕk), and upon\ntransitioning to s, are updated with prog(s, ϕi) for all i.\n2.2\nSolution Approaches for MO-PLTL SSPs\nExisting solution approaches compile the MO-PLTL SSP\ninto the problem of minimising the cost of reaching certain\naccepting states with the required probabilities. This analysis is performed in an augmented state space which is the\nsynchronised product of the regular state space S and the\nmode space M. More precisely, the augmented state space\nis S× = S × M; the initial state is t0 = ⟨s0, ⃗ϕ0⟩ where\nthe initial mode ⃗ϕ0 is such that ϕ0i = prog(s0, ψi) for all\ni, the set of goal states is G× = G × M; the action applicability function is A×(⟨s,ϕ⟩) = A(s), the transition probability distribution is P ×(⟨s′,⃗ϕ′⟩|⟨s,⃗ϕ⟩, a) = P(s′|s, a) if\nϕ′\ni = prog(ϕi, s′) for all i and 0 otherwise; and the accepting\nstates with respect to the ith constraint of the MO-PLTL SSP\nare Ti = {⟨s,⃗ϕ⟩ ∈ G×| idle(s, ϕi)}.\nIn theory, this reachability problem can be solved by a\nlinear program, known as the dual LP, whose variables are the\n2More permissive options are possible, provided that the synchronised product described in Subsection 2.2 is an SSP.\noccupation measures xt,a representing the expected number\nof times action a will be performed in state t = ⟨s,⃗ϕ⟩ ∈ S×\nwhen executing the policy – see (Altman 1999; Baier and\nKatoen 2008; Baumgartner, Thi´ebaux, and Trevizan 2018):\nmin\nxt,a≥0\nX\nt∈S×,a∈A×(t)\nxt,aC(a)\n(LP1)\ns.t.\nin(t) =\nX\nt′∈S×,a∈A×(t′)\nxt′,aP ×(t|t′, a)\n∀t ∈ S× (C1)\nout(t) = P\na∈A×(t) xt,a\n∀t ∈ S×\\ G× (C2)\nout(t0) − in(t0) = 1\n(C3)\nout(t) − in(t) = 0\n∀t ∈ S×\\ (G×∪ {t0}) (C4)\nP\ntg∈G×in(t) = 1\n(C5)\nP\nt∈Ti in(t) ∈ zi\n∀ψi ∈ ⃗ψ (C6)\nLP1 can be viewed as a ﬂow problem where xt,a describes\nthe ﬂow leaving state t via action a and xt,aP(t′|t, a) units\nof ﬂow reaching each successor state t′. The objective represents the total expected cost to reach the goal (sink) from the\ninitial state (source). The functions in(t) and out(t) in C1C2 are shorthand for the incoming and outgoing ﬂow for state\nt. C3-C5 represent, respectively, the source, the ﬂow preservation and the sink of the ﬂow network, and C6 enforces\nthe constraints on the probability of reaching an accepting\nstate. The optimal solution x∗ of LP1 can be converted into\nan optimal policy π∗(t, a) = x∗\nt,a/ P\na′∈A×(t) x∗\nt,a′.\nLP1, with its |S| × Q\ni 22|ψi| × |A| variables, is unacceptably large. Hence, Baumgartner et al. (2018) investigated a\nheuristic search approach, PLTL-dual, which only applies\nLP1 to a subset of the augmented state space, constructed on\nthe ﬂy, starting from t0. At each iteration, the search expands\nthe fringe states reachable under the optimal policy found at\nthe previous iteration and runs the LP. The search stops if\nthe LP ﬁnds a proper policy as this policy is guaranteed to\nbe optimal. If the MO-PLTL SSP is unsolvable, an infeasible\nLP will be produced at some iteration when or before the\ncomplete augmented state space is generated.\nThe search is guided by k + 1 admissible heuristics to\nevaluate fringe states: a cost heuristic that under-estimates\nthe expected cost of reaching the goal, guiding the search\ntowards cheap policies; and one heuristic for each PLTL constraint that over-estimates3 the probability of the respective\nformula, guiding the search towards valid policies. These\nheuristics map probability distributions P over the fringe of\nS× to R+. The set F of reachable fringe states is the support of P, i.e., F = {⟨s,⃗ϕ⟩ ∈ S×|P(s, ⃗ϕ) > 0}. A heuristic hψi for PLTL constraint ψi is admissible if, for all P,\nhψi(P) ≥ P\n⟨s,⃗ϕ⟩∈F P(s, ⃗ϕ) × Pr∗(ϕi|s), where Pr∗(ϕi|s)\nis the maximum probability of satisfying ϕi from state ⟨s,⃗ϕ⟩\nover the set of optimal policies.\nBaumgartner et al. (2018) uses the hpom heuristic (Trevizan, Thi´ebaux, and Haslum 2017) to estimate cost, and a\nheuristic hBA based on the NBA representation of LTL for3Note that over-estimates of probabilities are sufﬁcient: PLTLdual obviates the need to compute under-estimates by converting\nevery constraint Pr(ψi)∈[zi, zi] into two lower bound constraints:\nPr(ψi) ∈ [zi, 1] and Pr(¬ψi) ∈ [1 − zi, 1].\nmulae to estimate probabilities. However, these heuristics do\nnot scale to NBAs exceeding 100 states.4 In the rest of this\npaper, we show that more powerful heuristics for estimating\nprobabilities can be directly obtained from the progressed\nformulae labelling the fringe states, and present two such\nheuristics, hpom\nψ\nwhich is based on projection and hdec\nψ which\nis based on a further decomposition of the formula. These\nheuristics apply to a single constraint, which we write as\nψ ≡ Pr(ψ) ∈ z to simplify notation.",
        "ltl projection": "In this section we introduce our heuristic for PLTL constraints\nbased on a set of projections, i.e., a set of smaller MO-PLTL\nSSPs obtained by ignoring different subsets of atomic propositions. Each of these projections simplify the underlying SSP\nand one of our novel contributions is to show how to simplify\nthe LTL formulae to obtain a meaningful heuristic. We ﬁrst\ndescribe the multi-variable projections, then we introduce our\nLTL formula projection and show how to integrate different\nprojections into a single heuristic.\n3.1\nUnderlying SSP Projection\nFor the remainder of this paper, we assume that the set of\natomic propositions L is compactly represented as in SAS+\n(B¨ackstr¨om 1992), using a set of multi-valued variables v∈V,\neach with a ﬁnite domain Dv. This allows us to represent each\nstate s∈S as a set of values, one per state variable, and we\ndenote by s[v]∈Dv the value of v in s. We also assume that\nthe actions in A are represented compactly using partial valuations over V, i.e., using functions from V to×v∈V(Dv∪{⊥})\nwhere s[v]=⊥ denotes that v has no assigned value. Using\nthis representation, an action a consists of: a partial valuation\npre(a) denoting its precondition; a set eff(a) of partial valuations representing the effects of the action; and a probability\ndistribution Pa(·) over effects e ∈ eff(a) representing the\nprobability of e being selected when a is applied. The result\nof applying an effect e in a state s∈S is the state res(s, e)∈S\nsuch that, for all state variables v of s, res(s, e)[v]=e[v] if\ne[v]̸=⊥ and res(s, e)[v]=s[v] otherwise.\nGiven a non-empty set of variables U ⊂ V, the projection of a state s onto U is a state sU ∈ ×v∈U Dv s.t.\nsU[v] = s[v] for all v ∈ U. The projection of an MOPLTL SSP onto U is ⟨LU, SU, sU\n0 , GU, A, P U, C, ψU⟩ where:\nLU = {(v = d)|v ∈ U, d ∈ Dv};\nSU = ×\nv∈U\nDv;\nGU = {sU|s ∈ G}; and\nif a is applicable in sU ∈ SU (i.e., pre(a)[v] ∈ {⊥, sU[v]}\nfor all v ∈ U) then\nP U(sU ′|sU, a) =\nX\ne∈eff(a)\ns.t. sU ′=res(sU,e)\nPa(e).\nNext, we show how to project the PLTL constraint ψ onto U\nto obtain ψU.\n4Using the LTLf semantics (De Giacomo and Vardi 2013) and\nﬁnite automata would not remedy this issue.\n3.2\nFormula Projection\nWhen projecting a formula ψ onto a set of variables U, we\nassume without loss of generality that ψ is in negation normal\nform without spurious ⊤ and ⊥, and we relax the PLTL constraint by assuming that each instance of a forgotten variable\n(in V \\ U) is independent of the others. That is, multiple instances of a forgotten variable can variously be assigned true\nor false at each state in a path. Under these assumptions, all\nliterals in ψ containing forgotten variables can be trivially replaced with ⊤, allowing the formula to be further simpliﬁed.\nFor instance, a formula F(v1 = 1)∧((v1 = 3)U(¬(v2 = 1)))\nwhen projected onto {v2} would become ⊤U(¬(v2 = 1)),\npreserving the requirement that v2 must eventually not be 1.\nWe now explain how to choose suitable sets U of variables\nto project onto. The aim of a PLTL heuristic is to extract\nsome information about how to satisfy a constraint; there\nis no point if after projection the formula is trivial (i.e., it\nsimpliﬁes to ⊤ or ⊥). Let minvars(ψ) be the minimal set of\nvariable combinations {C1, . . . , Cn}, such that the projection\nof ψ onto the variables U is non-trivial if and only if there\nis some combination Cj ∈ minvars(ψ) s.t. Cj ⊆ U. It can\nclearly be seen that the set minvars(ψ) can be computed\nrecursively as follows:\nminvars(⊤) = minvars(⊥) = ∅\nminvars((v = d)) = minvars(¬(v = d)) = {{v}}\nminvars(ψ1 ∧ ψ2) = reduce(minvars(ψ1) ∪ minvars(ψ2))\nminvars(ψ1 ∨ ψ2) = reduce({C1 ∪ C2|C1 ∈ minvars(ψ1),\nC2 ∈ minvars(ψ2)})\nminvars(Xψ) = minvars(ψ)\nminvars(ψ1Uψ2) = minvars(ψ1Rψ2) = minvars(ψ2)\nwhere the reduce operator removes combinations that are subsumed by others. Consider our earlier example ψ = F(v1 =\n1) ∧ ((v1 = 3)U(¬(v2 = 1))). Projecting onto {v1} will\npreserve the left hand side of the ∧, and {v2} only the right\nhand side of the U. Note that projecting ψ onto {v1, v2} is\nalso non-trivial, but any set U with {v1, v2} ⊆ U also has\n{v1} ⊆ U, so {v1, v2} does not appear in the minimal set\nof combinations. Hence minvars(ψ) = {{v1}, {v2}}. We\nchoose a set of combinations Pψ randomly from minvars(ψ)\nsuch that Pψ covers the set of variables in ψ. Our heuristic\nprojects the MO-PLTL SSP onto each of these combinations\nin Pψ and ties these projections together as explained below.\n3.3\nTying Projections\nFor each combination Cj in Pψ, we build a dual LP (LP1) for\nthe projection of the MO-PLTL problem onto Cj. We denote\nthe set of reachable augmented states in this projection S×\nj .\nThese projections are tied together by extra tying constraints which enforce that each projection should use each\naction an equal number of times in expectation:\nP\nt∈S×\nj xt,a =P\nt∈S×\nj′ xt,a\n∀Cj, Cj′ ∈Pψ, a∈A\n(C7)\nThe heuristic hpom\nψ (P) injects P(s, ϕ) ﬂow into each network\nat the projection of each fringe state ⟨s,ϕ⟩ ∈ F, and maximises the amount of ﬂow reaching the accepting states of\nthe projection.5 The heuristic estimate is that maximum.\n5It is possible that for a fringe state the projection of that state is\nhpom\nψ\nis admissible. The proof follows from the fact that\nthe literal independence assumption makes formulae easier\nto satisfy. Hence any path from a state in F which reaches a\ngoal state and satisﬁes ψ can be projected onto each S×\nj and\nwill reach a goal and satisfy the projection of ψ.",
        "ltl decomposition": "We now present a second admissible heuristic hdec\nψ for a single PLTL constraint. This heuristic combines projection with\na complementary relaxation of PLTL constraints, so it is assumed in this section that the underlying SSP and ψ have\nalready been projected onto a subset of the SAS+ variables\nas in the previous section. We call this second relaxation decomposition, as it decomposes the progression formulae into\nsub-formulae. Decomposition over-estimates the probability\nof satisfying a formula in conjunctive normal form (CNF)\nby summing the probability of disjuncts and averaging the\nprobability of conjuncts.\nFor the rest of this paper, we assume progression also converts formulae to CNF. Any LTL formula can be expanded to\nthe form V\ni(αi∨W\nj φij) where each αi is a ﬁnite disjunction\nof literals, and each φij is an LTL formula in negation normal\nform preﬁxed by the next operator X. During progression,\neach αi can be evaluated against the current state interpretation, and we refer to the resulting form as CNF, to W\nj φij\nas a clause, and to φij as an X-literal. While this CNF transformation results in an exponential blowup in formula size,\nit is only necessary in hdec\nψ , and is used in conjunction with\nformula projection. In practice, projection simpliﬁes the formula such that the exponential blowup doesn’t signiﬁcantly\naffect the heuristic’s performance.\nFor convenience, we represent a formula in CNF as a set\nof sets; a set Ψ represents the formula V\nΦ∈Ψ\nW\nφ∈Φ φ. The\nset of X-literals in a CNF formula Ψ is D(Ψ), called the\ndecomposition of Ψ. It is convenient to deﬁne the set of\nX-literals which might arise from progression of a formula\nψ. We denote this set Σ(ψ), and it is the set of temporal\nsubformulae of ψ preﬁxed by X.\nWe denote the probability of satisfying a formula Ψ from\na state s as Pr(Ψ|s). To estimate the probability Pr(Ψ|s) we\nobserve the following inequalities:\nPr(Ψ|s) ≤ P\nΦ∈Ψ Pr(Φ|s)/|Ψ|\n(1)\nPr(Φ|s) ≤ P\nφ∈Φ Pr(φ|s)\n(2)\nPr(φ|s)= max\na∈A(s)\nX\ns′∈S\nP(s′|s, a)Pr(prog(s′, φ)|s′)\n(3)\nApplied recursively, these inequalities ﬁnd an over-estimate\nfor the probability while only optimising actions in states\nwith X-literal modes in Σ(ψ).\nTo encode this optimisation in our LP framework, we\nuse a ﬂow network which exhibits duplication of ﬂow. The\nvariables are occupation measures xs,φ,a representing ﬂow\nleaving the pair ⟨s,φ⟩ via action a. Under SSP dynamics with\nprogression, the ﬂow along xs,φ,a would be shared between\nthe pairs ⟨s′,prog(s′, φ)⟩ such that s′ is reachable via a, but\nnot present in an LP, in which case that LP is extended to include\nthis state and all projected states reachable from it.\nFigure 1: An example of ﬂow from multiple states labelled\nwith CNF formulae being redistributed to decomposed modes.\nDotted lines denote the movement of ﬂow by redistribution,\nand solid lines denote occupation measures.\ninstead is duplicated and redirected to pairs ⟨s′,φ′⟩ for φ′∈\nD(prog(s′, φ)). Let IΨ,φ = |{Φ ∈ Ψ|φ ∈ Φ}| be the number\nof occurrences of φ in Ψ. We deﬁne the functions:\nin(s, Ψ) ≡\nX\ns′∈S,a∈A(s′)\nφ∈Σ(ψ):prog(s,φ)=Ψ\nP(s|s′, a)xs′,φ,a\nout(s, φ) ≡ P\na∈A(s) xs,φ,a\nrec(s, φ) ≡ P\nΨ:φ∈D(Ψ)\nIΨ,φ\n|Ψ| in(s, Ψ)\nThe function in() represents the ﬂow into states with CNF\nmodes, rec() represents ﬂow received from these to X-literal\nmodes, and out() represents the ﬂow leaving one of these\nstate-X-literal pairs. Note that rec() represents both Eqs. (1)\nand (2) by splitting ﬂow evenly between clauses and subsequently duplicating ﬂow to each X-literal. See Fig. 1 for an\nexample of this ﬂow redistribution.\nFlow in this network sinks at pairs ⟨s,Ψ⟩ not only where\ns∈G but also if Ψ=⊤ or Ψ=⊥, as there is no decomposition\nfor ⊤ and ⊥. Let the set of these sinks be F. We maximise\nﬂow into the accepting sinks, i.e., ⟨s,Ψ⟩∈T or Ψ=⊤. Let\nthe set of these pairs be ˆG⊂F. The amount of ﬂow reaching\nthe accepting sinks is represented by the variable sinkacc.\nThe following linear constraints deﬁne the decomposition\nﬂow network, using only O(|S × Σ(ψ) × A |) variables.\nmax\nsinkacc\n(LP2)\ns.t.\nxs,φ,a ≥ 0\n∀s ∈ S, φ ∈ Σ(ψ), a ∈ A(s)\n(C8)\nsinkacc ≤ 1\n(C9)\nout(s, φ)−rec(s, φ) ≤\nX\nΨ:⟨s,Ψ⟩∈F\nP(s, Ψ)× IΨ,φ\n|Ψ|\n∀s∈S\\G,\nφ∈Σ(ψ) (C10)\nsinkacc −\nX\n⟨s,Ψ⟩∈F∩ˆG\nP(s, Ψ) =\nX\n⟨s,Ψ⟩∈ˆG\nin(s, Ψ)\n(C11)\nHere C10 requires that ﬂow leaving a state must enter it,\nand the right hand side allows for ﬂow to be sourced from\nfringe states. Note that the right hand side will be 0 when\nthere is no fringe state feeding into the decomposed state/XA\nA1\nB\nB1\nB2\nC\nC1\nC2\nC3\nFigure 2: An example of ﬂow synchronised between both\nnetworks. The blue/red labels above/below transitions show\nﬂow in the primary/secondary network.\nliteral pair, i.e., ∄⟨s,Ψ⟩ ∈ F with φ ∈ D(Ψ). This constraint\nis an inequality so that ﬂow can be leaked from anywhere in\nthe network, a technique previously used to solve SSPs with\ndead ends in (Trevizan, Teichteil-K¨onigsbuch, and Thi´ebaux\n2017). C11 deﬁnes sinkacc, taking into account fringe states\nwhich project to the goal.",
        "integration with pltl-dual": "The state-of-the-art heuristic search algorithm for solving\nMO-PLTL SSPs is PLTL-dual (Baumgartner, Thi´ebaux, and\nTrevizan 2018). We integrate both hpom\nψ\nand hdec\nψ with PLTLdual to evaluate their performance. PLTL-dual interfaces\nwith heuristics in a unique way, iteratively solving LPs representing progressively larger subsets of the state space, and\nincluding the heuristic computation in this same LP. In this\nway, the heuristic is computed for all fringe states at once,\nsimultaneously with ﬁnding the shortest path.\nThe vast majority of the information extracted from heuristics in PLTL-dual is via tying constraints. For estimating\nthe cost, PLTL-dual maintains an instance of hpom (Trevizan,\nThi´ebaux, and Haslum 2017), and this is tied to the heuristic\nfor each constraint, so actions necessary to satisfy a constraint\nare used in the cost heuristic also.\nWhile hpom\nψ\ncan be tied directly with PLTL-dual, tying hdec\nψ\nﬁrst requires the introduction of the concept of ﬂow retracing.\n5.1\nFlow Retracing\nTo over-estimate the probability, the network in the previous\nsection (which we call the primary network) ﬁnds a probabilistic path through a relaxation of MO-PLTL SSP dynamics.\nBecause of ﬂow duplication, occupation measures in the primary network don’t correspond to the original dynamics,\ncompromising the use of tying constraints. To bridge this\ngap, we introduce a secondary network which retraces ﬂow\nthat reached accepting states without duplication. This can\nbe thought of as ﬁnding a probability distribution on paths\nthrough the primary network to accepting states, as illustrated\nin Fig. 2. The secondary network is represented by further\nconstraints which ﬁnd this probability distribution concurrently with the optimisation of the primary network. It is this\nsecondary network that we tie to hpom in PLTL-dual.\nFig. 2 illustrates the secondary network tracing ﬂow\nthrough the primary network. Since there is no accepting\npath from state B to the goal, primary ﬂow into it is leaked\nafter being redistributed. The probabilistic action out of A1\nlets only 0.5 units of ﬂow into state C, all of which eventually\nreaches accepting goal states. The ﬂow entering C1 may be\nleaked even if there is a path to an accepting goal state, as the\nsecondary network ﬂow is bottlenecked by the 0.5 units of\nprimary ﬂow from A1 to C. As such, the secondary network\ntraces only 0.5 units of ﬂow through the primary network.\nThe secondary network uses an all-outcomes determinisation (Yoon, Fern, and Givan 2007), allowing ﬂow to “choose”\nwhich outcome of an action it follows, and which X-literal\nto be distributed to. The variables in the secondary network\nare occupation measures for both states with CNF modes and\nthose with X-literals. These are denoted ys,Ψ,φ and ys,φ,a,s′\nrespectively. The ﬁrst represents ﬂow being redistributed in\nthe state s from the mode Ψ to φ, the second represents ﬂow\nleaving s with φ via action a and reaching the state s′.\nFor convenience we deﬁne the following functions for the\nsecondary network. These are self-explanatory, but have a\nnotable distinction between X-literal modes and CNF modes,\nsimilarly to the primary network:\nin2(s, Ψ) =\nX\ns′∈S,φ∈Σ(ψ),a∈A(s′):\nΨ=prog(s,φ)∧P (s|s′,a)>0\nys′,φ,a,s\nout2(s, φ) =\nX\na∈A(s),s′∈S:\nP (s|s′,a)>0\nys,φ,a,s′\nin2(s, φ) =\nX\nΨ:φ∈D(Ψ)\nys,Ψ,φ\nout2(s, Ψ) =\nX\nφ∈D(Ψ)\nys,Ψ,φ\nThe secondary network retraces ﬂow reaching accepting\nstate pairs in ˆG. As not all the ﬂow reaching the fringe necessarily can leave the secondary network through these states,\nthe ﬂow entering it is similarly limited. We add the variables\nis,Ψ for each fringe state to represent the amount of ﬂow entering the secondary network from that state. The constraints\nfor ﬂow through the secondary network are:\nys,φ,a,s′ ≥ 0\n∀s, s′ ∈S, φ∈Σ(ψ), a∈A(s):P(s′|s, a)>0 (C12)\nys,Ψ,φ ≥ 0\n∀s ∈ S, Ψ ∈ 22Σ(ψ), φ ∈ D(Ψ) (C13)\n0 ≤ is,Ψ ≤ 1\n∀⟨s,Ψ⟩ ∈ F \\ F (C14)\nis,Ψ ≤ P(s, Ψ)\n∀⟨s,Ψ⟩ ∈ F \\ F (C15)\nout2(s, Ψ) − in2(s, Ψ) = is,Ψ\n∀⟨s,Ψ⟩ ∈ F \\ F (C16)\nout2(s, Ψ) − in2(s, Ψ) = 0\n∀⟨s,Ψ⟩∈S × 22Σ(ψ) \\(F∩F) (C17)\nout2(s, φ) − in2(s, φ) = 0\n∀⟨s,φ⟩ ∈ S × Σ(ψ) (C18)\nP\n⟨s,Ψ⟩∈ˆG in2(s, Ψ) = P\n⟨s,Ψ⟩∈F is,Ψ\n(C19)\nThe input variables is,Ψ must not exceed the actual ﬂow\nentering from the associated fringe state (C15), and the ﬂow\nentering the network must be the same as the ﬂow leaving it\n(C19). Similarly, the ﬂow entering each state must equal the\nﬂow leaving it (C17, C18), allowing for the ﬂow entering the\nnetwork at fringe states (C16). Note that for C13 and C17,\nonly the variables for the set of reachable CNF formulae need\nto be generated, rather than the full mode set 22Σ(ψ).\nAs well as secondary network’s constraints (C12-C19), we\nadd constraints between the two networks, upper-bounding\nthe secondary network relative to the primary network. Flow\nalong actions is restricted by C21, and ﬂow redistributed in\ndecomposition is upper bounded by C22 and C23, where\nthe right hand side of C22 takes into account ﬂow entering\nthe network from the fringe. C20 forces the ﬂow leaving the\nsecondary network to match the ﬂow leaving the primary\nnetwork. The combination of C11, C19 and C20 make the\nﬂow entering each network and leaving each network via\npairs in ˆG identical.\nP\n⟨s,Ψ⟩∈ˆG in(s, Ψ) = P\n⟨s,Ψ⟩∈ˆG in2(s, Ψ)\n(C20)\nys,φ,a,s′ −xs,φ,a×P(s′|s, a) ≤ 0\n∀ys,φ,a,s′ (C21)\nys,Ψ,φ−in(s, Ψ)× IΨ,φ\n|Ψ| ≤\nX\nΨ:⟨s,Ψ⟩∈F\nP(s, Ψ)× IΨ,φ\n|Ψ|\n∀⟨s,Ψ⟩ ∈ F,\nφ ∈ D(Ψ) (C22)\nys,Ψ,φ−in(s, Ψ)× IΨ,φ\n|Ψ| ≤ 0\n∀⟨s,Ψ⟩∈S×22Σ(ψ)\\F, φ∈D(Ψ) (C23)\n5.2\nTying Constraints\nAs mentioned above, PLTL heuristics in PLTL-dual are tied\nto the cost heuristic which consists of a set of projections,\none of each variable v, and ignores the PLTL constraints.\nThe cost heuristic can be tied to hpom\nψ\nusing constraints\nsimilar to C7. The distribution P is then deﬁned by the ﬂow\ninto the fringe states in PLTL-dual and the cost and PLTL\nheuristics are computed concurrently with the path optimisation.\nTying hdec\nψ to the cost heuristic is also relatively straightforward. Let xd,a be the occupation measure for the action a\nand the value d∈Dv of the projection onto some variable v\nused by the cost heuristic. Each projection in hdec\nψ is then tied\nthis the projection onto v using the tying constraints below:\nX\ns,s′∈S,φ∈Σ(ψ):\nP (s|s′,a)>0\nys,φ,a,s′ ≤\nX\nd∈Dv\nxd,a\n∀a ∈ A (C24)\nThe choice of v in C24 doesn’t matter, as all projections\nused by the cost heuristic are all tied together through equality\nconstraints and will yield the same result. Also, note that\nthe inequality in C24 forces actions necessary to satisfy the\nconstraint to be taken in each of the variable projections of\nthe cost heuristic, but not vice versa. This is because, the\nnetwork for each given PLTL constraint reaches its sink as\nsoon as an accepting state is reached for that constraint, rather\nthan continuing executing actions to reach the goal.\n5.3\nAdmissibility of hdec\nψ\nin PLTL-dual\nWe provide a sketch of the proof of the admissibility of hdec\nψ\nwhen integrated with PLTL-dual. Let ˆx be a solution to LP1,\ninducing a valid policy π and let z = Pr(ψ | s0, π). W.l.o.g.,\nwe show that hdec\nψ (⟨s0, Ψ0⟩) ≥ z (the case of other extended\nstates ⟨s, Ψ⟩ can be handled by using them as initial state and\nadjusting z appropriately). Also, let out(s, Ψ | π) be the ﬂow\nout of ⟨s, Ψ⟩ in our solution ˆx.\nSuppose\nfor\nthe\nsake\nof\ncontradiction\nthat\nhdec\nψ (⟨s0, Ψ0⟩) < z. The proof sketch is as follows: We\nﬁrst construct a solution to the secondary network from ˆx\nsatisfying C12-C19, then a solution for the primary network\nsatisfying C8-C11 and show that both networks satisfy\nconstraints C20-C23.\nTo construct the solution to the secondary network,\nlet Ts0,Ψ0,π\nbe the set of ﬁnite trajectories t\n=\n⟨s0, Ψ0⟩, ..., ⟨sn, Ψn⟩ from ⟨s0, Ψ0⟩ reachable under π, such\nthat t |= Ψ0. By the semantics of progression, for each trajectory t there exists at least one sequence of sets of LTL\nformulae St\n0, ..., St\nn (called satisfying assignments) such that\nSt\ni ⊆ D(Ψi),\n∀Φ ∈ Ψi, St\ni ∩ Φ ̸= ∅,\n∀φt\nij ∈ St\ni, ∀Φk ∈ prog(φt\nij, si+1), |St\ni+1 ∩ Φk| = 1 and\n∀φt\nij ∈ St\ni, t[i...n] |= φt\nij\nwhere t[i...n] is the subsequence of t starting at index i.\nThese sets represent, at each step, the literals in the formula\nwhich will be satisﬁed by the remainder of the trajectory. We\ncan iteratively assign each φt\nij several weights wt\nijk, where\nwt\n0j0 =\nIΨ0,φt\n0j\n|Ψ0| , and wt\nijk =\nIprog(φt\n(i−1)k,st\ni),φt\nij\n|prog(φt\n(i−1)k,st\ni)| · P\nl wt\n(i−1)kl.\nThese weights have the property that ∀i P\nj,k wt\nijk = 1.\nWe deﬁne all ys,φ,a,s′ and ys0,Ψ0,φ as follows:\nys,φ,a,s′ :=\nX\nt∈Ts0,Ψ0,π\nPr(t) ·\n\u0010\nX\ni,j,k s.t. st\ni=s,\nφt\nij=φ,at\ni=a,st\ni+1=s′\nwt\nijk\n\u0011\nys0,Ψ0,φ :=\nX\nt∈Ts0,Ψ0,π\nPr(t) ·\n\u0010\nX\nj s.t. φ=φt\n0j\nwt\n0j0\n\u0011\nand all other ys,Ψ,φ as:\nys,Ψ,φ :=\nX\nt∈Ts0,Ψ0,π\nPr(t) ·\n\u0010\nX\ni,j,k s.t. st\ni=s,\nprog(φt\n(i−1)k,st\ni)=Ψ,φt\nij=φ\nwt\nijk\n\u0011\nUnder this construction, as the weights at each step sum\nto 1, then the ﬂow entering accepting goal states in the secondary network is P\nt∈Ts0,Ψ0,π Pr(t) = z.\nFor the primary network, we construct a solution from\nthe trajectories T ′\ns0,Ψ0,π, which is the set of trajectories induced by π, not conditioned on satisfying Ψ0. For a trajectory t, for each φt\nij ∈ D(Ψt\ni) we assign a weight vt\nij.\nNote that unlike weights wt\nijk, these are not limited to a\nset of satisfying assignments. We assign vt\n0j =\nIΨ0,φt\n0j\n|Ψ0| , and\nvt\nij = P\nk\nIprog(φt\n(i−1)k,st\ni),φt\nij\n|prog(φt\n(i−1)k,st\ni)| ·vt\n(i−1)k. These weights have the\nproperty that, for φij ∈ Si, vt\nij ≥ P\nk wt\nijk. We then assign\nxs,φ,a :=\nX\nt∈T ′\ns0,Ψ0,π\nPr(t) ·\n\u0010\nX\ni,j s.t. st\ni=s,\nφt\nij=φ,at\ni=a\nvt\nij\n\u0011\nwith an exception for xs,φ,ag which route ﬂow into accepting\nstates. These variables are set to\nxs,φ,ag :=\nys,φ,ag,g\nPr(g | s, ag)\nwhere g is a goal state. Due to this, the primary network has\nexactly z units of ﬂow entering accepting states.\nNote that tying constraints are consistent with this construction when π is projected onto other cost constraints.\nNow that we have a valid ﬂow for both primary and secondary networks, we need to show that they satisfy constraints C20–C23, i.e., the primary network upper-bounds\nthe secondary network. Consider an action a from a state\n⟨s, φ⟩. By construction, outcomes of a must be taken proportionately to P(s′ | s, a) by the secondary network except\nwhen the associated trajectories are not in Ts0,Ψ0,π. All trajectories in T ′\ns0,Ψ0,π are included in the primary network,\nhence satisfying the upper bound in C21. From this we\nhave in(s, Ψ) ≥ in2(s, Ψ), and as weights are distributed\nequivalently in both the primary and secondary network,\nconstraints C22 and C23 are also satisﬁed. Lastly, C20 is\ntrue by construction, as both networks admit exactly z ﬂow\ninto accepting states which contradicts our assumption that\nhdec\nψ (⟨s0, Ψ0⟩) < z; therefore hdec\nψ is admissible.",
        "experimental results": "The heuristics were evaluated in comparison with the NBA\nheuristic hBA (Baumgartner, Thi´ebaux, and Trevizan 2018)\nand the trivial heuristic (h(P) = 1), as well as PRISM. As\nthe results for the 2019 Comparison of Tools for the Analysis\nof Quantitative Formal Models (Hahn et al. 2019) shows,\nPRISM is still one of the fastest model checkers available.\nWe used the default options of PRISM and the “-lp” ﬂag to\nuse their LP approach to MO-PLTL SSPs because, without\nthis ﬂag, PRISM was unable to solve any of our benchmarks.\nThe experiments were ran on an Intel i7-7700@3.6GHz using\nGurobi 8.1.1 on a single thread and a 20mins and 4Gb cutoff.\nFor evaluation, we use the Factory and Wall-e domains\nfrom (Baumgartner, Thi´ebaux, and Trevizan 2018), and a\nnew domain called Priority Search. All domains include at\nleast one non co-safe and one non-safe formula. The problems in our experiments are represented in PPDDL (Younes\net al. 2005) and translated to probabilistic SAS+ using Fast\nDownward (Helmert 2006). For PRISM, the probabilistic\nSAS+ problems are translated to PRISM’s multi-valued language. Both PPDDL and PRISM versions of the problems are\navailable at https://gitlab.com/fwt/mo-pltl-ssps-benchmarks.\nThe Priority Search domain is based on the Search and Rescue (SAR) domain (Trevizan, Thi´ebaux, and Haslum 2017)\nand, in both domains, the agent controls a robot locating\nmissing victims of a disaster in an n × n grid. Each position is randomly initialised as possibly containing a victim\nwith probability p. As in the SAR domain, the actions represent the agent moving in the environment and exploring\nthe current position; however, the fuel resource from SAR is\nomitted to avoid confounding effects in the experiment since\nhpom can handle them with ease which could be seen as an\nunfair advantage against the baselines. The constraints in our\ndomain are also different from SAR which does not have any\nPLTL constraints. The ﬁrst constraint requires that eventually\nall unknown locations must be searched. A contiguous line of\nn − 1 locations is randomly initialised as the “danger zone”.\nThe second and third constraints are that locations in the\ndanger zone must be searched ﬁrst, and with probability at\nleast 0.8, the robot must not stay inside the danger zone for\nmore than 2 steps.\nThe results, as the aggregate of multiple runs for each parameterisation of each domain, are presented in Fig. 3 for\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n100\n101\n102\n103\nWall-e (n)\nprism\ntrivial\nnba100\ndecomp\nsplit\n2-1\n3-0\n3-1\n3-2\n4-0\n4-1\n4-2\n4-3\n5-0\n5-1\n5-2\n5-3\n5-4\n6-0\n6-1\n6-2\n6-3\n6-4\n6-5\n7-0\n7-1\n7-2\n7-3\n7-4\n7-5\n7-6\n8-0\n8-1\n8-2\n8-3\n8-4\n8-5\n8-6\n8-7\n9-0\n9-1\n9-2\n9-3\n100\n102\nFactory (n-k)\nFigure 3: The solution time in seconds averaged over 10 runs for each problem.\nn\np\nPRISM\nTrivial\nHeuristic\nhBA\nhdec\nψ\nhdec\nψ -pos\nhpom\nψ\nhpom\nψ -pos\n4\n0.25\n100%\n100%\n75%\n100%\n100%\n100%\n100%\n1.72±0.03\n1.82±0.17\n652±158\n5.49±0.37\n6.69±1.35\n4.25±0.30\n4.15±0.82\n0.50\n100%\n100%\n100%\n100%\n100%\n100%\n100%\n57.6±1.94\n74.1±24.9\n272±88.8\n167±53.8\n60.4±23.2\n141±50.6\n32.3±12.6\n0.75\n0%\n0%\n0%\n0%\n55%\n0%\n75%\nn.a.\nn.a.\nn.a.\nn.a.\n624±222\nn.a.\n265±143\n5\n0.25\n100%\n100%\n100%\n100%\n100%\n100%\n100%\n34.7±1.68\n46.0±130\n127±36.2\n128±30.0\n75.2±15.6\n91.6±23.3\n40.4±11.0\n0.50\n0%\n0%\n0%\n0%\n30%\n0%\n70%\nn.a.\nn.a.\nn.a.\nn.a.\n600±457\nn.a.\n330±147\n0.75\n0%\n0%\n0%\n0%\n0%\n0%\n15%\nn.a.\nn.a.\nn.a.\nn.a.\nn.a.\nn.a.\n503±1124\nTable 1: Results for Priority Search(n,p) problems averaged over 20 runs for each problem. The results are reported as “X Y±Z”\nwhere X is the percentage of problems successfully solved and Y and Z are the average and its 95% conf. interval for the cpu-time\nover solved problems. Best values for each problem is highlighted.\nWall-e and Factory, and in Table 1 for Priority Search. The\nplots show the average time over 10 runs to solve problems\n(with its 95% conﬁdence interval) in the Factory and Walle domains. The graphs use a log scale, and we omit points\nwhich had less than 100% coverage. Table 1 shows the results\nfor 6 parameterisations of the Priority Search domain with\n20 runs per entry. Since the algorithms considered compute\nthe optimal solution and do not rely on sampling, 10 runs\nis enough to account for minor sources of randomisation,\ne.g., tie breaking. The only exception is the random choice of\ncombinations used by hpom\nψ\n(Section 3.2); however, for both\nwall-e and factory domains, there is only one possible combination and, for priority search domain, the 20 runs account\nfor the multiple possible combinations. Table 2 presents relevant statistics of selected problems for each approach, e.g.,\nsize of the SSP, LPs, and automata, namely Deterministic\nRabin Automata (DRA) for PRISM and NBA for PLTL-dual\nwith the hBA heuristic.\nIn the Wall-e domain, either hpom\nψ\nor hdec\nψ dominates all the\nother planners for n>3 and they are at least one order of magnitude faster than the others planners for the largest problems\nsolved by them (n∈{6, 7}). hpom\nψ\nhas the best performance\nup to n=8 because it provides better guidance than the other\nheuristics: hpom\nψ\nexplored on average 31.3% (95%ci: ±0.02)\nand 8.2% (95%ci: ±0.03) of the states explored by hdec\nψ and\nhBA respectively for n≥4. However, hpom\nψ\nis unable to scale\nup as well as hdec\nψ . For instance, for n=7, hpom\nψ\nuses almost\nthe double of LP variables as hdec\nψ for heuristic encoding (see\nTable 2). This is because the Wall-e domain has at most 2\nstate variables in each constraint and, more importantly, only\na single state variable in its largest constraint, making the\nhpom\nψ\nconstraint relaxation ineffective. For n=16, hdec\nψ solved\n9 out of 10 runs in 18m40s and exceeded the 20mins cutoff\nin a single run while all runs exceeded the cutoff for n=17.\nFor Factory, hpom\nψ\ndominates all the other algorithms for\nanything but small problems and is 1 to 2 orders of magnitude\nfaster than the other planners for n∈{7,8} and all k. Moreover, hpom\nψ\nis the only planner able to scale up to n=8, k>4\nand n = 9. As in Wall-e, the dominance of hpom\nψ\nis because\nit is more informative than the other heuristics: it explored\non average 44.5% (95%ci: ±0.09) of the states explored by\nthe second best planner for n>4 and all k. This can also be\nobserved in the size of the LP solved using hpom\nψ , for instance,\nin problem 8-3, hpom\nψ\nuses 50% less LP variable to encode\nits heuristic than hdec\nψ and its last LP solved is 20% of the\nsize of hdec\nψ ’s last LP. Notably, it was slower to apply hdec\nψ\nthan the trivial heuristic in factory, which can be attributed to\ndecomposition not being informative enough to make up for\nWall-e (n)\nFactory (n-k)\nPriority Search (n, p)\nproblem\n5\n7\n15\n4-3\n8-3\n9-3\n4, 0.5\n5, 0.25\n5, 0.50\nSSP\nSize of S\n90\n182\n870\n256\n65K\n262K\n4K\n3K\n212K\nSize of A\n315\n671\n3K\n1K\n736K\n3M\n38K\n31K\n2M\nPRISM\nSize of largest DRA\n2,857\n–\n–\n29,979\n–\n–\n256\n128\n–\n# LP vars. used\n80,920\n–\n–\n3,572\n–\n–\n112,863\n92,467\n–\nhBA\nlargest NBA size\n48\n256\n–\n13\n137\n266\n512\n256\n–\n# LP vars. for heur.\n8,913\n3,498\n–\n418\n224\n248\n2,265\n3,921\n–\n# vars. used in last LP\n18,474\n33,337\n–\n1,552\n42,513\n–\n47,878\n43,885\n–\n% time solving LPs\n90.1%\n90.1%\n–\n41.9%\n96.5%\n–\n59.9%\n72.9%\n–\ntrivial h.\n# vars. used in last LP\n10,681\n33,492\n–\n1,070\n40,836\n–\n33,613\n31,125\n% time solving LPs\n82.3%\n90.4%\n–\n44.3%\n95.4%\n–\n83.1%\n77.4%\n–\nhdec\nψ\n# LP vars. for heur.\n5,454\n13,412\n116,444\n785\n4,683\n6,110\n9,264\n13,943\n23,822\n# vars. used in last LP\n9,533\n20,134\n145,071\n1,965\n36,891\n–\n27,457\n32,188\n68,421\n% time solving LPs\n70.9%\n74.6%\n81.4%\n58.5%\n96.7%\n–\n85.6%\n88.4%\n89.6%\nhpom\nψ\n# LP vars. for heur.\n5,230\n26,334\n–\n446\n2,022\n2,596\n15,447\n21,719\n40,834\n# vars. used in last LP\n7,376\n29,427\n–\n972\n7,383\n28,435\n28,879\n35,515\n78,110\n% time solving LPs\n19.3%\n30.7%\n–\n16.1%\n81.6%\n84.3%\n74.2%\n74.1%\n85.5%\nTable 2: Statistics for each approach in selected problems. For priority search, the statistic are for hdec\nψ -pos and hpom\nψ -pos.\nits computation time. For instance, hdec\nψ expanded only 8%\nfewer states than with the trivial heuristic on average, as the\nsingle directional tying constraints (C24) don’t capture the\nbidirectional relationship between machines and production.\nLastly, for the Priority Search domain, the difference between hdec\nψ and hpom\nψ\nis not statistically signiﬁcant and they\nwere dominated by PRISM for n = 4, p ∈ {0.25, 0.5} and\nn=5, p=0.25. hdec\nψ and hpom\nψ\nunderperform in this domain\nbecause the choice of variables made by the process in Section 3 is sub-optimal. This can be veriﬁed by manually including the robot’s position in the projections which we refer to\nas hpom\nψ -pos and hdec\nψ -pos. Considering hpom\nψ -pos and hdec\nψ -pos,\nwe have that hpom\nψ -pos is statistically tied with PRISM in the\nsmall problems and dominates all other planners for large\ninstances (n = 4, p = 0.75 and n = 5, p ≥ 0.5). While statistically tied on time for small problem, hpom\nψ -pos expanded\n15.1% (95%ci: ±0.06) of the states visited by PRISM on average, thus solving much smaller LPs, e.g., for n=5, p=0.25,\nits ﬁnal LP’s size is on average 39% of that of PRISM’s single LP. This advantage allows hpom\nψ -pos to scale up to larger\nproblems than PRISM. Moreover, hdec\nψ -pos is slower than\nPRISM for small problem but it is still capable to scale up to\nlarger problems than PRISM. We can also see that hpom\nψ -pos\nprovides better guidance than hdec\nψ -pos, for instance hpom\nψ -pos\nexpanded 77.6% (95%ci: ±5.65) of the states expanded by\nhdec\nψ -pos on problem n = 5, p = 0.25. Note however that\nPRISM is a more general tool that can solve a larger class of\nproblems than MO-PLTL SSPs.",
        "conclusion": "We presented new admissible heuristics for probabilistic planning with MO-PLTL constraints, and showed they compared\nfavourably to the only other heuristic available for these\nproblems (Baumgartner, Thi´ebaux, and Trevizan 2018). The\nstrength and novelty of our heuristics lie in principled ways\nof choosing sets of variables on which to project LTL formulae and relaxing the computation of their probabilities. These\ncontributions are enabled by progression, showing promise\nfor progression-based approaches to PLTL heuristics.\nIn related work, Lacerda et al. (2015) deﬁne a “task progression” metric that estimates the number of transitions\nrequired to reach an accepting state from a given state of a\nﬁnite automaton for a co-safe LTL formula. This is then used\nin a multi-objective MDP, to reward the extent to which an\nLTL formula that cannot be satisﬁed by any policy has progressed towards an accepting state. The metric could be seen\nas an admissible heuristic for co-safe LTL, which is however\nnot informed by the possible transitions of the environment.\nOutside of probabilistic planning, there is a body of work\non heuristics for deterministic planning with temporally extended goals and preferences expressed using LTL variants\n(Baier, Bacchus, and McIlraith 2009; Bienvenu, Fritz, and\nMcIlraith 2011). Among those, (Bienvenu, Fritz, and McIlraith 2011) uses progression to optimistically evaluate formulae at fringe states, assuming the part of the formula that\ncannot be idled to false in the current state is true. In addition\nto being designed for a different problem where satisfaction\nprobability needs to be evaluated, our projection heuristic\ncan be more informative, as it is only optimistic over a subset of the formula’s variables. The remainder of heuristic\nsearch approaches to planning with LTL constraints compiles\nLTL and ﬁnite LTL variants into various types of automata\nwhose description can directly be incorporated in the factored\nplanning problem descriptions. They are then handled using\nstandard heuristics that are not LTL-aware (Rintanen 2000;\nBaier and McIlraith 2006; Edelkamp 2006; Torres and Baier\n2015; Camacho et al. 2017).\nOur future work includes decreasing the large number of\nvariables introduced to retrace accepting ﬂow, since they are\ngreatly responsible for the overhead in hdec\nψ . We also plan to\nexperiment with techniques to split automata (Camacho et al.\n2018) to improve PLTL-dual and the heuristics. Finally, we\nwould like to extend the heuristic search approach to deal\nwith more expressive logics (Baumgartner, Thi´ebaux, and\nTrevizan 2017) and partially observable domains (Santana,\nThi´ebaux, and Williams 2016; Walraven and Spaan 2018).",
        "summary_en": "Probabilistic planning subject to multi-objective probabilistic temporal logic (PLTL) constraints models the problem of computing safe and robust behaviours for agents in stochastic environments. This paper presents novel admissible heuristics to guide the search for cost-optimal policies for these problems. These heuristics project and decompose LTL formulae obtained by progression to estimate the probability that an extension of a partial policy satisfies the constraints. Their computation with linear programming is integrated with the recent PLTL-dual heuristic search algorithm, enabling more aggressive pruning of regions violating the constraints. The paper's experiments show that they further widen the scalability gap between heuristic search and verification approaches to these planning problems.",
        "summary_zh": "这篇论文介绍了一种新颖的可接受启发式方法。受多目标概率时序逻辑（PLTL）约束的概率规划模拟了为随机环境中的代理计算安全和稳健行为的问题，本文提出的方法用于指导为这些问题寻找成本最优的策略。这些启发式预测并分解通过渐进式获得的LTL公式，以估计部分策略的扩展满足约束条件的概率。它们的线性规划计算与最新的PLTL双启发式搜索算法相结合，能够更积极地修剪违反约束条件的区域。实验结果表明，它们进一步拉大了启发式搜索和验证方法在这些规划问题上的可扩展性差距。"
    },
    {
        "title": "Lifelong Path Planning with Kinematic Constraints for Multi-Agent Pickup and Delivery",
        "abstract": "The Multi-Agent Pickup and Delivery (MAPD) problem models applications where a large number of agents attend to a stream of incoming pickup-and-delivery tasks. Token Passing (TP) is a recent MAPD algorithm that is efﬁcient and effective. We make TP even more efﬁcient and effective by using a novel combinatorial search algorithm, called Safe Interval Path Planning with Reservation Table (SIPPwRT), for single-agent path planning. SIPPwRT uses an advanced data structure that allows for fast updates and lookups of the current paths of all agents in an online setting. The resulting MAPD algorithm TP-SIPPwRT takes kinematic constraints of real robots into account directly during planning, computes continuous agent movements with given velocities that work on non-holonomic robots rather than discrete agent movements with uniform velocity, and is complete for wellformed MAPD instances. We demonstrate its beneﬁts for automated warehouses using both an agent simulator and a standard robot simulator. For example, we demonstrate that it can compute paths for hundreds of agents and thousands of tasks in seconds and is more efﬁcient and effective than existing MAPD algorithms that use a post-processing step to adapt their paths to continuous agent movements with given velocities.",
        "introduction": "In the Multi-Agent Pickup and Delivery (MAPD) problem\n(Ma et al. 2017), m agents a1 . . . am attend to a stream of incoming pickup-and-delivery tasks in a given 2-dimensional\n4-neighbor grid with blocked and unblocked cells of size\nL × L each. Agents have to avoid collisions with each other.\nA task τj is characterized by a pickup cell sj and a delivery\ncell gj. The task is inserted into the system at an unknown\ntime. The task set T contains the unassigned tasks in the system. An agent not executing a task is called a free agent. It\ncan be assigned any one task τj ∈ T at a time and then has\nto move from its current cell via cell sj to cell gj, implying\nthat it has to move an object from cell sj to cell gj and can\ncarry at most one object at a time. Once it arrives at cell sj, it\nstarts to execute task τj and is called a task agent. Later, once\nit arrives at cell gj, it has executed task τj and becomes a\nfree agent again. A MAPD instance is solved iff all tasks are\nexecuted in a bounded amount of time after they have been\ninserted into the system. The MAPD problem models applications such as warehouse robots that move shelves (Wurman, D’Andrea, and Mountz 2008), aircraft towing robots\nthat move planes (Morris et al. 2016), and ofﬁce delivery\nrobots that move packages (Veloso et al. 2015).\nMost MAPD algorithms solve the multi-agent pathﬁnding problem (Ma and Koenig 2017) in an inner loop. The\nmulti-agent pathﬁnding problem is to compute collision-free\npaths for multiple agents and is NP-hard to solve optimally\n(Yu and LaValle 2013b; Ma et al. 2016b). Ways of solving\nit (and its variants) include reductions to other well-studied\ncombinatorial problems (Yu and LaValle 2013a; Erdem et\nal. 2013; Surynek 2015) and dedicated algorithms based on\nsearch and other techniques (Silver 2005; Standley 2010;\nWang and Botea 2011; Luna and Bekris 2011; Sharon et\nal. 2013; Goldenberg et al. 2014; Wagner and Choset 2015;\nSharon et al. 2015; Cohen et al. 2016; Ma and Koenig 2016;\nMa, Kumar, and Koenig 2017; Nguyen et al. 2017). See (Ma\net al. 2016a; Felner et al. 2017) for complete surveys.\nToken Passing (TP) (Ma et al. 2017) is a recent MAPD\nalgorithm that is efﬁcient and effective. It assumes, like\nmany multi-agent pathﬁnding algorithms, discrete agent\nmovements in the main compass directions with uniform\nvelocity but can use MAPF-POST (H¨onig et al. 2016a;\n2016b) in a post-processing step to adapt its paths to continuous forward movements with given translational velocities\nand point turns with given rotational velocities. However,\nthe resulting paths might then not be effective since planning is oblivious to this transformation. TP needs to repeatedly plan time-minimal paths for agents that avoid collisions\nwith the paths of the other agents. We show how TP can be\nmade even more efﬁcient by using Safe Interval Path Planning with Reservation Table (SIPPwRT), our contribution\nto improve SIPP (Phillips and Likhachev 2011) for this and\nmany other applications. We also show how TP can be made\nmore general by letting SIPPwRT directly compute continuous forward movements and point turns with given velocities. The resulting MAPD algorithm TP-SIPPwRT guarantees a safety distance between agents and solves all wellformed MAPD instances.",
        "tp-sippwrt": "TP (Ma et al. 2017) is a recent MAPD algorithm that assumes discrete agent movements in the main compass directions with a uniform velocity of typically one cell per\ntime unit on a grid. It is similar to Cooperative A* (Silver\n2005) and can be generalized to a fully distributed MAPD\nalgorithm. We describe TP very brieﬂy but its important implication for this paper is that agents repeatedly plan paths\nfor themselves (in Steps TP1 and TP3 below), considering\nthe other agents as dynamic obstacles that follow their paths\nand with which collisions need to be avoided. The agents\nuse space-time A* for this single-agent path planning.\nA set of endpoints is any subset of cells that contains at\nleast all start cells of agents and all pickup and delivery cells\nof tasks. The pickup and delivery cells are called task endpoints. The other endpoints are called non-task endpoints. A\nMAPD instance is well-formed iff the number of tasks is ﬁnite, there are no fewer non-task endpoints than agents, and\nthere exists a path between any two endpoints that does not\npass through other endpoints (C´ap, Vokr´ınek, and Kleiner\n2015; Ma et al. 2017). TP solves all well-formed MAPD instances (Ma et al. 2017).\nTP operates as follows for a given set of endpoints: It uses\na token (a synchronized block of shared memory) that stores\nthe task set and the current paths, one for each agent. The\nsystem repeatedly updates the task set in the token to contain all unassigned tasks in the system and then sends the\ntoken to some agent that is currently not following a path.\nThe agent with the token considers all tasks in the task set\nwhose pickup and delivery cells are different from the end\ncells of all paths in the token. TP1: If such tasks exist, then\nthe agent assigns itself that task among these tasks whose\npickup cell it can arrive at the earliest, removes the task from\nthe task set, computes two time-minimal paths in the token,\none that moves the agent from its current cell to the pickup\ncell of the task and then one that moves the agent from the\npickup cell to the delivery cell of the task, concatenates the\ntwo paths into one path, and stores the resulting path. TP2:\nIf no such tasks exist and the agent is not in the delivery cell\nof any task in the task set, then it stores the empty path in\nthe token (to wait at its current cell). TP3: Otherwise, the\nagent computes and stores a time-minimal path in the token\nthat moves the agent from its current cell to some endpoint\nthat is different from both the delivery cells of all tasks in\nthe task set and from the end cells of all paths in the token.\n(This rule is necessary to avoid deadlocks.) Each path the\nagent computes has two properties: (1) It avoids collisions\nwith all other paths in the token; (2) No other paths in the\ntoken use its end cell after its end time. Finally, the agent\nreleases the token, follows its path, and waits at the end cell\nof the path.\nWe now show how TP can be made more general by replacing space-time A* with SIPPwRT, a version of SIPP that\ncomputes continuous forward movements and point turns\nwith given velocities rather than discrete agent movements\nin the main compass directions with uniform velocity. We\nmake some simplifying assumptions throughout this paper\neven though TP-SIPPwRT and SIPPwRT could easily be\ngeneralized beyond them, mostly because these assumptions\nare necessary to compare TP-SIPPwRT against state-of-theart MAPD algorithms and, as a bonus, make it easier to explain TP-SIPPwRT: We assume that each agent ai is a disk\nwith radius Ri ≤ L/2 and use its center as its reference\npoint. The conﬁguration of an agent is a pair of its location\n(cell) and orientation (main compass direction). Agents always move from the center of their current unblocked cell\nto the center of an adjacent unblocked cell via the following available actions, besides waiting: a point turn π/2 rads\n(ninety degrees) in either clockwise or counterclockwise direction with a given rotational velocity and a forward movement to the center of the adjacent cell with a given translational velocity. The agents can accelerate and decelerate\ninﬁnitely fast. The paths of two agents are free of collisions\niff the interiors of the agent disks never intersect when they\nfollow their paths.",
        "sippwrt": "Space-time A* and SIPP are two versions of A* that both\nplan time-minimal paths for agents from their current cells to\ngiven goal cells, considering the other agents as dynamic obstacles that follow their paths and with which collisions need\nto be avoided. They both assume discrete agent movements\nin the main compass directions with a uniform velocity of\ntypically one cell per time unit on a grid. Space-time A* operates on pairs of cells and time steps, while SIPP groups\ncontiguous time steps during which a cell is not occupied\ninto safe (time) intervals for that cell and thus operates on\npairs of cells and safe intervals. This affords the A* search\nof SIPP pruning opportunities because it is always preferable for an agent to arrive at a cell earlier during the same\nsafe interval since it can then simply wait at the cell. Thus, if\nthe A* search of SIPP has already found a path that arrives\nat some cell at some time during some safe interval and then\ndiscovers a path that arrives at the same cell at a later time\nin the same safe interval, then it can prune the latter path\nwithout losing optimality. SIPP has already been used for\nrobotics applications (Narayanan, Phillips, and Likhachev\n2012; Yakovlev and Andreychuk 2017). We generalize it to\ncontinuous forward movements and point turns with given\nvelocities in the following, where a safe interval for a cell\nis now a maximal contiguous interval during which the cell\nis not occupied by dynamic obstacles. Since SIPPwRT, the\nresulting version of SIPP, is guaranteed to discover collisionfree paths (like space-time A*) when used as part of TP, TPSIPPwRT, the resulting version of TP, continues to solve all\nwell-formed MAPD instances.\nReservation Table and Safe Intervals\nSIPP represents the path of each dynamic obstacle as a\nchronologically ordered list of cells occupied by the dynamic obstacle, which is not efﬁcient since SIPP has to iterate through all these lists to calculate all safe intervals of\na given cell. On the other hand, space-time A* maintains a\nreservation table that is indexed by a cell and a time step,\nwhich allows for the efﬁcient calculation of all safe intervals\nof a given cell.\nSIPPwRT improves upon SIPP using a version of a reservation table that handles continuous agent movements with\ngiven velocities and is indexed by a cell. A reservation table entry of a given cell is a priority queue that contains all\nreserved intervals for that cell in increasing order of their\nlower bounds. A reserved interval for a cell is a maximal\ncontiguous interval during which the cell is occupied by\nsome dynamic obstacle. The reservation table allows SIPPwRT to implement all operations efﬁciently that are needed\nby TP-SIPPwRT, namely to (1) calculate all safe intervals of\na given cell; (2) add reservation table entries after a new path\nhas been calculated; and (3) delete reservation table entries\nthat refer to irrelevant times in the past in order to keep the\nreservation table small.\nFunction GetSafeIntervals. GetSafeIntervals(cell) returns\nall safe intervals for cell cell in increasing order of their\nlower bounds. The safe intervals for the cell are obtained as\nthe complements of the reserved intervals for the cell with\nrespect to interval [0, ∞]. For safe interval i = [i.lb, i.ub]\nand a dynamic obstacle departing from cell cell at time i.lb,\ndep cfg[cell, i] is the conﬁguration of the dynamic obstacle\nat time i.lb. It is NULL iff i.lb ≤ current t. Similarly, for\nsafe interval i = [i.lb, i.ub] and the dynamic obstacle arriving at cell cell at time i.ub, arr cfg[cell, i] is the conﬁguration of the dynamic obstacle at time i.ub. It is NULL iff\ni.ub = ∞.\nTime Offsets\nThe safe intervals of a cell represent the times during which\nthe cell is not occupied. However, this does not mean that\nan agent can arrive at any of those times at the cell since\nthe agent might still collide with a dynamic obstacle that has\njust departed from the cell or is about to arrive at the cell.\nThus, the lower and upper bounds of a safe interval have to\nbe tightened using the following time offsets.\nFunction Offset(cfg1, cfg2) returns the time offset ∆T that\nexpresses the minimum amount of time the center of some\nunknown agent a1 with safety radius R1 and translational\nvelocity vtrans,1 needs to depart from the center of a cell\ncfg1.cell = l with conﬁguration cfg1 before the center of\nsome unknown agent a2 with safety radius R2 and translational velocity vtrans,2, coming from some cell l′, arrives at\nthe center of the same cell cfg2.cell = l with conﬁguration\ncfg2 to avoid a collision. The time offset is zero iff either\ncfg1 = NULL or cfg2 = NULL, meaning that either agent\na1 or agent a2 does not exist. The calculation of the time\noffset requires only knowledge of the conﬁgurations, safety\nradii, and velocities of both agents.1\nAssume that agent a2 departs from cell l′ at time 0 (and\nthus arrives at cell l at time t′ =\nL\nvtrans,2 ) and agent a1 departs\nfrom cell l at time td ≤\nL\nvtrans,2 . D(t) is the distance between\nthe agents, where t is the amount of time elapsed after agent\na2 departs from cell l′. It must hold that D(t) ≥ R1 + R2 to\navoid that the two agents collide. We distinguish three cases\nto calculate the time offset ∆T:\n1In the pseudocode of SIPPwRT, we show how to keep track of\nthe conﬁgurations but do not include the safety radii and velocities\nin the conﬁgurations for ease of readability (although this needs to\nbe done in case they are not the same for all agents and times).\nl′\nl\na1\nR1\nvtrans,1\na2\nR2\nvtrans,2\nL\nL = vtrans,2t\nvtrans,1(t − td)\nL − vtrans,2t\nFigure 1: Left: Two agents move in the same direction. Middle: D is at its minimum for the vtrans,1 < vtrans,2 case. Right:\nD is at its minimum for the vtrans,1 ≥ vtrans,2 case.\nl\nl′\na1\nR1\nvtrans,1\na2\nR2\nvtrans,2\nL\nD\nvtrans,1(t − td)\nL − vtrans,2t\nFigure 2: Left: Two agents move in orthogonal directions.\nRight: D is at its minimum.\n(a) Same Direction.\nBoth agents move in the same direction (meaning that the orientations of conﬁgurations cfg1\nand cfg2 are equal), see Figure 1 (left), where gray lines connect the centers of cells. In this case, D(t) = L − vtrans,2t +\nvtrans,1(t − td). We now distinguish two sub-cases to show\nthat the time offset is ∆T =\nR1+R2\nmin(vtrans,1,vtrans,2).\n(a1) Case vtrans,1 < vtrans,2. This case is shown in Figure\n1 (middle). D(t) decreases as the time t increases. D(t) thus\nreaches its minimum at the time t = t′′ =\nL\nvtrans,2 when agent\na2 arrives at cell l. Substituting t =\nL\nvtrans,2 back into D(t) ≥\nR1 + R2, we have\nD(t) = L − vtrans,2t + vtrans,1(t − td)\n= vtrans,1(\nL\nvtrans,2\n− td) ≥ R1 + R2.\nTherefore, td ≤\nL\nvtrans,2 − R1+R2\nvtrans,1 . The time offset ∆T is thus\n∆T = t′ − max td =\nL\nvtrans,2 − (\nL\nvtrans,2 − R1+R2\nvtrans,1 ) = R1+R2\nvtrans,1 .\n(a2) Case vtrans,1 ≥ vtrans,2. This case is shown in Figure\n1 (right). D(t) decreases before agent a1 starts to move and\nthen increases as the time t increases. D(t) thus reaches its\nminimum at the time t = td when agent a1 starts to move.\nSubstituting t = td back into D(t) ≥ R1 + R2, we have\nD(t) = L − vtrans,2t + vtrans,1(t − td) = L − vtrans,2td\n≥ R1 + R2.\nTherefore, td ≤ L−(R1+R2)\nvtrans,2\n. The time offset is thus ∆T =\nt′ − max td =\nL\nvtrans,2 − L−(R1+R2)\nvtrans,2\n= R1+R2\nvtrans,2 .\n(b) Orthogonal Directions.\nBoth agents move in orthogonal directions, see Figure 2. In this case, D(t)\n=\np\n(vtrans,1(t − td))2 + (L − vtrans,2t)2. We determine the\ntime t at which D(t) ≥ 0 reaches its minimum by solving\n∂D2\n∂t\n= 0. Substituting the result t =\nv2\ntrans,1td+vtrans,2L\nv2\ntrans,1+v2\ntrans,2\ninto\nD2 ≥ (R1 + R2)2, we have\nD2(t) = (L − vtrans,2t)2 + (vtrans,1(t − td))2\n= (vtrans,1(vtrans,2td − L))2\n(v2\ntrans,1 + v2\ntrans,2)\n≥ (R1 + R2)2.\nSince L\n≥\nvtrans,2td, we have vtrans,1(L − vtrans,2td)\n≥\nq\nv2\ntrans,1 + v2\ntrans,2(R1\n+\nR2).\nTherefore,\ntd\n≤\nvtrans,1L−√\nv2\ntrans,1+v2\ntrans,2(R1+R2)\nvtrans,1vtrans,2\n. The time offset is thus ∆T\n= t′ − max td =\nL\nvtrans,2 −\nvtrans,1L−√\nv2\ntrans,1+v2\ntrans,2(R1+R2)\nvtrans,1vtrans,2\n=\n√\nv2\ntrans,1+v2\ntrans,2(R1+R2)\nvtrans,1vtrans,2\n.\n(c) Opposite Directions.\nBoth agents move in opposite\ndirections, that is, agent a1 moves from cell l to cell l′\nand agent a2 moves from cell l′ to cell l. The time offset is set to allow agent a1 to arrive at cell l′ even before\nagent a2 departs from cell l′. In this case, the time offset is\n∆T =\nL\nvtrans,1 +\nL\nvtrans,2 , which is the sum of the times that agent\na1 needs to move from cell l to cell l′ and that agent a2 needs\nto move from cell l′ to cell l. We later show that SIPPwRT\navoids collisions when it uses all bounds simultaneously.\nIncreased/Decreased Bounds\nThe algorithm calls the following functions to tighten the\nlower and upper bounds of safe interval i during which an\nagent can stay at cell l = cfg.cell safely.\nThe algorithm calls Function GetLB1(cfg, i) for an agent\na2 to return maxj(j.lb + Offset(dep cfg[cfg.cell, j], cfg)).\nHere, j.lb + Offset(dep cfg[cfg.cell, j], cfg) is the increased\nlower\nbound\nfor\neach\nsafe\ninterval\nj\nin\nGetSafeIntervals(cfg.cell) with j.lb\n≤\ni.lb. For agent\na2 that arrives at cell l = cfg.cell from another cell l′ with\nconﬁguration cfg, the idea is to prevent it from colliding\nwith any dynamic obstacle a1 that departs from cell l with\nconﬁguration dep cfg[cfg.cell, j] before a2 arrives at cell l.\nThe algorithm calls Function GetUB1(cfg, i) for an agent\na1 to return minj(j.ub − Offset(cfg, arr cfg[cfg.cell, j])).\nHere,\nj.ub\n−\nOffset(cfg, arr cfg[cfg.cell, j])\nis\nthe\ndecreased upper bound for each safe interval j\nin\nGetSafeIntervals(cfg.cell) with j.ub\n≥\ni.ub. For agent\na1 that departs from cell l = cfg.cell with conﬁguration cfg,\nthe idea is to prevent it from colliding with any dynamic\nobstacle a2 that arrives at cell l from another cell l′ with\nconﬁguration arr cfg[cfg.cell, j] after a1 departs from cell l.\nThe algorithm calls Function GetLB2(cfg, i) for an agent\na1 to return maxj(j.lb +\nL\nv′\ntrans −\nL\nvtrans ). Here, j.lb +\nL\nv′\ntrans −\nL\nvtrans is the increased lower bound for each safe interval j in GetSafeIntervals(cfg.cell) where the orientation of\ndep cfg[cfg.cell, j] is the same as that of cfg and j.lb ≤ i.lb.\nFor agent a1 that departs from cell l = cfg.cell with conﬁguration cfg and translational velocity vtrans and moves also\ntoward cell l′, the idea is to prevent it from arriving at cell\nl′ earlier than (and thus “passing through”) any dynamic obstacle a2 that departs from cell l before agent a1 with conﬁguration dep cfg[cfg.cell, j] and translational velocity v′\ntrans\nand moves also toward cell l′.\nThe algorithm calls Function GetUB2(cfg, i) for an agent\na1 to return minj(j.lb +\nL\nv′\ntrans −\nL\nvtrans ). Here, j.lb +\nL\nv′\ntrans −\nL\nvtrans is the decreased upper bound for each safe interval j in GetSafeIntervals(cfg.cell) where the orientation of\ndep cfg[cfg.cell, j] is the same as that of cfg and j.lb ≥ i.ub.\nFor agent a1 that departs from cell l = cfg.cell with conﬁguration cfg and translational velocity vtrans and moves also\ntoward cell l′, the idea is to prevent it from arriving at cell l′\nlater than (and thus “being passed through” by) any dynamic\nobstacle a2 that departs from cell l after agent a1 with conﬁguration dep cfg[cfg.cell, j] and translational velocity v′\ntrans\nand moves toward cell l′.\nAdmissible H-Values\nStep TP1 of TP-SIPPwRT requires an agent to use SIPPwRT twice, namely (1) to plan a time-minimal path from its\ncurrent conﬁguration to a candidate set of endpoints (pickup\ncells) and (2) to plan a time-minimal path from the resulting conﬁguration to a particular endpoint (a delivery cell).\nStep TP3 requires the agent to use SIPPwRT once to plan a\ntime-minimal path from its current conﬁguration to a candidate set of endpoints (to avoid deadlocks). The agent always\nmoves along each path with given (ﬁxed) translational and\nrotational velocities (unless it waits). Thus, SIPPwRT has to\nplan only paths to a given set G of one or more endpoints.\nBy ignoring the dynamic obstacles, we determine the admissible h-values needed for the A* search of SIPPwRT to plan\ntime-minimal paths as follows: We calculate a time-minimal\npath that excludes waiting for the agent from each conﬁguration cfg to each conﬁguration cfg′ whose cell is an endpoint\n(by searching backward once from each conﬁguration cfg′).\nWe then use the minimum heuristic (Stern, Goldenberg,\nand Felner 2017) h(cfg, G) = mincfg′.cell∈G h(cfg, cfg′) as\nadmissible h-value of conﬁguration cfg, where h(cfg, cfg′)\nis the calculated time of the time-minimal path from cfg\nto cfg′. In practice, if set G is large and endpoints are\ndensely distributed across the grid, it is more efﬁcient to\nuse h(cfg, G) = 0 (as we do for Step TP3 of TP-SIPPwRT)\nsince it can be calculated faster even though SIPPwRT might\nexpand more nodes.\nPseudocode\nAlgorithm 1 shows the pseudocode of SIPPwRT, which\nplans a time-minimal path for an agent with translational\nvelocity vtrans and rotational velocity vrot from its conﬁguration start cfg at time current t to a cell in set G. SIPPwRT\nperforms a regular A* search with nodes that are pairs of\nconﬁgurations of the agent and safe intervals. The g-value\ng[n] of a node n = ⟨n.cfg, n.int⟩ with conﬁguration n.cfg\nand safe interval n.int = [n.int.lb, n.int.ub] is the earliest\ndiscovered time in n.int when the agent can be in conﬁguration n.cfg. The start node is n = ⟨start cfg, [current t, ∞]⟩\nwith g[n] = current t. The safe interval n.int of the start\nnode expresses that the agent can wait forever in its current\nconﬁguration. A node n is a goal node iff the cell of its conﬁguration is in set G and the agent can wait forever in its\nconﬁguration (n.int.ub = ∞).\nIn our implementation of SIPPwRT, each action is a turnand-move action, i.e., a point turn into one of the four compass directions followed by a wait (when necessary) and\nthen a forward movement to a neighboring unblocked cell.\nSince only forward movements deﬁne the temporal constraints between safe intervals of neighboring cells, the state\nAlgorithm 1: SIPPwRT.\n1 Function SIPPwRT(start cfg, G, current t, vtrans, vrot)\n2\nnstart ← NewNode(⟨start cfg, [current t, ∞]⟩);\n3\ng[nstart] ← current t;\n4\nOPEN ← {nstart};\n5\nwhile OPEN ̸= ∅ do\n6\nn ← arg minn′∈OPEN(g[n′] + h(n′.cfg, G));\n7\nOPEN ← OPEN \\ {n};\n8\nif n.cell ∈ G and n.int.ub = ∞ then\n9\nreturn path from start cfg to n.cfg;\n10\nsuccessors ← GetSuccessors(n);\n11\nforeach n′ ∈ successors do\n12\nif g[n′] is undeﬁned then\n13\ng[n′] ← ∞;\n14\nif g[n′] > g[n] + cost[n, n′] then\n15\nparent[n′] ← n;\n16\ng[n′] ← g[n] + cost[n, n′];\n17\nif n′ /∈ OPEN then\n18\nOPEN ← OPEN ∪ {n′};\n19\nreturn no path exists (does not happen for well-formed MAPD instances);\n20 Function GetSuccessors(n)\n21\nsuccessors ← ∅;\n22\nforeach legal turn-and-move action in n do\n23\ncfg t ← conﬁguration resulting from executing the point turn of action in\nn.cfg (cfg t.cell = n.cfg.cell);\n24\nub1 ← GetUB1(cfg t, n.int);\n25\nlb2 ← GetLB2(cfg t, n.int);\n26\nub2 ← GetUB2(cfg t, n.int);\n27\nlb ← max((g[n] + ∆tturn(action, vrot)), lb2);\n28\nub ← min(ub1, ub2);\n29\nif lb ≤ ub then\n30\ncfg′ ← conﬁguration resulting from executing the forward movement in\naction in n.cfg t;\n31\ni′.lb ← lb + ∆tmove(action, vtrans);\n32\ni′.ub ← ub + ∆tmove(action, vtrans);\n33\nsafeIntervals ← GetSafeIntervals(cfg′.cell);\n34\nforeach i′′ ∈ safeIntervals do\n35\nlb1 ← GetLB1(cfg′, i′′);\n36\nif [lb1, i′′.ub] ∩ i′ ̸= ∅ then\n37\nt′ ← max(i′.lb, lb1);\n38\nn′ ← NewNode(⟨cfg′, i′′⟩);\n39\ncost[n, n′] ← t′ − g[n];\n40\nsuccessors ← successors ∪ {n′};\n41\nreturn successors;\nspace of our search remains unaffected by the use of turnand-move actions instead of separate point turn, move, and\nwait actions independently.\nFunction GetSuccessors. GetSuccessors(n) calculates the\nsuccessors of node n by considering all legal turn-and-move\nactions action available to the agent in conﬁguration n.cfg\n[Line 22]. Assume that executing the point turn of action\naction takes ∆tturn(action, vrot) time units and results in conﬁguration cfg t with which the agent departs from its current\ncell [Line 23]. The agent must depart from its current cell no\nlater than lb and no earlier than ub to avoid colliding with\ndynamic obstacles that also visit its current cell [Lines 2428]. If the agent can depart from its current cell [Line 29],\nthen assume that executing the forward movement of action action in conﬁguration cfg t takes ∆tmove(action, vtrans)\ntime units and results in successor conﬁguration cfg′ [Line\n30]. The agent waits an appropriate amount of time in conﬁguration cfg t after the point turn, then executes the forward movement, and arrives in conﬁguration cfg′ in interval\ni′ = [lb + ∆tmove(action, vtrans), ub + ∆tmove(action, vtrans)]\n[Lines 31-32]. The successors of node n are generated by\nFigure 3: Left: Small simulated warehouse environment.\nRight: Large simulated warehouse environment.\nTable 1: Experiment 1. (Inapplicable entries are dashed.)\nalgorithm\nvtask\ndiscrete\nsrvc time\ndiscrete\nmakespan\nsrvc\ntime\nmake\nspan\nplan\ntime\npost-proc\ntime\nthpt\nstdy\nthpt\nTP-SIPPwRT 0.50\n–\n–\n944.03 2,475.58\n0.90\n–\n0.397 0.433\n0.75\n601.69 1,755.22\n0.92\n0.552 0.632\n1.00\n435.26 1,392.00\n0.83\n0.689 0.782\nCENTRAL\n0.50\n325.28\n1,163\n1,049.51 2,617.00 1,161.44\n264.66\n0.370 0.406\n0.75\n691.90 1,895.68\n254.36\n0.504 0.552\n1.00\n520.36 1,553.00\n269.91\n0.609 0.670\nTP-A*\n0.50\n329.83\n1,204\n1,026.23 2,628.22\n1.00\n267.38\n0.373 0.408\n0.75\n675.65 1,909.45\n295.54\n0.508 0.558\n1.00\n505.81 1,570.77\n278.74\n0.609 0.683\nprocessing all safe intervals i′′ = [i′′.lb, i′′.ub] for the new\ncell cfg′.cell of the agent [Lines 33-34]. The lower bound of\nsafe interval i′′ is increased from i′′.lb to lb1 to ensure that\nthe agent can arrive at its new cell without colliding with\ndynamic obstacles that also visit its new cell [Line 35]. The\nupdated safe interval [lb1, i′′.ub] is intersected with interval i′ [Line 36]. If their intersection is non-empty, then the\nagent can arrive at its successor conﬁguration during safe interval i′′. Only the earliest time t′ in the intersection needs\nto be considered (since the agent can simply wait in its successor conﬁguration and the later times in the intersection\ncan thus be pruned, as argued earlier) [Line 37]. The resulting successor of node n is n′ = ⟨cfg′, i′′⟩ [Line 38], and\nthe cost (here: time) of the transition from node n to node\nn′ is cost[n, n′] = t′ − g[n] [Line 39] (consisting of executing the point turn of action action for ∆tturn(action, vrot)\ntime units, waiting for t′ − g[n] − ∆tturn(action, vrot) −\n∆tmove(action, vtrans) time units, and then executing the forward movement of action action for ∆tmove(action, vtrans)\ntime units), so that g[n′] = g[n] + cost[n, n′] = t′ is the\nearliest discovered time in n′.int = i′′ when the agent can\nbe in conﬁguration n′.cfg = cfg′.\nMain Routine. The main routine of SIPPwRT performs a\nregular A* search. It initializes the g-value of the start node\nand inserts the node into the OPEN list [Lines 2-4]. It then\nrepeatedly removes a node n with the smallest sum of gvalue and h-value g[n] + h(n.cfg, G) from the OPEN list\n[Lines 6-7] and processes it: If the node is a goal node, then it\nreturns the path found by following the parent pointers from\nthe node to the start node [Lines 8-9]. Otherwise, it generates\nthe successors of the node [Line 10]. For each successor, it\ninitializes its g-value to inﬁnity if the g-value is still undeﬁned [Lines 12-13]. It then checks whether the g-value of\nthe successor can be reduced by changing its parent pointer\nto node n [Line 14]. If so, it changes the parent pointer of the\nsuccessor, reduces its g-value, and inserts it into the OPEN\nlist (if necessary) [Lines 15-18].\nTheorem 1. The path returned by SIPPwRT from the start\nconﬁguration to a goal is free of collisions.\nWe prove Theorem 1 in the technical report.\nSince all heuristics used by SIPPwRT are admissible\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n200\n400\n600\n800 1000 1200 1400 1600 1800 2000 2200 2400 2600\nTP-SIPPwRT\nCENTRAL\nTP-A*\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\nTP-SIPPwRT\nCENTRAL\nTP-A*\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nTP-SIPPwRT\nCENTRAL\nTP-A*\nFigure 4: Number of tasks executed per second in a moving 100-second window (t − 100, t] (that is, throughput at time t) as a\nfunction of time t for different MAPD algorithms. Left: vtask = 0.50 m/s. Middle: vtask = 0.75 m/s. Right: vtask = 1.00 m/s.\nTable 2: Experiment 2.\nvtask\n0.50\n0.75\n1.00\nagts task\nfreq\nsrvc\ntime\nmake\nspan\nplan\ntime thpt\nsrvc\ntime\nmake\nspan\nplan\ntime thpt\nsrvc\ntime\nmake\nspan\nplan\ntime thpt\n10\n1\n2,809.72 6,771.00 0.84 0.146 1,834.97 4,764.28 0.86 0.213 1,357.21 3,818.00 0.72 0.270\n2\n3,029.59 6,759.41 0.85 0.157 2,077.68 4,768.89 0.84 0.215 1,584.62 3,784.00 0.73 0.274\n5\n3,181.97 6,789.41 0.86 0.155 2,185.29 4,748.33 0.86 0.225 1,710.76 3763.71 0.75 0.274\n10\n3,215.43 6,775.00 0.84 0.159 2,252.70 4,762.45 0.88 0.219 1,750.19 3,749.00 0.75 0.280\n20\n1\n1,228.35 3,557.58 0.90 0.295\n745.48 2,540.33 0.89 0.411\n502.50 2,000.71 0.76 0.511\n2\n1,450.40 3,503.00 0.89 0.298\n966.27 2,493.67 0.91 0.392\n714.20 1,980.00 0.79 0.489\n5\n1,591.79 3,519.83 0.89 0.292 1,088.86 2,481.85 0.88 0.416\n844.32 1,966.00 0.81 0.507\n10\n1,661.62 3,502.83 0.88 0.290 1,136.08 2,479.45 0.90 0.417\n892.22 1,964.00 0.81 0.507\n30\n1\n723.03 2,482.41 0.94 0.396\n389.15 1,763.50 0.91 0.551\n222.21 1,431.71 0.82 0.672\n2\n944.03 2,475.58 0.90 0.397\n601.69 1,755.22 0.92 0.552\n435.26 1,392.00 0.83 0.689\n5\n1,079.62 2,435.83 0.90 0.398\n728.33 1,724.18 0.92 0.555\n563.14 1,372.71 0.83 0.688\n10\n1,126.47 2,468.00 0.93 0.393\n779.67 1,737.00 0.92 0.550\n612.06 1,380.00 0.84 0.065\n40\n1\n484.93 2,023.58 0.90 0.484\n225.18 1,471.12 0.95 0.657\n101.16 1,252.00 0.85 0.765\n2\n701.23 1,945.00 0.94 0.503\n432.11 1,430.33 0.95 0.674\n298.04 1,122.71 0.89 0.847\n5\n830.73 2,054.00 0.89 0.470\n563.25 1,368.67 0.94 0.693\n427.23 1,073.00 0.87 0.870\n10\n880.46 1,905.00 0.88 0.506\n605.10 1,382.67 0.94 0.686\n469.92 1,095.71 0.89 0.853\n50\n1\n331.66 1,680.41 0.98 0.641\n122.98 1,262.00 0.98 0.771\n63.45 1,140.41 0.96 0.845\n2\n557.10 1,676.58 0.97 0.581\n335.58 1,192.51 0.96 0.804\n219.99\n968.00 0.92 0.976\n5\n683.56 1,674.41 0.97 0.573\n454.42 1,153.51 0.93 0.814\n344.44\n931.00 0.91 0.992\n10\n729.07 1,644.41 0.97 0.582\n502.03 1,200.94 0.98 0.784\n389.78\n926.00 0.93 0.996\nTable 3: Experiment 3.\nvtask\n0.50\n0.75\n1.00\nagts\nsrvc\ntime\nmake\nspan\nplan\ntime thpt stdy\nthpt\nsrvc\ntime\nmake\nspan\nplan\ntime thpt stdy\nthpt\nsrvc\ntime\nmake\nspan\nplan\ntime thpt stdy\nthpt\n100 877.94 2,891.58 5.72 0.70 0.81 489.46 2,130.67\n5.83 0.91 1.15 289.80 1,671.00\n5.15 1.16 1.49\n150 525.07 2,269.58 6.49 0.88 1.15 253.49 1,602.00\n6.67 1.20 1.64 122.69 1,396.71\n5.57 1.37 1.98\n200 353.46 1,905.58 7.08 1.03 1.47 154.76 1,504.63\n7.35 1.31 1.97 117.21 1,276.12\n9.50 1.50 2.04\n250 267.07 1,762.24 9.35 1.13 1.71 147.90 1,271.67 12.83 1.52 1.99 132.45 1,297.00 15.76 1.48 2.02\nas argued earlier, using the argument in (Phillips and\nLikhachev 2011) together with Theorem 1, it is straightforward to show that SIPPwRT returns a time-minimal path to\na given set G of one or more endpoints that does not collide with the paths of other agents in the token and is complete for the single-agent path-planning problems for function calls TP1 and TP3. We can thus rely on the proof of\nTheorem 3 in (Ma et al. 2017) to show that TP-SIPPwRT is\ncomplete for well-formed MAPD instances.\nTheorem 2. TP-SIPPwRT solves all well-formed MAPD\ninstances.",
        "simulated automated warehouses": "We demonstrate the beneﬁts of TP-SIPPwRT for automated\nwarehouses using both an agent simulator with perfect path\nexecution and a standard robot simulator with imperfect\npath execution resulting from unmodeled kinodynamic constraints and motion noise by the MAPD algorithms. Figure\n3 (left) shows an example on the agent simulator with 50\nagents and cells of size 1 m × 1 m. Grey cells in columns\nof grey cells are potential start cells for the agents. Colored disks are the actual start cells, which are drawn randomly from all potential start cells and are the non-task\nendpoints. All agents face north in their start cells. Grey\ncells other than the start cells are task endpoints (that would\nhouse shelves in a warehouse even though we do not model\nshelves here). The pickup and delivery cells of all tasks are\ndrawn randomly from all task endpoints. White cells are\nnon-endpoints.\nThe agents model circular warehouse robots. All agents\nuse the same rotational velocity vrot. The following rules impose restrictions on their legal movements and translational\nvelocities: All free agents can move with high translational\nfree velocity vtrans = vfree through all cells because warehouse robots that do not carry shelves can move through\nall cells, including those that house shelves. All task agents\ncan move with slow translational task velocity vtrans = vtask\nthrough only the pickup and delivery endpoints of their tasks\nand all other non-endpoints since warehouse robots that\ncarry shelves cannot move through cells that house shelves.",
        "experimental results": "We now report our experimental results on a 2.50 GHz Intel\nCore i5-2450M laptop with 6 GB RAM. Videos of sample\nexperiments can be found at\nhttp://idm-lab.org/project-p.html\nExperiment 1: MAPD Algorithms and Task Velocity. We\ncompare TP-SIPPwRT for vtask = 0.50, 0.75, and 1.00 m/s\non the agent simulator in the small simulated warehouse environment of Figure 3 (left) to two MAPD algorithms that\nboth assume discrete agent movements with uniform velocity to the four neighboring cells, namely the original TP\n(Ma et al. 2017) (labeled TP-A*) and CENTRAL (Ma et al.\n2017), which repeatedly uses the Hungarian Method (Kuhn\n1955) to assign tasks to agents and then Conﬂict-Based\nSearch (Sharon et al. 2015) to plan paths for the agents. We\nﬁrst convert the paths produced by these two MAPD algorithms from containing movements in the four compass directions to containing forward movements and point turns.\nWe then use MAPF-POST (H¨onig et al. 2016a) to adapt the\npaths in polynomial time to continuous agent movements\nwith given velocities. Since MAPF-POST guarantees safety\ndistances between agents of L/\n√\n2 = 0.71 m, we use the\nsame radius of R = 0.5L/\n√\n2 = 0.35 m for all agents. We\nuse a runtime limit of 5 minutes per instance. We use 30\nagents (agts) since CENTRAL, the most runtime-intensive\nof our MAPD algorithms, can handle only slightly more than\n30 agents without any timeouts. We use vfree = 1.00 m/s\nand vrot = π/2 = 1.57 rad/s. We generate one sequence\nof 1,000 tasks and insert them in the generated order into\nthe system with a task frequency (task freq) of 2 tasks in the\nbeginning of every second.\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2.6\n0\n400\n800\n1200\n1600\n2000\n2400\n2800\n100\n150\n200\n250\ntasks added\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2.6\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n2200\n100\n150\n200\n250\ntasks added\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2.6\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n100\n150\n200\n250\ntasks added\nFigure 5: Number of tasks inserted (in light grey) and executed per second in a moving 100-second window (t − 100, t] as a\nfunction of time t for different numbers of agents. Left: vtask = 0.50 m/s. Middle: vtask = 0.75 m/s. Right: vtask = 1.00 m/s.\nFigure 4 visualizes the throughput at time t [number of\ntasks that ﬁnish execution per second in the 100-second\nwindow (t − 100, t]], measured in tasks per second, as\na function of t, measured in seconds. The steady state is\nthe time interval when the throughput remains mostly unchanged, determined by visual inspection of the graphs. We\nuse as steady state t ∈ [501, 2100] for vtask = 0.50 m/s,\nt ∈ [501, 1500] for vtask = 0.75 m/s, and t ∈ [501, 1100] for\nvtask = 1.00 m/s. The throughput at time t of TP-SIPPwRT\ndecreases earlier than the ones of TP-A* and CENTRAL because fewer still unexecuted tasks are available toward the\nend for TP-SIPPwRT than for them. Thus, TP-SIPPwRT is\nmore effective than them.\nTable 1 reports the discrete service (srvc) time [time until a task has ﬁnished execution after insertion into the system according to the original plan with discrete agent movements, averaged over all tasks], discrete makespan [time\nwhen the last task has ﬁnished execution according to the\noriginal plan with discrete agent movements], service (srvc)\ntime [time until a task has actually ﬁnished execution after\ninsertion into the system, averaged over all tasks], makespan\n[time when the last task has actually ﬁnished execution],\nplanning (plan) time [execution time of the MAPD algorithm], and post-processing (post-proc) time [execution time\nof MAPF-POST], all measured in seconds, as well as the\nthroughput (thpt) [throughput at time t averaged over all\ntimes t whose throughputs are positive] and the throughput in the steady state (stdy thpt) [throughput at time t averaged over all times in the steady state], both measured in\nnumber of tasks per second. Service time, makespan, and\nthroughput measure effectiveness, while the planning and\npost-processing times measure efﬁciency. The planning time\nof TP-SIPPwRT is less than one second for 30 agents and\n1,000 tasks. It is on par with the one of TP-A* and smaller\nthan the one of CENTRAL. Furthermore, TP-SIPPwRT does\nnot have any post-processing time while both TP-A* and\nCENTRAL have post-processing times of more than 250\nseconds. Thus, TP-SIPPwRT is more efﬁcient than them.\nThe service time and makespan of TP-SIPPwRT are smaller\nthan the ones of TP-A* and CENTRAL, while its throughput\nis larger. Thus, TP-SIPPwRT is more effective than them.\nExperiment 2: Number of Agents, Task Frequency, and\nTask Velocity. We run TP-SIPPwRT with the same setup\nas in Experiment 1 (including the same sequence of 1,000\ntasks) for vtask = 0.50, 0.75, and 1.00 m/s, 10, 20, 30, 40,\nand 50 agents, and task frequencies of 1, 2, 5, and 10 tasks\nFigure 6: Screenshots for Experiment 3 at t = 35 s. Left:\nAgent simulator. Right: Robot simulator.\nper second. Table 2 shows that the planning time of TPSIPPwRT is less than one second for up to 50 agents and\n1,000 tasks. As expected, the service time decreases as the\ntask frequency decreases; the service time and makespan decrease and the throughput increases as the number of agents\nincreases; and the service time and makespan decrease and\nthe throughput increases as the task velocity increases.\nExperiment 3: Environment Size, Number of Agents,\nand Task Velocity. We run TP-SIPPwRT with the same\nsetup as in Experiment 1 but in the large simulated warehouse environment of Figure 3 (right) for 100, 150, 200, and\n250 agents and vtask = 0.50, 0.75, and 1.00 m/s. We use\none sequence of 2,000 tasks and a task frequency of 2 tasks\nper second. Figure 5 visualizes the throughput at time t. We\nuse as steady state t ∈ [501, 1000]. Table 3 shows that the\nplanning time of TP-SIPPwRT is less than 16 seconds for\nup to 250 agents and 2,000 tasks, justifying our claim that\nit can compute paths for hundreds of agents and thousands\nof tasks in seconds. Similarly to before, the service time and\nmakespan decrease and the throughput and planning time\nincrease as the number of agents increases; and the service\ntime and makespan decrease and the throughput increases as\nthe task velocity increases. There is an exception due to the\ncongestion resulting from many agents for 250 agents and\nvtask = 1.00 m/s.\nExperiment 4: Robot Simulator. We created a custom\nmodel of the kinodynamic constraints of a differential-drive\nCreate2 robot from iRobot for the robot simulator V-REP\n(Rohmer, Singh, and Freese 2013). Create2 robots have a\ncylindrical shape with radius 0.175 m and can reach a translational speed of 0.5 m/s and a rotational speed of 4.2 rad/s.\nWe use vfree = 0.40 m/s, vtask = 0.20 m/s, vrot = π =\n3.14 rad/s, and R = 0.40 m as conservative values to allow the robots to follow their paths safely despite unmodeled\nkinodynamic constraints and motion noise by TP-SIPPwRT.\nWe implemented a PID controller that uses [x, y, θ]T (given\nby V-REP) as the current state and the desired next cell with\nthe associated desired arrival time (given by TP-SIPPwRT)\nas the goal state. The PID controller corrects for heading errors by orienting the robot to face the desired next cell while\nsimultaneously adjusting the translational speed to let the\nrobot arrive at the desired next cell at the desired arrival time.\nWe limit our experiment to the small warehouse environment of Figure 6 for 10 robots due to the slow runtime of VREP. We use one sequence of 20 tasks and a task frequency\nof 2 tasks per second. The planning time of TP-SIPPwRT\nis 2 ms. All robots follow their paths safely, resulting in a\nservice time of 90.57 s and a makespan of 171.16 s.",
        "conclusion": "We presented the efﬁcient and effective algorithm TPSIPPwRT for the Multi-Agent Pickup and Delivery problem. We suggest the following future research directions: (1)\nMake existing (even optimal) multi-agent pathﬁnding algorithms more general by combining them with our SIPPwRT\nto compute continuous agent movements with given velocities. The resulting algorithms could, for example, be used\nto make CENTRAL more general. (2) Include additional\nkinodynamic constraints into SIPPwRT and TP-SIPPwRT,\nsuch as acceleration and deceleration constraints, to allow\nrobots to follow their paths even more safely. (3) Make TPSIPPwRT decentralized.",
        "summary_en": "The Multi-Agent Pickup and Delivery (MAPD) problem models applications where a large number of agents attend to a stream of incoming pickup-and-delivery tasks. Token Passing (TP) is a recent MAPD algorithm that is efficient and effective. This paper makes TP even more efficient and effective by using a novel combinatorial search algorithm, called Safe Interval Path Planning with Reservation Table (SIPPwRT), for single-agent path planning. SIPPwRT uses an advanced data structure that allows for fast updates and lookups of the current paths of all agents in an online setting. The resulting MAPD algorithm TP-SIPPwRT takes kinematic constraints of real robots into account directly during planning, computes continuous agent movements with given velocities that work on non-holonomic robots rather than discrete agent movements with uniform velocity, and is complete for wellformed MAPD instances. The paper demonstrates its benefits for automated warehouses using both an agent simulator and a standard robot simulator. For example, the paper demonstrates that it can compute paths for hundreds of agents and thousands of tasks in seconds and is more efficient and effective than existing MAPD algorithms that use a post-processing step to adapt their paths to continuous agent movements with given velocities.",
        "summary_zh": "这篇论文介绍了一种针对多代理取货和送货问题的终身路径规划方法。多代理取货和送货（MAPD）问题模拟的是大量代理处理接收到的取货和送货任务流的应用。令牌传递（TP）是一种最新的MAPD算法，高效且有效。作者在单个代理路径规划中使用了一种名为带预约表的安全间隔路径规划（SIPPwRT）的新型组合搜索算法，从而使TP算法更加高效。该算法采用先进的数据结构，可在在线环境下快速更新和查找所有代理的当前路径。由此产生的MAPD算法TP-SIPPwRT 在规划过程中直接考虑了真实机器人的运动学约束，以给定速度计算适用于非自主机器人的连续代理运动，而不是以均匀速度计算离散代理运动，并且对于成型良好的MAPD实例是完整的。最后，作者通过代理模拟器和标准机器人模拟器进行了实验验证，结果表明该方法在自动化仓库中具有显著的优势"
    },
    {
        "title": "Safe Interval Path Planning with Kinodynamic Constraints",
        "abstract": "Safe Interval Path Planning (SIPP) is a powerful algorithm for solving a single-agent pathfinding problem where the agent is confined to a graph and certain vertices/edges of this graph are blocked at certain time intervals due to dynamic obstacles that populate the environment. The original SIPP algorithm relies on the assumption that the agent is able to stop instantaneously. However, this assumption often does not hold in practice, e.g. a mobile robot moving at a cruising speed cannot stop immediately but rather requires gradual deceleration to a full stop that takes time. In other words, the robot is subject to kinodynamic constraints. Unfortunately, as we show in this work, in such a case, the original SIPP is incomplete. To this end, we introduce a novel variant of SIPP that is provably complete and optimal for planning with acceleration/deceleration. In the experimental evaluation, we show that the key property of the original SIPP still holds for the modified version: it performs much fewer expansions compared to A* and, as a result, is notably faster.",
        "introduction": "Planning a path in the presence of both static and moving\nobstacles is a challenging problem with topical applications\nin robotics, video games, and other domains. When the environment is fully observable and the trajectories of the dynamic obstacles are known (e.g., are predicted by the robot’s\nperception system or by a global observer), it is reasonable\nto account for them while planning. A prominent method\nthat is tailored for such a setting is Safe Interval Path Planning (SIPP) (Phillips and Likhachev 2011). This is a searchbased algorithm that operates on a graph where vertices correspond to the configurations of the agent (e.g., position,\nheading, velocity, etc.) and edges correspond to the transitions between them. Due to the dynamic obstacles, certain\nvertices/edges of this graph are blocked at certain time intervals. SIPP accounts for that and constructs plans in which an\nagent can wait at the vertices to avoid collisions. An explicit\nassumption of SIPP is that the agent can start/stop moving instantaneously. Under this assumption, the algorithm is\nprovably complete and optimal (w.r.t. the given spatial and\ntemporal discretization).\nA\nB\nC\nt0 = 6\nv = 1\nt0 = 12\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nDistance (m)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n..\nTime(s)\nFigure 1: An example of the challenging path planning instance with dynamic obstacles.\nIn our work, we adopt an assumption (more realistic from\nthe practical perspective) that decelerating to a full stop and,\nsimilarly, accelerating from the full stop takes time. In other\nwords, the agent is subject to kinodynamic constraints. This\nmakes SIPP inapplicable as shown in Fig. 1. In this example, the environment is discretized to the 3 × 10 grid,\nwhere the distance between the centers of the grid cells is\n1 m, and the agent has to go from the leftmost cell B0 to\nthe rightmost one B9. Moreover, two dynamic obstacles are\npresent in the environment that block cells B5 and B7 during the intervals shown in black in the bottom part of the\nfigure. Assume that the agent’s cruising speed is 1 m s−1. At\nthis speed, it takes 1 s to move between the cells. Furthermore, the maximum acceleration/deceleration of the agent\nis 0.5 m s−2, which means that accelerating to the cruising\nspeed from the full stop takes 2 s and 1 m. Therefore, when\nstarting to move from the full stop, the agent will arrive at\nthe neighboring cell not in 1 s, but rather in 2 s. If the algorithm ignores that, like the original SIPP, which assumes\ninstantaneous acceleration, then the constructed plan, shown\nin red, will actually lead to the collision (if it is applied with\nreal kinodynamic constraints) as shown by the dashed red\nline. A straightforward modification of SIPP that takes accelerating/decelerating actions into account is to apply only\ndynamically valid transitions, i.e., to prevent the wait in the\nstates where the velocity is not zero. However, this variant\nwill report failure to find the solution while producing a partial plan shown in blue. In this plan, the agent successfully\nstops before the first obstacle and waits for 2 s, but then is\nunable to pass the second obstacle as it is running late to\narrive at B7. There is, however, a collision-free plan. It is\nshown in green. The reason why SIPP fails to find it is that,\nintuitively, SIPP postpones the wait actions and cannot reason about the consequences of waiting in different states of\nthe search tree. This does not violate the theoretical guarantees when waiting is available in any state. On the other\nhand, when waiting is not always readily available, i.e., the\nagent needs time to stop, the algorithm becomes evidently\nincomplete. We will elaborate on this and provide technical\ndetails in the following sections of the paper.\nTo the best of our knowledge, no works on SIPP exist\nthat directly address this issue. As a result, all known SIPPbased algorithms are incomplete in the setting where kinodynamic constraints of the agent have to be taken into account when planning. Our work fills this gap and presents\nan algorithm that is provably complete and optimal under\nsuch constraints: Safe Interval Path Planning with Interval\nProjection (SIPP-IP). To empirically evaluate it, we have\nconducted a wide range of experiments in which we compare SIPP-IP to several baselines that include other (noncomplete) variants of SIPP one may think of and A*. Empirical results clearly show that SIPP-IP outperforms them all,\nas it is able to solve instances that are unsolvable by other\nplanners and its runtime is two orders of magnitude lower\nthan the one of A*.",
        "related work": "Search-based planning with predictably moving obstacles\ncan be straightforwardly implemented as A* (Hart, Nilsson,\nand Raphael 1968) with discretized time. Taking the time dimension into account, however, leads to significant growth\nof the search space, especially when fine-grained time discretization is needed. To this end, (Phillips and Likhachev\n2011) introduced SIPP, based on the idea of compressing\nsequences of timesteps into the time intervals and searching over these intervals. SIPP is provably complete and optimal under several assumptions, including that the agent\ncan start/stop moving instantaneously. Later, numerous variants of SIPP emerged, enhancing the original algorithm, e.g.,\nany-angle SIPP (Yakovlev and Andreychuk 2021), anytime\nSIPP (Narayanan, Phillips, and Likhachev 2012), different\nvariants of bounded-suboptimal SIPP (Yakovlev, Andreychuk, and Stern 2020). Moreover, SIPP and its modifications\nare widely used as building blocks of some of the state-ofthe-art multi-agent pathfinding solvers (Cohen et al. 2019;\nLi et al. 2022). None of these variants consider kinodynamic\nconstraints.\nNext, we mention only the works that to some extent\ndeal with taking the agent’s kinematic and/or kinodynamic\nconstraints into account. (Ma et al. 2019) introduced SIPPwRT that allowed for the planning with different velocities. Still, acceleration actions and effects were not considered in this work. Similarly, (Yakovlev, Andreychuk, and\nVorobyev 2019) described any-angle variant of SIPP that\nsupported non-uniform velocities. Interestingly, the authors\nevaluate their planner on real robots. To plan safe trajectories for them, they suggested inflating the sizes of the moving obstacles, which alternatively can be seen as extending\nthe blocked time intervals of certain graph vertices/edges.\nThis ad-hoc technique was shown to perform reasonably\nwell in practice, but in general, it raises the question of how\nto choose the offset. For example, if one enlarges the blocked\nintervals in the setting described in the Introduction (recall\nFig. 1) by 1 s, SIPP will not find a solution. In (Ali and\nYakovlev 2021), accelerating actions were straightforwardly\nintegrated into SIPP, which, again, makes the algorithm incomplete (as we show in this work). (Cohen et al. 2019)\nsuggested a variant of SIPP for the problem setting that assumes arbitrary motion patterns. However, this was not the\nmain focus of the paper. Consequently, no techniques were\nproposed to take special care of the accelerating motions,\nand the empirical evaluation considered only the motions\nthat start and end with zero velocity. Overall, to the best of\nour knowledge, no SIPP variant existed prior to this work\nthat would be provably complete and optimal when the assumption of the original SIPP that “the robot can start/stop\ninstantaneously” does not hold.",
        "problem statement": "We assume a discretized timeline T = 0, 1, .... The agent\nis associated with a graph G = (V, E), with the start and\ngoal vertices: start, goal ∈ V . A vertex of the graph corresponds to the state of the agent, alternatively known as the\nconfiguration, in which the agent can reside without colliding with the static obstacles. Each configuration explicitly\nencompasses the velocity, vel, of the agent. For example, it\nmay be comprised of the agent’s coordinates and orientation\nas well as of its velocity: v = (x, y, θ, vel). Configurations\nwith the same position/orientation but different velocities,\nindeed, correspond to different vertices of V . Configurations\nwith vel = 0 are of special interest, as the agent may stay\nput (wait) only in them.\nAn edge e = (v, u) ∈ E represents a motion primitive\n(Pivtoraiko and Kelly 2011), a small kinodynamically feasible fragment of the agent’s motion that transfers the agent\nfrom v to (distinct) u. We assume that for any v ∈ V , a finite set of such motions is available and each motion takes\nan integer number of timesteps. Practicality-wise, this assumption is mild, as any real-valued duration may be approximated with a certain precision and represented as an\ninteger. The duration of the motion defines the weight of the\nedge, w(e) ∈ N.\nBased on the source/target velocity, each motion (edge)\ncan be classified as either accelerating, decelerating, or uniform. In the latter case, the agent’s velocity at the target\nconfiguration is the same as in the source one, while in the\nfirst two cases, it changes (increases and decreases, respectively). The presence of accelerating/decelerating motions\nmakes our problem distinguishable from the works that assumed instantaneous acceleration1.\nBesides the agent and the static obstacles, the environment is populated by a fixed number of moving obstacles.\nWe assume that their trajectories are known and converted\n(by an auxiliary procedure) to the collision and safe intervals\nassociated with the graph vertices and edges; i.e., for each\nvertex, a finite sequence of non-overlapping time intervals\nSI(v) is given, which is the sequence of the safe intervals\nat which the agent can be configured at v without colliding with any moving obstacle. Similarly, safe intervals are\ndefined for each edge, SI(e). If the agent executes a move\ne at any timestep outside SI(e), a collision with a moving\nobstacle occurs.\nA (timed) path for the agent, or a trajectory, is a sequence\nπ = (e1, t1), (e2, t2), ...(eL, tL), where ti ∈ T is the time\nmoment the motion defined by the edge ei is started. The\ncost of the trajectory is the timestep when the agent reaches\nthe final vertex, cost(π) = eL + w(eL). A trajectory is feasible if the target vertex of ei matches the source vertex of\nei+1 and ti+1 ≥ ti + w(ei) (with t0 = 0). Moreover if\nti+1 > ti + w(ei), then the velocity at the source of ei+1\nmust be zero.\nA feasible trajectory can also be seen as a plan composed\nof the move and wait actions, where the move actions are\ndefined by the graph edges and the wait actions might occur\nat the vertices where the agent’s velocity is zero. The duration of the wait action occurring after the i-th move action is\ncomputed as δ = ti+1 − (ti + w(ei)). As w(ei) is integer, δ\nis integer as well.\nA collision between the trajectory π and the dynamic obstacles occurs iff either of the conditions occur: i) the agent\nstarts executing some motion primitive e at the timestep\nwhich is outside of SI(e); ii) according to π, the agent waits\nat a vertex v outside of SI(v).\nThe problem now is to find a feasible collision-free trajectory π that transfers the agent from start to goal on a given\nG (with the annotated safe intervals of vertices/edges). In\nthis work, we are interested in solving this problem optimally, i.e., in reaching the goal as early as possible.",
        "method": "As our method relies on SIPP, which in turn relies on A*\nwith timesteps, we first discuss the background and then\ndelve into the details of the suggested approach. We assume\nthat a reader is familiar with vanilla A* for static graphs.\nBackground\nA* with time steps\nThe straightforward way to solve the\nconsidered problem is to use A* for searching the state\nspace whose nodes are the tuples n = (v, t), where v is\n1Please note that our problem formulation, as well as the suggested method, is agnostic to how the state variables, e.g. velocity,\nchange while the agent is executing a motion primitive. We are interested in the values of the state variables only at the endpoints of\na motion primitive.\nthe graph vertex and t is the time step by which it is reached.\nWhen expanding a node, the successors are generated as follows. First, for each e = (v, u), if e is feasible, the node\nnmove = (u, t + w(e)) is added to the successors. Indeed,\nif the application of a motion e at time step t results in a\ncollision, the resulting node is discarded. Second, if v allows for the wait, then the node corresponding to the wait\naction of the minimal possible duration of one time step\nnwait = (v, t + 1) is also added to successors. Again, if\nthe safe intervals of v do not cover the time step t + 1, this\nnode is discarded.\nOther parts of the search algorithm are exactly the same\nas in A*. Notably, the g-values of the nodes equal the time\ncomponents of their identifiers. I.e., the g-value of the node\nn = (v, t) equals t, as this is the minimal known estimate of\nthe cost (w.r.t to the current iteration of the algorithm) form\nstart to v.\nSIPP\nIn many scenarios, A* with time steps generates numerous nodes of the form (v, t), (v, t + 1), (v, t + 2), etc.,\nwhich are created via the use of the atomic wait actions. This\nleads to the growth of the search tree and slows down the\nsearch. To this end, SIPP relies on the idea of compressing\nthe sequential wait actions to reduce the number of the considered search nodes.\nThe search nodes of SIPP are identified as n\n=\n(v, [lbj, ubj]), where [lbj, ubj] is the j-th safe interval of\nv. For each node, SIPP stores the earliest arrival time:\nt ∈ [lbj, ubj]. Whenever a lower-cost path to n is found,\nt(n) is updated, and this value serves as the g-value of the\nnode similarly to A* with time steps.\nWhen expanding a node, SIPP does not generate any separate successor corresponding to the wait action. Instead, it\ngenerates only the successors that correspond to the “wait\nand move” actions that land into the neighboring configurations within their safe intervals. This means that to generate\na successor, SIPP waits the minimal amount of time possible\nso that when the move to the new configuration is used, we\narrive at the new safe interval as early as possible. For example, if SIPP performs a move from a node n = (v, [5, 10]) to\na node n′ = (v′, [15, 18]) and the duration of that move is 5\nand t(n) = 7, then t(n′) is 15, meaning that after reaching n\nat t = 7, the agent waits there for 3 time steps, starts moving\nat t = 10, and arrives to n′ at t = 15. This move cannot\nbe performed earlier than t = 10, as in this case, it will end\noutside the safe interval of n′.\nSIPP is much faster than A* and is provably complete and\noptimal under the following assumptions: inertial effects are\nneglected, the agent can start/stop moving instantaneously,\nand, consequently, the wait actions are available at all configurations.\nWhat if the wait actions are not always available\nThe\nintrinsic SIPP assumption that the agent can wait at any configuration often does not hold in practice, as many mobile\nagents cannot start/stop instantaneously. To reflect this, the\naccelerating/decelerating motions should be introduced, and\nthe velocity variable should be added to the agent’s configuration (as done in our problem statement). In this setting,\nSIPP is incomplete.\n2\n2\n((A, =0),[0,5]) \n=0 \n1\nA\nB\nC\nD\nTime\n1\n2\n3\n5\n6\n7\nPosition\n((B, =1),[0,\n)) \n=2\n2\n((B, =1),[2,7]) \n=2\n2\n((C, =0),[5,\n)) \n=5\n2\n((D, =0),[7,\n)) \n=7\n((D, =1),[7,\n)) \n=7\n((D, =1),[6,9]) \n=6\n4\n1\n((C, =1),[5,8]) \n=5\nFigure 2: An example where the standard SIPP fails to find\na solution, while SIPP-IP succeeds. The search tree of SIPP\nis shown in green. The search tree of SIPP-IP is indicated in\npurple.\nStatement 1. SIPP is incomplete when the agent is subject\nto kinodynamic constraints and wait actions are not available at any configuration.\nProof. We prove by presenting an example for which SIPP\nis not able to find a solution whilst it exists. Consider a problem depicted in Fig. 2. The agent needs to reach cell D from\ncell A. The motion primitives are defined as follows:\n• (accelerating motion) The agent starts moving from a cell\nwith vel = 0 and ends with vel = 1 in the neighboring\ncell. The cost (duration) is 2 time steps.\n• (uniform motion) The agent starts moving from a cell\nwith vel = 1 and ends with vel = 1 in the neighboring cell. The cost (duration) is 1 time step.\n• (decelerating motion) The agent starts moving from a cell\nwith vel = 1 and ends with vel = 0 in the neighboring\ncell. The cost (duration) is 2 time steps.\nIndeed, the agent can wait when vel = 0. The duration\nof the wait action is 1 time step. For the sake of brevity, we\nignore the agent’s orientation and motions that change it.\nSIPP starts with expanding the initial search node\n((A, vel = 0), [0, 5]) in which only the accelerating motion\nis applicable. This results in generating the node ((B, vel =\n1), [0, ∞)) with the arrival time t = 2, shown in green in\nFig. 2. The agent cannot wait in this configuration. Thus,\nit can only continue moving, arriving at C either at t = 3\nusing the uniform motion action or at t = 4 using the decelerated motion. Both moves are invalid, as they do not reach\nC within its safe interval which starts at 5. Thus, no successors are generated and no search nodes that can be expanded\nare left in the SIPP’s search tree. The algorithm terminates,\nfailing to report a solution. The latter, however, does exist:\nthe agent needs to wait for 2 time steps at A and then continue moving towards D. In this way, it will arrive at C at\nt = 5 satisfying the safe interval constraint.\nTime\nFigure 3: Projection of the time interval in SIPP-IP.\nNext, we present a modification of SIPP that is complete\nand optimal under the considered assumptions.\nSIPP-IP: Safe Interval Path Planning with (Wait)\nInterval Projection\nIdea\nThe main reason why standard SIPP fails to solve\nplanning instances like the one presented above is because\nthe information about possible wait actions is not propagated\nfrom predecessors to successors. In the considered example, when achieving the search node ((B, vel = 1), [0, ∞)),\nSIPP “forgets” that the agent can wait in the predecessor and\nperform the move action at any time step until the end of the\npredecessor’s safe interval. This is not a problem when the\nagent can wait at any configuration, but leads to incompleteness in the case we are considering.\nTo this end, we substitute the safe interval as the search\nnode’s identifier with another time interval which we refer\nto as the wait interval. For any search node, this interval belongs to the safe interval of the graph vertex. Indeed, nodes\nwith the same vertex but different wait intervals should be\ndistinguished. The wait interval of a search node incorporates information about all possible wait-and-move actions\nthat can be performed in its predecessor. When a node is expanded, the waiting interval is projected forward to all of\nits successors; thus, the information about the possible waitand-move actions is propagated from the root of the search\ntree (start node) along all of its branches. We call the modification of SIPP that implements this principle Safe Interval\nPath Planning With (Wait) Interval Projection, or SIPP-IP.\nHenceforth, we will refer to the wait interval of a SIPP-IP\nnode as time interval (or, simply, interval). When talking\nabout the safe intervals of the graph vertices, we will never\nomit “safe” to avoid confusion.\nProjecting intervals\nThe role of the projection operation\nis to propagate the information on all of the available waitand-move actions from the predecessor to the successor. Formally, the input for the projection procedure is a SIPP-IP\nsearch node, n = (v, [tl, tu]), where [tl, tu] resides inside\none of the safe intervals of v, and a graph edge e = (v, v′)\nalong which the interval should be propagated. The output\nis the set of time intervals TI = {ti = [t′, t′′]}, s.t.:\n• each resultant interval belongs to one of the safe intervals\nof the target vertex: ∀tik ∈ TI ∃si ∈ SI(v′) : tik ⊆ si\n• resultant intervals do not overlap: tik ∩ til = ∅, ∀k ̸= l\n• for any transition, specified by e, that starts at any time\nstep of the source interval, the time step corresponding\nto the end of this transition belongs to one of the resultant intervals if the transition is valid (no collisions occur\nwhen performing it): ∀t ∈ [tl, tu] s.t. (e, t) is a collisionfree transition ∃ˆt ∈ TI s.t. ˆt = t + w(e)\nA general example of how projecting interval operation\nworks is depicted in Fig. 3. The source graph vertex is denoted by the blue oval, and the target vertex (where the outgoing edge lands) is shown in green. Black cylinders correspond to the blocked intervals of the vertices. The time\ninterval to be projected is marked in yellow. The resultant\ntime intervals are shown in cyan, green, and orange. Observe\nthat due to the existence of the intermediate unsafe interval\nin the target vertex, the projected time interval is, first, split\ninto the two ones (pink transitions landing in unsafe intervals are ruled out). Second, the transitions that land in the\nsafe intervals of the destination vertex but that lead to a collision in the course of the transition (gray arrows) are also\npruned. Thus, the lower time interval is trimmed, while the\nupper is split into two ones.\nA straightforward technical implementation of the projecting operation may involve the sequential application of e\nto the time steps forming the input interval with further filtering out the invalid transitions and grouping the resultant time\nsteps into the intervals. This resembles A* with time steps,\nbut the resultant atomic search states of A* are compressed\ninto the interval states of SIPP-IP. Thus, the search tree of\nSIPP-IP is more compact. Indeed, more computationally efficient implementations of the projection operation may be\nsuggested, depending on how the collision detection mechanism is specified2.\nTo demonstrate how projecting helps in solving instances\nthat were unsolvable for standard SIPP, recall the example\ndepicted in Fig. 2. When expanding the start node SIPPIP projects, the time interval of A, which is [0, 5] (coincides with the safe interval for the start node), to the successor. This results in [2, 7] interval. When expanding the\nnode ((B, vel = 1), [2, 7]), we can now apply both uniform\nmotion action and decelerating action by trying to commit\nthem at any time step that belongs to the time interval of\n(B, vel = 1) (via the projection operation, again). In such a\nway, the uniform motion action from B to C will be committed at t = 4, following the SIPP’s principle of reaching the\nsuccessor as early as possible, i.e. at t = 5. The projection of\nthe interval of B will give us [5, 8]. Finally, when expanding\n((C, vel = 1), [5, 8]) the node ((D, vel = 0), [7, ∞)) will\nbe generated. When this node will be chosen for expansion,\nthe search will report finding the solution.\nSIPP-IP description\nSIPP-IP starts with forming the start\nnode from the graph vertex. Initially, the start interval contains only the first time step, tstart, provided as input. If the\nstart vertex allows for waiting, i.e., the velocity of the initial\nconfiguration is zero, the upper bound of the node’s interval\nis extended to the upper bound of the safe interval (of the\nstart vertex), in which tstart resides.\n2See more in the pre-print available on arXiv.\nAlgorithm 1: SIPP-IP\nFunction\nfindPath(vstart, tstart, vgoal, G(V, E), SI):\n1\nOPEN ← ϕ, CLOSED ← ϕ\n2\nti = [tstart, tstart]\n3\nif vstart.vel = 0 then\n4\nti.tu ← upper bound of SI(vstart, ti)\n5\nnstart ← (vstart, ti)\n6\nf(nstart) ← tstart + h(vstart)\n7\nAdd nstart to OPEN\n8\nwhile OPEN ̸= ϕ do\n9\nn ← state from OPEN with minimal f-value\n10\nremove n from OPEN, insert n to CLOSED\n11\nif n.v = vgoal then\n12\nreturn π ← ReconstructPath(n, nstart)\n13\nsucc ← getSuccessors(n, G(V, E), SI)\n14\nfor each n′ in succ do\n15\nif n′ in CLOSED or in OPEN then\n16\ncontinue\n17\nf(n′) ← n′.tl + h(n′.v)\n18\nAdd n′ to OPEN\n19\nreturn ϕ\nFunction getSuccessors(n, G(V, E), SI):\n20\nSUCC = ϕ\n21\nfor each e = (n.v, v′) in available motions do\n22\nintrvls = projectIntervals(n, e, SI)\n23\nif v′.vel = 0 then\n24\nfor each ti in intrvls do\n25\nti.tu ← upper bound of SI(v′, ti)\n26\nfor ti in intrvls do\n27\ninsert (v′, ti) to SUCC\n28\nreturn SUCC\nThen SIPP-IP follows the general outline of SIPP/A*. At\neach iteration, it, first, selects the best node from OPEN,\ni.e., the one with the minimal f-value. The f-value of a\nSIPP-IP node n = (v, [tl, tu]) is defined as f(n) = g(n) +\nh(n) = tl + h(v). Similarly to SIPP, the g-value of the node\nis the earliest time step the agent can arrive at n, that is tl.\nThe h-value is independent of the time interval and is defined for graph vertices. It estimates the travel time from the\nvertex to the goal (e.g., it equals the straight-line distance\nbetween the vertices divided by the maximum speed of the\nagent). As in SIPP, we assume the heuristic to be consistent.\nAfter selecting a node, its successors are generated. For\neach outgoing edge, we apply the projecting operation as\ndescribed above. As a result, for each edge, we obtain a set\nof projected time intervals. If the target configuration allows\nfor waiting (the velocity is zero), then we extend the upper\nbounds of the projected intervals to the upper bounds of the\ncorresponding safe intervals of the target vertex. As a result,\neach of the generated successors implicitly encompasses the\ninformation both about all possible transitions from the predecessor and all possible wait actions in the current node.\nFinally, each generated successor is inserted into OPEN\nin case it is not already present in the search tree. The algorithm stops when a node corresponding to the goal vertex\nis extracted from OPEN. At this stage, the path can be reconstructed. To do so, we go backward from the goal node\nand, at each iteration, do the following. Let n be the current node (initially set to the goal node, ngoal), t – the time\nvariable (initially equal to ngoal.tl), nparent – parent of n,\nand e the transition between them. If the agent can wait at\nn, then we add the tuple (e, n.tl − cost(e)) to π and change\nn = nparent and t = n.tl −cost(e). If the agent cannot wait\nat n, we add (e, t − cost(e)) to π and change n = nparent\nand t = t − cost(e). This is repeated until we reach the start\nnode. If the final value t does not match tstart, then the agent\nhas to wait at the start (for t − tstart time steps).\nThe pseudo-code of SIPP-IP is presented in Algorithm 1.\nTheoretical Properties of SIPP-IP\nAccording to the definition of the SIPP-IP state, we\ncan view each SIPP-IP state as a sequence of A*-withTime-Steps (A*-TS) states. That is nSIP P −IP (v, [tl, tu]) =\n{sA∗−T S(v, t) : t ∈ [tl, tu]}. We will use this relation along\nin the next proofs.\nTheorem 1. SIPP-IP is complete.\nProof. In the presented function getSuccessors, we try to use\nall available edges on the input vertex. Then, for each edge\nusing the function projectIntervals, we get the intervals that\ncontain all possible valid timesteps at which we can get at\nthe target node of the edge starting from a timestep in the\ntime interval of the input state (by definition of projectIntervals). This is equivalent to generating all A*-TS states from\nthe A*-TS states included in the input state using an edge.\nBy using all the edges, we generate all A*-TS that can be\ngenerated by the move action. Directly after that, we check\nif the wait action is available at the target vertex, and then,\nwe extend the resulting time intervals to the upper bound\nof the safe interval at the target vertex, where the interval\nis located. This is equivalent to the application of the wait\nactions on all A*-TS states at the target vertex. As a result, in getSuccessors, all valid A*-TS successors are generated (and capsulated by SIPP-IP states). So, SIPP-IP can\nbe viewed as a modified version of A*-TS which, at every iteration, expands several states at the same time, generates all\nthe successors of these states, and inserts them into OPEN\n(by definition, all generated A*-TS states are reformed into\nSIPP-IP states without any loss). As a result, SIPP-IP will\nalways generate and expand all A*-TS states, and as A*-TS\nis complete, so is SIPP-IP.\nLemma 1. The A*-TS state with the minimum f-value\nin a SIPP-IP state nSIP P −IP (v, [tl, tu]) is the state\nnA∗−T S(v, tl).\nProof. Recall that f-value of a A*-TS state is equal to\nn.t + h(n.v). As v of all A*-TS states in one SIPP-IP state\nare identical, the h-values of them are equal. As a result, the\nstate with the minimal time i.e., tl is the state with the minimal f-value.\nTheorem 2. SIPP-IP is optimal.\nProof. As SIPP-IP is a complete algorithm and the cost\n(time) is included as an identifier in the state, it is guaranteed that the optimal state will be expanded. Therefore,\nit is sufficient to prove that the first expanded state with\nthe goal vertex is the optimal one i.e., contains the A*TS state with optimal (minimal) time. Let us again view\nthe SIPP-IP states as a sequence of extracted A*-TS states.\nLet the first expanded SIPP-IP state with the goal vertex be nSIP P −IP (vgoal, [tl, tu]) that contains the A*-TS\nstate nA∗−T S(vgoal, tl). Let us suppose that nA∗−T S is\nnot the optimal A*-TS state, but there exists another state\nn′\nA∗−T S with the minimal f-value in OPEN f(n′) that is\nless than f(n). According to Lemma 1, n′\nA∗−T S is located\nat the bottom of a SIPP-IP state n′\nSIP P −IP in OPEN, i.e.,\nn′\nA∗−T S.t = n′\nSIP P −IP .tl. As in SIPP-IP, the states are ordered by the values f(nSIP P −IP ) = n.tl + h(n.v) that is\nequal to the f-value of the bottom A*-TS state f(nA∗−T S),\nthe state n′\nSIP P −IP should have been expanded before the\nstate nSIP P −IP because f(n′\nA∗−T S) < f(nA∗−T S), which\nresults in a contradiction. Therefore, there is no A*-TS state\nwith f-value less than f(nA∗−T S), and hence nSIP P −IP is\nthe optimal state.",
        "empirical evaluation": "We have used five different maps from the MovingAI benchmark (Sturtevant 2012) for the experiments: empty (sized\n64x64), room (64x64), warehouse (84x170), random\n(128x128) and Sydney (256x256). Each map was populated with an increasing number of moving obstacles (MOs),\nand for each number, 200 different instances were generated\nthat differ in trajectories of MOs. These trajectories were\ngenerated by randomly assigning start and goal locations for\neach MO and letting it go from start to goal using random\nspeeds. Moreover, MO may wait at any cell for a random\nnumber of time steps. For each map, we generated instances\nwith the densities of MOs varying from 1/25 to 1/3, where\nthe density is the ratio of the number of MOs to the number\nof free cells. The start and goal locations of the agent were\nfixed to the top-left and bottom-right corners of the map.\nNote that some instances in our dataset (especially with high\ndensities of MO) are not generally solvable.\nThe agent was modeled as a disk whose diameter equals\nthe length of the grid cell. The configuration is defined as\n(x, y, θ, vel) where x, y are the coordinates of the cell, θ ∈\n{0◦, 90◦, 180◦, 270◦} is the orientation, and vel ∈ {0, 2} is\nthe velocity of the agent.\nThe following motion primitives were defined: accelerating, decelerating, and uniform. Accelerating (decelerating)\nprimitive: move with the fixed acceleration of 0.5 cell/s2\n(−0.5 cell/s2) from a configuration with zero (2 cell/s) velocity until reaching maximum (zero) velocity. The agent\ntraverses four cells this way without changing its orientation. Uniform motion primitive: go with the maximum velocity (2 cell/s) one cell forward (the orientation does not\nchange). Additionally, if the velocity is zero, the agent can\n25 20 15 10\n5\n4\n3\n#Free_Cells/#MO\n0\n20\n40\n60\n80\n100\nSuccess Rate(%)\nroom-64-64-16\n25 20 15 10\n5\n4\n3\n#Free_Cells/#MO\nempty_64_64\n25 20 15 10\n5\n4\n3\n#Free_Cells/#MO\nwarehouse-10-20-10-2-2\n25 20 15 10\n5\n4\n3\n#Free_Cells/#MO\nrandom128_10_0\n25 20 15 10\n5\n4\n3\n#Free_Cells/#MO\nSydney_2_256\nSIPP-IP\nSIPP1\nSIPP2\nA*\nFigure 4: Success Rates of the evaluated algorithms.\nroom\nempty\nwarehouse\nrandom\nSydney\n0\n1\n2\n%\n45\n13\n55\n125\n535\n25\n10\n40\n73\n420\n72\n19\n108\n208\n1381\nRuntime relative to A*\nSIPP-IP\nSIPP1\nSIPP2\nFigure 5: Runtimes of SIPP-IP, SIPP1 and SIPP2 compared\nto A*. Absolute values (in ms) are shown above the bars.\nrotate 90◦ clockwise or counterclockwise in 2 s or wait for\none time step. The time step is chosen to be 0.1 s.\nCollision checking is conservative. We prohibit the agent\nand any MO from touching the same cell at the same time.\nTherefore, the blocked intervals of any cell were reserved for\nthe MO when it touched this cell even partially. The agent is\nallowed to use a motion primitive only if it does not result in\ntouching a cell in a blocked interval. Moreover, if the time\nlimits of touching some cell for an agent are not integers,\nthey are extended to the closest integers in a safer manner.\nWe compared SIPP-IP to A* with time steps and two\nstraightforward extensions of SIPP: SIPP1 and SIPP2.\nSIPP1 is a modification that generates only kinodynamically\nfeasible successors for the configurations where the velocity is not zero. SIPP2 further allows to re-expand the search\nnodes (which corresponds to allowing to reach configurations with non-zero velocities at different time steps in the\nsame safe interval). When implementing SIPP1/SIPP2, we\nused the techniques from SIPPwRT (Ma et al. 2019); thus,\nthe latter can be considered to be included in the comparison. The C++ source code of all planners is publicly available https://github.com/PathPlanning/SIPP-IP.\nThe experiments were conducted on a PC with Intel Core\ni7-10700F CPU @ 2.90GHz × 16 and 32Gb of RAM. We\nimposed a limit of 100, 000, 000 of generated nodes for all\nalgorithms. For each instance, we tracked whether the algorithm produced a solution and recorded the algorithm’s\nruntime and the solution cost.\nFig. 4 presents the Success Rate (SR) plots, where SR is\nthe ratio of the successfully solved instances to all of the instances. Indeed, all SIPP-IP competitors have lower SR (except the empty map, where A* and SIPP-IP have the same\nSR). For SIPP1 and SIPP2, this is explained by their incompleteness. For A* this is explained by numerous violations\nof the imposed limit (of 100, 000, 000) on the number of the\ngenerated nodes.\nroom empty warehouse random Sydney Factor\nSIPP1 93%\n82%\n69%\n89%\n97%\n>\nSIPP2 90%\n79%\n66%\n85%\n92%\nSIPP1 77%\n46%\n16%\n55%\n30%\n> 5%\nSIPP2 60%\n39%\n11%\n44%\n20%\nSIPP1 42%\n7%\n1%\n7%\n4%\n> 50%\nSIPP2 22%\n3%\n0%\n1%\n3%\nTable 1: Percentage of SIPP1/SIPP2 solutions that have\nhigher costs compared to SIPP-IP.\nFor each map, we also analyzed the cost of the instances\nthat were successfully solved by SIPP-IP, SIPP1, and SIPP2.\nThe results are presented in Table 1. Each cell in the table tells in how many instances (in percent) the cost of the\nSIPP1/SIPP2 solution exceeded the cost of the optimal solution found by SIPP-IP by a fixed factor. Evidently, in most\ninstances, the costs of competitors are larger than of SIPPIP’s costs, and for certain maps (e.g. room), the percentage\nof the instances where their cost notably exceeds (by more\nthan 50%) SIPP-IP’s cost is significant (up to 42%).\nFinally, the algorithms’ runtime analysis is presented in\nFig. 5. Indeed, all versions of SIPP, including the complete\none, SIPP-IP, are significantly faster than A* and reduce\ncomputation time by two orders of magnitude. Moreover,\nSIPP-IP is faster than SIPP2. Still, it is outperformed by\nSIPP1. This is expected, as SIPP1 exploits a straightforward\nexpansion strategy missing numerous successors SIPP-IP\nwould generate. Absolute (averaged) values of the runtime\nof SIPP-IP (less than 0.1 s on all maps, except the largest\none) suggest that the proposed planner can be utilized in\nreal robotic systems, where taking into account kinodynamic\nconstraints may be of vital importance.",
        "conclusion": "In this work, we have presented a provably complete and\noptimal variant of the prominent Safe Interval Path Planning algorithm capable to handle kinodynamic constraints.\nWe have shown that straightforward ways to extend SIPP\nfail in the considered setup, and therefore a more involved\nalgorithm is needed. The latter has been presented and analyzed theoretically and empirically. The directions for future\nresearch include embedding the suggested algorithm within\nthe multi-agent path planning solver and conducting experiments on real robots.",
        "summary_en": "Safe Interval Path Planning (SIPP) is a powerful algorithm for solving a single-agent pathfinding problem where the agent is confined to a graph and certain vertices/edges of this graph are blocked at certain time intervals due to dynamic obstacles that populate the environment. The original SIPP algorithm relies on the assumption that the agent is able to stop instantaneously. However, this assumption often does not hold in practice, e.g. a mobile robot moving at a cruising speed cannot stop immediately but rather requires gradual deceleration to a full stop that takes time. In other words, the robot is subject to kinodynamic constraints. To this end, the paper introduces a novel variant of SIPP that is provably complete and optimal for planning with acceleration/deceleration. In the experimental evaluation, the paper shows that the key property of the original SIPP still holds for the modified version: it performs much fewer expansions compared to A* and, as a result, is notably faster.",
        "summary_zh": "这篇论文介绍了安全区间路径规划算法（SIPP）的一个新颖变体，用于解决受运动动力学约束的单个代理在动态障碍物环境下移动的问题。原始的SIPP算法假设代理可以立即停止，但这种假设在实践中通常不成立。因此，该论文引入了一种新的SIPP变体，可以考虑加速和减速等动力学约束，保证了完备性和最优性。实验结果表明，新版本的SIPP比A*算法扩展次数更少，因此速度更快。"
    },
    {
        "title": "Hierarchical Text Classification as Sub-hierarchy Sequence Generation",
        "abstract": "Hierarchical text classification (HTC) is essential for various real applications. However, HTC models are challenging to develop because they often require processing a large volume of documents and labels with hierarchical taxonomy. Recent HTC models based on deep learning have attempted to incorporate hierarchy information into a model structure. Consequently, these models are challenging to implement when the model parameters increase for a large-scale hierarchy because the model structure depends on the hierarchy size. To solve this problem, we formulate HTC as a sub-hierarchy sequence generation to incorporate hierarchy information into a target label sequence instead of the model structure. Subsequently, we propose the Hierarchy DECoder (HiDEC), which decodes a text sequence into a sub-hierarchy sequence using recursive hierarchy decoding, classifying all parents at the same level into children at once. In addition, HiDEC is trained to use hierarchical path information from a root to each leaf in a sub-hierarchy composed of the labels of a target document via an attention mechanism and hierarchy-aware masking. HiDEC achieved state-of-the-art performance with significantly fewer model parameters than existing models on benchmark datasets, such as RCV1-v2, NYT, and EURLEX57K.",
        "introduction": "Hierarchical text classification (HTC) uses a hierarchy such\nas a web taxonomy to classify a given text into multiple\nlabels. Moreover, classification tasks are essential in realworld applications because of the tremendous amount of\ndata on the web that should be properly organized for applications such as product navigation (Kozareva 2015; Cevahir\nand Murakami 2016) and news categorization (Lewis et al.\n2004; Sandhaus. 2008).\nRecent HTC research using deep learning can be categorized into local and global approaches. In the local approach (Peng et al. 2018; Kowsari et al. 2017; Shimura,\nLi, and Fukumoto 2018; Banerjee et al. 2019; Dumais and\nChen 2000; Wehrmann, Cerri, and Barros 2018), a classifier is built for each unit after the entire hierarchy is split\ninto a set of small units. Subsequently, the classifiers are\napplied in sequence according to a path from a root to target labels in a top-down manner. In contrast, in the global\nFigure 1: Example of converting the target labels of two documents to the sub-hierarchy sequences. The existing global\nmodel uses the entire hierarchy twice. In contrast, the proposed HiDEC uses the sufficiently small sub-hierarchies relevant to the documents twice.\napproach (Zhao et al. 2018; Wang et al. 2021; Peng et al.\n2021; Wang et al. 2022; Zhou et al. 2020; Mao et al. 2019;\nChen et al. 2021; Sinha et al. 2018; Deng et al. 2021; Wu,\nXiong, and Wang 2019; Yang et al. 2018), a classifier for\nall labels in the entire hierarchy is built, excluding the hierarchy structure through flattening. A document hierarchy\ninformation can be obtained using a structure encoder and\nmerged with text features from a text encoder. The global approach achieves superior performance to the local approach\nowing to the effective design of the structure encoders, such\nas the graph convolution network (Kipf and Welling 2017)\nand Graphormer (Ying et al. 2021).\nHowever, the global approach has scalability limitations\nbecause the structure encoders require a disproportionate\nnumber of parameters as the size of the hierarchies increases. For example, HiAGM (Zhou et al. 2020) and HiMatch (Chen et al. 2021), recent GCN-based structure encoders, require a weight matrix to convert text features to\nlabel features. In HGCLR (Wang et al. 2022), edge and spatial encodings were employed to represent the relationship\nbetween two nodes.\nIn existing global models, a large model size is inevitable\nbecause they attempt to incorporate the entire hierarchy information with respect to the document labels into a model\nstructure. In contrast, employing a sub-hierarchy comprising\na set of paths from a root to each target label is sufficient because most labels are irrelevant to a target document. To this\nend, we formulate HTC as a sub-hierarchy sequence generation using an encoder-decoder architecture to incorporate\nthe sub-hierarchy information into a target label sequence instead of the model structure. Figure 1 shows the differences\nbetween the proposed approach and the existing global models. For example, given a hierarchy and two documents with\ndifferent labels in Figure 1-(a), a global model attempts to\ncapture the entire hierarchy information with respect to the\ndocument labels, as shown in Figure 1-(b). For further improvement, the document labels are converted into a subhierarchy sequence using a depth-first search on an entire\nhierarchy and parse tree notation, as shown in Figure 1-(c)\nand -(d).\nBased on this idea, we propose a Hierarchy DECoder\n(HiDEC)1, which recursively decodes the text sequence\ninto a sub-hierarchy sequence by sub-hierarchy decoding\nwhile remaining aware of the path information. The proposed method comprises hierarchy embeddings, hierarchyaware masked self-attention, text-hierarchy attention, and\nsub-hierarchy decoding, similar to the decoder of Transformer (Vaswani et al. 2017). Hierarchy-aware masked selfattention facilitates learning all hierarchy information in a\nsub-hierarchy sequence. A hierarchy-aware mask captures\nthe sub-hierarchy information by considering the dependencies between each label and its child labels from the\nsub-hierarchy sequence at once in the training step. Subsequently, two features generated from the hierarchy-aware\nmasked self-attention and a text encoder are merged through\ntext-hierarchy attention. Note that HiDEC does not require\nadditional parameters for the structure encoder as used in\nglobal models, such as HiAGM, HiMatch, and HGCLR.\nConsequently, the parameters of the model increase linearly\nwith respect to the classes in a hierarchy. In the inference\nstep, sub-hierarchy decoding is recursively applied to expand from parent to child labels in a top-down manner. As\na result, all parent labels at the same depth are expanded simultaneously. Thus, the maximum recursions are the depth\nof the entire hierarchy and not the sequence length.\nA series of experiments show that HiDEC outperforms state-of-the-art (SOTA) models on two small-scale\ndatasets, RCV1-v2 and NYT, and a large-scale dataset,\nEURLEX57K. Consequently, HiDEC achieves better performance with significantly fewer parameters on the three\nbenchmark datasets. Thus, the proposed approach can solve\nscalability problems in large-scale hierarchies.\nThe contributions of this paper can be summarized as follows:\n• This paper formulates HTC as a sub-hierarchy sequence\ngeneration using an encoder-decoder architecture. We\ncan incorporate the hierarchy information into the subhierarchy sequence instead of the model structure, as all\ndependencies are aware by parse tree notation.\n• This paper proposes a Hierarchy DECoder (HiDEC)\n1Code is available on https://github.com/SangHunIm/HiDEC\nthat recursively decodes the text sequence into a subhierarchy sequence by sub-hierarchy decoding while remaining aware of the path information.\n• This paper demonstrates the superiority of HiDEC by\ncomparing SOTA models on three benchmark HTC\ndatasets (RCV1-v2, NYT, and EURLEX57K). The results reveal the role of HiDEC in HTC through in-depth\nanalysis.",
        "related work": "The critical point to HTC is the use of hierarchy information,\ndenoted as relationships among labels. For example, the relationships include root-to-target labels (path information),\nparent-to-child, and the entire hierarchy (holistic information).\nResearch on HTC can be categorized into local and\nglobal approaches. In the local approach, a set of classifiers are used for small units of classes, such as for-eachclass (Banerjee et al. 2019), for-each-parent (Dumais and\nChen 2000; Kowsari et al. 2017), for-each-level (Shimura,\nLi, and Fukumoto 2018), and for-each-sub-hierarchy (Peng\net al. 2018). In (Kowsari et al. 2017), HDLTex was introduced as a local model that combined a deep neural network\n(DNN), CNN, and RNN to classify child nodes. Moreover,\nHTrans (Banerjee et al. 2019) extended HDLTex to maintain path information across local classifiers based on transfer learning from parent to child. HMCN (Wehrmann, Cerri,\nand Barros 2018) applied global optimization to the classifier of each level to solve the exposure bias problem. Finally,\nHR-DGCNN (Peng et al. 2018) divided the entire hierarchy\ninto sub-hierarchies using recursive hierarchical segmentation. Unfortunately, applying this to large-scale hierarchies\nis challenging because many parameters are required from a\nset of local classifiers for small units of classes.\nIn the global approach, the proposed methods employed\na single model with path (Zhao et al. 2018; Wang et al.\n2021; Peng et al. 2021; Mao et al. 2019; Sinha et al. 2018;\nDeng et al. 2021) or holistic (Zhou et al. 2020; Chen et al.\n2021; Wang et al. 2022) information. For instance, HNATC\n(Sinha et al. 2018) obtained path information using a sequence of outputs from the previous levels to predict the output at the next level. In HiLAP-RL (Mao et al. 2019), reinforcement learning was exploited in that HTC was formalized as a pathfinding problem. Moreover, HE-AGCRCNN\n(Peng et al. 2021) and HCSM (Wang et al. 2021) used capsule networks. Recent research (Zhou et al. 2020; Chen et al.\n2021; Wang et al. 2022) has attempted to employ holistic information of an entire hierarchy using a structure encoder\nwith GCN (Kipf and Welling 2017) and Graphormer (Ying\net al. 2021). HiAGM (Zhou et al. 2020) propagated text\nthrough GCN, HiMatch (Chen et al. 2021) improved HiAGM by adapting semantic matching between text and label\nfeatures from text features and label embeddings. In addition, HGCLR (Wang et al. 2022) attempted to unify a structure encoder with a text encoder using a novel contrastive\nlearning method, where BERT (Devlin et al. 2019) and\nGraphormer were employed for the text and structure encoders, respectively. Therefore, classification was performed\nFigure 2: (a): The overall architecture of HiDEC. A feature vector from a text encoder is decoded to a sub-hierarchy sequence,\nwhich is expanded one level at once by starting from the root. (b) and (c): Illustration of input and output in training and\ninference, respectively. In step 3 of (a) and the right of (c), HiDEC correctly generates the sub-hierarchy (sequence) we expected.\n/E means “[END]” token.\nusing a hierarchy-aware text feature produced by the text encoder. Finally, it reported SOTA performance on RCV1-v2\nand NYT but is infeasible in large-scale hierarchies because\nof the large model size caused by incorporating holistic information into the model structure.",
        "proposed methods": "The HTC problem can be defined using a tree structure. A\nhierarchy is represented as a tree G = (V, ⃗E) where V =\n{v1, v2, . .. , vC} is a set of C-labels in the hierarchy, and\n⃗E = {(vi, vj)|vi ∈ V, vj ∈ child(vi)} is a set of edges between a label vi and a child vj of vi. D = {d1, d2, . .. , dK}\nis a collection of K documents. A document dk has a subhierarchy Gdk = (V dk, ⃗Edk) converted from assigned labels where V dk = Ldk ∪ {vdk\ni |vdk\ni\n∈ ancestor(vdk\nj ), vdk\nj\n∈\nLdk} and ⃗Edk = {(vi, vj)|vj ∈ V dk, vi ∈ parent(vj)},\nwhere Ldk = {vdk\n1 , vdk\n2 , . . . , vdk\nt } is a label set of document\ndk. In other words, Gdk is constructed using all the labels\nassigned to dk and their ancestors. ˆGdk\n0\n= ({vroot}, ∅) is\nthe initial sub-hierarchy of HiDEC, which has a root and no\nedges. Based on ˆGdk\n0 , recursive hierarchy decoding is defined by expanding ˆGdk\np\nfor p times from p=0. The goal of\ntraining HiDEC is given by ˆGdk\np = Gdk.\nIn Figure 2-(a), the overall architecture of HiDEC is presented with a demonstration of the recursive hierarchy decoding. The remainder of this section presents the details of\nthe proposed model.\nText Encoder\nIn the proposed model, a text encoder can use any model\nthat outputs the text feature matrix of all the input tokens,\nsuch as GRU and BERT (Devlin et al. 2019). For simplicity, let us denote dk as T = [w1, w2, .. . , wN] where wn\nis a one-hot vector for an index of the n-th token. Initially,\na sequence of tokens was converted into word embeddings\nH0(= W0T) ∈ RN×e where W0 is the weight matrix of\nthe word embedding layer, and e is an embedding dimension. Given H0, the hidden state H from the text encoder\ncan be computed using Equation 1:\nH = TextEncoder(H0).\n(1)\nHierarchy DECoder (HiDEC)\nHierarchy Embedding Layer\nThe sub-hierarchy embeddings, as shown in Figure 1, is obtained by initially constructing a sub-hierarchy sequence from a document dk.\nThis process consists of two steps. First, a sub-hierarchy\nGdk = (V dk, ⃗Edk) of dk is built with its target labels.\nSecond, a sub-hierarchy sequence S following a parse tree\nnotation is generated from Gdk. Three special tokens, “(”,\n“)”, and “[END]”, are used to properly represent the subhierarchy. The tokens “(” and “)” denote the start and end of\na path from each label, respectively, whereas the “[END]”\ntoken indicates the end of a path from a root. For example,\nS=[( R ( A ( D ( I ( [END] ) ) ) ) ( B ( F ( [END] ) ) )\n( C ( [END] ) ) )] is constructed in Figure 1 with a label\nset [C,F,I]. Once again, the tokens in S are represented as\none-hot vectors for further processing. Subsequently, these\ntokens can be represented as ¯S = [s1, s2, .. . , sM] where\nsi = Iv is a one-hot vector for a label v and the special\ntokens. Finally, the sub-hierarchy embeddings U0 are constructed after explicitly incorporating the level information,\nsimilar to Transformer’s position encoding (Vaswani et al.\n2017), using Equations 2 and 3:\n¯U0 = Ws¯S,\n(2)\nU0 = LevelEmbedding( ¯U0).\n(3)\nHierarchy-Aware Masked Self-Attention\nThis component is responsible for capturing hierarchy information, similar to the structure encoder in global models (Zhou et al.\n2020; Chen et al. 2021; Wang et al. 2022). However, only\na sub-hierarchy from the entire hierarchy, which is thought\nto be highly relevant information for classification, is used\nbased on the self-attention mechanism used by Transformer\nAlgorithm 1: Recursive Hierarchy Decoding in Inference\nIndices: Hierarchy depth P, Number of attentive layers R\nInput: Text feature matrix from text encoder H\nOutput: Predicted label set L\n//HiDEC\n1: L = ∅\n2: ˆG0 = ({vroot}, ∅)\n3: for p = 0, . . . , P − 1 do\n//Sub-hierarchy embedding\n4:\nConvert ˆGp to sub-hierarchy sequence Sp\n5:\nCompute U0 from Sp with Eq.2, 3\n6:\nGenerate masking matrix M with Eq.5\n//Attentive layers\n7:\nfor r = 0, . . . , R − 1 do\n8:\nUr+1 = Attention(Ur, H, M) with Eq.4, 6, 7\n9:\nend for\n10:\nU = UR\n//Sub-hierarchy expansion\n11:\nfor si ∈ Sp do\n12:\nif si /∈ special token set then\n13:\nvi = si\n14:\nfor vj ∈ child(vi) do\n15:\ncij = Ui · WS · Ivj with Eq.8\n16:\npi = sigmoid(ci) with Eq.9\n17:\nGet ˆyi from pi by thresholding\n18:\nfor vk ∈ ˆyi do\n19:\nVp = Vp ∪ {vk}\n20:\n⃗Ep = ⃗Ep ∪ {(vi, vk)}\n21:\nend for\n22:\nend for\n23:\nend if\n24:\nend for\n25:\nˆGp+1 = (Vp, ⃗Ep)\n26: end for\n//Label assignment\n27: for i = 0, . . . , |VP | do\n28:\nif vi ∈ leaf( ˆGP ) then\n29:\nL = L ∪ {vi}\n30:\nelse if vi ==“[END]” then\n31:\nL = L ∪ {parent(vi)}\n32:\nend if\n33: end for\n34: return L\n(Vaswani et al. 2017). To compute self-attention scores,\nwe applied hierarchy-aware masking to incorporate hierarchy information. The self-attention mechanism of Transformer was exploited with a minor modification concerning\nhierarchy-aware masking. The hierarchy-aware masked selfattention of r-th layer is computed using Equation 4:\n˙Ur = MHA(Wr\nQUr−1, Wr\nKUr−1, Wr\nV Ur−1, M), (4)\nwhere MHA is the multi-head attention, the same as that of\nTransformer. Wr\nQ,Wr\nK,Wr\nV are projection weight matrices\nfor the query, key, and value, respectively. Moreover, M is\nthe hierarchy-aware mask defined as follows:\nMij =\n\u001a−1e9\nif vi /∈ ancestor(vj)\n0\nelse\n.\n(5)\nWe ignore the dependency between two labels if they are\nnot the same label and not an ancestor by setting Mij =\n−1e9. This setting makes the model attend to the path information relevant to the input documents and ignores the\nhierarchy information at the lower-level labels than each label to learn a sub-hierarchy sequence at once in a training\nstep. Note that the dependencies of the three special tokens\nwith respect to the other tokens, including themselves, are\nconsidered.\nText-Hierarchy Attention\nIn text-hierarchy attention, we\ncan compute the attention scores of labels by dynamically\nreflecting the importance of tokens in an input document. A\nnew sub-hierarchy matrix ¨Ur of r-th layer is computed by\ncombining the text feature matrix H from the encoder and\n˙Ur without a masking mechanism using Equation 6:\n¨Ur = MHA(Wr\nQ ˙Ur, Wr\nKH, Wr\nV H, −).\n(6)\nSubsequently, the output of r-th layer Ur is obtained\nusing a position-wise feed-forward network (FFN) using\nEquation 7:\nUr = FFN( ¨Ur).\n(7)\nConsequently, the output of the final layer, U, in HiDEC\nis used in the sub-hierarchy expansion.\nSub-Hierarchy Decoding\nSub-hierarchy decoding is crucial in generating a sub-hierarchy using recursive hierarchy\ndecoding. This results in a target sub-hierarchy if HiDEC\nfunctions as expected. For each label, the classification to\nchild labels is performed using the sub-hierarchy matrix U\nusing Equations 8 and 9:\ncij = Ui · WS · Ivj\n∀vj ∈ child(vi),\n(8)\npi = sigmoid(ci),\n(9)\nwhere cij is a similarity score of child vj under a parent\nvi, and pi is the probability of a child vj obtained using a\ntask-specific probability function such as sigmoid. The three\nspecial tokens are excluded when selecting the parent vi.\nWe reduced the label space of HTC by focusing on the\nchild labels of a parent label of interest. During training, we\nuse binary cross-entropy loss functions as shown in Equation\n10:\nL = − 1\nMJ\nM\nX\ni=0\nJ\nX\nj=0\nyijlog(pij) + (1 − yij)log(1 − pij),\n(10)\nwhere J = |child(vi)| indicates the number of child labels\nfor parent vi. Moreover, yij and pij denote a target label of\nj-th child label of vi and its output probability, respectively.\nAt the inference time, recursive hierarchy decoding is performed using a threshold. The details of the recursive hierarchy decoding are described in Algorithm 1. The number of\ndecoding steps is the same as the maximum depth of the hierarchy. At each decoding step, all tokens except the special\ntokens are expanded. Decoding ends if the tokens are leaf labels or “[END]”. Finally, the labels associated with “[END]”\nor leaf labels are assigned to the input as predictions.\nDataset\n|L|\nDepth Avg\nTrain\nVal\nTest\nRCV1-v2\n103\n4\n3.24 20,833 2,316 781,265\nNYT\n166\n8\n7.60 23,345 5,834\n7,292\nEURLEX57K 4,271\n5\n5.00 45,000 6,000\n6,000\nTable 1: Data statistics. |L| denotes the number of labels.\nDepth and Avg are the maximum hierarchy depth and the average number of assigned labels for each text, respectively.",
        "experiments": "Datasets and Evaluation Metrics\nTable 1 lists the data statistics used in the experiments. For\nthe standard evaluation, two small-scale datasets, RCV1-v2\n(Lewis et al. 2004) and NYT (Sandhaus. 2008), and one\nlarge-scale dataset, EURLEX57K (Chalkidis et al. 2019),\nwere chosen. RCV1-v2 comprises 804,414 news documents,\ndivided into 23,149 and 781,265 documents for training\nand testing, respectively, as benchmark splits. We randomly\nsampled 10% of the training data as the validation data for\nmodel selection. NYT comprises 36,471 news documents\ndivided into 29,179 and 7,292 documents for training and\ntesting, respectively. For a fair comparison, we followed the\ndata configurations of previous work (Zhou et al. 2020; Chen\net al. 2021). In particular, EURLEX57K is a large-scale hierarchy with 57,000 documents and 4,271 labels. Benchmark\nsplits of 45,000, 6,000, and 6,000 were used for training,\nvalidation, and testing, respectively. We used Micro-F1 for\nthree datasets and Macro-F1 for RCV1-v2 and NYT.\nImplementation Details\nAfter text cleaning and stopword removal, words with two\nor more occurrences were selected to retain the vocabulary.\nConsequently, the vocabulary sizes for the three benchmark\ndatasets were 60,000.\nFor the text encoder, we opted the simplest encoder, a bidirectional GRU with a single layer, for a fair comparison to\nprevious work (Zhou et al. 2020; Chen et al. 2021). The size\nof the hidden state was set to 300. The word embeddings\nin the text encoder were initialized using 300-dimensional\nGloVe (Pennington, Socher, and Manning 2014). In contrast to the GRU-based encoder, we used BERT, bert-baseuncased (Devlin et al. 2019), to demonstrate the generalization ability of the pre-trained encoder. The output hidden\nmatrix from the last layer of BERT was used as the context\nmatrix H in Equation 1.\nFor HiDEC, a layer with two heads were used for both\nGRU-based encoder and BERT. The label and level embeddings with 300- and 768-dimension for the GRU-based encoder and BERT, respectively, were initialized using a normal distribution with µ=0 and σ=300−0.5. The hidden state\nsize in the attentive layer was the same as the label embedding size. The FFN comprised two FC layers with 600- and\n3,072-dimension feed-forward filter for the GRU-based encoder and BERT, respectively. Based on an empirical test,\nwe removed the residual connection from the original Transformer decoder (Vaswani et al. 2017). The threshold for recursive hierarchy decoding was set to 0.5. A dropout with a\nprobability of 0.5, 0.1, and 0.1 was applied to the embedding\nlayer and behind every FFN and attention, respectively.\nFor optimization, Adam optimizer (Kingma and Ba 2015)\nwas utilized with learning rate lr=1e-4, β1=0.9, β2=0.999,\nand eps=1e-8. The size of the mini-batch was set to 256 for\nGRU-based models. With BERT as a text encoder model, we\nset lr and the mini-batch size to 5e-5 and 64, respectively.\nThe lr was controlled using a linear schedule with a warmup rate of 0.1. Gradient clipping with a maximum gradient\nnorm of 1.0 was performed to prevent gradient overflow.\nAll models were implemented using PyTorch (Paszke\net al. 2019) and trained using NVIDIA A6000. The average score of the ten different models was utilized as the proposed model performance, where the model with the best\nperformance was selected using Micro-F1 on the validation\ndata.\nComparison Models\nWe selected various baseline models from recent work. For\nRCV1-v2 and NYT, TextRCNN (Lai et al. 2015), HiAGM\n(Zhou et al. 2020), HiMatch (Chen et al. 2021), HTCInfoMax (Deng et al. 2021), and HGCLR (Wang et al. 2022)\nwere chosen. TextRCNN comprises bi-GRU and CNN layers and is a hierarchy-unaware model used as a text encoder\nin HiAGM and HiMatch. HiAGM combines text and label features from the text encoder and GCN-based structure\nencoder, respectively, using text propagation. HTCInfoMax\nand HiMatch improved HiAGM with a prior distribution and\nsemantic matching loss, respectively. They can be improved\nby replacing the text encoder with PLM. HGCLR directly\nembeds hierarchy information into the text encoder using\nGraphormer during training. For EURLEX57K, BiGRUATT (Xu et al. 2015), HAN (Yang et al. 2016), CNNLWAN (Mullenbach et al. 2018), BiGRU-LWAN (Chalkidis\net al. 2019), and HiMatch (Chen et al. 2021) were chosen. BiGRU-ATT and HAN are strong baselines for text\nclassification tasks with an attention mechanism. CNN and\nBiGRU-LWAN extended BiGRU-ATT with label-wise attention. Owing to the large model size, applying HiMatch\nto EURLEX57K is infeasible. According to the paper (Chen\net al. 2021), the text-propagation module weakly influences the performance compared with the original approach.\nTherefore, we simplified HiMatch by removing the textpropagation module.\nExperimental Results\nTable 2 summarizes the performance of HiDEC and the\nother models on (a) two small-scale datasets, RCV1-v2 and\nNYT, and (b) a large-scale dataset, EURLEX57K. HiDEC\nachieved the best performance on the three datasets regardless of whether BERT was used (Devlin et al. 2019) as the\ntext encoder. This highlights the effectiveness of HiDEC using sub-hierarchy information rather than entire-hierarchy\ninformation. In addition, we found that HiDEC highly benefits from PLM compared to other models, as the performance gains of HiDEC with and without BERT are relatively large. For example, in MicroF1 on RCV1-v2, the gain\nbetween HiDEC and HiDEC with PLM was 2.42. In contrast, the gain between HiMatch (Chen et al. 2021) and HiModel\nRCV1-v2\nNYT\nMicro Macro Micro Macro\nw/o Pretrained Language Models\nTextRCNN∗ (Zhou et al. 2020) 81.57 59.25 70.83 56.18\nHiAGM (Zhou et al. 2020)\n83.96 63.35 74.97 60.83\nHTCInfoMax (Deng et al. 2021) 83.51 62.71\nHiMatch (Chen et al. 2021)\n84.73 64.11\nHiDEC\n85.54 65.08 76.42 63.99\nw/ Pretrained Language Models\nBERT∗ (Wang et al. 2022)\n85.65 67.02 78.24 65.62\nHiAGM (Wang et al. 2022)\n85.58 67.93 78.64 66.76\nHTCInfoMax (Wang et al. 2022) 85.53 67.09 78.75 67.31\nHiMatch (Chen et al. 2021)\n86.33 68.66\nHGCLR (Wang et al. 2022)\n86.49 68.31 78.86 67.96\nHiDEC\n87.96 69.97 79.99 69.64\n(a)\nModel\nEURLEX57K\nMicro\nw/o Pretrained Language Models\nBiGRU-ATT∗ (Chalkidis et al. 2019)\n68.90\nHAN∗ (Chalkidis et al. 2019)\n68.00\nCNN-LWAN∗ (Chalkidis et al. 2019)\n64.20\nBiGRU-LWAN∗ (Chalkidis et al. 2019)\n69.80\nHiMatch\n71.11\nHiDEC\n71.23\nw/ Pretrained Language Models\nBERT∗\n73.20\nHiDEC\n75.29\n(b)\nTable 2: Performance comparison on three datasets, (a) RCV1-v2 and NYT, (b) EURLEX57K. ∗ denotes models without\nhierarchy information.\nModel\nRCV1-v2\nNYT\nEURLEX57K\n103\n166\n4,271\nw/o Pretrained Language Models\nTextRCNN∗\n18M\n18M\n19M\nHiAGM\n31M\n42M\n5,915M\nHiMatch\n37M\n52M\n6,211M\nHiDEC\n20M\n20M\n21M\nw/ Pretrained Language Models\nBERT∗\n109M\n109M\n112M\nHGCLR\n120M\n121M\n425M\nHiDEC\n123M\n123M\n127M\nTable 3: Model parameter comparison. ∗ denotes models\nwithout hierarchy information.\nMatch with PLM was 1.60. On EURLEX57K, training the\nhierarchy-aware models was infeasible because of the large\nmodel size2 caused by the structure encoder for 4,271 labels, except for HiDEC and simplified HiMatch. However,\nthis does not apply to BERT because of the model size. Similar to RCV1-v2 and NYT, HiDEC with BERT exhibited the\nbest performance.\nModel Parameters\nTable 3 summarizes the parameters for different models\non three benchmark datasets. The label sizes are RCV1-v2\n(103) < NYT (166) << EURLEX57K (4,271). The table\nshows that the parameters of the existing models increase\ndramatically as the label size increases. Note that HiDEC requires significantly fewer parameters even though the label\n2Except for HiDEC and simplified HiMatch, we encountered\nout-of-memory with NVIDIA A6000 48GB for other models in\ntraining.\nModel\nRCV1-v2\nEURLEX57K\nMicro Macro\nMicro\nHiDEC\n85.54 64.04\n71.31\n+ Residual connection\n84.69 60.32\n69.97\n− Hierarchy-aware masking 85.46 63.73\n71.22\n− Level Embedding\n85.51 63.91\n70.90\nTable 4: Ablation studies on RCV1-v2 and EURLEX57K.\nsize increases. In the extreme case on EURLEX57K, HiDEC\nonly requires 21M parameters, which are 295x smaller than\nHiMatch with 6,211M. Consequently, the parameters in\nHiDEC increase linearly with respect to the labels in a hierarchy because no extra parameters are required for the structure encoder and sub-tasks. HiAGM and HiMatch require\nmerging parameters projecting text features into label features for text propagation. In addition, HGCLR needs edge\nand spatial encoding parameters for the structure encoder.\nHowever, HiDEC does not require these parameters because\nattentive layers play the same role. Only the label embeddings increase according to the label size in a hierarchy.\nAblation Studies\nTable 4 shows the ablation studies of each component\nin HiDEC without PLM on RCV1-v2 and EURLEX57K.\nHiDEC differs from the original Transformer decoder\n(Vaswani et al. 2017) in the absence of a residual connection and the existence of hierarchy-aware masking and level\nembedding. We observed that adding the residual connection and eliminating hierarchy-aware masking and level embedding also had a negative effect. Among them, adding a\nresidual connection had the most negative effect. We presumed that the essential information from previous features\nfor HTC was hindered by the residual connection.\nFigure 3: Heatmaps of the attention scores in HiDEC on RCV1-v2. (a) and (b) are heatmap of the hierarchy-aware masked\nself-attention scores and text-hierarchy attention scores, respectively. Attention scores over 0.3 are clipped in all the heatmaps\nand the similar colors indicate the same level.\n1\n2\n3\n4\n5\n6\n7\n8\nLevel\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nMicro-F1\nHiDEC\nHiMatch\nHiAGM\nTextRCNN\nFigure 4: The level-wise performance of GRU-based HTC\nmodels on NYT.\nInterpretation of Attentions\nWe investigated the roles of hierarchy-aware masked selfattention and text-hierarchy attention by visualizing the attention scores on RCV1-v2, as shown in Figure 3. The selfattention scores are shown in Figure 3-(a). In (α), the attention score between “(” and “C17” is relatively high where\n“(” is a starting path from “C17”. The score shows that\nthe special tokens “(” and “)” were appropriately associated\nwith the corresponding labels. In (β), the dependency between child “E21” and parent “ECAT” was well-described\nbecause the attention scores for child labels under parent\n“ECAT” were high. In (γ), the results show a dependency\nbetween the label assignment sequence – [“(”, “END”, “)”]\nand the label “E21”. From these three examples, we can conclude that hierarchy-aware masked self-attention effectively\ncaptures the path dependencies. Figure 3-(b) shows the attention scores between the input tokens and a sub-hierarchy\nsequence. Some tokens, such as the “rating” and “moody,”\nhave high attention scores for the descendants of “CCAT”\nand itself, where “CCAT” denotes “CORPORATE/INDUSTRIAL”. In contrast, some tokens like “issuer,” “municipal,”\nand “investors” have high attention scores for the descendants of “EACT” and itself, where “EACT” denotes “ECONOMICS”. This result indicates that the labels are associated with different tokens to different degrees.\nLevel-Wise Performance\nFigure 4 depicts the level-wise performance of the models using a GRU-based text encoder on NYT. It shows the\neffectiveness of the hierarchy-aware models by comparing\nTextRCNN increases as the level increases. Among them,\nHiDEC consistently achieved the best performance at all\nlevels. Note that significant improvements were obtained at\nthe deep levels, implying that sub-hierarchy information is\nmore powerful in capturing the structure information of a\ntarget document than the entire hierarchy information.",
        "conclusion": "This paper addressed the scalability limitations of recent\nHTC models due to the large model size of the structure encoders. To solve this problem, we formulated HTC as a subhierarchy sequence generation using an encoder-decoder architecture. Subsequently, we propose Hierarchy DECoder\n(HiDEC) which recursively decodes the text sequence into\na sub-hierarchy sequence by sub-hierarchy decoding while\nstaying aware of the path information. HiDEC achieved\nstate-of-the-art performance with significantly fewer model\nparameters than existing models on benchmark datasets,\nsuch as RCV1-v2, NYT, and EURLEX57K. In the future,\nwe plan to extend the proposed model to extremely largescale hierarchies (e.g., MeSH term indexing or product navigation) and introduce a novel training strategy combining\ntop-down and bottom-up methods that can effectively use a\nhierarchy structure.",
        "summary_en": "Hierarchical text classification (HTC) is essential for various real applications. However, HTC models are challenging to develop because they often require processing a large volume of documents and labels with hierarchical taxonomy. Recent HTC models based on deep learning have attempted to incorporate hierarchy information into a model structure. Consequently, these models are challenging to implement when the model parameters increase for a large-scale hierarchy because the model structure depends on the hierarchy size. To solve this problem, this paper formulates HTC as a sub-hierarchy sequence generation to incorporate hierarchy information into a target label sequence instead of the model structure. Subsequently, the paper proposes the Hierarchy DECoder (HiDEC), which decodes a text sequence into a sub-hierarchy sequence using recursive hierarchy decoding, classifying all parents at the same level into children at once. In addition, HiDEC is trained to use hierarchical path information from a root to each leaf in a sub-hierarchy composed of the labels of a target document via an attention mechanism and hierarchy-aware masking. HiDEC achieved state-of-the-art performance with significantly fewer model parameters than existing models on benchmark datasets, such as RCV1-v2, NYT, and EURLEX57K.",
        "summary_zh": "这篇论文介绍了一种名为Hierarchy DECoder（HiDEC）的模型用于层次文本分类（HTC），解决了现有基于深度学习的HTC模型在处理大规模层次结构时存在的问题。该模型通过子层次序列生成的方式来将层次信息融入目标标签序列，从而避免了模型结构随层次大小增加而复杂化的问题。HiDEC利用递归层次解码将文本序列解码为子层次序列，并通过注意力机制和层次感知的掩蔽训练模型。在RCV1-v2、NYT和EURLEX57K等基准数据集上的实验结果表明，HiDEC在模型参数数量较少的情况下取得了最先进的性能表现。"
    },
    {
        "title": "Weakly-Supervised Hierarchical Text Classification",
        "abstract": "Hierarchical text classiﬁcation, which aims to classify text documents into a given hierarchy, is an important task in many real-world applications. Recently, deep neural models are gaining increasing popularity for text classiﬁcation due to their expressive power and minimum requirement for feature engineering. However, applying deep neural networks for hierarchical text classiﬁcation remains challenging, because they heavily rely on a large amount of training data and meanwhile cannot easily determine appropriate levels of documents in the hierarchical setting. In this paper, we propose a weakly-supervised neural method for hierarchical text classiﬁcation. Our method does not require a large amount of training data but requires only easy-to-provide weak supervision signals such as a few class-related documents or keywords. Our method effectively leverages such weak supervision signals to generate pseudo documents for model pre-training, and then performs self-training on real unlabeled data to iteratively reﬁne the model. During the training process, our model features a hierarchical neural structure, which mimics the given hierarchy and is capable of determining the proper levels for documents with a blocking mechanism. Experiments on three datasets from different domains demonstrate the efﬁcacy of our method compared with a comprehensive set of baselines.",
        "introduction": "Hierarchical text classiﬁcation, which aims at classifying text\ndocuments into classes that are organized into a hierarchy,\nis an important text mining and natural language processing\ntask. Unlike ﬂat text classiﬁcation, hierarchical text classiﬁcation considers the interrelationships among classes and\nallows for organizing documents into a natural hierarchical\nstructure. It has a wide variety of applications such as semantic classiﬁcation (Tang, Qin, and Liu 2015), question\nanswering (Li and Roth 2002), and web search organization\n(Dumais and Chen 2000).\nTraditional ﬂat text classiﬁers (e.g., SVM, logistic regression) have been tailored in various ways for hierarchical text\nclassiﬁcation. Early attempts (Ceci and Malerba 2006) disregard the relationships among classes and treat hierarchical\nclassiﬁcation tasks as ﬂat ones. Later approaches (Dumais\nand Chen 2000; Liu et al. 2005; Cai and Hofmann 2004)\ntrain a set of local classiﬁers and make predictions in a topdown manner, or design global hierarchical loss functions\nthat regularize with the hierarchy. Most existing efforts for\nhierarchical text classiﬁcation rely on traditional text classiﬁers. Recently, deep neural networks have demonstrated\nsuperior performance for ﬂat text classiﬁcation. Compared\nwith traditional classiﬁers, deep neural networks (Kim 2014;\nYang et al. 2016) largely reduce feature engineering efforts by\nlearning distributed representations that capture text semantics. Meanwhile, they provide stronger expressive power over\ntraditional classiﬁers, thereby yielding better performance\nwhen large amounts of training data are available.\nMotivated by the enjoyable properties of deep neural networks, we explore using deep neural networks for hierarchical text classiﬁcation. Despite the success of deep neural\nmodels in ﬂat text classiﬁcation and their advantages over\ntraditional classiﬁers, applying them to hierarchical text classiﬁcation is nontrivial because of two major challenges.\nThe ﬁrst challenge is that the training data deﬁciency prohibits neural models from being adopted. Neural models are\ndata hungry and require humans to provide tons of carefullylabeled documents for good performance. In many practical\nscenarios, however, hand-labeling excessive documents often\nrequires domain expertise and can be too expensive to realize.\nThe second challenge is to determine the most appropriate\nlevel for each document in the class hierarchy. In hierarchical\ntext classiﬁcation, documents do not necessarily belong to\nleaf nodes and may be better assigned to intermediate nodes.\nHowever, there are no simple ways for existing deep neural\nnetworks to automatically determine the best granularity for\na given document.\nIn this work, we propose a neural approach named\nWeSHClass, for Weakly-Supervised Hierarchical Text\nClassiﬁcation and address the above two challenges. Our\napproach is built upon deep neural networks, yet it requires\nonly a small amount of weak supervision instead of excessive\ntraining data. Such weak supervision can be either a few (e.g.,\nless than a dozen) labeled documents or class-correlated keywords, which can be easily provided by users. To leverage\nsuch weak supervision for effective classiﬁcation, our approach employs a novel pretrain-and-reﬁne paradigm. Specifically, in the pre-training step, we leverage user-provided\nseeds to learn a spherical distribution for each class, and\nthen generate pseudo documents from a language model\nguided by the spherical distribution. In the reﬁnement step,\nwe iteratively bootstrap the global model on real unlabeled\ndocuments, which self-learns from its own high-conﬁdent\npredictions.\nWeSHClass automatically determines the most appropriate level during the classiﬁcation process by explicitly\nmodeling the class hierarchy. Speciﬁcally, we pre-train a\nlocal classiﬁer at each node in the class hierarchy, and aggregate the classiﬁers into a global one using self-training.\nThe global classiﬁer is used to make ﬁnal predictions in a\ntop-down recursive manner. During recursive predictions, we\nintroduce a novel blocking mechanism, which examines the\ndistribution of a document over internal nodes and avoids\nmandatorily pushing general documents down to leaf nodes.\nOur contributions are summarized as follows:\n1. We design a method for hierarchical text classiﬁcation using neural models under weak supervision. WeSHClass\ndoes not require large amounts of training documents but\njust easy-to-provide word-level or document-level weak\nsupervision. In addition, it can be applied to different classiﬁcation types (e.g., topics, sentiments).\n2. We propose a pseudo document generation module that\ngenerates high-quality training documents only based on\nweak supervision sources. The generated documents serve\nas pseudo training data which alleviate the training data\nbottleneck together with the subsequent self-training step.\n3. We propose a hierarchical neural model structure that\nmirrors the class taxonomy and its corresponding training\nmethod, which involves local classiﬁer pre-training and\nglobal classiﬁer self-training. The entire process is tailored\nfor hierarchical text classiﬁcation, which automatically\ndetermines the most appropriate level of each document\nwith a novel blocking mechanism.\n4. We conduct a thorough evaluation on three real-world\ndatasets from different domains to demonstrate the effectiveness of WeSHClass. We also perform several case\nstudies to understand the properties of different components in WeSHClass.",
        "problem formulation": "We study hierarchical text classiﬁcation that involves treestructured class categories. Speciﬁcally, each category can\nbelong to at most one parent category and can have arbitrary\nnumber of children categories. Following the deﬁnition in\n(Silla and Freitas 2010), we consider non-mandatory leaf prediction, wherein documents can be assigned to both internal\nand leaf categories in the hierarchy.\nTraditional supervised text classiﬁcation methods rely on\nlarge amounts of labeled documents for each class. In this\nwork, we focus on text classiﬁcation under weak supervision.\nGiven a class taxonomy represented as a tree T , we ask the\nuser to provide weak supervision sources (e.g., a few classrelated keywords or documents) only for each leaf class in T .\nThen we propagate the weak supervision sources upwards in\nT from leaves to root, so that the weak supervision sources\nof each internal class are an aggregation of weak supervision\nsources of all its descendant leaf classes. Speciﬁcally, given\nM leaf node classes, the supervision for each class comes\nfrom one of the following:\n1. Word-level supervision: S = {Sj}|M\nj=1, where Sj =\n{wj,1, . . . , wj,k} represents a set of k keywords correlated\nwith class Cj;\n2. Document-level supervision: DL = {DL\nj }|M\nj=1, where\nDL\nj = {Dj,1, . . . , Dj,l} denotes a small set of l (l ≪\ncorpus size) labeled documents in class Cj.\nNow we are ready to formulate the hierarchical text classiﬁcation problem. Given a text collection D = {D1, . . . , DN},\na class category tree T , and weak supervisions of either S\nor DL for each leaf class in T , the weakly-supervised hierarchical text classiﬁcation task aims to assign the most likely\nlabel Cj ∈ T to each Di ∈ D, where Cj could be either an\ninternal or a leaf class.",
        "pseudo document generation": "To break the bottleneck of lacking abundant labeled data for\nmodel training, we leverage user-given weak supervision to\ngenerate pseudo documents, which serve as pseudo training\ndata for model pre-training. In this section, we ﬁrst introduce how to leverage weak supervision sources to model\nclass distributions in a spherical space, and then explain how\nto generate class-speciﬁc pseudo documents based on class\ndistributions and a language model.\nModeling Class Distribution\nWe model each class as a\nhigh-dimensional spherical probability distribution which has\nbeen shown effective for various tasks (Zhang et al. 2017).\nWe ﬁrst train Skip-Gram model (Mikolov et al. 2013) to learn\nd-dimensional vector representations for each word in the\ncorpus. Since directional similarities between vectors are\nmore effective in capturing semantic correlations (Banerjee\net al. 2005; Levy, Goldberg, and Dagan 2015), we normalize\nall the d-dimensional word embeddings so that they reside\non a unit sphere in Rd. For each class Cj ∈ T , we model\nthe semantics of class Cj as a mixture of von Mises Fisher\n(movMF) distributions (Banerjee et al. 2005; Gopal and Yang\n2014) in Rd:\nf(x | Θ) =\nm\nX\nh=1\nαhfh(x | µh, κh) =\nm\nX\nh=1\nαhcd(κh)eκhµT\nh x,\nwhere Θ = {α1, . . . , αm, µ1, . . . , µm, κ1, . . . , κm}, ∀h ∈\n{1, . . . , m}, κh ≥ 0, ∥µh∥ = 1, and the normalization constant cd(κh) is given by\ncd(κh) =\nκd/2−1\nh\n(2π)d/2Id/2−1(κh),\nwhere Ir(·) represents the modiﬁed Bessel function of the\nﬁrst kind at order r. We choose the number of components in\nmovMF for leaf and internal classes differently:\n• For each leaf class Cj, we set the number of vMF component m = 1, and the resulting movMF distribution is\nequivalent to a single vMF distribution, whose two parameters, the mean direction µ and the concentration parameter\nκ, act as semantic focus and concentration for Cj.\n• For each internal class Cj, we set the number of vMF\ncomponent m to be the number of its children classes. Recall that we only ask the user to provide weak supervision\nsources at the leaf classes, and the weak supervision source\nof Cj are aggregated from its children classes. The semantics of a parent class can thus be seen as a mixture of the\nsemantics of its children classes.\nWe ﬁrst retrieve a set of keywords for each class given the\nweak supervision sources, then ﬁt movMF distributions using\nthe embedding vectors of the retrieved keywords. Speciﬁcally,\nthe set of keywords are retrieved as follows: (1) When users\nprovide related keywords Sj for each class j, we use the average embedding of these seed keywords to ﬁnd top-n closest\nkeywords in the embedding space; (2) When users provide\ndocuments DL\nj that are correlated with class j, we extract n\nrepresentative keywords from DL\nj using tf-idf weighting. The\nparameter n above is set to be the largest number that does\nnot result in shared words across different classes. Compared\nto directly using weak supervision signals, retrieving relevant\nkeywords for modeling class distributions has a smoothing\neffect which makes our model less sensitive to the weak\nsupervision sources.\nLet X be the set of embeddings of the n retrieved keywords\non the unit sphere, i.e.,\nX = {xi ∈ Rd | xi drawn from f(x | Θ), 1 ≤ i ≤ n},\nwe use the Expectation Maximization (EM) framework\n(Banerjee et al. 2005) to estimate the parameters Θ of the\nmovMF distributions:\n• E-step:\np(zi = h | xi, Θ(t)) =\nα(t)\nh fh(xi | µ(t)\nh , κ(t)\nh )\nPm\nh′=1 α(t)\nh′ fh′(xi | µ(t)\nh′ , κ(t)\nh′ )\n,\nwhere Z = {z1, . . . , zn} is the set of hidden random variables that indicate the particular vMF distribution from\nwhich the points are sampled;\n• M-step:\nα(t+1)\nh\n= 1\nn\nn\nX\ni=1\np(zi = h | xi, Θ(t)),\nr(t+1)\nh\n=\nn\nX\ni=1\np(zi = h | xi, Θ(t))xi,\nµ(t+1)\nh\n=\nr(t+1)\nh\n∥r(t+1)\nh\n∥\n,\nId/2(κ(t+1)\nh\n)\nId/2−1(κ(t+1)\nh\n)\n=\n∥r(t+1)\nh\n∥\nPn\ni=1 p(zi = h | xi, Θ(t)).\nwhere we use the approximation procedure based on Newton’s method (Banerjee et al. 2005) to derive an approximation of κ(t+1)\nh\nbecause the implicit equation makes\nobtaining an analytic solution infeasible.\nLanguage Model Based Document Generation\nAfter obtaining the distributions for each class, we use an LSTMbased language model (Sundermeyer, Schl¨uter, and Ney\n2012) to generate meaningful pseudo documents. Speciﬁcally, we ﬁrst train an LSTM language model on the entire\ncorpus. To generate a pseudo document of class Cj, we sample an embedding vector from the movMF distribution of Cj\nand use the closest word in embedding space as the beginning\nword of the sequence. Then we feed the current sequence\nto the LSTM language model to generate the next word and\nattach it to the current sequence recursively 1. Since the beginning word of the pseudo document comes directly from\nthe class distribution, the generated document is ensured to\nbe correlated to Cj. By virtue of the mixture distribution\nmodeling, the semantics of every children class (if any) of\nCj gets a chance to be included in the pseudo documents,\nso that the resulting trained neural model will have better\ngeneralization ability.",
        "the hierarchical classiﬁcation model": "In this section, we introduce the hierarchical neural model\nand its training method under weakly-supervised setting.\nLocal Classiﬁer Pre-Training\nWe construct a neural classiﬁer Mp (Mp could be any text\nclassiﬁer such as CNNs or RNNs) for each class Cp ∈ T if\nCp has two or more children classes. Intuitively, the classiﬁer\nMp aims to classify the documents assigned to Cp into its\nchildren classes for more ﬁne-grained predictions. For each\ndocument Di, the output of Mp can be interpreted as p(Di ∈\nCc | Di ∈ Cp), the conditional probability of Di belonging\nto each children class Cc of Cp, given Di is assigned to Cp.\nThe local classiﬁers perform local text classiﬁcation at\ninternal nodes in the hierarchy, and serve as building blocks\nthat can be later ensembled into a global hierarchical classiﬁer. We generate β pseudo documents per class and use\nthem to pre-train local classiﬁers with the goal of providing each local classiﬁer with a good initialization for the\nsubsequent self-training step. To prevent the local classiﬁers\nfrom overﬁtting to pseudo documents and performing badly\non classifying real documents, we use pseudo labels instead\nof one-hot encodings in pre-training. Speciﬁcally, we use a\nhyperparameter α that accounts for the “noises” in pseudo\ndocuments, and set the pseudo label l∗\ni for pseudo document\nD∗\ni (we use D∗\ni instead of Di to denote a pseudo document)\nas\nl∗\nij =\n\u001a(1 − α) + α/m\nD∗\ni is generated from class j\nα/m\notherwise\n(1)\nwhere m is the total number of children classes at the corresponding local classiﬁer. After creating pseudo labels, we\npre-train each local classiﬁer Mp of class Cp using the pseudo\ndocuments for each children class of Cp, by minimizing the\nKL divergence loss from outputs Y of Mp to the pseudo\n1In case of long pseudo documents, we repeatedly generate several sequences and concatenate them to form the entire document.\nlabels L∗, namely\nloss = KL(L∗∥Y) =\nX\ni\nX\nj\nl∗\nij log l∗\nij\nyij\n.\nGlobal Classiﬁer Self-Training\nAt each level k in the class taxonomy, we need the network\nto output a probability distribution over all classes. Therefore,\nwe construct a global classiﬁer Gk by ensembling all local\nclassiﬁers from root to level k. The ensemble method is\nshown in Figure 1. The multiplication operation conducted\nbetween parent classiﬁer output and children classiﬁer output\ncan be explained by the conditional probability formula:\np(Di ∈ Cc) = p(Di ∈ Cc ∩ Di ∈ Cp)\n= p(Di ∈ Cc | Di ∈ Cp)p(Di ∈ Cp),\nwhere Di is a document; Cc is one of the children classes of\nCp. This formula can be recursively applied so that the ﬁnal\nprediction is the multiplication of all local classiﬁers’ outputs\non the path from root to the destination node.\nLevel 0 \u000bRoot)\nLocal Classiﬁer\nLevel 1 (Politics)\nLocal Classiﬁer\nLevel 1 (Sports)\nLocal Classiﬁer\nLevel 2\n(Military)\nLevel 2\n(Gun Control)\nLevel 2\n(Hockey)\nLevel 2\n(Basketball)\nLevel 2\n(Tennis)\np(Di 2 Politics) = 0.05\np(Di 2 Sports) = 0.95\np(Di 2 Military|Di 2 Politics) = 0.34\np(Di 2 Basketball|Di 2 Sports) = 0.8\n0.34\n0.66\n0.1\n0.1\n0.8\np(Di 2 Military) = 0.05 ⇥ 0.34 = 0.017\np(Di 2 Basketball) = 0.95 ⇥ 0.8 = 0.76\nFigure 1: Ensemble of local classiﬁers.\nGreedy top-down classiﬁcation approaches will propagate\nmisclassiﬁcations at higher levels to lower levels, which can\nnever be corrected. However, the way we construct the global\nclassiﬁer assigns documents soft probability at each level,\nand the ﬁnal class prediction is made by jointly considering all classiﬁers’ outputs from root to the current level via\nmultiplication, which gives lower-level classiﬁers chances to\ncorrect misclassiﬁcations made at higher levels.\nAt each level k of the class taxonomy, we ﬁrst ensemble\nall local classiﬁers from root to level k to form the global\nclassiﬁer Gk, and then use Gk’s prediction on all unlabeled\nreal documents to reﬁne itself iteratively. Speciﬁcally, for\neach unlabeled document Di, Gk outputs a probability distribution yij of Di belonging to each class j at level k, and we\nset pseudo labels to be (Xie, Girshick, and Farhadi 2016):\nl∗∗\nij =\ny2\nij/fj\nP\nj′ y2\nij′/fj′ ,\n(2)\nwhere fj = P\ni yij is the soft frequency for class j.\nThe pseudo labels reﬂect high-conﬁdent predictions, and\nwe use them to guide the ﬁne-tuning of Gk, by iteratively (1)\ncomputing pseudo labels L∗∗ based on Gk’s current predictions Y and (2) minimizing the KL divergence loss from Y to\nL∗∗. This process terminates when less than δ% of the documents in the corpus have class assignment changes. Since Gk\nis the ensemble of local classiﬁers, they are ﬁne-tuned simultaneously via back-propagation during self-training. We will\ndemonstrate the advantages of using global classiﬁer over\ngreedy approaches in the experiments.\nBlocking Mechanism\nIn hierarchical classiﬁcation, some documents should be classiﬁed into internal classes because they are more related to\ngeneral topics rather than any of the more speciﬁc topics,\nwhich should be blocked at the corresponding local classiﬁer\nfrom getting further passed to children classes.\nWhen a document Di is classiﬁed into an internal class\nCj, we use the output q of Cj’s local classiﬁer to determine\nwhether or not Di should be blocked at the current class: if\nq is close to a one-hot vector, it strongly indicates that Di\nshould be classiﬁed into the corresponding child; if q is close\nto a uniform distribution, it implies that Di is equally relevant\nor irrelevant to all the children of Cj and thus more likely\na general document. Therefore, we use normalized entropy\nas the measure for blocking. Speciﬁcally, we will block Di\nfrom being further passed down to Cj’s children if\n−\n1\nlog m\nm\nX\ni=1\nqi log qi > γ,\n(3)\nwhere m ≥ 2 is the number of children of Cj; 0 ≤ γ ≤ 1 is a\nthreshold value. When γ = 1, no documents will be blocked\nand all documents are assigned into leaf classes.\nInference\nThe hierarchical classiﬁcation model can be directly applied\nto classify unseen samples after training. When classifying\nan unseen document, the model will directly output the probability distribution of that document belonging to each class\nat each level in the class hierarchy. The same blocking mechanism can be applied to determine the appropriate level that\nthe document should belong to.\nAlgorithm Summary\nAlgorithm 1 puts the above pieces together and summarizes\nthe overall model training process for hierarchical text classiﬁcation. As shown, the overall training is proceeded in a\ntop-down manner, from root to the ﬁnal internal level. At\neach level, we generate pseudo documents and pseudo labels to pre-train each local classiﬁer. Then we self-train the\nensembled global classiﬁer using its own predictions in an\niterative manner. Finally we apply blocking mechanism to\nblock general documents, and pass the remaining documents\nto the next level.",
        "experiments": "Experiment Settings\nDatasets and Evaluation Metrics\nWe use three corpora\nfrom three different domains to evaluate the performance of\nour proposed method:\nAlgorithm 1: Overall Network Training.\nInput: A text collection D = {Di}|N\ni=1; a class category tree\nT ; weak supervisions W of either S or DL for each\nleaf class in T .\nOutput: Class assignment C = {(Di, Ci)}|N\ni=1, where\nCi ∈ T is the most speciﬁc class label for Di.\n1 Initialize C ← ∅;\n2 for k ← 0 to max level − 1 do\n3\nN ← all nodes at level k of T ;\n4\nforeach node ∈ N do\n5\nD∗ ← Pseudo document generation;\n6\nL∗ ← Equation (1);\n7\npre-train node.classifier with D∗, L∗;\n8\nGk ← ensemble all classiﬁers from level 0 to k;\n9\nwhile not converged do\n10\nL∗∗ ← Equation (2);\n11\nself-train Gk with D, L∗∗;\n12\nDB ← documents blocked based on Equation (3);\n13\nCB ← DB’s current class assignments;\n14\nC ← C ∪ (DB, CB);\n15\nD ← D − DB;\n16 C′ ← D’s current class assignments;\n17 C ← C ∪ (D, C′);\n18 Return C;\nTable 1: Dataset Statistics.\nCorpus name\n# classes\n(level 1 + level 2)\n# docs\nAvg. doc length\nNYT\n5 + 25\n13, 081\n778\narXiv\n3 + 53\n230, 105\n129\nYelp Review\n3 + 5\n50, 000\n157\n• The New York Times (NYT): We crawl 13, 081 news\narticles using the New York Times API 2. This news corpus\ncovers 5 super-categories and 25 sub-categories.\n• arXiv: We crawl paper abstracts from arXiv website3 and\nkeep all abstracts that belong to only one category. Then\nwe include all sub-categories with more than 1, 000 documents out of 3 largest super-categories and end up with\n230, 105 abstracts from 53 sub-categories.\n• Yelp Review: We use the Yelp Review Full dataset (Zhang,\nZhao, and LeCun 2015) and take its testing portion as our\ndataset. The dataset contains 50, 000 documents evenly\ndistributed into 5 sub-categories, corresponding to user\nratings from 1 star to 5 stars. We consider 1 and 2 stars as\n“negative”, 3 stars as “neutral”, 4 and 5 stars as “positive”,\nso we end up with 3 super-categories.\nTable 1 provides the statistics of the three datasets; Table 2\nand 3 show some sample sub-categories of NYT and arXiv\ndatasets. We use Micro-F1 and Macro-F1 scores as metrics\nfor classiﬁcation performances.\nBaselines\nWe compare our proposed method with a wide\nrange of baseline models, described as below:\n2http://developer.nytimes.com/\n3https://arxiv.org/\nTable 2: Sample subcategories of NYT Dataset.\nSuper-category (# children)\nSub-category\nPolitics (9)\nabortion, surveillance, immigration, . . .\nArts (4)\ndance, television, music, movies\nBusiness (4)\nstocks, energy companies, economy, . . .\nScience (2)\ncosmos, environment\nSports (7)\nhockey, basketball, tennis, golf, . . .\nTable 3: Sample subcategories of arXiv Dataset.\nSuper-category (# children)\nSub-category\nMath (25)\nmath.NA, math.AG, math.FA, . . .\nPhysics (10)\nphysics.optics, physics.ﬂu-dyn, .. .\nCS (18)\ncs.CV, cs.GT, cs.IT, cs.AI, cs.DC, . . .\n• Hier-Dataless (Song and Roth 2014): Dataless hierarchical text classiﬁcation 4 can only take word-level supervision sources. It embeds both class labels and documents\nin a semantic space using Explicit Semantic Analysis\n(Gabrilovich and Markovitch 2007) on Wikipedia articles,\nand assigns the nearest label to each document in the semantic space. We try both the top-down approach and\nbottom-up approach, with and without the bootstrapping\nprocedure, and ﬁnally report the best performance.\n• Hier-SVM (Dumais and Chen 2000; Liu et al. 2005): Hierarchical SVM can only take document-level supervision\nsources. It decomposes the training tasks according to the\nclass taxonomy, where each local SVM is trained to distinguish sibling categories that share the same parent node.\n• CNN (Kim 2014): The CNN text classiﬁcation model 5\ncan only take document-level supervision sources.\n• WeSTClass (Meng et al. 2018): Weakly-supervised neural\ntext classiﬁcation can take both word-level and documentlevel supervision sources. It ﬁrst generates bag-of-words\npseudo documents for neural model pre-training, then bootstraps the model on unlabeled data.\n• No-global: This is a variant of WeSHClass without the\nglobal classiﬁer, i.e., each document is pushed down with\nlocal classiﬁers in a greedy manner.\n• No-vMF: This is a variant of WeSHClass without using\nmovMF distribution to model class semantics, i.e., we randomly select one word from the keyword set of each class\nas the beginning word when generating pseudo documents.\n• No-selftrain: This is a variant of WeSHClass without\nself-training module, i.e., after pre-training each local classiﬁer, we directly ensemble them as a global classiﬁer at\neach level to classify unlabeled documents.\nParameter Settings\nFor all datasets, we use Skip-Gram\nmodel (Mikolov et al. 2013) to train 100-dimensional word\nembeddings for both movMF distributions modeling and classiﬁer input embeddings. We set the pseudo label parameter\n4https://github.com/CogComp/cogcomp-nlp/tree/master/\ndataless-classiﬁer\n5https://github.com/alexander-rakhlin/\nCNN-for-Sentence-Classiﬁcation-in-Keras\nα = 0.2, the number of pseudo documents per class for pretraining β = 500, and the self-training stopping criterion\nδ = 0.1. We set the blocking threshold γ = 0.9 for NYT\ndataset where general documents exist and γ = 1 for the\nother two.\nAlthough our proposed method can use any neural model\nas local classiﬁers, we empirically ﬁnd that CNN model always results in better performances than RNN models, such\nas LSTM (Hochreiter and Schmidhuber 1997) and Hierarchical Attention Networks (Yang et al. 2016). Therefore, we\nreport the performance of our method by using CNN model\nwith one convolutional layer as local classiﬁers. Speciﬁcally,\nthe ﬁlter window sizes are 2, 3, 4, 5 with 20 feature maps\neach. Both the pre-training and the self-training steps are\nperformed using SGD with batch size 256.\nWeak Supervision Settings\nThe seed information we use\nas weak supervision for different datasets are described as follows: (1) When the supervision source is class-related keywords, we select 3 keywords for each leaf class; (2) When\nthe supervision source is labeled documents, we randomly\nsample c documents of each leaf class from the corpus (c = 3\nfor NYT and arXiv; c = 10 for Yelp Review) and use them\nas given labeled documents. To alleviate the randomness, we\nrepeat the document selection process 10 times and show the\nperformances with average and standard deviation values.\nWe list the keyword supervisions of some sample classes\nfor NYT dataset as follows: Immigration (immigrants, immigration, citizenship); Dance (ballet, dancers, dancer); Environment (climate, wildlife, ﬁsh).\nQuantitative Comparision\nWe show the overall text classiﬁcation results in Table 4.\nWeSHClass achieves the overall best performance among\nall the baselines on the three datasets. Notably, when the\nsupervision source is class-related keywords, WeSHClass\noutperforms Hier-Dataless and WeSTClass, which shows\nthat WeSHClass can better leverage word-level supervision\nsources in hierarchical text classiﬁcation. When the supervision source is labeled documents, WeSHClass has not only\nhigher average performance, but also better stability than the\nsupervised baselines. This demonstrates that when training\ndocuments are extremely limited, WeSHClass can better\nleverage the insufﬁcient supervision for good performances\nand is less sensitive to seed documents.\nComparing WeSHClass with several ablations, Noglobal, No-vMF and No-self-train, we observe the effectiveness of the following components: (1) ensemble of local\nclassiﬁers, (2) modeling class semantics as movMF distributions, and (3) self-training. The results demonstrate that all\nthese components contribute to the performance of WeSHClass.\nComponent-Wise Evaluation\nIn this subsection, we conduct a series of breakdown experiments on NYT dataset using class-related keywords as\nweak supervision to further investigate different components\nin our proposed method. We obtain similar results on the\nother two datasets.\nPseudo Documents Generation\nThe quality of the generated pseudo documents is critical to our model, since highquality pseudo documents provide a good model initialization. Therefore, we are interested in which pseudo document\ngeneration method gives our model best initialization for the\nsubsequent self-training step. We compare our document generation strategy (movMF + LSTM language model) with the\nfollowing two methods:\n• Bag-of-words (Meng et al. 2018): The pseudo documents\nare generated from a mixture of background unigram distribution and class-related keywords distribution.\n• Bag-of-words + reordering: We ﬁrst generate bag-of-words\npseudo documents as in the previous method, and then use\nthe globally trained LSTM language model to reorder the\npseudo documents by greedily putting the word with the\nhighest probability at the end of the current sequence. The\nbeginning word is randomly chosen.\nWe showcase some generated pseudo document snippets\nof class “politics” for NYT dataset using different methods in\nTable 5. Bag-of-words method generates pseudo documents\nwithout word order information; bag-of-words method with\nreordering generates text of high quality at the beginning, but\npoor near the end, which is probably because the “proper”\nwords have been used at the beginning, but the remaining\nwords are crowded at the end implausibly; our method generates text of high quality.\nTo compare the generalization ability of the pre-trained\nmodels with different pseudo documents, we show their subsequent self-training process (at level 1) in Figure 2(a). We\nnotice that our strategy not only makes self-training converge\nfaster, but also has better ﬁnal performance.\nGlobal Classiﬁer and Self-training\nWe proceed to study\nwhy using self-trained global classiﬁer on the ensemble of\nlocal classiﬁers is better than greedy approach. We show the\nself-training procedure of the global classiﬁer at the ﬁnal level\nin Figure 2(b), where we demonstrate the classiﬁcation accuracy at level 1 (super-categories), level 2 (sub-categories) and\nof all classes. Since at the ﬁnal level, all local classiﬁers are\nensembled to construct the global classiﬁer, self-training of\nthe global classiﬁer is the joint training of all local classiﬁers.\nThe result shows that the ensemble of local classiﬁers for joint\ntraining is beneﬁcial for improving the accuracy at all levels.\nIf a greedy approach is used, however, higher-level classiﬁers will not be updated during lower-level classiﬁcation, and\nmisclassiﬁcation at higher levels cannot be corrected.\nBlocking During Self-training\nWe demonstrate the dynamics of the blocking mechanism during self-training. Figure 2(c) shows the average normalized entropy of the corresponding local classiﬁer output for each document in NYT\ndataset, and Figure 2(d) shows the total number of blocked\ndocuments during the self-training procedure at the ﬁnal\nlevel. Recall that we enhance high-conﬁdent predictions to\nreﬁne our model during self-training. Therefore, the average\nnormalized entropy decreases during self-training, implying\nthere is less uncertainty in the outputs of our model. Correspondingly, fewer documents will be blocked, resulting in\nmore available documents for self-training.\nTable 4: Macro-F1 and Micro-F1 scores for all methods on three datasets, under two types of weak supervisions.\nMethods\nNYT\narXiv\nYelp Review\nKEYWORDS\nDOCS\nKEYWORDS\nDOCS\nKEYWORDS\nDOCS\nMacro\nMicro\nMacro\nAvg. (Std.)\nMicro\nAvg. (Std.)\nMacro\nMicro\nMacro\nAvg. (Std.)\nMicro\nAvg. (Std.)\nMacro\nMicro\nMacro\nAvg. (Std.)\nMicro\nAvg. (Std.)\nHier-Dataless\n0.593\n0.811\n0.374\n0.594\n0.284\n0.312\nHier-SVM\n0.142 (0.016)\n0.469 (0.012)\n0.049 (0.001)\n0.443 (0.006)\n0.220 (0.082)\n0.310 (0.113)\nCNN\n0.165 (0.027)\n0.329 (0.097)\n0.124 (0.014)\n0.456 (0.023)\n0.306 (0.028)\n0.372 (0.028)\nWeSTClass\n0.386\n0.772\n0.479 (0.027)\n0.728 (0.036)\n0.412\n0.642\n0.264 (0.016)\n0.547 (0.009)\n0.348\n0.389\n0.345 (0.027)\n0.388 (0.033)\nNo-global\n0.618\n0.843\n0.520 (0.065)\n0.768 (0.100)\n0.442\n0.673\n0.264 (0.020)\n0.581 (0.017)\n0.391\n0.424\n0.369 (0.022)\n0.403 (0.016)\nNo-vMF\n0.628\n0.862\n0.527 (0.031)\n0.825 (0.032)\n0.406\n0.665\n0.255 (0.015)\n0.564 (0.012)\n0.410\n0.457\n0.372 (0.029)\n0.407 (0.015)\nNo-self-train\n0.550\n0.787\n0.491 (0.036)\n0.769 (0.039)\n0.395\n0.635\n0.234 (0.013)\n0.535 (0.010)\n0.362\n0.408\n0.348 (0.030)\n0.382 (0.022)\nWeSHClass\n0.632\n0.874\n0.532 (0.015)\n0.827 (0.012)\n0.452\n0.692\n0.279 (0.010)\n0.585 (0.009)\n0.423\n0.461\n0.375 (0.021)\n0.410 (0.014)\nTable 5: Sample generated pseudo document snippets of class “politics” for NYT dataset.\nDoc #\nBag-of-words\nBag-of-words + reordering\nmovMF + LSTM language model\n1\nhe’s cup abortion bars have pointed use of\nlawsuits involving smoothen bettors rights\nin the federal exchange, limewire ...\nthe clinicians pianists said that the legalizing of the proﬁling of the . . .abortion abortion abortion identiﬁcation abortions . . .\nabortion rights is often overlooked by the\npresident’s 30-feb format of a moonjock\nperiod that offered him the rules to . . .\n2\nﬁrst tried to launch the agent in immigrants\nwere in a lazar and lakshmi deﬁnition of\nyerxa riding this we get very coveted as . . .\nmajorities and clintons legalization, moderates and tribes lawfully ...lawmakers clinics immigrants immigrants immigrants . ..\nimmigrants who had been headed to the\nunited states in benghazi, libya, saying that\nmr. he making comments describing .. .\n3\nthe september crew members budget security administrator lat coequal representing a\nfederal customer, identiﬁed the bladed ...\nthe impasse of allowances overruns pensions entitlement . . . funding ﬁnancing budgets budgets budgets budgets taxpayers . . .\nbudget increases on oil supplies have grown\nmore than a ezio of its 20 percent of energy\nspaces, producing plans by 1 billion .. .\n0\n200\n400\n600\n800\nIterations\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nF1 scores\nMacro-F1\nMicro-F1\nBOW\nBOW-reorder\nLSTM\n(a) Pseudo documents generation\n0\n500\n1000 1500 2000 2500 3000 3500\nIterations\n0.5\n0.6\n0.7\n0.8\n0.9\nF1 scores\nMacro-F1\nMicro-F1\nlevel 1\nlevel 2\nall\n(b)\nGlobal\nclassiﬁer\nselftraining\n0\n500 1000 1500 2000 2500 3000 3500\nIterations\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nAvg. normalized entropy\n(c) Average normalized entropy\n0\n500 1000 1500 2000 2500 3000 3500\nIterations\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n# of blocked docs\n(d) Number of blocked documents\nFigure 2: Component-wise evaluation on NYT dataset.",
        "related work": "Weakly-Supervised Text Classiﬁcation\nThere exist some previous studies that use either word-based\nsupervision or limited amount of labeled documents as weak\nsupervision sources for the text classiﬁcation task. WeSTClass (Meng et al. 2018) leverages both types of supervision\nsources. It applies a similar procedure of pre-training the\nnetwork with pseudo documents followed by self-training on\nunlabeled data. Descriptive LDA (Chen et al. 2015) applies an\nLDA model to infer Dirichlet priors from given keywords as\ncategory descriptions. The Dirichlet priors guide LDA to induce the category-aware topics from unlabeled documents for\nclassiﬁcation. (Ganchev et al. 2010) propose to encode prior\nknowledge and indirect supervision in constraints on posteriors of latent variable probabilistic models. Predictive text\nembedding (Tang, Qu, and Mei 2015) utilizes both labeled\nand unlabeled documents to learn text embedding speciﬁcally\nfor a task. Labeled data and word co-occurrence information\nare ﬁrst represented as a large-scale heterogeneous text network and then embedded into a low dimensional space. The\nlearned embedding are fed to logistic regression classiﬁers\nfor classiﬁcation. None of the above methods are speciﬁcally\ndesigned for hierarchical classiﬁcation.\nHierarchical Text Classiﬁcation\nThere have been efforts on using SVM for hierarchical classiﬁcation. (Dumais and Chen 2000; Liu et al. 2005) propose\nto use local SVMs that are trained to distinguish the children classes of the same parent node so that the hierarchical\nclassiﬁcation task is decomposed into several ﬂat classiﬁcation tasks. (Cai and Hofmann 2004) deﬁne hierarchical\nloss function and apply cost-sensitive learning to generalize\nSVM learning for hierarchical classiﬁcation. A graph-CNN\nbased deep learning model is proposed in (Peng et al. 2018)\nto convert text to graph-of-words, on which the graph convolution operations are applied for feature extraction. FastXML\n(Prabhu and Varma 2014) is designed for extremely large\nlabel space. It learns a hierarchy of training instances and\noptimizes a ranking-based objective at each node of the hierarchy. The above methods rely heavily on the quantity and\nquality of training data for good performance, while WeSHClass does not require much training data but only weak\nsupervision from users.\nHierarchical dataless classiﬁcation (Song and Roth 2014)\nuses class-related keywords as class descriptions, and projects\nclasses and documents into the same semantic space by retrieving Wikipedia concepts. Classiﬁcation can be performed\nin both top-down and bottom-up manners, by measuring the\nvector similarity between documents and classes. Although\nhierarchical dataless classiﬁcation does not rely on massive\ntraining data as well, its performance is highly inﬂuenced\nby the text similarity between the distant supervision source\n(Wikipedia) and the given unlabeled corpus.",
        "conclusion": "We proposed a weakly-supervised hierarchical text classiﬁcation method WeSHClass. Our designed hierarchical network structure and training method can effectively leverage\n(1) different types of weak supervision sources to generate\nhigh-quality pseudo documents for better model generalization ability, and (2) class taxonomy for better performances\nthan ﬂat methods and greedy approaches. WeSHClass outperforms various supervised and weakly-supervised baselines\nin three datasets from different domains, which demonstrates\nthe practical value of WeSHClass in real-world applications.\nIn the future, it is interesting to study what kinds of weak\nsupervision are most effective for the hierarchical text classiﬁcation task and how to combine multiple sources together\nto achieve even better performance.",
        "summary_en": "Hierarchical text classification, which aims to classify text documents into a given hierarchy, is an important task in many real-world applications. Recently, deep neural models are gaining increasing popularity for text classification due to their expressive power and minimum requirement for feature engineering. However, applying deep neural networks for hierarchical text classification remains challenging, because they heavily rely on a large amount of training data and meanwhile cannot easily determine appropriate levels of documents in the hierarchical setting. Therefore, this paper proposes a weakly-supervised neural method for hierarchical text classification. The method does not require a large amount of training data but requires only easy-to-provide weak supervision signals such as a few class-related documents or keywords. The method effectively leverages such weak supervision signals to generate pseudo documents for model pre-training, and then performs self-training on real unlabeled data to iteratively refine the model. During the training process, the paper's model features a hierarchical neural structure, which mimics the given hierarchy and is capable of determining the proper levels for documents with a blocking mechanism. Experiments on three datasets from different domains demonstrate the efficacy of the method compared with a comprehensive set of baselines.",
        "summary_zh": "这篇论文介绍了一种用于层次文本分类的弱监督神经网络方法，为了解决深度神经网络在层次文本分类中所面临的挑战，即对大量训练数据的依赖性以及在层次结构中确定文档适当级别的困难。该方法通过利用少量类相关文档或关键词等弱监督信号生成伪文档进行模型预训练，然后在真实的无标记数据上进行自我训练，迭代优化模型。论文中提出的模型采用层次神经结构，在训练过程中模拟给定的层次结构，通过阻塞机制确定文档的适当层次。在三个不同领域的数据集上的实验结果表明，该方法与一系列基准方法相比非常有效。"
    },
    {
        "title": "CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling",
        "abstract": "In real-world applications of natural language generation, there are often constraints on the target sentences in addition to ﬂuency and naturalness requirements. Existing language generation techniques are usually based on recurrent neural networks (RNNs). However, it is non-trivial to impose constraints on RNNs while maintaining generation quality, since RNNs generate sentences sequentially (or with beam search) from the ﬁrst word to the last. In this paper, we propose CGMH, a novel approach using Metropolis-Hastings sampling for constrained sentence generation. CGMH allows complicated constraints such as the occurrence of multiple keywords in the target sentences, which cannot be handled in traditional RNN-based approaches. Moreover, CGMH works in the inference stage, and does not require parallel corpora for training. We evaluate our method on a variety of tasks, including keywords-to-sentence generation, unsupervised sentence paraphrasing, and unsupervised sentence error correction. CGMH achieves high performance compared with previous supervised methods for sentence generation. Our code is released at https://github.com/NingMiao/CGMH",
        "introduction": "Natural language generation oftentimes involves constraints\non the generated sentences. The constraints can be categorized into the following types: (1) Hard constraints, such as\nthe mandatory inclusion of certain keywords in the output\nsentences; and (2) Soft constraints, such as requiring the\ngenerated sentences to be semantically related to a certain\ntopic. Figure 1 illustrates an example of advertisement generation, where “BMW” and “sports” should appear in the advertising slogan. Hence, “BMW, the sports car of the future”\nis a valid sentence as an advertisement.\nExisting sentence generation methods are mostly based\non recurrent neural networks (RNNs), which generate a sentence sequentially from left to right (Sutskever, Vinyals,\nand Le 2014). However, it is non-trivial to impose constraints during the left-to-right generation in RNNs. Previous work proposes a backward-forward generation apFigure 1: CGMH generates a sentence with the constraint of\nkeyword inclusion. At each step, CGMH proposes a candidate modiﬁcation of the sentence, which is accepted or rejected according to a certain acceptance rate.\nproach (Mou et al. 2015), which could only generate sentences with one keyword. Additionally, researchers propose grid beam search to incorporate constraints in machine translation (Post and Vilar 2018; Hasler et al. 2018;\nHokamp and Liu 2017). It works with the translation task because the source and target are mostly aligned and the candidate set of translations is small. But grid beam search would\nnot work well with general sentence generation, which has\nmany more candidate sentences.\nIn this paper, we propose Constrained Generation by\nMetropolis-Hastings sampling (CGMH), a novel approach\nthat addresses sentence generation with constraints. Different from previous sentence samplers working in the variational latent space (Bowman et al. 2016), CGMH directly\nsamples from the sentence space using the MetropolisHastings (MH) algorithm (Metropolis et al. 1953). MH is an\ninstance of the general Markov chain Monte Carlo (MCMC)\nsampling. It deﬁnes local operations in the sentence space\n(e.g., word replacement, deletion, and insertion). During\nsampling, we randomly choose a word and an operation to\nform a proposal for transition. The proposal is either accepted or rejected according to an acceptance rate computed\nby a pre-speciﬁed stationary distribution. Compared with\nGibbs sampling (another MCMC method), MH is more ﬂexible to generate sentences with arbitrary lengths.\nIt is then straightforward to impose constraints on the generated sentences by introducing a matching function (which\nindicates the degree to which the constraints are satisﬁed) to\nmanipulate the stationary distribution of MH. For hard constraints, the matching function could be a binary indicator,\nruling out the possibility of an infeasible sentence. For soft\nconstraints, the matching function could be, for example, a\nmeasure of semantic similarity. In both cases, we are able to\nsample sentences with constraints.\nOur proposed CGMH can be applied to a variety of\ntasks. We ﬁrst experiment keywords-to-sentence generation,\nwhere keywords are hard constraints. In this task, CGMH\noutperforms state-of-the-art constrained generation models\nin both negative likelihood (ﬂuency) and human evaluation.\nWe also conduct experiments on two generation tasks with\nsoft constraints, namely, paraphrase generation and sentence\nerror correction. Results show that, without a parallel corpus, CGMH signiﬁcantly outperforms other unsupervised\nmodels, achieving close results to state-of-the-art supervised\napproaches.\nIn summary, our contributions include\n• We propose CGMH, a general framework for sentence\nsampling that can cope with both hard and soft constraints.\n• We design the proposal distribution and stationary distributions for MH sentence sampling in three tasks, including keywords-to-sentence generation, paraphrase generation, and sentence error correction. Experimental results\nshow that CGMH achieves high performance compared\nwith previous methods.\n• We make it possible to accomplish the above tasks in an\nunsupervised fashion, which does not require a parallel\ncorpus as is needed in previous approaches.",
        "related work": "In recent years, sentence generation is mostly based on the\nrecurrent neural network (RNN) because of its capability\nof learning highly complicated structures of language. In\nmost tasks, RNN-based sentence generation is modeled as\nmax a posteriori (MAP) inference, and people use greedy\nsearch or beam search to approximate the most probable sentences (Sutskever, Vinyals, and Le 2014).\nFor sentence sampling, the most na¨ıve approach, perhaps,\nis to sample words from RNN’s predicted probabilities stepby-step, known as forward sampling in the Bayesian network regime. The prototype-then-edit model (Guu et al.\n2018) ﬁrst samples prototypes (sentences in the training set),\nand then edits the prototypes to obtain new sentences; it\ncan be thought of as sampling from an RNN-deﬁned kernel density. Bowman et al. (2016) use variational autoencoders (VAEs) to sample sentences from a continuous latent\nspace. However, these approaches allow neither soft constraints that require ﬂexible manipulation of sentence probabilities, nor hard constraints that specify one or more given\nwords.\nBerglund et al. (2015) propose a Gibbs sampling model\nthat uses a bi-directional RNN to alternatively replace a token from its posterior distribution. Su et al. (2018) further\napply it to control the sentiment of a sentence. However, the\nshortcoming of Gibbs sampling is obvious: it cannot change\nthe length of sentences and hence is not able to solve complicated problems such as sentence generation from keywords. Our paper extends Gibbs sampling with word insertion and deletion. This results in a Metropolis-Hastings sampler, enabling more ﬂexible sampling. Harrison, Purdy, and\nRiedl (2017) utilize MH to sample an event sequence in the\ntask of story generation, which cannot be directly used for\nsentence generation.\nAnother line of work tackles the problem of constrained sentence generation from a searching perspective. In neural machine translation, for example, grid beam\nsearch (Hokamp and Liu 2017, GBS) makes use of 2dimensional beam search to seek sentences that satisfy the\nconstraints, whereas constrained beam search (Anderson et\nal. 2017, CBS) utilizes a ﬁnite-state machine to assist searching. Post and Vilar (2018) and Hasler et al. (2018) further\naccelerate the search process. In machine translation, the\nsearch space is limited and highly conditioned on the source\nsentence. But in other generation tasks, there may be many\nmore candidate sentences; GBS fails when the greedy pruning is unable to ﬁnd sentences satisfying the constraints due\nto the large search space.\nSentence generation with soft constrains is also related to\ncontrolling latent features of a sentence (Prabhumoye et al.\n2018; Li et al. 2018), such as the meaning and sentiment.\nHu et al. (2017) apply a discriminator to VAE to generate\nsentences with speciﬁed sentiments, and Shen et al. (2017)\nachieve style transfer by cross-alignment with only nonparallel data. Such approaches require an explicit deﬁnition\nof the latent feature (e.g., sentiment), supported by large labeled datasets.\nThe difference between our model and previous work is\nthat our MH sampling framework is applicable to both hard\nand soft constraints. It immediately enables several nontrivial applications, including unsupervised paraphrase generation, unsupervised error correction, and keyword-based\nsentence generation.",
        "approach": "In this section, we describe our CGMH model (referring to\nConstrained Generation by Metropolis-Hastings) in detail.\nWe ﬁrst introduce the general framework of MH sampling,\nand then we design MH components—including proposal\ndesign, stationary distributions, and acceptance decision—\nin the scenario of constrained sentence generation.\nThe Metropolis-Hastings Algorithm\nThe Metropolis-Hastings (MH) algorithm is a classical\nMarkov chain Monte Carlo (MCMC) sampling approach.\nMCMC generates a sequence of correlated samples by iteratively jumping from one state to another, according to\nthe transition matrix of a Markov chain. For sentence generation, each state of the Markov chain is a particular sentence. Under mild conditions, the distribution of samples\nwill converge to the stationary distribution of the Markov\nchain. Therefore, we need to design a Markov chain whose\nstationary distribution is the desired sentence distribution.\nHowever, it is sometimes difﬁcult to directly specify the\ntransition probability of the Markov chain to obtain an arbitrary stationary distribution. The MH sampler solves this\nproblem in a two-step fashion. It ﬁrst proposes a tentative\ntransition, but then accepts or rejects the proposal according\nan acceptance rate. The acceptance/rejection rate is computed by the desired stationary distribution and the proposal\ndistribution. This ensures the detailed balance condition,\nwhich in turn guarantees that MH converges to the desired\ndistribution.\nMore speciﬁcally, let π(x) be the distribution from which\nwe want to sample sentences (x denotes a particular sentence). MH starts from a (possibly) arbitrary state x0 (an initial sentence or a sequence of keywords). At each step t, a\nnew sentence x′ is proposed based on a proposal distribution\ng(x′|xt−1), where xt−1 denotes the sentence of the last step.\nThen the proposal could be either accepted or rejected, given\nby the acceptance rate\nA(x′|xt−1) = min{1, A∗(x′|xt−1)}\n(1)\nwhere\nA∗(x′|xt−1) =\nπ(x′)g(xt−1|x′)\nπ(xt−1)g(x′|xt−1)\n(2)\nIn other words, the proposal is accepted with a probability\nof A(x′|xt−1), and the next sentence xt = x′. With a probability of 1 − A(x′|xt−1), the proposal is otherwise rejected\nand xt = xt−1. Theoretically, the distribution of sample xn\nwill converge to π(x) as n → ∞ for irreducible and aperiodic Markov chains. In practice, initial several samples are\ndiscarded as they are subject to the initial states x0. If the\nsamples converge to the stationary distribution, we say the\nMarkov chain mixes or burns in. Readers may refer to Gelman et al. (2013) for details of MH sampling.\nThe MH framework is a ﬂexible sampling algorithm because: (1) The proposal distribution could be arbitrary, as\nlong as the Markov chain is irreducible and aperiodic; (2)\nThe stationary distribution could also be arbitrarily speciﬁed, which will be reﬂected in Equation 2 to correct the proposal distribution; and (3) We can safely ignore a normalization factor of the stationary distribution and only specify an\nunnormalized measure, because π(·) appears in both numerator and denominator of Equation 2. All these allow ﬂexible\nmanipulation of the stationary distribution.\nThe design of proposal distributions and stationary distributions relies heavily on applications, which will be described in the rest of this section.\nProposals\nWe design a set of simple yet effective proposals, including\nword replacement, insertion, and deletion. That is, we randomly select a word at each step, and for the selected word,\nwe randomly perform one of the three operations with probability [pinsert, pdelete, preplace], which is set to [1/3, 1/3, 1/3]\nin our experiments. We further describe the operations as\nfollows.\nReplacement.\nAssume that the sentence at the current step\nis x = [w1, · · · , wm−1, wm, wm+1, · · · , wn], where n is\nsentence length, and that we decide to propose a replacement action for the mth word wm.\nGiven all other words in the current sentence, we need to\nchoose a new word for the m-th position by the conditional\nprobability:\ngreplace(x′|x) = π(w∗\nm = wc|x−m) =\nπ(w1, · · · , wm−1, wc, wm+1, · · · , wn)\nP\nw∈V π(w1, · · · , wm−1, w, wm+1, · · · , wn)\n(3)\nwhere w∗\nm is the new word for position m, wc is a candidate\nword for w∗\nm, x′ = [w1, · · · , wm−1, wc, wm+1, · · · , wn]\nis the candidate sentence, V is the set of all words, and\ngreplace(x′|x) is the probability of choosing x′ as the target\nof replacement action from x. However, it is difﬁcult to\ncompute π(w∗\nm = wc|x−m) for all wc ∈ V, because we\nhave to compute π(w1, · · · , wm−1, wc, wm+1, · · · , wn) for\neach candidate sentence separately. This results from different words in the middle of a sentence and thereafter different\nRNN hidden states.\nWe propose to pre-select a subset of plausible candidate\nwords. It is easy to compute π(w1, ..., wm−1, w∗\nm = wc)\nas well as π(w∗\nm = wc, wm+1, ..., wn) by a forward and a\nbackward language model, and π(w1, ..., w∗\nm = wc, xn) is\nno greater than either of them. We thus build a pre-selector\nQ to discard wc with low forward or backward probability:\nQ(wc) = min( π(w1, ..., wm−1, w∗\nm = wc),\nπ(w∗\nm = wc, wm+1, ..., wn))\n(4)\nAfter pre-selection, we compute the conditional probability of selected words by Equation (3), from which we sample\na word for replacement.\nInsertion and deletion.\nInserting a word is done in a twostep fashion: we ﬁrst insert a special token, placeholder\n<PHD>, at the position that we are currently working on,\nand then use (3) to sample a real word to replace the placeholder. As a result, ginsert takes a similar form to (3).\nDeleting\nis,\nperhaps,\nthe\nsimplest\noperation,\nand\nwe directly remove the current word. Suppose x\n=\n[w1, · · · , wm−1, wm, wm+1 · · · wn] and we are about to\ndelete the word wm. Then gdelete(x′|xt−1) equals 1 if x′ =\n[w1, · · · , wm−1, wm+1 · · · wn], or 0 for other sentences.\nInsertion and deletion ensure the ergodicity of the\nMarkov chain, as in the worst case, any two sentences, x\nand x′, are still reachable by ﬁrst deleting all words in x,\nand then inserting all words in x′ in order. In addition, word\nreplacement is an intuitive operation that helps reach “semantically neighboring” states more easily. Therefore we\ninclude it as one of our proposals. It should be noticed that,\nalthough the replacement action is restricted to top-ranked\ncandidate words for efﬁciency purposes, this does not affect\nthe ergodicity of the Markov chain.\nStationary Distribution\nIn the proposed CGMH framework, we would like to obtain sentences from a desired distribution π(x), known as the\nstationary distribution of the Markov chain. For constrained\nsentence generation, CGMH allows ﬂexible design of the\nstationary distribution.\nGenerally, the stationary distribution π(x) can be deﬁned\nas\nπ(x) ∝ p(x) · X 0\nc (x) · · · X n\nc (x)\n|\n{z\n}\nconstraints\n(5)\nwhere p(x) is the probability of a sentence in general, and\nX 0\nc (x), · · · , X n\nc (x) are scoring functions indicating the degree to which a constraint is satisﬁed. Technically, CGMH\nworks with both hard and soft constraints. For a hard constraint, X i\nc is an indicator function, which equals 1 if the\nith constraint is satisﬁed, or 0 otherwise. For a soft constraint, X i\nc is a “smoothed” indicator function showing how\nthe sentence satisﬁes the (soft) constraint. By multiplying\nthese scoring functions together, we could impose more than\none constraints.\nThe design of the scoring functions is ﬂexible but task related. In our paper, we apply the CGMH framework to three\ndifferent tasks.\nSentence generation with keywords.\nIn this task, we\nwould like to generate a sentence given one or more keywords as constraints. It has been previously explored in various applications including question answering (Yin et al.\n2016) and dialog systems (Mou et al. 2016). Most previous\nwork makes use of attention or copying mechanisms to impose the keyword constraints in a soft manner, which means\nthat the constraint may not be satisﬁed (Yin et al. 2016;\nGu et al. 2016).\nIn our CGMH framework, it is natural to impose hard constraints by restricting the support of the stationary distribution to feasible solutions. In particular, we have\nπ(x) ∝ pLM(x) · Xkeyword(x)\nwhere pLM is a general sentence probability computed by a\nlanguage model and Xkeyword is the indicator function showing if the keywords are included in the generated sentence.\nIn other words, the stationary distribution is proportional to\nthe language model probability if all constraints are satisﬁed (keywords appearing in the sentence), or 0 otherwise.\nDuring generation, the initial sentence x0 is simply a sequence of keywords, and then we perform sampling to generate valid sentences.\nUnsupervised paraphrase generation.\nParaphrase generation aims to synthesize literally different sentences that\nconvey the same meaning as the input sentence. It is an important task in NLP, and can be a key component in downstream applications such as data augmentation for NLP. Previous state-of-the-art paraphrase generation methods require\nparallel data for training, which is not always available.\nIn our paper, we view paraphrase generation as sampling\nfrom a distribution, where the sentences are (1) ﬂuent by\nthemselves and (2) close in meaning to the input sentence\nx∗. The former property can be captured by a traditional\nlanguage model, whereas the latter can be modeled as a constraint. Concretely, we have\nπ(x) ∝ pLM(x) · Xmatch(x|x∗)\n(6)\nHere, pLM(x) is also the probability given by a language\nmodel, indicating the ﬂuency of x. Xmatch(x|x∗) is a matching score. In our experiments, we have several choices for\nXmatch(·|·):\n• Keyword matching (KW) as hard constraints. We observe\nthat paraphrases typically share some keywords with the\noriginal sentence. In this variant, we use Rake (Rose et\nal. 2010) to extract keywords and keep them ﬁxed during\nsampling. That is Xmatch(x|x∗) = 1 if x and x∗ share the\nsame keywords, and 0 otherwise.\n• Word embedding similarity as a soft constraint. Embeddings map discrete words to real-valued vectors, providing a softer way of measuring similarity. In this matching\nfunction, we enhance keyword matching with embedding\nsimilarity. For any word w in a sentence x, we ﬁrst ﬁnd\nthe closest word in the input sentence x∗ by computing\ntheir cosine similarity (Pennington, Socher, and Manning\n2014). Then either the minimum or the average of these\ncosine measures is taken as the matching score, resulting\nin two variants (WVM and WVA).\n• Skip-thoughts similarity (ST) as a soft constraint. The kipthoughts approach trains sentence embeddings by predicting surrounding sentences (Kiros et al. 2015). We compute the cosine similarity between the skip-thought embeddings of x and x∗, and use it as the matching score.\nTheoretically speaking, we may start sampling from any\nsentence, once the stationary distribution is deﬁned. However, it would take too long for the Markov chain to\nmix/burn-in (i.e., samples are from the desired distribution).\nWe thus use the original sentence as the initial state, i.e.,\nx0 = x∗. This is similar to the warm start in Gibbs sampling\nfor contrastive divergence estimation (Hinton, Osindero, and\nTeh 2006).\nUnsupervised sentence error correction.\nPrevious work\nof sentence error correction also depends on parallel\ndata (Felice et al. 2014; Junczys-Dowmunt and Grundkiewicz 2016; Sakaguchi, Post, and Van Durme 2017; Chollampatt, Hoang, and Ng 2016). Our CGMH framework allows us to generate samples from a distribution of correct\nsentences, starting from an erroneous one as the input x∗.\nIn this application, we use the same stationary distribution (Equation 6) as in the unsupervised paraphrase setting,\nwhere pLM is trained on a general corpus ensuring the ﬂuency (correctness), and Xmatch(·|·)—assumed insensitive to\ngrammatical errors—imposes a soft constraint of semantic\nrelevance.\nAcceptance Rate\nIn MH, both proposals and stationary distributions can be\nspeciﬁed. The way to ensure that the samples are indeed\nfrom the desired distribution is to correct the proposal distribution by a probability of acceptance or rejection, given by\nan acceptance rate in Equations (1) and (2).\nIn our approach, we have three types of proposals,\nnamely, deletion, insertion, and replacement. We thus breakdown our acceptance rate (before taking min{1, ·}) as\nA∗\nreplace(x′|x) = preplace · greplace(x|x′) · π(x′)\npreplace · greplace(x′|x) · π(x)\n≈ π(wm|x−m) · π(x′)\nπ(w′m|x−m) · π(x) = 1\n(7)\nA∗\ninsert(x′|x) = pdelete · gdelete(x|x′) · π(x′)\npinsert · ginsert(x′|x) · π(x)\n=\npdelete · π(x′)\npinsert · ginsert(x′|x) · π(x)\n(8)\nA∗\ndelete(x′|x) = pinsert · ginsert(x|x′) · π(x′)\npdelete · gdelete(x′|x) · π(x)\n= pinsert · ginsert(x|x′) · π(x′)\npdelete · π(x)\n(9)\nIn particular, (7) is trivially true because word replacement could be thought of as a step of Gibbs sampling, which\nis in turn a step of MH sampling whose acceptance rate is\nguaranteed to be 1. (8) and (9) are reciprocal because deletion and insertion are the inverse operation to each other.",
        "experiments": "We evaluated our approach on a variety of tasks including sentence generation from keywords, unsupervised paraphrase generation, and unsupervised sentence error correction. We also conducted in-depth analysis of the proposed\nCGMH method.\nKeywords-to-Sentence Generation\nFor keywords-to-sentence generation, we trained our language model on randomly chosen 5M sentences from the\nOne-Billion-Word Corpus (Chelba et al. 2013).1 We held out\na 3k-sentence set to provide keywords for testing. For each\nsentence, we randomly sampled one or more words as the\nconstraint(s). Our language models are simply a two-layer\nLSTM with a hidden size of 300. We chose 50k most frequent words as the dictionary.\nFor MH sampling, we used the sequence of keywords as\nthe initial state, and chose the sentence with the lowest perplexity after 100 steps as the output. We set the maximum\nsampling step to 200.\nWe tested the negative likelihood (NLL) of sentences to\nevaluate their ﬂuency. NLL is given by a third-party n-gram\nlanguage model trained on the English monolingual corpus\nof WMT18.2 We also invited 3 volunteers to score the ﬂuency of generated sentences. Volunteers were asked to score\n100 samples from each method according to their quality.\nScores range from 0 to 1, where 1 indicates the best quality.\nTable 1 compares our method with current state-of-theart approaches of constrained generation, namely, the grid\nbeam search approach (GBS) (Hokamp and Liu 2017) and\n1http://www.statmt.org/lm-benchmark/\n2http://www.statmt.org/wmt18/translation-task.html\n#keyword(s)\nCGMH\nGBS\nsep-B/F\nasyn-B/F\n1\nNLL\n7.04\n7.42\n7.80\n8.30\nHuman\n0.45\n0.32\n0.11\n0.09\n2\nNLL\n7.57\n8.72\nHuman\n0.61\n0.55\n3\nNLL\n8.26\n8.59\nHuman\n0.56\n0.49\n4\nNLL\n7.92\n9.63\nHuman\n0.65\n0.55\nTable 1: Results of NLL and human evaluation on sentences\nwith 1 to 4 keywords. Sentences with lower NLL and higher\nhuman evaluation scores are better.\nKeyword(s)\nGenerated Sentences\nfriends\nMy good friends were in danger .\nproject\nThe ﬁrst project of the scheme .\nhave, trip\nBut many people have never\nmade the trip .\nlottery, scholarships\nBut the lottery has provided\nscholarships .\ndecision, build,\nThe decision is to build a new\nhome\nhome .\nattempt, copy,\nThe ﬁrst attempt to copy the\npainting, denounced\npainting was denounced .\nTable 2: Sentences generated from keywords by CGMH.\nStatistic\nValue\nMean intra-annotator std\n0.098\nMean intra-model std\n0.280\np-value (1 keyword)\n< 0.01\np-value (2 keywords)\n< 0.05\np-value (3 keywords)\n< 0.05\np-value (4 keywords)\n< 0.01\nTable 3: Statistics of human evaluation for keywords-tosentence generation.\ntwo variants of the backward forward model (sep-B/F and\nasyn-B/F) (Mou et al. 2015).\nCGMH outperforms previous work in both NLL and human evaluations. The two variants of B/F cannot generate more than one keywords. GBS is designed for machine\ntranslation; it works well when the candidate sentence space\nis small. But for general sentence generation, pruning in grid\nmakes the keywords sometimes unable to ﬁnd appropriate\npreﬁxes. Table 2 provides several examples of keywords-tosentence generation by CGMH.\nWe present statistics of human evaluation in Table 3.\n“Mean intra-annotator std” is the mean standard deviation\nof scores from different volunteers, whereas “Mean intramodel std” is the mean standard deviation of each model.\nThis implies that the volunteers achieve high consistency\nwith each other, and that the gap between different models\nis large. p-values in this table are between CGMH and GBS;\nfor CGMH and B/F models, p-values are lower than 0.001.\nThis shows that the results given by human annotation are\nstatistically signiﬁcant.\nModel\nBLEU-ref\nBLEU-ori\nNLL\nOrigin Sentence\n30.49\n100.00\n7.73\nVAE-SVG (100k)\n22.50\nVAE-SVG-eq (100k)\n22.90\nVAE-SVG (50k)\n17.10\nVAE-SVG-eq (50k)\n17.40\nSeq2seq (100k)\n22.79\n33.83\n6.37\nSeq2seq (50k)\n20.18\n27.59\n6.71\nSeq2seq (20k)\n16.77\n22.44\n6.67\nVAE (unsupervised)\n9.25\n27.23\n7.74\nCGMH w/o matching\n18.85\n50.28\n7.52\nw/ KW\n20.17\n53.15\n7.57\nw/ KW + WVA\n20.41\n53.64\n7.57\nw/ KW + WVM\n20.89\n54.96\n7.46\nw/ KW + ST\n20.70\n54.50\n7.78\nTable 4: Performances of different paraphrase models.\nIdeal paraphrase generator should achieve higher BLEU-ref,\nlower BLEU-ori, and lower NLL scores.\nType\nExamples\nOri\nwhat ’s the best plan to lose weight\nRef\nwhat is a good diet to lose weight\nGen\nwhat ’s the best way to slim down quickly\nOri\nhow should i control my emotion\nRef\nhow do i control anger and impulsive emotions\nGen\nhow do i control my anger\nOri\nwhy do my dogs love to eat tuna ﬁsh\nRef\nwhy do my dogs love eating tuna ﬁsh\nGen\nwhy do some dogs like to eat raw tuna and raw ﬁsh\nTable 5: Paraphrase generation given by CGMH w/\nKW+WVM. For each sample, we show the original sentence\n(Ori), the reference paraphrase (Ref), and the generated sentence (Gen).\nStep\nState (Sentence)\nProposal\nOrigin\nwhat movie do you like most .\nreplace what with which\n1\nwhich movie do you like most .\ndelete most\n2\nwhich movie do you like .\ninsert best\n3\nwhich movie do you like best .\nreplace like with think\n4\nwhich movie do you think best .\ninsert the\n5\nwhich movie do you think the best .\ninsert is\nOutput\nwhich movie do you think is the best .\nTable 6: An example of the sampling process given by\nCGMH w/ KW+WVM.\nUnsupervised Paraphrase Generation\nWe followed previous work of supervised paraphrase generation (Gupta et al. 2017; Prakash et al. 2016; Gupta et\nal. 2017; Li et al. 2017) and used a standard benchmark,\nthe Quora dataset,3 to evaluate each model. The dataset contains 140k pairs of paraphrase sentences, and 260k pairs of\nnon-paraphrase sentences. We followed the standard dataset\nsplit, which holds out 3k and 30k for validation and testing,\nrespectively.\nFor supervised baselines, we varied the training samples\nto be 100k, 50k, and 20k pairs, so that we could evaluate\nthe effect of different parallel data sizes in supervised training. For unsupervised paraphrase generation, we only need\n3https://www.kaggle.com/c/quora-question-pairs/data\na non-parallel corpus to train the language model. The sentences in the test set, however, are questions, so it is improper to use generic language models (e.g., trained on OneBillion Corpus) to judge the likelihood of a question. Instead, we trained language models on all the training samples that do not appear in the validation and test sets. The\nlanguage models are of the same structure as the ones for\nkeywords-to-sentence generation, except that we reduce the\ndictionary size to 30k because of fewer training samples.\nPrevious work uses the BLEU score (Papineni et al. 2002)\nagainst a ground truth reference (denoted as BLEU-ref) to\nevaluate the quality of the generated paraphrase (Gupta et\nal. 2017). We observe that it is insufﬁcient because simply\ncopying the input sentence itself yields the highest BLEUref score (Table 4). We thus propose to use the BLEU score\nagainst the original input sentence (denoted as BLEU-orig)\nas an auxiliary measure. Ideally, BLEU-ref should be high,\nwhereas BLEU-ori should be low. We tried different initialization states, including using exact or corrupted original sentences. We also attempted to start from a totally random state. As a lot of samples are generated, we chose the\nﬁrst sentence with BLEU-ori score lower than 55 to compare with other models. (The number is chosen empirically\nin order to get paraphrases with obvious literal difference.)\nWe compare our approach with supervised methods, including sequence-to-sequence models, VAE-SVG and VAESVG-eq (Gupta et al. 2017). VAE-SVG is a VAE conditioned on the original sentence, and VAE-SVG-eq is a variant of VAE-SVG which shares parameters between encoder\nand decoder.\nWe would also like to compare paraphrase generator in\nthe unsupervised setting as our CGMH. However, we cannot ﬁnd an existing dedicated model. We ﬁnd it possible\nto train a variational autoencoder (VAE) with non-parallel\ncorpus and sample sentences from the variational latent\nspace (Bowman et al. 2016).\nTable 4 shows, compared with VAE, that our method\nachieves a fairly close BLEU-ref score to the best supervised approaches. Moreover, CGMH even outperforms the\nsupervised methods when the training set is not large enough\n(≤50k).\nAdmittedly, CGMH has higher BLEU-ori scores than supervised methods, indicating that the generated samples are\ncloser to the input. This, however, makes sense because\nCGMH samples sentences from a distribution speciﬁed in\nan unsupervised fashion, as opposed to rewriting words and\nphrases in an ad hoc fashion to make the expressions different, as is learned in the supervised setting. However, we only\nconsider paraphrases with BLEU-ori less than 55, which\nhas assured a signiﬁcant literal difference. Future research\ncould address this problem by designing proper heuristics to\nmanipulate the stationary distribution, which is beyond the\nscope of our paper (but shows the ﬂexibility of our model).\nTable 5 shows examples of generated paraphrases. We see\nqualitatively that CGMH yields fairly good samples in terms\nof both closeness in meaning and difference in expressions.\nTable 6 gives a real example of the paraphrase generation\nprocess with CGMH.\nModel\n#parallel data\nGLEU\nAMU\n2.3M\n44.85\nCAMB-14\n155k\n46.04\nMLE\n720k\n52.75\nNRL\n720k\n53.98\nCGMH\n0\n45.5\nTable 7: Results of different models on sentence correction.\nOri\nEven if we are failed , We have to try to get a new things .\nRef\nEven if we all failed , we have to try to get new things .\nGen\nEven if we are failing , We have to try to get some new things .\nOri\nIn the world oil price very high right now .\nRef\nIn today ’s world , oil prices are very high right now .\nGen\nIn the world , oil prices are very high right now .\nTable 8: Examples of sentence correction by CGMH.\nUnsupervised Error Correction\nWe evaluated our method on JFLEG (Napoles, Sakaguchi,\nand Tetreault 2017),4 a newly released dataset for sentence\ncorrection. It contains 1501 sentences (754 for validation\nand 747 for test), each with 4 revised references. We adopted\nthe GLEU metric (Napoles et al. 2015), which measures sentence ﬂuency and grammaticality.\nThis benchmark dataset does not contain training samples. Various studies have not only proposed new models,\nbut also collected parallel data for themselves, each containing millions of samples, shown in Table 7. However, we used\nnone of them.\nWe adopted the same language models (trained on OneBillion-Word) as in keywords-to-sentence generation to approximate sentence probabilities. To better handle typos and\ntense errors, we employ en package5 to provide an additional\ncandidate word set containing possible words with similar\nspellings or the same root. For MH sampling, we start from\nthe original sentence, and simply output the 100th sample.\nAs the original erroneous sentence has low probability from\na language model perspective, the goal of sentence correction can be formulated as jumping to a nearby sentence with\nhigh probability. We would like to encourage MH to explore\nmore probable states by further rejecting proposals if the\nlikelihood is becoming too small.\nThe performance of CGMH on error correction is surprisingly promising, as shown in Table 7. CGMH achieves comparable results to CAMB14, a rule-based system for error\ncorrection (Felice et al. 2014). CGMH even outperforms the\nAMU system (Junczys-Dowmunt and Grundkiewicz 2016),\nwhich is built on phrase-based machine translation with\n2.3M parallel training pairs and intensively engineered linguistic features. We observe some performance gap between CGMH and other supervised approaches, namely,\nMLE (Maximal Likelihood Estimation) and NRL (Neural\nReinforcement Learning) (Sakaguchi, Post, and Van Durme\n2017). Nevertheless, our initial success of CGMH shows a\npromising direction of unsupervised error correction.\n4https://github.com/keisks/jﬂeg\n5https://www.clips.uantwerpen.be/pages/pattern-en\nFigure 2: Generation quality with corrupted initial states. At\neach situation, 0/5%/10%/100% of the words in initial sentences are randomly replaced with other words.\nModel Analysis\nDespite successful applications in the above experiments,\nwe now analyze CGMH in more detail. k Acceptance rate\nand ergodicity. Table 9 shows the acceptance rate in the\nparaphrase generation task. We see that the word replacement has 100% acceptance rate as it is essentially a Gibbs\nstep, guaranteed by Equation 7. For word deletion and addition, the acceptance rate is lower, but still in a reasonable\nrange; it allows the sampler to generate sentences with different lengths, as opposed to Gibbs sampling. In our experiment, it takes about 150 steps to obtain a ﬂuent sentence\nfrom a sequence of keywords. For paraphrase generation, it\ntakes less than 50 steps for more than 20% of words being\nchanged, showing that CGMH is efﬁcient for practical use.\nInitial state of the Markov chain. Theoretically, the initial state of the Markov chain does not affect the stationary\ndistribution, given that the chain is irreducible and aperiodic.\nFigure 2 shows that, in the experiment of paraphrase, corrupting the initial state x0 by a small fraction does not signiﬁcantly affect model performance. But if we corrupt 100%\n(i.e., start with random initial states), the performance is very\nlow at the beginning but improves gradually. However, it is\ndifﬁcult to obtain satisfactory performance after 100 epochs.\nThis shows that warm start is useful for CGMH sentence\nsampling.\nIt should be emphasized that CGMH sampling does not\nsolely reply on the initial state being a valid sentence. In\nkeywords-to-sentence generation, we start from a sequence\nof keywords, but CGMH eventually yields samples which\nare generally ﬂuent sentences.\nComparison with VAE. We would like to compare\nCGMH (sampling from the sentence space) with VAE (sampling from the latent space). VAE is a probabilistic model\nthat imposes a prior distribution on the latent space and then\ndecodes a sentence in a deterministic manner by a RNN. In\npractice, we observe the variance of VAE samples will increase as the generation proceeds. This is shown by the blue\ncurve in Figure 3, as the word overlap rate (the ratio of the\nreference sentences containing words at a speciﬁc position\nof the generated ones) goes down for words far from the beginning of a sentence. This is possibly because RNN can be\nthought of as an autoregressive Bayesian network generating words conditioned on previous ones. Hence error will\naccumulate during generation. However, CGMH does not\nModel\nRep\nAdd\nDel\nMean\nCGMH w/o matching\n100\n9.2\n5.1\n32.9\nw/ KW\n100\n8.0\n4.4\n32.5\nw/ KW + WVM\n100\n10.8\n2.9\n32.7\nTable 9: Acceptance rate (%) in the paraphrase generation\ntask. Word replacement has 100% acceptance rate as shown\nin Equation (8).\nFigure 3: Overlap rates of CGMH and VAE for each word\nposition of sentences.\nseverely suffer from this problem, because there is not an\nexplicit generation direction (order) for CGMH. At the same\ntime, CGMH has the ability to self-correct, which is shown\nin the experiment of error correction.\nIf we would like to sample diversiﬁed sentences, CGMH\nis also better than VAE, because the diversity is distributed\nacross the entire sentence in CGMH.",
        "conclusion": "In this paper, we present CGMH for constrained sentence\ngeneration by Metropolis-Hastings (MH) sampling, where\nwe propose word-level operations including replacement,\ninsertion, and deletion as proposals, and design several stationary distributions for different tasks. We evaluated our results on keywords-to-sentence generation, paraphrase generation, and error correction. Our CGMH framework not only\nmakes unsupervised learning feasible in these applications,\nbut also achieves high performance close to state-of-the-art\nsupervised approaches.",
        "summary_en": "In real-world applications of natural language generation, there are often constraints on the target sentences in addition to fluency and naturalness requirements. Existing language generation techniques are usually based on recurrent neural networks (RNNs). However, it is non-trivial to impose constraints on RNNs while maintaining generation quality, since RNNs generate sentences sequentially (or with beam search) from the first word to the last. Therefore,this paper proposes CGMH, a novel approach using Metropolis-Hastings sampling for constrained sentence generation. CGMH allows complicated constraints such as the occurrence of multiple keywords in the target sentences, which cannot be handled in traditional RNN-based approaches. Moreover, CGMH works in the inference stage, and does not require parallel corpora for training. The paper evaluates the method on a variety of tasks, including keywords-to-sentence generation, unsupervised sentence paraphrasing, and unsupervised sentence error correction. CGMH achieves high performance compared with previous supervised methods for sentence generation.",
        "summary_zh": "这篇论文介绍了一种名为CGMH的新方法来生成受约束的句子。传统的基于循环神经网络（RNN）的语言生成技术通常难以在保持生成质量的同时对RNN施加约束。本文提出的CGMH利用Metropolis-Hastings采样来生成受约束的句子，并允许处理复杂的约束条件。此外，CGMH在推断阶段工作，不需要平行语料库进行训练。作者在多项任务上对该方法进行了评估，实验结果表明，CGMH在句子生成方面与先前监督方法相比的有更高的性能。"
    },
    {
        "title": "Show Me How To Revise: Improving Lexically Constrained Sentence Generation with XLNet",
        "abstract": "Lexically constrained sentence generation allows the incorporation of prior knowledge such as lexical constraints into the output. This technique has been applied to machine translation, and dialog response generation. Previous work usually used Markov Chain Monte Carlo (MCMC) sampling to generate lexically constrained sentences, but they randomly determined the position to be edited and the action to be taken, resulting in many invalid reﬁnements. To overcome this challenge, we used a classiﬁer to instruct the MCMCbased models where and how to reﬁne the candidate sentences. First, we developed two methods to create synthetic data on which the pre-trained model is ﬁne-tuned to obtain a reliable classiﬁer. Next, we proposed a two-step approach, “Predict and Revise”, for constrained sentence generation. During the predict step, we leveraged the classiﬁer to compute the learned prior for the candidate sentence. During the revise step, we resorted to MCMC sampling to revise the candidate sentence by conducting a sampled action at a sampled position drawn from the learned prior. We compared our proposed models with many strong baselines on two tasks, generating sentences with lexical constraints and text inﬁlling. Experimental results have demonstrated that our proposed model performs much better than the previous work in terms of sentence ﬂuency and diversity. Our code, pre-trained models and Appendix are available at https://github.com/NLPCode/MCMCXLNet.",
        "introduction": "Recently, there has been much interest in generating sentences in a controlled manner. Lexically constrained sentence generation aims to incorporate some pre-speciﬁed keywords or phrases into the generated sentences, and has been\nwidely used for many natural language processing (NLP)\ntasks. For example, Mou et al. (2016) alleviated generating\nuniversal dialog responses by injecting a keyword into the\ndialogue replies. Some researchers (Hokamp and Liu 2017;\nPost and Vilar 2018) incorporated the domain-speciﬁc terminologies into the translated sentences.\nTo generate sentences with lexical constraints, Mou et al.\n(2015) proposed a novel backward and forward language\nmodel (B/F-LM). Liu et al. (2019b) extended B/F-LM by introducing a discriminator. However, the capability of these\nmodels is limited, as they can generate sentences with only\none lexical constraint. To generate sentences with multiple\nlexical constraints, Hokamp and Liu (2017) proposed grid\nbeam search (GBS) by controlling the decoding process. But\nthis may degrade the quality of the generated sentence since\nthe constraints are not considered when generating previous\ntokens. Applying GBS to unconditional generative models\nwill suffer from this issue more seriously because the solution space is much wider than that of conditional generative\nmodels. In addition, beam search-based models tend to generate generic and repetitive sentences in unconditional text\ngeneration (Holtzman et al. 2020).\nAnother line of work applies MCMC sampling to lexically constrained sentence generation. Berglund et al. (2015)\nﬁrst used Gibbs sampling to generate sentences from the\nbidirectional RNN. Then, Wand and Cho (2019) extended\nGibbs sampling to generate sentences from BERT (Devlin\net al. 2019). Su et al. (2018) incorporated soft constraints\nsuch as sentiments via discriminators into the generated sentences. Furthermore, Miao et al. (2019) proposed CGMH,\nwhich can generate ﬂexible-length sentences under the given\nlexical constraints with replacement, insertion, and deletion\nactions. Different from GBS, MCMC-based models generate the constrained sentence via many reﬁnements instead\nof one pass, which allows them to utilize both the past and\nfuture contexts to reﬁne tokens one by one iteratively.\nHowever, previous MCMC-based models typically conduct many invalid and redundant reﬁnements because of\nthe randomly chosen actions and positions. This problem is\nmore serious when generating sentences with lexical constraints since it needs many more operations to generate a\ncomplete sentence. To demonstrate this problem, we showed\nan example in Figure 1. It is easy for us to realize that a token should be inserted before “very” to complete this sentence. Therefore, operations such as replacing a token with\nanother token or inserting a token at some other position are\nvery likely to be rejected since these reﬁnements can hardly\nimprove the quality of the candidate sentence. Without sufﬁcient reﬁnements, these models will risk generating ungrammatical sentences.\nTo reduce the redundant reﬁnements conducted by\nMCMC-based models, we proposed a two-step approach,\n“Predict and Revise”, for constrained sentence generation,\nshown in Figure 1. Intuitively, suppose a classiﬁer can tell\n<BOS> This film very interesting . <EOS>\nToken-level Classifier\nPredict\nCopy\nReplace\nInsert\np0,0 p0,1\np0,2\np0,3\np1,0\np2,0\np1,1\np2,1\np1,2\np2,2\np1,3\np2,3\np0,4\np1,4\np2,4\np0,5\np1,5\np1,6\np0,6\np2,5 p2,6\nAction: Insert \nPostion: 3\nCandidate Generator\nMCMC Sampling\n<BOS> This film is very interesting . <EOS>\nThe learned prior table\nRevise\nSample\nTop K candidates\nSample\nPre-process\nCreate synthetic datasets\nFine-tune XLNet\nTrain FLM and BLM as \nthe candidate generator\nRefine \nFigure 1: Illustration of our approach. In the predict step, we leveraged the token-level classiﬁer to compute the learned prior for\nthe current sentence. Then, we sampled a proposed action and position from the learned prior. In the revise step, we generated\nthe top K candidate sentences, from which we sampled a candidate sentence with MCMC.\nthe model where and how to reﬁne the candidate sentence,\nthen we will undoubtedly avoid many invalid operations,\nthus improving the quality of the generated sentences. To\ntrain a reliable classiﬁer, we proposed two methods (random and masked language model) to create synthetic data.\nWhen creating synthetic data for the replacement action, the\nrandom method replaces the chosen token with a randomly\nsampled token, while the masked language model method\nmasks the chosen token and leverages the permutation-based\nlanguage model, i.e., XLNet (Yang et al. 2019), to predict\nthe masked token. A token from the top N tokens is drawn\nas the replaced token. These two methods are complementary to each other, contributing to training a classiﬁer. Then,\nwe ﬁne-tuned XLNet on the synthetic dataset to obtain a\ntoken-level classiﬁer, which provides accurate revision information including the position to be revised and the action\nto be conducted. In the predict step, an action and a position\nare drawn from the learned prior given by the classiﬁer for\nthe candidate sentence. In the revise step, we used the candidate generator to generate the top K candidate sentences,\nand drew a candidate sentence with MCMC.\nThe main contributions of our work are threefold: (1) we\nproposed two approaches to create the synthetic dataset and\nﬁne-tuned XLNet to obtain a reliable classiﬁer; (2) we proposed a two-step approach for constrained sentence generation, guided by the classiﬁer to reﬁne the candidate sentence;\n(3) we conducted extensive experiments on two tasks, generating sentences with lexical constraints and text inﬁlling.\nExperiment results show that our proposed model outperforms previous baselines in sentence ﬂuency and diversity.",
        "problem deﬁnition": "Generating Sentences with Lexical Constraints\nGenerating sentences with lexical constraints aims to incorporate the given lexical constraints into the output. Given the\nconstraints c1, c2, · · · , ck, this task is deﬁned as follows:\nX∗ = arg max\nX P(X|c1, c2, · · · , ck),\n(1)\nwhere X is the sentence containing the given lexical\nconstraints. This task is meant to ﬁnd a ﬂuent sentence by\nmaximizing the conditional probability.\nText Inﬁlling\nText inﬁlling aims to inﬁll the missing portions of a\nsentence based on the past and future contexts to make\nthe sentence complete and ﬂuent. In this paper, we follow\na more general deﬁnition of text inﬁlling (Zhu, Hu, and\nXing 2019), where the number of missing portions is\narbitrary, and each portion can be inﬁlled with an arbitrary\nnumber of tokens. Given an incomplete sentence XB =\n{x1, · · · , xi−1, m, xi+1, · · · , xj−1, m, xj+1, · · · , xn},\nan\narbitrary number of tokens can be ﬁlled into each blank\nm to make it meaningful. Therefore, text inﬁlling can be\nformulated as follows:\nX∗ = arg max\nX P(X|XB).\n(2)\nText inﬁlling is a special case of constrained sentence generation, where the known portions can be regarded as lexical\nconstraints. Compared with generating sentences with lexical constraints, tokens are permitted to be inserted only at\nspeciﬁc positions, where blanks are located.",
        "methodology": "The overview of our proposed approach is demonstrated in\nFigure 1. In this section, we will ﬁrst introduce how to create the synthetic dataset for training a classiﬁer. Then, we\nwill describe how to train a classiﬁer and use it to compute\nthe learned prior for the candidate sentence. Finally, we will\ndemonstrate the process of generating candidate sentences\nwith the candidate generator and reﬁning the current candidate sentence with MCMC sampling.\nCreating the Synthetic Dataset\nWe aim to obtain a three-class token-level classiﬁer, which is\nexpected to tell us how to reﬁne the candidate sentence. We\nuse labels 0, 1, 2 to indicate copy, replacement, and insertion\nactions. The copy action indicates that the current token does\nnot need to be modiﬁed. The replacement action indicates\nthat the current token should be replaced with another token\nto make the sentence more ﬂuent. Similarly, the insertion action means some tokens should be inserted before the current\ntoken to complete the sentence. To train a reliable classiﬁer,\nwe will create the synthetic dataset D = {(X, L)}, where\n(X, L) denotes a data instance. We used One-Billion-Word\ncorpus1 to construct the synthetic dataset. Each time we randomly selected a sentence from One-Billion-Word corpus.\nAssume the selected sentence is “Opponents of the tariff say\nU.S. manufacturing would suffer under the climate bill regardless of trade policy changes.” Then, we randomly chose\na continuous segment from this sentence, “manufacturing\nwould suffer under the climate bill.”\nTo construct synthetic data for the insertion action, we\nrandomly deleted some tokens from the selected part. Then,\nwe obtained a synthetic input pair, X={<BOS>, manufacturing, suffer, under, the, climate, bill, <EOS>} and\nL = {0, 2, 2, 0, 0, 0, 0, 2}. In the above example, the labels for “manufacturing” and “<EOS>” are 2, which means\nwe need to insert something before these tokens. Since we\ndeleted “would” from the selected sub-sentence, the label\nfor “suffer” is also 2. Labels for the remaining tokens are 0.\n“<BOS>” and “<EOS>” are special tokens, which represent the start and end of a sentence, respectively.\nWe proposed two methods (random and masked language\nmodel) to create synthetic data for the replacement action.\nThe random method replaces the chosen token with a randomly sampled token. For example, replacing the token\n“would” with “(”. In this case, the classiﬁer can easily infer the label for “(” is 1. Nevertheless, this case is far from\nthe real mistakes made by humans. Therefore, the classiﬁer\nwill struggle to infer labels in scenarios where tense errors\nappear, or some inappropriate words are used. To solve this\nproblem, the masked language model method uses XLNet\nto mimic mistakes made by humans. Speciﬁcally, we ﬁrst\nreplaced the chosen token, e.g., “would” with the masked\ntoken “<MASK>”. Then, we used XLNet to predict the\nmasked token “<MASK>” and sampled a token from the\ntop N predicted tokens with the highest probability. (In our\nexperiments, we set N to 20). Before using XLNet to create\nsynthetic data, we ﬁne-tuned it on the masked dataset constructed with One-Billion-Word to give reliably predicted tokens. These two methods are complementary to each other,\nand we combined them to create the synthetic dataset.\nThe Token-level Classiﬁer\nAfter getting the synthetic dataset, we ﬁne-tuned XLNet on\nthem to get a token-level classiﬁer. We chose XLNet as the\nclassiﬁer because it can not only be used as a classiﬁer but\nalso serve as a transformer-based language model (Vaswani\net al. 2017). We also conducted experiments using BERT\nas the classiﬁer and GPT-2 as the language model, but the\nresults are worse than XLNet because BERT and GPT-2 do\nnot share the same vocabulary.\nIn this paper, we also conducted experiments with LSTMbased language models. However, the learned prior given\nby XLNet cannot be directly used by LSTM-based language models. Since XLNet uses SentencePiece (Kudo and\nRichardson 2018) to tokenize the text, each token of LSTMbased language models may be divided into multiple subtokens. To solve this problem, we use the label of the ﬁrst\nsub-token as the label for the given token. For example, the\n1http://www.statmt.org/lm-benchmark/\ntoken “ﬁne-tuning” is divided into four sub-tokens (“ ﬁne”,\n“-”, “tun” and “ing”) by XLNet. The label of “ ﬁne” will be\nregarded as the label of “ﬁne-tuning” when feeding the outputs of XLNet classiﬁer to LSTM-based language models.\nAfter getting the learned prior table P shown in Figure 1,\nwe set the replacement probability of the “<EOS>” token\nand lexical constraints to zero to make sure the given keywords appear in the output. In addition, we set the probabilities of the “<BOS>” token to zero to prohibit any modiﬁcation at the starting position. We computed the sum of these\nprobabilities across the horizontal dimension of the table to\ncompute the probabilities for the replacement and insertion\nactions, from which we sampled an action a. Then, we drew\na position from the row of the sampled action.\nThe Candidate Generator and MCMC Sampling\nSuppose the candidate sentence at time step t is Xt =\n[x1, · · · , xi, · · · , xn]. Assume the proposed action and position is replacement and i, respectively. We need to compute\nthe probability of replacing the i-th token with another token. Using Gibbs sampling (Geman and Geman 1984), we\ncomputed the conditional distribution as follows:\nq(xi = wj|x−i) =\np(x1, · · · , xi = wj, · · · , xn)\nP\nw∈V p(x1, · · · , xi = w, · · · , xn),\n(3)\nwhere V denotes the vocabulary, and x−i denotes tokens of\nXt except xi. p(x1, · · · , xi = wj, · · · , xn) denotes the sentence probability, computed by a forward language model.\nIt is non-trivial to compute the conditional distribution\nwith Equation (3) since we need to compute probabilities for\n|V | candidate sentences. Previous work resorted to a candidate generator to solve this problem, which takes x−i as\ninputs and outputs the top K tokens with the highest probabilities. Then, we get K candidate sentences by replacing the\ni-th token with the top K tokens. In this paper, we test four\ndifferent candidate generators. First, we trained a forward\nlanguage model (FLM), a backward language model (BLM),\nand a masked language model (MLM). FLM takes the tokens before xi as inputs (FLM(x<i)), while BLM takes the\ntokens after xi as inputs (BLM(x>i)). To leverage both the\npast and future contexts, MLM takes the tokens before and\nafter xi as inputs (MLM(x<i, x>i)). Our last method combines FLM and BLM (FLM(x<i) × BLM(x>i)), and also\nuses the contextual information to predict xi. Not surprisingly, the last method performs better than using only FLM\nor BLM as the candidate generator. However, we also found\nthat the last method performs slightly better than MLM. We\nspeculated that the conditional distribution given by the last\nmethod is more consistent with Equation (3). Therefore, in\nthe following experiments, we use the last method as our\ncandidate generator, and we set K to 50.\nAssume the proposed action is insertion. We need to insert a special token, “<MASK>”, before xi. We used the\ncandidate generator to generate the top K candidate sentences. Then, we used FLM to compute sentence probabilities for them. Finally, we sampled a sentence X′ from them.\nSimilar to the replacement action, a sentence with a higher\nprobability is more likely to be selected. The difference is\nthat the proposal distribution for insertion is asymmetric. To\nAlgorithm 1 Constrained Sentence Generation with XLNet\n1: Create the synthetic dataset, and train a token-level classiﬁer C on the synthetic training set.\n2: Train FLM and BLM on the training set.\n3: Set lexical constraints as the initial state X0 of MCMC.\n4: for t ← 1 to T do\n5:\nCompute the learned prior C(Xt−1), and draw an action A and a position P from C(Xt−1).\n6:\nCompute the top K candidate tokens with the candidate generator for (A, P, Xt−1).\n7:\nCreate K candidate sentences with the top K tokens,\nand compute probabilities for them with FLM.\n8:\nDraw a sentence X′ from the candidate sentences\nbased on the normalized sentence probabilities.\n9:\nCompute the acceptance rate A(X′|Xt−1), and draw\na number α from Uniform [0,1].\n10:\nIf α < A(X′|Xt−1) then Xt ← X′ else Xt ← Xt−1.\n11: end for\n12: Output the sentence X∗ with the lowest NLL.\nmake it meet the detailed balance condition, the MetropolisHastings (MH) algorithm (Metropolis et al. 1953; Hastings\n1970; He et al. 2017a,b) introduces an acceptance term:\nAinsert(X′|Xt)\n=\nmin(1, A⋆\ninsert(X′|Xt)),\n(4)\nA⋆\ninsert(X′|Xt)\n≈\nq(Xt|X′) × p(X′)\nq(X′|Xt) × p(Xt)\n(5)\n≈\nP\nX∈S p(X)\np(Xt)\n,\n(6)\nwhere S is the set of candidate sentences for the insertion\naction. When computing the acceptance rate, we ignored\nthe probabilities for the chosen action and position, and we\nfound ignoring these terms improves the experiment results.\nWe summarize the process of our proposed approach in\nAlgorithm 1. We ﬁrst created synthetic datasets and then\nﬁne-tuned XLNet on them to get the classiﬁer. Next, we\ntrained FLM and BLM and used them as the candidate generator. Finally, we reﬁned the candidate sentence with the\nclassiﬁer and MCMC sampling.",
        "experiments": "Generating Sentences with Lexical Constraints\nExperiment Setups and Baselines.\nWe selected 6M,\n0.3M and 1K sentences from One-Billion-Word corpus\nas the training, validation, and test sets, respectively. We\ntrained forward and backward LSTM-based language models on the training set. Similarly, we ﬁne-tuned the pretrained XLNet (base-cased version) model on the training\nset to get forward and backward XLNet-based language\nmodels. For each language model, we selected the checkpoint with the lowest validation loss. For both LSTM-based\nand XLNet-based models, the forward and backward language models serve as candidate generators. The forward\nlanguage models are also used to compute sentence probabilities. We ﬁne-tuned XLNet (base-cased version) on the\nsynthetic dataset to get our classiﬁer. The experiment setups\nfor language models and the classiﬁer are shown in the Appendix. We constructed four kinds of test sets for constrained\nsentence generation by varying the length of lexical constraints k from 1 to 4. For each case, we randomly extracted\n1, 000 sets of lexical constraints from the test sentences.\nTo compare with previous work, we implemented two\nvariants of the backward and forward language model (sepB/F and asyn-B/F), and GBS. After these models are welltrained, we ran beam search decoding (beam width = 10)\nto generate sentences with the extracted lexical constraints.\nAs for CGMH, we used the code and the pre-trained model\nprovided by the author to generate sentences.\nIn this paper, we proposed four models. The LSTM-based\nMCMC model (L-MCMC) uses LSTM-based models as the\ngenerator and language models. L-MCMC w/ deletion is our\nimplementation for CGMH. As shown in Table 1, sentences\ngenerated by our implementation have lower NLL values\nindicating higher quality. L-MCMC only uses replacement\nand insertion actions, which outperforms L-MCMC w/ deletion. Therefore, all our proposed models don’t use the deletion action. When reﬁning the generated sentence, L-MCMC\nrandomly chooses a position and an action. In comparison,\nthe LSTM-based MCMC w/ classiﬁer (L-MCMC-C) samples a position and an action from the learned prior table provided by the classiﬁer. Similarly, we also extended MCMC\nsampling to XLNet-based models and proposed the XLNetbased MCMC (X-MCMC) and the XLNet-based MCMC w/\nclassiﬁer (X-MCMC-C). Our models are implemented with\nHuggingFace (Wolf et al. 2019).\nFor a fair comparison, all MCMC-based models were run\nfor 200 steps to generate sentences with the given constraints\nand then output the sentence with the lowest NLL. In addition, sep-B/F, asyn-B/F, GBS, L-MCMC, and L-MCMC-C\nuse the same LSTM model structure.\nAutomatic Evaluation for Quality.\nTo automatically\nevaluate the quality of generated sentences, we followed\n(Wang and Cho 2019) by computing negative log-likelihood\n(NLL) of sentences. A lower NLL value means that the generated sentence is more ﬂuent and coherent. We used the\npre-trained GPT-2 small (117M) to measure NLL values of\nsentences. We also computed BLEU scores (Papineni et al.\n2002) between generated sentences and human references.\nA high BLEU score indicates a model can generate sentences similar to human references. NLL and BLEU results\nare shown in Table 1. Our proposed models (L-MCMC-C\nand X-MCMC-C) outperform baselines in NNL.\nIt is worth mentioning that sep-B/F, asyn-B/F, and GBS\nachieve relatively low NLL and high BLEU scores. Does\nit mean these models can generate well-formed sentences?\nHoltzman et al. (2020) found that beam search decoding\ntends to lead to degeneration. Language models are expected\nto assign low NLL scores to high-quality sentences, yet they\nalso give low NLL scores to repetitive and generic sentences.\nWe found that beam search-based models (sep-B/F, asynB/F, and GBS) may easily fall into repetitive loops, which\nis consistent with what is observed in previous work (Holtzman et al. 2020). We showed the percentage of sentences\nMetrics\nNLL (GPT-2) (↓)\nBLEU (bigram) (↑)\nHuman evaluation (↑)\nModels\nk=1\nk=2\nk=3\nk=4\nk=1\nk=2\nk=3\nk=4\nk=1\nk=2\nk=3\nk=4\nsep-B/F\n4.432\n7.1%\n0.333\nasyn-B/F\n4.304\n7.1%\n0.369\nGBS\n3.985\n4.163\n4.178\n4.209\n6.6%\n9.8%\n12.8%\n16.0%\n0.495\n0.384\n0.445\n0.421\nCGMH (200)\n5.079\n5.103\n5.227\n5.246\n1.3%\n3.9%\n7.3%\n11.5%\n0.345\n0.344\n0.400\n0.408\nL-MCMC w/ deletion (200)\n4.923\n4.775\n4.856\n4.874\n1.0%\n3.9%\n7.7%\n12.5%\nL-MCMC (200)\n4.549\n4.564\n4.617\n4.672\n2.5%\n5.9%\n10.3%\n14.4%\n0.433\n0.459\n0.469\n0.429\nL-MCMC-C (50)\n4.425\n4.389\n4.491\n4.487\n1.6%\n4.8%\n8.9%\n14.2%\nL-MCMC-C (200)\n3.762\n3.773\n3.819\n3.904\n3.7%\n7.5%\n12.4%\n16.7%\n0.575\n0.520\n0.573\n0.557\nX-MCMC (200)\n4.225\n4.319\n4.427\n4.463\n1.7%\n4.5%\n8.3%\n12.4%\n0.471\n0.516\n0.487\n0.516\nX-MCMC-C (60)\n4.152\n4.259\n4.398\n4.490\n1.9%\n4.7%\n8.3%\n12.1%\nX-MCMC-C (200)\n3.532\n3.683\n3.779\n3.879\n2.9%\n6.1%\n10.0%\n14.4%\n0.681\n0.589\n0.621\n0.655\nMetrics\nSelf-BLEU (4-gram) (↓)\nDistinct (bigram) (↑)\nEntropy (4-gram) (↑)\nHuman Reference\n8.2%\n9.0%\n9.0%\n9.0%\n81.5%\n80.7%\n80.7%\n80.7%\n9.882\n9.875\n9.875\n9.875\nsep-B/F\n81.7%\n19.5%\n7.664\nasyn-B/F\n80.2%\n20.3%\n7.924\nGBS\n84.2%\n80.6%\n73.4%\n67.9%\n16.2%\n20.9%\n27.0%\n31.8%\n6.741\n7.555\n8.117\n8.567\nCGMH (200)\n19.8%\n14.9%\n11.7%\n11.4%\n65.6%\n68.0%\n69.9%\n71.0%\n8.605\n8.871\n9.124\n9.301\nL-MCMC w/ deletion (200)\n14.4%\n13.8%\n10.5%\n9.2%\n74.1%\n74.8%\n76.2%\n77.0%\n8.309\n8.676\n8.961\n9.172\nL-MCMC (200)\n20.1%\n16.5%\n13.8%\n13.3%\n64.4%\n67.3%\n68.6%\n68.6%\n9.014\n9.260\n9.497\n9.669\nL-MCMC-C (50)\n20.2%\n16.3%\n14.2%\n12.6%\n68.6%\n70.8%\n71.1%\n73.1%\n8.591\n8.901\n9.144\n9.339\nL-MCMC-C (200)\n33.0%\n28.3%\n24.1%\n21.9%\n54.1%\n57.5%\n60.1%\n62.3%\n9.219\n9.415\n9.577\n9.685\nX-MCMC (200)\n11.3%\n9.1%\n8.3%\n7.1%\n77.5%\n78.9%\n79.4%\n79.3%\n8.832\n9.141\n9.391\n9.559\nX-MCMC-C (60)\n12.6%\n11.1%\n8.1%\n8.5%\n75.5%\n77.3%\n79.3%\n80.0%\n8.781\n9.018\n9.202\n9.372\nX-MCMC-C (200)\n19.0%\n15.1%\n13.6%\n12.1%\n69.6%\n72.6%\n74.0%\n74.2%\n9.097\n9.279\n9.453\n9.577\nTable 1: Results on One-Billion-Word test sets with different k. (Numbers in brackets refer to the number of time steps.)\ncontaining n-gram repetitions in Table 6 in the Appendix. In\naddition, we found that some generic sub-sequences (‘I am\ngoing to’, ‘he said’, ‘referring to’, ‘adding that’, etc.) frequently appear in the generated sentences. Both repetitive\nn-grams and generic sub-sequences will help these models\nto achieve low NLL and high BLEU scores. Therefore, it is\ninsufﬁcient to assess the generated sentences with NLL and\nBLEU. To complement them, we also automatically measured the diversity of each model’s generations.\nAutomatic Evaluation for Diversity.\nWe used SelfBLEU (Zhu et al. 2018) to measure the diversity of each\nmodel’s generations. Self-BLEU evaluates how one sentence resembles the other generated ones. For each sentence,\nwe treated it as a hypothesis and the others as references. We\nalso used distinct n-gram (Li et al. 2016) and entropy (Zhang\net al. 2018) to measure diversity. Distinct n-gram reﬂects the\npercentage of unique n-grams. Entropy further considers the\ndistribution of n-grams. Lower Self-BLEU or higher distinct\nn-gram and entropy values indicate higher diversity. From\nTable 1, we can see that sep-B/F, asyn-B/F and GBS have\nhigher Self-BLEU, lower distinct bigram and lower entropy\nscores, indicating they have low sample diversity, which is\nconsistent with our analysis above. By comparison, MCMCbased models have high sample diversity.\nWhen we ran L-MCMC-C for 50 steps, it achieved similar\nperformance in sentence quality, and diversity to L-MCMC\nran for 200 steps. Similarly, X-MCMC-C with 60 reﬁnement\nsteps is on par with X-MCMC with 200 reﬁnement steps. We\nshowed NLL and Self-BLEU achieved by L-MCMC and LMCMC-C with different reﬁnement steps when k = 4 in\nFigure 2(b). We can see that the proposed model (L-MCMCC) requires much fewer reﬁnement steps to achieve similar\nsentence quality and diversity compared to L-MCMC. The\nimprovements achieved by our proposed models are mainly\ndue to the classiﬁer’s guidance, which reduces the number\nof invalid reﬁnements. To verify this point, we showed the\nacceptance rates of LSTM-based models in Figure 2(a) and\nshowed the acceptance rates of XLNet-based models in the\nAppendix. We can see that applying the classiﬁer signiﬁcantly improves the acceptance rates for replacement and\ninsertion actions. In addition, adding the classiﬁer brings a\nsmall amount of overhead. For example, the running time of\nthe classiﬁer only accounts for about 1/7 of X-MCMC-C.\nWhen we ran L-MCMC-C and X-MCMC-C for 200 steps,\nNLL values declined dramatically. Meanwhile, Self-BLEU\nincreased, and distinct bigram decreased. Increasing the\nnumber of time steps will improve sentence quality but degrade sentence diversity. It seems to indicate that sentence\nquality and diversity contradict each other. To further analyze this phenomenon, we showed the relationship between\nNLL and Self-BLEU when k = 4 in Figure 2(c). We can see\nthat NLL values decrease with the increase of time steps,\nwhile Self-BLEU values increase for both models. Therefore, it is nontrivial to improve both sentence quality and diversity. Generic sentences with high probabilities favor sentence quality in terms of NLL but disfavor sentence diversity. Another advantage of our method is that we can make\na trade-off between quality and diversity by controlling the\nnumber of time steps.\nWhen comparing L-MCMC with X-MCMC (or LMCMC-C with X-MCMC-C), we found the latter outperk=1\nk=2\nk=3\nk=4\nk=1\nk=2\nk=3\nk=4\nThe number of keywords\n0\n20\n40\n60\n80\n100\nAcceptance rate (%)\n78.2\n78.4\n78.4\n78.4\n54.3\n54.1\n54.0\n54.5\n26.1\n27.6\n29.0\n31.0\n12.3\n13.2\n14.4\n15.4\nReplace (L-MCMC-C)\nReplace (L-MCMC)\nInsert (L-MCMC-C)\nInsert (L-MCMC)\n(a) Acceptance rates vs. k.\n0.10\n0.15\n0.20\n0.25\nThe Self-BLEU values\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nThe NLL values\n50\n100\n200\n400\n600800\n50\n100\n200\n400\n600\n800\nL-MCMC\nL-MCMC-C\n(b) NLL vs. Self-BLEU.\n0\n100\n200\n300\n400\n500\n600\n700\n800\nThe number of time steps\n0\n1\n2\n3\n4\n5\n6\nThe NLL values\n5.37\n4.93\n4.67\n4.47\n4.36\n4.26\n4.49\n4.18\n3.90\n3.73\n3.70\n3.66\nL-MCMC (NLL)\nL-MCMC-C (NLL)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nThe Self-BLEU values\n0.070.09\n0.13\n0.17\n0.18\n0.19\n0.13\n0.17\n0.22\n0.26\n0.27\n0.28\nL-MCMC (Self-BLEU)\nL-MCMC-C (Self-BLEU)\n(c) NLL and Self-BLEU vs. time steps.\nFigure 2: In subﬁgure (a), k is the number of keywords. In subﬁgure (b), numbers near lines denote the number of time steps.\nforms the former in both sentence quality (NLL) and diversity (Self-BLEU and distinct bigram), mainly because the\nlatter uses the pre-trained model as language models.\nHuman Evaluation.\nWe also conducted a human evaluation to further compare these models. We selected 100 sentences for each model and invited three volunteers to assess\nthe sentences in terms of ﬂuency. Concretely, we ﬁrst chose a\ngroup of sentences generated by different models and shufﬂed them to avoid bias. Then, the annotators ranked these\nsentences and assigned scores (0 to 1) to them, where 1 indicates the best quality. We showed the results of human evaluation in Table 1. Our proposed models (X-MCMC-C and LMCMC-C) also outperform baselines in human evaluation.\nWe also performed paired t-test comparisons between the\nproposed model (X-MCMC-C) and baselines, and p-values\nare less than 0.01, indicating the differences between the\nproposed model and baselines are statistically signiﬁcant.\nAblation Study.\nWe performed an ablation study to\ndemonstrate the importance of each design. We mainly focused on two aspects: the effectiveness of the learned prior\nand the importance of the synthetic dataset. In Table 2, we\ncompared L-MCMC-C (row 1) with ﬁve variants (rows 2-6).\nFor a fair comparison, all models use LSTM-based language\nmodels and are run for 200 steps.\nRemoving the learned prior for actions (row 2) (actions\nare randomly sampled, but positions are still sampled from\nthe learned prior) results in a slight increase in NLL, while\nremoving the learned prior for positions (row 3) causes a\nsharp decline in performance, which is nearly as poor as LMCMC (row 7). Therefore, the learned prior for positions is\nmuch more important than that of actions. Further, we used\none-third of the synthetic dataset to train the classiﬁer (row\n4). The results of row 4 are slightly worse than those of row\n1. Based on row 4, if we remove the random method and\nonly use the masked LM (XLNet) to create the synthetic\ndataset (row 5), NLL values will increase marginally. By\ncomparison, removing the masked LM (row 6) has a detrimental effect on performance. The random and masked LM\nmethods are complementary, and both are indispensable for\ntraining the classiﬁer.\nTo summarize, both the learned prior and the synthetic\ndataset play an essential role in our proposed model.\nMetrics\nNLL (GPT-2) (↓)\n#\nModels variants\nk=1\nk=2\nk=3\nk=4\n1\nL-MCMC-C\n3.762\n3.773\n3.819\n3.904\n2\n– prior for actions\n3.850\n3.861\n3.893\n3.932\n3\n– prior for positions\n4.498\n4.513\n4.605\n4.646\n4\n– size of dataset\n3.921\n3.950\n4.015\n4.030\n5\n– random\n3.954\n3.982\n4.024\n4.066\n6\n– masked LM\n4.063\n4.108\n4.167\n4.189\n7\nL-MCMC\n4.549\n4.564\n4.617\n4.672\nTable 2: Ablation study on our model.\nConstraints\nperson, home, problems, depression\nGBS\nA person familiar with the matter said : “ The problems of\ndepression are home to the people of the United States .\nCGMH\nThe person ’s head of home health problems have sparked\ndepression .\nL-MCMC-C\nThe average person who ’s lost money since he was a child at\nhome has no problems with depression .\nX-MCMC-C\nOne person was sent home with mental health problems and\nsevere depression .\nTable 3: Sentences generated with lexical constraints.\nSamples and Analysis.\nWe showed some sentences generated by our proposed models and baselines in Table 3.\nOur proposed models can generate high-quality, lexically\nconstrained sentences. As for GBS, the lexical constraint\n“home” conﬂicts with the previous tokens since GBS is not\naware of the future lexical constraints when generating previous tokens. Therefore, forcing to incorporate the lexical\nconstraint “home” degrades the quality of the generated sentence. The sentence generated with CGMH lacks coherence\nand ﬂuency. Since CGMH reﬁnes the generated sentence\nrandomly, it still needs more reﬁnements. More generated\nsentences are shown in Table 8 in the Appendix.\nText Inﬁlling\nExperiment Setups and Baselines.\nWe used 1, 000 sentences from the test set of One-Billion-Word corpus to create test sets for text inﬁlling. Following (Liu et al. 2019a),\nwe resorted to two mask strategies (random and middle) and\nthree mask ratios (r = 25%, 50%, or 75%) to construct test\nsets for text inﬁlling. The random mask strategy randomly\nremoves r of tokens from the original sentence. The middle\nMetrics\nNLL (GPT-2) (↓)\nBLEU (4-gram) (↑)\nModels\nMiddle\nRandom\nMiddle\nRandom\n25%\n50%\n75%\n25%\n50%\n75%\n25%\n50%\n75%\n25%\n50%\n75%\nHuman Reference\n4.022\n4.022\n4.022\n4.022\n4.022\n4.022\n100%\n100%\n100%\n100%\n100%\n100%\nTemplate\n65.1%\n36.5%\n9.0%\n49.5%\n16.1%\n2.6%\nFLM\n4.417\n4.479\n4.223\n4.434\n4.727\n4.543\n68.4%\n39.5%\n12.2%\n66.4%\n31.9%\n10.5%\nBLM\n4.453\n4.460\n4.130\n4.545\n4.860\n4.620\n68.3%\n39.6%\n12.8%\n65.9%\n31.2%\n9.9%\nF+B LM\n4.288\n4.355\n4.130\n4.309\n4.605\n4.457\n69.1%\n40.0%\n12.5%\n68.3%\n33.4%\n10.9%\nBayesian MCMC\n4.188\n4.268\n4.242\n4.164\n4.310\n4.349\n68.7%\n39.6%\n12.3%\n69.7%\n35.4%\n10.5%\nTIGS (strategy 1)\n4.985\n5.515\n5.865\n5.183\n5.911\n6.268\n65.8%\n37.2%\n9.9%\n56.1%\n21.8%\n4.3%\nTIGS (strategy 2)\n4.416\n4.341\n4.042\n4.638\n5.018\n4.852\n67.9%\n39.7%\n12.3%\n62.9%\n28.5%\n7.9%\nL-MCMC\n4.216\n4.313\n4.303\n4.178\n4.367\n4.436\n68.5%\n39.7%\n12.5%\n69.0%\n34.4%\n10.1%\nL-MCMC-C\n4.146\n4.053\n3.796\n4.135\n4.199\n4.106\n69.0%\n40.3% 12.9% 70.0% 36.1% 10.5%\nL-MCMC-C (w/o BP)\n70.9%\n43.1%\n14.6%\n70.9%\n37.6%\n11.8%\nTable 4: NLL and BLEU results for different mask strategies and rates. (“BP” refers to the brevity penalty.)\nmask strategy removes r of tokens from the middle of the\noriginal sentence. Therefore, we have six types of test sets.\nTo compare our proposed model with previous work, we\nimplemented several strong baselines. The forward language\nmodel (FLM) and the backward language model (BLM) ﬁlls\nthe blanks from left to right, and from right to left with beam\nsearch (beam width = 10), respectively. F+B LM (Wang\net al. 2016) ﬁlls the blanks by FLM and BLM and then\nselects the output with the lowest NLL. Bayesian MCMC\n(Berglund et al. 2015) initializes the blanks with random values and uses the replacement action to reﬁne the ﬁlled values, but it can only generate sentences with a ﬁxed length.\nWe also implemented TIGS (Liu et al. 2019a), which iteratively reﬁnes the ﬁlled tokens with gradient descent. Before\nreﬁning with TIGS, we resorted to two strategies to initialize\nthe blanks with random values or values predicted by FLM\nwith greedy search. All models use LSTM-based language\nmodels with the same structure (the setups for LSTM-based\nlanguage models have been introduced in the ﬁrst task). For\na fair comparison, we ran 20 iterations for Bayesian MCMC,\nTIGS, L-MCMC, and L-MCMC-C.\nAutomatic Evaluation.\nFollowing previous work (Liu\net al. 2019a), we also resorted to NLL and BLEU to automatically evaluate the inﬁlled sentences. Similar to our ﬁrst\ntask, NLL is measured by GPT-2, and BLEU is computed\nbetween the inﬁlled sentences and human references. BLEU\nmeasures how similar the inﬁlled sentence is to the ground\ntruth, while NLL assesses the ﬂuency and coherence of the\ninﬁlled sentences. We showed the results of NLL and the\ncorpus-level BLEU (Papineni et al. 2002) in Table 4. Our\nproposed model achieves the lowest NLL scores in all cases,\nwhich means the inﬁlled sentences of our proposed model\nare more ﬂuent than those of previous methods. One advantage of our proposed model is that it does not need to know\nthe number of blanks when inﬁlling. In contrast, other baselines need to know the number of blanks before inﬁlling.\nTherefore, our proposed models (L-MCMC and L-MCMCC) may generate sentences with different lengths from the\nground truths. Even though the BLEU algorithm with the\nbrevity penalty will penalize our model when it generates\nshorter sentences, our model still outperforms baselines in\nmost cases in terms of BLEU. Compared with TIGS, the\nproposed model does not need any initialization before inﬁlling. In addition, TIGS is sensitive to initialization strategies. TIGS performs much better when initialized with the\nresults of FLM, which may be unfair to other models. Compared with L-MCMC, L-MCMC-C signiﬁcantly improves\nthe performance in NLL and BLEU, mainly beneﬁting from\nthe learned prior given by the classiﬁer. We showed some\ninﬁlled sentences in Table 9 in the Appendix.",
        "related work": "Pre-trained Language Models\nRecently, many downstream NLP tasks are driven by largescale pre-trained language models such as GPT (Radford\n2018), GPT-2 (Radford et al. 2019), BERT (Devlin et al.\n2019), ROBERTA (Liu et al. 2019c), ELECTRA (Clark\net al. 2020), SpanBERT (Joshi et al. 2020), XLNet (Yang\net al. 2019), MASS (Song et al. 2019) and BART (Lewis\net al. 2020). However, these models cannot be directly applied to lexically constrained sentence generation.\nGenerating Sentences with Lexical Constraints\nB/F LMs (Mou et al. 2015; Liu et al. 2019b) are limited to\ngenerating text with one lexical constraint. GBS (Hokamp\nand Liu, 2017) can generate sentences with multiple lexical\nconstraints but degrades the generation quality and diversity.\nCGMH (Miao et al. 2019) revises candidate sentences randomly, causing many invalid operations. To solve this problem, we used an XLNet-based token-level classiﬁer to guide\nMCMC-based models to reﬁne the candidate sentence.",
        "conclusion": "In this paper, we aim to reduce the redundant reﬁnements\nconducted by previous MCMC-based models. To achieve\nthis, we used a token-level classiﬁer to instruct MCMCbased models where and how to reﬁne the candidate sentence. Compared with previous MCMC-based approaches,\nour proposed model can iteratively reﬁne the candidate sentence with the learned prior given by the pre-trained classiﬁer. Experiment results show that our proposed model can\ngenerate ﬂuent and diverse sentences for constrained sentence generation, outperforming all baselines.",
        "summary_en": "Lexically constrained sentence generation allows the incorporation of prior knowledge such as lexical constraints into the output. This technique has been applied to machine translation, and dialog response generation. Previous work usually used Markov Chain Monte Carlo (MCMC) sampling to generate lexically constrained sentences, but they randomly determined the position to be edited and the action to be taken, resulting in many invalid refinements. To overcome this challenge, this paper used a classifier to instruct the MCMC-based models where and how to refine the candidate sentences. First, the paper developed two methods to create synthetic data on which the pre-trained model is fine-tuned to obtain a reliable classifier. Next, the paper proposed a two-step approach, “Predict and Revise”, for constrained sentence generation. During the predict step, the paper leveraged the classifier to compute the learned prior for the candidate sentence. During the revise step, the paper resorted to MCMC sampling to revise the candidate sentence by conducting a sampled action at a sampled position drawn from the learned prior. The paper compared the proposed models with many strong baselines on two tasks, generating sentences with lexical constraints and text infilling. Experimental results have demonstrated that the proposed model performs much better than the previous work in terms of sentence fluency and diversity.",
        "summary_zh": "这篇论文介绍了一种新的词汇约束的句子生成技术。之前的研究通常使用马尔科夫链蒙特卡洛（MCMC）采样来生成词汇约束句子，但它们随机确定要编辑的位置和采取的操作，导致很多无效的改进。为了解决这一问题，作者使用分类器指导基于MCMC的模型对候选句子进行改进。首先，他们开发了两种方法来生成合成数据，然后对预训练模型进行微调，以获得可靠的分类器。接下来，他们提出了一个两步方法“预测和修订”，用于生成约束句子。在预测步骤中，利用分类器计算候选句子的学习先验。在修订步骤中，利用MCMC采样，在从所学先验中抽取的抽样位置上进行抽样操作，来修订候选句子。作者在两个任务上比较了他们提出的模型与许多基准，实验结果表明，他们提出的模型在句子流畅性和多样性方面比先前的工作表现更好。"
    },
    {
        "title": "An Affect-Rich Neural Conversational Model with Biased Attention and Weighted Cross-Entropy Loss",
        "abstract": "Affect conveys important implicit information in human communication. Having the capability to correctly express affect during human-machine conversations is one of the major milestones in artiﬁcial intelligence. In recent years, extensive research on open-domain neural conversational models has been conducted. However, embedding affect into such models is still under explored. In this paper, we propose an endto-end affect-rich open-domain neural conversational model that produces responses not only appropriate in syntax and semantics, but also with rich affect. Our model extends the Seq2Seq model and adopts VAD (Valence, Arousal and Dominance) affective notations to embed each word with affects. In addition, our model considers the effect of negators and intensiﬁers via a novel affective attention mechanism, which biases attention towards affect-rich words in input sentences. Lastly, we train our model with an affect-incorporated objective function to encourage the generation of affect-rich words in the output responses. Evaluations based on both perplexity and human evaluations show that our model outperforms the state-of-the-art baseline model of comparable size in producing natural and affect-rich responses.",
        "introduction": "Affect is a psychological experience of feeling or emotion.\nAs a vital part of human intelligence, having the capability to recognize, understand and express affect and emotions\nlike human has been arguably one of the major milestones\nin artiﬁcial intelligence (Picard 1997).\nOpen-domain conversational models aim to generate coherent and meaningful responses when given user input sentences. In recent years, neural network based generative conversational models relying on Sequence-to-Sequence network (Seq2Seq) (Sutskever, Vinyals, and Le 2014) have\nbeen widely adopted due to its success in neural machine\ntranslation. Seq2Seq based conversational models have the\nadvantages of end-to-end training paradigm and unrestricted\nresponse space over conventional retrieval-based models. To\nmake neural conversational models more engaging, various\ntechniques have been proposed, such as using stochastic latent variable (Serban et al. 2017) to promote response diversity and encoding topic (Xing et al. 2017) into conversational models to produce more coherent responses.\nHowever, embedding affect into neural conversational\nmodels has been seldom explored, despite that it has many\nbeneﬁts such as improving user satisfaction (Callejas, Griol,\nand L´opez-C´ozar 2011), fewer breakdowns (Martinovski\nand Traum 2003), and more engaged conversations (Robison, McQuiggan, and Lester 2009). For real-world applications, Fitzpatrick, Darcy, and Vierhile (2017) developed\na rule-based empathic chatbot to deliver cognitive behavior\ntherapy to young adults with depression and anxiety, and obtained signiﬁcant results on depression reduction. Despite\nof these beneﬁts, there are a few challenges in the affect\nembedding in neural conversational models that existing approaches fail to address: (i) It is difﬁcult to capture the emotion of a sentence, partly because negators and intensiﬁers\noften change its polarity and strength. Handling negators and\nintensiﬁers properly still remains as a challenge in sentiment\nanalysis. (ii) It is difﬁcult to embed emotions naturally in responses with correct grammar and semantics (Ghosh et al.\n2017).\nIn this paper, we propose an end-to-end single-turn opendomain neural conversational model to address the aforementioned challenges to produce responses that are natural and affect-rich. Our model extends Seq2Seq model with\nattention (Luong, Pham, and Manning 2015). We leverage an external corpus (Warriner, Kuperman, and Brysbaert\n2013) to provide affect knowledge for each word in the Valence, Arousal and Dominance (VAD) dimensions (Mehrabian 1996). We then incorporate the affect knowledge into\nthe embedding layer of our model. VAD notation has been\nwidely used as a dimensional representation of human emotions in psychology and various computational models, e.g.,\n(Wang, Tan, and Miao 2016; Tang et al. 2017). 2D plots of\nselected words with extreme VAD values are shown in Figure 1. To capture the effect of negators and intensiﬁers, we\npropose a novel biased attention mechanism that explicitly\nconsiders negators and intensiﬁers in attention computation.\nTo maintain correct grammar and semantics, we train our\nSeq2Seq model with a weighted cross-entropy loss that encourages the generation of affect-rich words without degrading language ﬂuency.\nOur main contributions are summarized as follows:\n• For the ﬁrst time, we propose a novel affective attention\nmechanism to incorporate the effect of negators and intensiﬁers in conversation modeling. Our mechanism in1\n2\n3\n4\n5\n6\n7\n8\n9\nValence (V)\n1\n2\n3\n4\n5\n6\n7\n8\n9\nArousal (A)\npedophile\nvacation\ngrain\ninsanity\ndementia\nparadise\n(a) V-A ratings\n1\n2\n3\n4\n5\n6\n7\n8\n9\nValence (V)\n1\n2\n3\n4\n5\n6\n7\n8\n9\nDominance (D)\npedophile\nvacation\ngrain\ninsanity\ndementia\nparadise\n(b) V-D ratings\n1\n2\n3\n4\n5\n6\n7\n8\n9\nArousal (A)\n1\n2\n3\n4\n5\n6\n7\n8\n9\nDominance (D)\npedophile\nvacation\ngrain\ninsanity\ndementia\nparadise\n(c) A-D ratings\nFigure 1: 2D plot of words with either highest or lowest ratings in valence (V), arousal (A) or dominance (D) in the corpus.\ntroduces only a small number of additional parameters.\n• For the ﬁrst time, we apply weighted cross-entropy loss in\nconversation modeling. Our affect-incorporated weights\nachieve a good balance between language ﬂuency and\nemotion quality in model responses. Our empirical study\ndoes not show performance degradation in language ﬂuency while producing affect-rich words.\n• Overall, we propose Affect-Rich Seq2Seq (AR-S2S), a\nnovel end-to-end affect-rich open-domain neural conversational model incorporating external affect knowledge.\nHuman preference test shows that our model is preferred\nover the state-of-the-art baseline model in terms of both\ncontent quality and emotion quality by a large margin.",
        "related work": "Prior studies on affective conversational systems mainly focused on rule-based systems, which require an extensive\nhand-crafted rule base. For example, Ochs, Pelachaud, and\nSadek (2008) designed an empathetic virtual agent that\ncan express emotions based on cognitive appraisal theories (Hewstone and Stroebe 2001), which require numerous\nevent-handling rules to be implemented. Another example\nis the Affect Listeners (Skowron 2010), which are conversational systems aiming to detect and adapt to the affective states of users. However, their detection and adaptation\nmechanisms heavily rely on hand-crafted features such as\nletter capitalization, punctuation and emoticons.\nIn recent years, there is an emerging research trend in endto-end neural network based generative conversational systems (Vinyals and Le 2015; Shang, Lu, and Li 2015). To\nimprove the content quality of neural conversational models, many techniques have been proposed, such as improving response diversity using Conditional Variational Autoencoders (CVAE) (Zhao, Zhao, and Eskenazi 2017) and encoding commonsense knowledge using external facts corpus\n(Ghazvininejad et al. 2018).\nHowever, few work investigated the problems in improving the emotion quality of neural conversational models.\nEmotional Chatting Machine (ECM) (Zhou et al. 2018) is a\nSeq2Seq conversational model that generates responses with\nuser-input emotions. It employs an internal memory module\nto model implicit emotional changes and an external memory module to help generate more explicit emotional words.\nThe main objective of ECM is to produce responses according to explicit user-input emotions. While our model focuses\non enriching affect in generated responses. Similar to ECM,\nMojitalk (Zhou and Wang 2018) presents a few generative\nmodels, including Seq2Seq, CVAE and Reinforced CVAE,\nto generate responses according to explicit user-input emojis. Both ECM and Mojitalk do not consider emotions in input sentences when generating emotional responses. In comparison, our model considers them naturally with focuses\non affect-rich words and avoids an additional step of determining which emotion to respond with during conversations.\nAsghar et al. (2018) introduces a Seq2Seq model with three\nextensions to incorporate affects into conversations. Similar to their work, we also adopt the approach of using VAD\nembedding to encode affects. However, we perform extra\npreprocessing on VAD embedding to improve model performance. In addition, we speciﬁcally consider the effect\nof negators and intensiﬁers via a novel affective attention\nmechanism when generating affect-rich responses.",
        "seq2seq with attention": "Prior to introducing our proposed model, we brieﬂy describe\nthe vanilla Seq2Seq model with attention. Seq2Seq model\nis a neural network model mapping the input sequence to\nthe output sequence. Speciﬁcally, it uses a Recurrent Neural Network (RNN) encoder to encode the variable length\ninput sequence X = (x1, x2, ..., xT ) as a vector of ﬁxed dimensionality hT and an RNN decoder to decode hT as the\nvariable length output sequence Y = (y1, y2, ..., yT ′ ). The\nobjective function of Seq2Seq is to maximize\np(Y |X) = p(y1|hT )\nT\n′\nY\nt′=2\np(yt′ |hT , y1, ..., yt′−1),\nht = f(ht−1, xt), ∀t = 1, 2, ..., T,\n(1)\nwhere ht denotes the hidden state of input sequence at time\nstep t and h0 is usually initialized as a zero vector. Function f denotes a non-linear transformation, which usually\ntakes the form of recurrent models such as Long Short-Term\nMemory (LSTM) (Hochreiter and Schmidhuber 1997) or\nGated Recurrent Units (GRU) (Cho et al. 2014).\nAfter encoding X as hT, the decoder updates its decoder\nhidden state st′ by taking the previous hidden state st′−1\nand previous output yt′−1 as inputs:\nst′ = g(st′−1, yt′−1), ∀t\n′ = 1, 2, ..., T\n′,\n(2)\nwhere g is another recurrent model, s0 = hT, and y0 is the\nstart of sequence (SOS) token.\nh1\ns1\ns2\ns3\ns4\ns5\ns6\nh2\nh3\nh7\nh6\nh5\nh4\ni\nso\nbad\ntoday\n.\nEOS\nfeel\nMessage Energy\nAffect Bias\nyou\nbe\nfine\n.\nEOS\nwill\nSOS, h7\nGlobal\nUniform\nLocal\nAttention\nWeighted Cross-Entropy Loss\neverything\nbe\nok\n.\nEOS\nwill\ntarget\nprediction\nsource\nSoftmax Layer\nLSTM Decoder\nAffective Attention\nLSTM Encoder\nAffective Embedding\nAffective Objective Function\nVAD Embedding\nWord \nEmbedding\nVAD \nEmbedding\nAffective \nEmbedding\nTerm \nFrequency\nFigure 2: Overall architecture of our proposed AR-S2S. This diagram illustrates decoding “ﬁne” and affect bias for “bad”.\nThe output word probability in equation (1) is given by\np(yt′ ) = softmax(Wost′ ), ∀t\n′ = 1, 2, ..., T\n′,\n(3)\nwhere Wo denotes a model parameter.\nThe attention mechanism (Luong, Pham, and Manning\n2015) is proposed to solve the problem of limited representation power of the ﬁnal input hidden state hT on which the\nentire decoding process is conditioned. Speciﬁcally, the attention mechanism focuses on different parts of the input\nsequence by computing a context vector ct′ at each decoding time step t\n′, ∀t\n′ = 1, 2, ..., T\n′, as the weighted average\nof all input hidden states ht, ∀t = 1, 2, ..., T, as follows:\nct′ =\nT\nX\nt=1\nαt′tht,\n(4)\nwhere the alignment vector αt′t is given by\nαt′t =\nexp(et′t)\nPT\nk=1 exp(et′k)\n,\n(5)\nwhere et′t = score(ht, st′ ) is the message energy function\nthat computes the energy or score between input hidden state\nht and output hidden state st′ . This message energy function\nis usually implemented as a Multilayer Perceptron (MLP).\nIn our case, we use a simple dot product operation due to\nits fast training and good performance (Luong, Pham, and\nManning 2015).\nThe context vector ct′ is then concatenated with the decoder hidden state st′ to form an attentional hidden state ˆst′\nas follows:\nˆst′ = tanh(Wc[ct′ ; st′ ]),\n(6)\nwhere [; ] denotes vector concatenation. Finally, ˆst′ replaces\nst′ in equation (3) to compute the output word probability.\nDimensions\nValues\nInterpretations\nValence\n3 - 7\npleasant - unpleasant\nArousal\n3 - 7\nlow intensity - high intensity\nDominance\n3 - 7\nsubmissive - dominant\nTable 1: Interpretations of clipped VAD embeddings.",
        "affect-rich seq2seq model": "In this section, we present our proposed model to produce\naffect-rich responses, which falls outside the capability of\nvanilla Seq2Seq models. The overall model architecture is\nillustrated in Figure 2.\nAffective Embedding\nOur model adopts Valence, Arousal and Dominance (VAD)\n(Mehrabian 1996) embedding to encode word affects as vectors of size 3 from an annotated lemma-VAD pairs corpus (Warriner, Kuperman, and Brysbaert 2013). This corpus\ncomprises 13,915 lemmas with VAD values annotated in the\n[1, 9] scale. To leverage this corpus, we assign VAD values to\nwords based on their lemmas. To increase coverage, we extend the corpus to 23,825 lemmas by assigning the average\nVAD values of their synonyms to absent lemmas. Furthermore, we empirically clip VAD values of all words to the\n[3, 7] interval to prevent words with extreme VAD values\nfrom repeatedly showing in the generated responses, as observed in our preliminary experiments. The interpretations\nof clipped VAD embedding are presented in Table 1. For example, word “nice” is associated with the clipped VAD values: (V: 6.95, A: 3.53, D: 6.47). For words whose lemmas\nare not in the extended corpus, comprising approximately\n10% of the entire training vocabulary, we assign them VAD\nvalues of [5, 3, 5], which are the clipped values of a neutral\nword. Note that a value of 3 in arousal (A) dimension is regarded as neutral because it has zero emotional intensity.\nFinally, to remove bias, we normalize VAD embedding as\nVAD(xt) = VAD(xt)−[5, 3, 5], where VAD(xt) ∈ R3 is the\nVAD embedding of word xt. We incorporate VAD embedding by concatenation as follows:\ne(xt) = [xt; λVAD(xt)],\n(7)\nwhere xt ∈ Rm denotes the word embedding of xt, e(xt) ∈\nRm+3 denotes the ﬁnal affective embedding of xt, m denotes the dimensionality of word vectors, and λ ∈ R+ denotes the affect embedding strength hyper-parameter to tune\nthe strength of VAD embeddings.\nIt is worth noting that the lemmas in our corpus were selected across multiple domains and are quite neutral (Brysbaert and New 2009). In addition, languages other than English, such as Spanish, Dutch, Finish, etc., also have such\nlemma-VAD pairs corpus, although in smaller sizes. Hence,\nour proposed conversational model has great potential to be\ndirectly applied to other languages.\nAffective Attention\nTo incorporate affect into attention naturally, we make\nthe intuitive assumption that humans pay extra attention\non affect-rich words during conversations. Speciﬁcally, our\nmodel biases attention towards affect-rich words in the input sentences, as well as considers the effect of negators and\nintensiﬁers. Our model employs an affect bias η augmenting\nthe affective strength of each word in the input sentences\ninto the energy function (see equation (5)) as follows:\net′t = ht\nT st′ + ηt,\n(8)\nwhere ht\nT st′ denotes the conventional dot product energy\nfunction and ηt is deﬁned as\nηt = γ||µ(xt)(1 + β) ⊗ VAD(xt)||2\n2,\nβ = tanh(Wbxt−1),\n(9)\nwhere ⊗ denotes element-wise multiplication, ||.||k denotes\nlk norm, Wb ∈ R3×m denotes a model parameter, β ∈ R3\nis a scaling factor in V, A and D dimensions in the [−1, 1] interval to scale the normalized VAD values of the current input word, γ ∈ R+ denotes the affective attention coefﬁcient\ncontrolling the magnitude of affect bias towards affect-rich\nwords in the input sentence, and µ(xt) ∈ R in the [0, 1] interval denotes a measure of term importance of xt (see the\nfollowing paragraph).\nTerm Importance\nThe introduction of term importance\nµ(xt) as weights in computing affective attention is inspired\nby the sentence embedding work (Arora, Liang, and Ma\n2016), where a simple weighted sum of word embedding\nalgorithm with weights being smoothed inverse term frequency can achieve good performance in textual similarity\ntasks. Term frequency has been widely adopted in information retrieval to compute the importance of a word. In our\nmodel, we propose three approaches, namely “uniform importance” (ui), “global importance” (gi), and “local impor−3\n−2\n−1\n0\n1\n2\n3\n4\n−2\n−1\n0\n1\n2\n3\n.\nnever\n.\nno\n.\nnot\n.\nvery\n.\nquite\n.\nrather\n.\nextremely\n.\nremarkably\n.\nexceptionally\n.\nextraordinarily\nFigure 3: 2D plot of the most frequent 30,000 words in our\ntraining dataset in GloVe embedding after PCA. Selected\ncommon negators and intensiﬁers are annotated in text.\ntance” (li) to compute µ(xt):\nµ(xt) =\n\n\n\n\n\n1\nui\na/(a + p(xt))\ngi\nlog(1/(p(xt)+ϵ))\nPt=T\nt=1 log(1/(p(xt)+ϵ))\nli\n,\n(10)\nwhere p(xt) denotes the term frequency of xt in the training\ncorpus, a denotes a smoothing constant that is usually set\nto 10−3 as suggested by Arora, Liang, and Ma (2016), and\nϵ is another small smoothing constant with value 10−8. We\ntake the log function in µli(xt) to prevent rare words from\ndominating the importance.\nModeling Negators and Intensiﬁers\nThe introduction of\nβ in equation (9) is to model the affect changes caused\nby negators and intensiﬁers. Often, negators make positive\nwords negative but with much lower intensity, and make\nnegative words less negative (Kiritchenko and Mohammad\n2016). Thus, β is expected to be negative for negators because negators tend to reduce the affect intensity of the following word (e.g., “not bad”). Intensiﬁers usually adjust the\nintensities of positive words and negative words but do not\nﬂip their polarities (Carrillo-de Albornoz and Plaza 2013).\nAs a result, β for extreme intensiﬁers (e.g., “extremely”)\nis expected to be larger than β for less extreme intensiﬁers (e.g., “very”). To speciﬁcally consider these phenomena, β is modeled to be a nonlinear transformation through\nthe word vector of xt−1. This idea is inspired by the observation that common negators and intensiﬁers share some\ncommon underlying properties in their word vector representations. Figure 3 shows that several common negators\nand intensiﬁers tend to cluster together in 2D plots in GloVe\nembedding (Pennington, Socher, and Manning 2014) after\napplying Principle Component Analysis (PCA).\nNote that our affective attention only considers unigram\nnegators and intensiﬁers, however, they are empirically\nfound as the majority of all negators and intensiﬁers. Statistics based on our training set indicate that the unigram intensiﬁer “very” occurs 364,913 times, in comparison, the composite intensiﬁer “not very” only occurs 2,838 times.\nAffective Objective Function\nThe conventional objective function of seq2seq model is to\nmaximize the probability of target response Y given input\nsequence X measured by cross-entropy loss. To encourage\nthe generation of affect-rich words, we incorporate VAD embedding of words into cross-entropy loss as follows:\nΨt′ = −|V |\n1 + δ||VAD(yt′ )||2\nP\nˆyt′ ∈V (1 + δ||VAD(ˆyt′ )||2) log(p(yt′ )),\n(11)\nwhere t\n′ = 1, 2, ..., T\n′, Ψt′ denotes the affective loss at decoding time step t\n′, yt′ denotes the target token at decoding\ntime step t\n′, V denotes the dataset vocabulary, and δ denotes\na hyper-parameter named affective loss coefﬁcient, which\nregulates the contribution of VAD embedding.\nOur proposed affective loss is essentially a weighted\ncross-entropy loss. The weights are constant and positively\ncorrelated with VAD strengths in l2 norm. The weight normalization is applied to ensure that our weights do not alter\nthe overall learning rate during optimization. Intuitively, our\naffective loss encourages affect-rich words to obtain higher\noutput probability, which effectively introduces a probability bias into the decoder language model towards affect-rich\nwords. This bias is controlled by our affective loss coefﬁcient δ. When δ = 0, our affective loss falls back to the\nconventional unweighted cross-entropy loss.\nIt is worth noting that our weighted cross-entropy loss incorporating external word knowledge, i.e., VAD in our case,\nis simple but effective in controlling the response style. Our\nloss function has many other potential application areas such\nas controlled neural text generation.",
        "experimental evaluation": "In this section, we present our datasets, evaluation methods,\nexperimental results and discussions. Following the experimental setup presented in (Zhou et al. 2018), we conduct\nmodel component test (MCT) to examine the effectiveness\nof our proposed affective attention and affective objective\nfunction in generating affect-rich responses. In addition, we\nconduct preference test (PT) between our best model (ARS2S) and the state-of-the-art baseline of comparable model\nsize to compare model responses. Finally, we conduct sensitivity analysis on the hyper-parameters introduced in our\nmodel to analyze their impacts on language ﬂuency and the\nnumber of distinct affect-rich words produced.\nDatasets\nWe use OpenSubtitles dataset (Tiedemann 2009) as our\ntraining dataset due to its large size. We use relatively\nless noisy Cornell Movie Dialog Corpus dataset (DanescuNiculescu-Mizil and Lee 2011) as our validation dataset for\nmore reliable tuning. We use DailyDialog dataset (Li et al.\n2017) for testing to examine model generalizations in different corpus domains.\nThe pairs in the training dataset are selected by a simple\nrule that the input sentence ends with a question mark and\nthe time interval between the pair of input and output sentences is less than 20 seconds. In addition, sound sequences\nsuch as “BANG” are removed. These pairs are then expanded\n(e.g., isn’t → is not), tokenized, and special symbols and\nnumbers were removed. Finally, the pairs with either input\nor output sentence longer than 20 words are removed. The\nvalidation and testing datasets are preprocessed by word expansion, tokenization and removal of special symbols and\nnumbers. Since we are modeling single-turn dialogue system, only the ﬁrst two utterances from each dialogue session\nin the testing dataset are extracted because using utterances\nin the middle would require context to respond.\nAfter data preprocessing, we randomly select 5 million\npairs from OpenSubtitles dataset as the training dataset with\na vocabulary comprising the most 30,000 frequent words,\ncovering 98.89% of all tokens. We randomly sample 100K\npairs from Cornell Movie Dialog Corpus dataset for validation and 10K pairs from DailyDialog dataset for testing.\nEvaluation Methods\nWe adopt perplexity metric to measure the language ﬂuency of a conversational model, as it is the only wellestablished automatic evaluation method in conversation\nmodeling. Other metrics such as BLEU (Papineni et al.\n2002) do not correlate well with human judgments (Liu et\nal. 2016). A model with lower perplexity indicates that it is\nmore conﬁdent about the generated responses. Note that a\nmodel with low perplexity does not guarantee to be a good\nconversational model because it may achieve so by always\ngenerating short responses.\nTo qualitatively examine model performance, we conduct\nwidely adopted human evaluations. We randomly sample\n100 input sentences from the testing dataset. For each input\nsentence, we then randomize the order of the responses generated by each comparison model. For each response, ﬁve\nhuman annotators are asked to evaluate two aspects:\n• +2: (content) The response has correct grammar and is\nrelevant and natural / (emotion) The response has adequate and appropriate emotions conveyed.\n• +1: (content) The response has correct grammar but is too\nuniversal / (emotion) The response has inadequate but appropriate emotions conveyed.\n• 0: (content) The response has either grammar errors or is\ncompletely irrelevant / (emotion) The response has either\nno or inappropriate emotions conveyed.\nExperiment 1: Model Component Test (MCT)\nWe compare the following models to examine the performance of our proposed affective attention and affective objective function on model perplexity and human evaluations:\nS2S: The standard Seq2Seq model with attention.\nS2S-UI, S2S-GI, S2S-LI: The standard Seq2Seq model\nwith our proposed affective attention using µui, µgi and µli\n(see equation (10)), respectively.\nS2S-AO: The standard Seq2Seq model with attention\nand our proposed affective objective function (see equation\n(11)).\nAR-S2S: our best model, which incorporates both µli and\naffective objective function.\nAll models have a word embedding of size 1027 (1024 +\n3) and hidden size of 1024. Both encoder and decoder have\ntwo layers of bi-directional LSTM. All models implement\nExperiment\nModel\n#Params\nPPL†\nPPL‡\nMCT (5M\npairs)\nS2S\n99M\n42.5\n124.3\nS2S-UI\n99M\n40.4\n116.4\nS2S-GI\n99M\n40.7\n120.3\nS2S-LI\n99M\n40.4\n117.0\nS2S-AO\n99M\n40.2\n115.7\nAR-S2S\n99M\n39.8\n113.7\nPT (3M\npairs)\nS2S\n66M\n41.2\n130.6\nS2S-Asghar\n66M\n46.4\n137.2\nAR-S2S\n66M\n40.3\n121.0\nTable 2: Model test perplexity. Symbol † indicates indomain perplexity obtained on 10K test pairs from the OpenSubtitles dataset. Symbol ‡ indicates out-domain perplexity\nobtained on 10K test pairs from the DailyDialog dataset.\naffective embedding. Parameters λ, δ and a are set to 0.1,\n0.15 and 10−3, respectively. Parameter γ for S2S-UI, S2SGI and S2S-LI are set to 0.5, 1 and 5, respectively. The beam\nsize is set to 20. Note that all models implement the maximum mutual information (MMI) objective function (Li et\nal. 2016) during inference to levitate the problem of generic\nresponses (e.g., “I don’t know”). For all models, a simple rerank operation is applied during inference to rank the generated responses ˆY based on their affective strength computed\nas\n1\n| ˆY |\nP\ny∈ ˆY ||VAD(y)||2. All models are initialized with a\nuniform distribution in the [−0.08, 0.08] interval, using the\nsame seed. We trained all models with a batch size of 64 for\n5 epochs using Adam (Kingma and Ba 2014) optimization\n(β1 = 0.9 and β2 = 0.999) with the learning rate of 0.0001\nthroughout the training process.\nResults\nTable 2 presents the results on model test perplexity in both MCT and PT (see Experiment 2). To analyze\nmodel generalization in different domains, we additionally\nreport test perplexity on in-domain test dataset, which is created using 10K test pairs from the OpenSubtitles dataset. All\nmodels have comparable perplexity on both in-domain and\nout-domain test datasets, empirically showing that our proposed methods do not cause performance degradation in language ﬂuency. One note is that the out-domain test perplexity for all models is quite large as compared to in-domain\nperplexity, as well as other dialog systems, e.g., (Vinyals\nand Le 2015). One possible reason is that our testing dataset\nis different from the training dataset in terms of both vocabulary and linguistic distributions (the former was created from daily conversations, whereas the latter was created from movie subtitles). As a result, the models may not\ngeneralize well.\nTables 3 and 4 present the evaluation results in MCT by\nﬁve human annotators on the content quality and emotion\nquality, respectively. The values in brackets denote performance improvement in percentage. The Fleiss’ kappa (Fleiss\nand Cohen 1973) for measuring inter-rater agreement is included as well. All models have “moderate agreement” or\n“substantial agreement”. For content quality, all models except S2S-AO have noticeably more +2 ratings than S2S. For\nemotion quality, it is clear that all of our proposed affective models have signiﬁcant improvement over S2S. Among\nModel (%)\n+2\n+1\n0\nScore\nKappa\nS2S\n22.4\n47.0\n30.6\n0.918\n0.544\nS2S-UI\n30.0\n48.6\n21.4\n1.086 (+18.3%)\n0.458\nS2S-GI\n28.6\n46.6\n24.8\n1.038 (+13.1%)\n0.413\nS2S-LI\n29.4\n47.2\n23.4\n1.060 (+15.5%)\n0.525\nS2S-AO\n25.0\n46.0\n29.0\n0.960 (+4.3%)\n0.482\nAR-S2S\n29.6\n44.8\n25.6\n1.040 (+13.3%)\n0.487\nTable 3: Human evaluations on content quality (MCT).\nModel (%)\n+2\n+1\n0\nScore\nKappa\nS2S\n19.0\n33.2\n47.8\n0.712\n0.613\nS2S-UI\n23.6\n36.0\n40.4\n0.832 (+16.9%)\n0.483\nS2S-GI\n26.0\n34.2\n39.8\n0.862 (+21.1%)\n0.652\nS2S-LI\n24.6\n36.4\n39.0\n0.856 (+20.2%)\n0.706\nS2S-AO\n22.6\n37.6\n39.8\n0.828 (+16.3%)\n0.602\nAR-S2S\n26.8\n37.2\n36.0\n0.908 (+27.5%)\n0.625\nTable 4: Human evaluations on emotion quality (MCT).\nMessage\nModel\nResponse\nMommy, can i\nstay up until\neleven of the\nclock?\nS2S (MCT)\nOf course you can\nstay up late.\nAR-S2S\n(MCT)\nOf course you can,\nsweetheart.\nYou\nare\nhome\nlate today, david.\nHow was school?\nS2S (MCT)\nIt was ﬁne.\nAR-S2S\n(MCT)\nGreat fun today.\nDo\nyou\nlike\nsinging?\nS2S (PT)\nYes, i do.\nS2S-Asghar\n(PT)\nI do not know.\nAR-S2S (PT)\nI love music.\nI’m pretty sure\nthat jim will\nturn out to be a\ngood lawyer.\nS2S (PT)\nHe will turn out to be\na good lawyer.\nS2S-Asghar\n(PT)\nI’m sure he will.\nAR-S2S (PT)\nThe best lawyer in the\nworld.\nTable 5: Sample responses for models in both MCT and PT.\nText in bold are affect-rich words.\nthe three affective attention mechanisms, S2S-LI achieves\nthe best overall performance. Note that the improvement\ngained by affective attention and affective objective function are partially orthogonal. One explanation is that by actively paying attention to affect-rich words in the input sentence, our model is able to produce more accurate affectrich words during decoding. Therefore, combing both techniques (AR-S2S) results in maximum improvement in emotion quality. Table 5 presents some sample responses in the\ntesting dataset.\nAnalysis of Affective Attention\nTo examine our hypothesis that our affective attention mechanism can correctly\ncapture the effect of negators and intensiﬁers, we plot the\nlearned parameter β (see equation (9)) in the Valence and\nArousal dimensions in Figure 4. It is obvious that our model\nsuccessfully learned to make β negative for negators. In addition, several extreme intensiﬁers such as “exceptionally”\nand “remarkably” have higher β than less extreme intensiﬁers such as “very” and “quite”, which is consistent with\nour hypothesis. One note is that our model does not learn\n−0.3\n−0.2\n−0.1\n0.0\n0.1\n0.2\n0.3\nValence (V)\n−0.10\n−0.05\n0.00\n0.05\n0.10\n0.15\nArousal (A)\nexceptionally\nextraordinarily\nremarkably\nextremely\nrather\nvery\nquite\nno\nnot\nnever\nBefore Training\n−1.0\n−0.8\n−0.6\n−0.4\n−0.2\n0.0\nValence (V)\n−1.0\n−0.8\n−0.6\n−0.4\n−0.2\n0.0\n0.2\nArousal (A)\nexceptionally\nextraordinarily\nremarkably\nextremely\nrather\nveryquite\nno\nnotnever\nAfter Training\nFigure 4: Learned parameter β (see equation (9)) in Valence (V) and Arousal (A) dimensions for several common\nnegators and intensiﬁers. Left sub-ﬁgure: before AR-S2S is\ntrained. Right sub-ﬁgure: after AR-S2S is trained.\ni\nwent\nto\njessie\ns\nbirthday partyyesterday\n.\nit\nwas\nvery\ngood\n.\ni\nwent\nto\njessie\ns\nbirthday partyyesterday\n.\nit\nwas\nvery\ngood\n.\ni\nwent\nto\njessie\ns\nbirthday partyyesterday\n.\nit\nwas\nvery\ngood\n.\ni\nwent\nto\njessie\ns\nbirthday partyyesterday\n.\nit\nwas\nvery\ngood\n.\nFigure 5: Learned attention on a sample input sentence from\nthe testing dataset. From top to bottom, the models are S2S,\nS2S-UI, S2S-GI and S2S-LI, respectively. Darker colors indicate larger strength.\nwell for some intensiﬁers such as “extremely”, whose β is\ncomparable to less extreme intensiﬁers such as “very”. This\nresult is not surprising because the impacts of intensiﬁers are\ndifﬁcult to be completely captured as they tend to vary depending on the following words (Kiritchenko and Mohammad 2016).\nFigure 5 shows the attention strength over a sample input sentence in the testing dataset. As expected, our proposed affective attention models place extra attention on\naffect-rich words, i.e., “good” in this case. In addition, S2SUI and S2S-LI have larger strengths than S2S-GI. This result is aligned with our model’s assumption because different “term importance” have different impacts on the attention strengths and the word “good” here is quite common\n(p(“good”) = 0.00143), which leads to the lower strength\nin S2S-GI.\nAnalysis of Affective Objective Function\nWe analyze the\ncapability of our proposed affective objective function in\nproducing affect-rich words. Table 6 presents the number\nof distinct affect-rich words in randomly selected 1K test\nresponses produced by different models. Affect-rich words\nare deﬁned as words with VAD strength in l2 norm exceeding the given threshold. It is clear that all S2S-AO models can produce more affect-rich words than S2S. In addition, the number of affect-rich words for every threshold increases steadily as the affective objective coefﬁcient δ increases, showing a good controllability of our model via δ.\nExperiment 2: Preference Test (PT)\nWe conduct human preference test to compare our ARS2S with the state-of-the-art baseline S2S-Asghar, the best\nThreshold for l2 Norm of VAD\nModel\n3\n2\n1\nS2S\n25\n104\n190\nS2S-AO (δ\n=\n0.5)\n36\n138\n219\nS2S-AO (δ = 1)\n50\n154\n234\nS2S-AO (δ = 2)\n69\n177\n256\nTable 6: Number of distinct affect-rich words (MCT).\nThreshold for l2 Norm of VAD\nModel\n3\n2\n1\nS2S\n21\n83\n157\nS2S-Asghar\n31\n120\n217\nAR-S2S\n52\n173\n319\nTable 7: Number of distinct affect-rich words (PT).\nmodel proposed in (Asghar et al. 2018). To the best of our\nknowledge, S2S-Asghar is the only model in the neural conversational model literature that aims to produce affect-rich\nresponses in an end-to-end manner (i.e., without explicit\nuser-input emotions). We also include S2S for comparison.\nTo make comparisons fair, we follow the speciﬁcations of\nS2S-Asghar and keep the number of parameters in all models comparable by reducing the size of our model. We use\na smaller training dataset with 3 million random pairs and\na vocabulary of size 20,000 due to the reduced model size.\nNote that our training dataset is still signiﬁcantly larger than\nthe original dataset used in (Asghar et al. 2018), which comprises only 300K pairs and a vocabulary size of 12,000. All\nmodels have a word embedding of size 1027, a single-layer\nLSTM encoder and a single-layer LSTM decoder. All training speciﬁcations remain the same as the MCT except that\nS2S-Asghar is trained for 4 epochs with conventional crossentropy loss and 1 more epoch with their proposed objective\nfunction, which includes a term to maximize affective content.\nFor human evaluation, we follow the same procedures as\nadopted in MCT except that ﬁve human annotators were\nasked to choose their preferred responses based on content\nquality and emotion quality, respectively, instead of annotating +2, +1 and 0. Ties are allowed.\nResults\nTable 7 shows the number distinct of affect-rich\nwords in randomly selected 1K responses produced by S2S,\nS2S-Asghar and our model. It is clear that our model produces signiﬁcantly more affect-rich words than both S2SAsghar and S2S.\nTable 8 shows the result of human evaluation. The Fleiss’\nkappa scores for content/emotion qualities are included in\nthe last column. All models have “moderate agreement” or\n“substantial agreement”. For content preference, our model\nscores relatively 21% higher than S2S-Asghar. For emotion preference, our model scores relatively 50% higher than\nS2S-Asghar. These ﬁndings show that our model is capable of producing better responses that are not only more appropriate in syntax and content, but also signiﬁcantly more\naffect-rich than the state-of-the-art model.\nModel (%)\nContent\nEmotion\nKappa\nS2S\n64\n26\n0.522/0.749\nS2S-Asghar\n66 (+3.1%)\n32 (+23.1%)\n0.554/0.612\nAR-S2S\n80 (+25.0%)\n49 (+88.5%)\n0.619/0.704\nTable 8: Human preference test (PT).\n0.5\n1.0\naﬀect embedding strength λ\n160\n180\n200\nperplexity\n0\n20\n40\naﬀective attention coeﬃcient γ\n160\n180\n200\nperplexity\n0\n1\n2\naﬀective objective coeﬃcient δ\n160\n180\n200\nperplexity\nFigure 6: Sensitivity analysis for affect embedding strength\nλ, affective attention coefﬁcient γ, and affective objective\ncoefﬁcient δ on model perplexity. The blue, red and green\ncurves (best viewed in color) in the middle sub-ﬁgure represent µui, µgi and µli (see equation (10)), respectively.\nExperiment 3: Sensitivity Analysis\nWe examine the impacts of the affect embedding strength\nλ, affective attention hyper-parameter γ, as well as affective\nloss hyper-parameter δ on model perplexity and the number of affect-rich words produced. Due to the large number\nof experiments, we conduct the sensitivity analysis using 1\nmillion pairs and a vocabulary of size 20,000. All training\nspeciﬁcations remain the same as MCT except that the number of LSTM layers is 1, the hidden layer size is 512 and the\nembedding layer size is 303.\nResults\nFigure 6 shows the plots of model test perplexity versus λ, γ and δ. Our model is fairly robust to a wide\nrange of λ, γ and δ, regardless of the type of term importance. It is worth noting that the generated responses tend to\nbecome shorter with γ ∈ [20, ∞], which may be caused by\nexcessive attention placed on affect-rich words during decoding. Another interesting ﬁnding is that our affective objective function slightly improves test perplexity. One possible explanation is that affect-rich words are less common\nthan generic words in our training corpus. As a result, our\nweighted cross-entropy loss placing extra weights on them\nimproves the overall prediction performance.\nFigure 7 shows the plots of the number of distinct affectrich words in randomly selected 1K test responses versus\nλ, γ and δ. The number of distinct words increases slightly\nwhen λ increases from 0 to 0.3, and then gradually decreases\nand stabilizes as λ increases from 0.3 to 1. For γ in all three\nterm importance, there is an initial boost in the number of\ndistinct words when γ is small, i.e., γ ∈ [0, 5]. However, as\nγ furher increases, the number of distinct words gradually\ndecreases, which may be caused by limited word space during decoding due to excessive attention on affect-rich words.\nAmong the three term importance proposed, local importance (µli) is slightly more robust against γ than the other\ntwo approaches. Finally, the number of distinct words consistently increases with δ, which is similar to our ﬁndings\nfrom Table 6. Note that the numbers in this sensitivity analysis are much smaller than MCT, which can be attributed to\nsmaller models and less training examples.\n0.2\n0.4\n0.6\n0.8\n1.0\naﬀect embedding strength λ\n0\n50\n100\nNo. of aﬀect-rich words\n0\n10\n20\n30\n40\n50\naﬀective attention coeﬃcient γ\n0\n50\n100\nNo. of aﬀect-rich words\n0.0\n0.5\n1.0\n1.5\n2.0\naﬀective objective coeﬃcient δ\n0\n50\n100\nNo. of aﬀect-rich words\nFigure 7: Sensitivity analysis for affect embedding strength\nλ, affective attention coefﬁcient γ, and affective objective\ncoefﬁcient δ on the number of distinct affect-rich words in\nrandomly selected 1K test responses. The solid, dashed and\ndotted curves correspond to l2 norm threshold of 1, 2 and\n3, respectively. The blue, red and green curves (best viewed\nin color) in the middle sub-ﬁgure represent µui, µgi and µli\n(see equation (10)), respectively.",
        "conclusion": "In this paper, we propose an end-to-end open-domain neural conversational model that produces affect-rich responses\nwithout performance degradation in language ﬂuency. Our\nmodel leverages external word-VAD knowledge to encode\naffect information into the conversational model. In addition, our model captures user emotions by paying extra attention to affect-rich words in input sentences and considering the effect caused by negators and intensiﬁers. Lastly,\nour model is trained with an affect-incorporated weighted\ncross-entropy loss to encourage the generation of affect-rich\nwords. Empirical studies on both model perplexity and human evaluations show that our model outperforms the stateof-the-art model of comparable size in producing natural and\naffect-rich responses.",
        "summary_en": "Affect conveys important implicit information in human communication. Having the capability to correctly express affect during human-machine conversations is one of the major milestones in artificial intelligence. In recent years, extensive research on open-domain neural conversational models has been conducted. However, embedding affect into such models is still under explored. Therefore, this paper proposes an endto-end affect-rich open-domain neural conversational model that produces responses not only appropriate in syntax and semantics, but also with rich affect. the paper's model extends the Seq2Seq model and adopts VAD (Valence, Arousal and Dominance) affective notations to embed each word with affects. In addition, the model considers the effect of negators and intensifiers via a novel affective attention mechanism, which biases attention towards affect-rich words in input sentences. Lastly, the paper trains the model with an affect-incorporated objective function to encourage the generation of affect-rich words in the output responses. Evaluations based on both perplexity and human evaluations show that the model outperforms the state-of-the-art baseline model of comparable size in producing natural and affect-rich responses.",
        "summary_zh": "这篇论文介绍了一种端到端的情感丰富的神经会话模型，为了在人机对话中正确表达情感，从而提升人工智能水平。该模型不仅能产生语法和语义恰当的回应，还能产生情感丰富的回应。模型采用VAD情感符号将每个单词嵌入情感，并通过新颖的情感注意力机制考虑否定词和强调词的影响，可以将注意力偏向输入句子中富含情感的词。最后，模型采用融合情感的目标函数进行训练，促使生成富含情感的词语。实验结果显示，该模型在困惑度和人工评估方面优于同规模的最先进基准模型，能够生成自然且情感丰富的响应。"
    },
    {
        "title": "Keyword-Guided Neural Conversational Model",
        "abstract": "We study the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. Solving this problem enables the application of conversational agents in many real-world scenarios, e.g., recommendation and psychotherapy. The dominant paradigm for tackling this problem is to 1) train a next-turn keyword classiﬁer, and 2) train a keyword-augmented response retrieval model. However, existing approaches in this paradigm have two limitations: 1) the training and evaluation datasets for next-turn keyword classiﬁcation are directly extracted from conversations without human annotations, thus, they are noisy and have low correlation with human judgements, and 2) during keyword transition, the agents solely rely on the similarities between word embeddings to move closer to the target keyword, which may not reﬂect how humans converse. In this paper, we assume that human conversations are grounded on commonsense and propose a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs (CKG) for both keyword transition and response retrieval. Automatic evaluations suggest that commonsense improves the performance of both next-turn keyword prediction and keyword-augmented response retrieval. In addition, both self-play and human evaluations show that our model produces responses with smoother keyword transition and reaches the target keyword faster than competitive baselines.",
        "introduction": "Building a human-like open-domain conversational agent\n(CA) has been one of the milestones in artiﬁcial intelligence (AI). Early conversational agents are primarily based\non rules (Weizenbaum 1966; Colby, Weber, and Hilf 1971),\ne.g., Eliza (Weizenbaum 1966), the ﬁrst CA developed\nin 60’s, simulates a Rogerian psychotherapist based on\nhand-crafted pattern matching rules. In recent years, with\nthe advancement of data-driven neural networks, neural\nopen-domain conversational models are becoming dominant\n(Vinyals and Le 2015; Lowe et al. 2015; Gao, Galley, and Li\n2018).\nHi, I like drinking dr. pepper and comics drawings and you?\nWow nice combination, I am eating pasta, italian is my favorite kind of food.\nI do love pasta but not egg noodles, since I eat only vegan.\nThat is a good one are you a vegan? I prefer meat hamburger is amazing.\nI really love jelly sandwiches, but hold the pb because i'm allergic.\nMe too, I enjoy ham and cheese sandwiches with orange juice.\nFigure 1: Illustration of keyword-guided conversations from\nself-play simulations. Keywords are highlighted in bold.\nGiven a random starting keyword “comics”, the agent\n(red) leads the conversation to the target keyword “juice”\nsmoothly and fast.\nRecent efforts in open-domain neural conversational\nmodels are primarily aiming to improve the response diversity (Li, Monroe, and Jurafsky 2016; Zhang et al. 2018b)\nand endowing responses with knowledge (Zhou et al. 2018b;\nDinan et al. 2019b), personality (Li et al. 2016a; Zhang\net al. 2018a), emotion (Zhou et al. 2018a; Zhong, Wang,\nand Miao 2019) and empathy (Rashkin et al. 2019; Zhong\net al. 2020). All the efforts mentioned above are focusing on\nmodels that passively respond to user messages. However, in\nmany real-world scenarios, e.g., conversational recommendation, psychotherapy and education, conversational agents\nare required to actively lead the conversation by smoothly\nchanging the conversation topic to a designated one. For example, during a casual conversation, the agent may actively\nlead the user to a speciﬁc product or service that the agent\nwants to introduce and recommend.\nIn this paper, we follow the line of research in (Tang et al.\n2019; Qin et al. 2020) and study the problem of imposing\nconversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. As illustrated\nin Figure 1, given a target keyword “juice” and a random\nstarting keyword “comics”, the agent is required to converse\nwith the user in multiple exchanges and lead the conversation to “juice”. The challenge of this problem lies in how\nto balance the tradeoff between maximizing keyword transition smoothness and minimizing the number of turns taken\nto reach the target. On the one hand, passively responding\nto the user solely based on the conversation context would\nHi, how was ur weekend?\nHey, it was good! How was yours?\nParty weekend, it was amazing\nWhat kind of party?\nKind of a get together with friends\nNice. I like to ride my bike if I've time on the weekend\nTraffic is major hassle here. I get mad\nYou should ride a bike instead of drive! Haha\nMy work place is a bit far\nWhere do you work? I sell insurance\nI work in a bank\nFigure 2: Illustration of keyword (in bold) transitions in a\nsample conversation from ConvAI2 (Zhang et al. 2018a).\nTransitions indicated by arrows are considered relevant. The\nrest keyword transitions, e.g., friends → ride, are irrelevant\n(but used in the training and evaluation datasets of existing\nstudies).\nachieve high smoothness but may take many turns to reach\nthe target, but on the other hand, directly jumping to the target word by ignoring the conversation context would minimize the number of turns but produce non-smooth keyword\ntransitions.\nTang et al. (2019) proposed to break down the problem into two sub-problems: next-turn keyword selection and\nkeyword-augmented response retrieval. Tang et al. (2019)\nproposed a next-turn keyword predictor and a rule-based\nkeyword selection strategy to solve the ﬁrst sub-problem,\nallowing the agent to know what is the next keyword to\ntalk about given the conversation history and the target keyword. In addition, Tang et al. (2019) proposed a keywordaugmented response retrieval model to solve the second subproblem, allowing the agent to produce a response that is\nrelevant to the selected keyword.\nHowever, there are two major limitations in existing studies (Tang et al. 2019; Qin et al. 2020). First, the training and\nevaluation datasets for next-turn keyword prediction are directly extracted from conversations without human annotations, thus, the majority of the ground-truth keyword transitions are noisy and have low correlations with human judgements. As illustrated in Figure 2, only a few keyword transitions in a conversation are considered relevant. In fact, in our\nhuman annotation studies of over 600 keyword transitions,\nwe found that around 70% of keyword transitions in the\nnext-turn keyword prediction datasets are rated as not relevant, which renders the trained next-turn keyword predictor\nin existing studies less reliable. Second, the rule-based keyword selection strategy primarily leverages the cosine similarity between word embeddings to select keywords that are\ncloser to the target keyword. Word embeddings are trained\nbased on the distributional hypothesis that words that have\nsimilar contexts have similar meanings, which may not reﬂect how humans relate words in conversational turn-taking.\nIn this paper, we assume that human conversations are\ngrounded on commonsense and propose a keyword-guided\nneural conversational model that can leverage external commonsense knowledge graphs (CKG) for both next-turn keyword selection and keyword-augmented response retrieval.\nHumans rely on commonsense to reason, and commonsense reasoning plays an important role in the cognitive process of conversational turn-taking (Schegloff 1991; Stocky,\nFaaborg, and Lieberman 2004; Lieberman et al. 2004). Relying on a CKG for keyword transition would allow the\nagent to select a more target-related keyword for the nextturn. Moreover, we leverage commonsense triplets from the\nCKG using Graph Neural Networks (GNN) for both nextturn keyword prediction and keyword-augmented response\nretrieval to achieve more accurate predictions.\nIn summary, our contributions are as follows:\n• We identify two limitations of existing studies in nextturn keyword selection: 1) noisy training and evaluation\ndatasets, and 2) unreliable keyword transition based on\nthe similarity between word embeddings.\n• For the ﬁrst time in this task, we propose to use CKG\nfor keyword transition and propose two GNN-based models to incorporate commonsense knowledge for next-turn\nkeyword prediction and keyword-augmented response retrieval, respectively.\n• We propose a large-scale open-domain conversation\ndataset for this task, obtained from Reddit. The linguistic\npatterns in Reddit are far more diverse than the ConvAI2\n(Zhang et al. 2018a) used in existing studies, which are\ncollected from only hundreds of crowd-workers.\n• We conduct extensive experiments and the results show\nthat grounding keyword transitions on CKG improves\noverall conversation smoothness and allows the agent\nto reach the target faster. In addition, leveraging commonsense triplets substantially improves the performance\nof both next-turn keyword prediction and keywordaugmented response retrieval. Finally, self-play and human evaluations show that our model produces smoother\nresponses and reaches the target keyword faster than competitive baselines.",
        "related work": "In recent years, several studies proposed to build conversational agents that can actively lead a conversation to a designated target keyword/goal (Tang et al. 2019; Wu et al.\n2019). Our work follows the task deﬁnition in (Tang et al.\n2019), which has been discussed in Introduction. Very recently, Qin et al. (2020) improved (Tang et al. 2019) in 1)\nnext-turn keyword prediction by only considering keyword\ntransitions that are present in the training dataset and 2)\nkeyword-augmented response retrieval by constraining that\nthe selected response must contain the predicted keyword or\na keyword closer to the target keyword. As a result, Qin et al.\n(2020) obtained the state-of-the-art performance on this task\nin terms of task success rate and transition smoothness.\nAnother line of research (Wu et al. 2019) focused on the\nspeciﬁc movie domain and proposed to use factoid knowledge graph to proactively lead the conversation from a random entity to a given entity. Our work differs from (Wu\net al. 2019) in that 1) we focus on open-domain conversations whereas they focus on movie domain; 2) we leverage commonsense knowledge graph for keyword transitions\nwhereas they leverage factoid knowledge graph for entity\ntransitions1; and 3) we allow the target to be any arbitrary\nkeyword whereas they constrain the target to be at most twohop away from the starting entity. Following the line of research in (Wu et al. 2019), Xu et al. (2020a) proposed to\nuse hierarchical reinforcement learning (HRL) to incorporate factoid knowledge graph for high-level topic selection\nand low-level in-depth topic-related conversation. Xu et al.\n(2020b) proposed a framework to represent prior information as a conversation graph (CG) and leverage policy learning to incorporate the CG into conversation generation.\nCommonsense has been studied extensively in recent neural conversational models (Young et al. 2018; Zhou et al.\n2018b; Zhang et al. 2020; Zhong et al. 2021). Zhou et al.\n(2018b) proposed graph attentions to statically incorporate\none-hop knowledge triplets into conversation understanding\nand dynamically generate knowledge-aware responses. Recently, Zhang et al. (2020) extended (Zhou et al. 2018b)\nto multi-hop knowledge triplets by proposing an attention\nmechanism to incorporate outer triplets and a GNN model\nto aggregate central triplets. Different from existing studies that leverage commonsense to improve the diversity and\ninformativeness of responses, we incorporate commonsense\ninto our approach for more reasonable keyword transition\nand more accurate response retrieval.",
        "our approach": "In this section, we ﬁrst introduce our task deﬁnition, and\nthen describe the CKG used in our paper, and ﬁnally\npropose the Commonsense-aware Keyword-guided neural\nConversational model (CKC).\nTask Deﬁnition\nGiven a conversation history of n utterances: x1:n\n=\nx1, ..., xn, we denote the sequence of keywords for xi as\nki, and the response to x1:n as y.\nBrieﬂy, given a target keyword t and a random initial utterance x1 with its keywords k1, the task of the agent is to\nchat with the user and lead the conversation to the target\nkeyword smoothly and fast. The target is only presented to\nthe agent and unknown to the user. We consider the target\nis achieved when an utterance (either by the user or by the\nagent) mentions the target keyword2.\nWe break down the task into two sub-problems: nextturn keyword selection and keyword-augmented response\nretrieval. We propose a CKG-aware next-turn keyword predictor and a CKG-guided keyword transition strategy to\nsolve the ﬁrst sub-problem. We then propose a CKG-aware\nkeyword-augmented response retrieval to solve the second\nsub-problem.\n1In our work, a keyword can be a named entity, e.g.,\nAAAI2021, or a generic content word, e.g., conference.\n2This is different from (Tang et al. 2019) where mentioning a\nsynonym of the target can be considered as success because we\nfound that synonyms are unreliable to measure the task success.\nkeywords & concepts\nHello, how are you today?\nHi, i'm justin. I'm on tour and just came in \nfrom a performance.\nThat's awesome, how do you like performing? \nI'm currently looking for a new job.\nIt is my life. I was born into it. Both of my \nparents are musicians.\nSounds like it was meant to be. My daughter \nloves to preform she's a prodigy really.\n...\nlife\nage\nparent\nliving PartOf\nRelatedTo\nCKG\ndaughter\nRelatedTo\nmusician\nplay_a_guitar\nCapableOf\n...\n...\nGated Graph Neural Network\nHierarchical GRU\nHierarchical Pooling\nConcat, Linear &  Softmax\nWord Embedding\nWord Embedding\nFigure 3: Illustration of our proposed CKG-aware next-turn\nkeyword prediction. We only use the most recent two utterances and their concepts and keywords as input. Words\nin bold denote keywords. Concepts are words or multi-word\nexpressions extracted from utterances based on the CKG vocabulary.\nCommonsense Knowledge Graph (CKG)\nIn this paper, we use ConceptNet (Speer, Chin, and Havasi\n2017) as our CKG. ConceptNet is a large-scale multilingual\nsemantic graph that describes general human knowledge in\nnatural language. Each node/concept on ConceptNet can be\na single word, e.g., “food” or a multi-word expression, e.g.,\n“having lunch”. The edges on ConceptNet represent the semantic relations between nodes and have weights suggesting\nthe conﬁdence score, e.g., ⟨having lunch, HasPrerequisite,\nfood⟩ with a weight of 2.83. The majority of edge weights\nare in [0, 10]. We only include triplets that satisfy the following requirements into our CKG: 1) the edge weight is at\nleast 1, 2) at least one node is in our keyword vocabulary3,\nand 3) the other node is in our word vocabulary4.\nCKG-Aware Next-Turn Keyword Prediction\nGiven a history of n utterances x1:n and n sequences of keywords k1:n, we propose a model that can predict the nextturn keywords kn+1. Note that kn+1 can include multiple\nkeywords, hence this is a multi-label classiﬁcation problem.\nOne major limitation of existing studies is that the training and evaluation datasets for next-turn keyword prediction\nare noisy, as discussed in Introduction. In this paper, we assume that human conversations are grounded on commonsense and leverage commonsense to 1) clean the training and\nevaluation datasets; and 2) propose a CKG-aware model for\nmore accurate next-turn keyword prediction.\nSpeciﬁcally, for each example in both training and evaluation datasets, we remove next-turn keywords that are not in\nthe immediate neighborhood of historical keywords. During\n3The keyword vocabulary is a subset of our word vocabulary\ncontaining frequent content words.\n4For a multi-word expression, we require that each single word\nto be in our word vocabulary.\nkeywords\ntop \nkeywords\ncandidates\nGreat. Very good to start young. I've been \nplaying guitar since I was three.\nconcepts\nHello, how are you today?\nHi, i'm justin. I'm on tour and just came in \nfrom a performance.\nThat's awesome, how do you like performing? \nI'm currently looking for a new job.\nIt is my life. I was born into it. Both of my \nparents are musicians.\nSounds like it was meant to be. My daughter \nloves to preform she's a prodigy really.\nlife\n...\nage\nparent\nliving\nPartOf\nRelatedTo\nCKG\ndaughter\nRelatedTo\nmusician\nplay_a_guitar\nCapableOf\n...\n...\nGated Graph Neural Network\nGRU\nWord Embedding\nWord Embedding\nGRU\nWord Embedding\nconcepts\nTrained Next-Turn \nKeyword Predictor\nContext Utterance Representation\nconcat\nconcepts\nCandidate Utterance Representation\nconcat\nconcepts\n...\nconcat\nUtterance Matching\nKeyword Matching\nkeywords\nscore\nFigure 4: Illustration of our proposed CKG-aware response retrieval model.\nmodel prediction in both training and evaluation, we also\nonly output keywords that are in the immediate neighborhood of input keywords. In other words, our model only outputs CKG-grounded keyword predictions.\nWe then propose a CKG-aware model that takes as input xn−1, xn, kn−1, kn and the CKG, and output kn+1. Note\nthat existing studies only use kn−1, kn and GRU (Cho et al.\n2014) to predict kn+1 (Tang et al. 2019; Qin et al. 2020).\nUsing longer context information does not improve performance in our experiments. An illustration of our model is\npresented in Figure 3.\nUtterance Representation\nWe obtain the utterance representation x ∈ Rd1 from the contextual utterances xn−1 and\nxn using a hierarchical GRU (HGRU) encoder, where d denotes the ﬁnal hidden state size of HGRU.\nCKG Graph Representation\nWe obtain a CKG graph\nrepresentation G ∈ RN×d2 using a Gated Graph Neural\nNetwork (GGNN) (Li et al. 2016b), where N denotes the\nnumber of nodes in the CKG and d2 denotes the hidden\nsize of GGNN. For each node on the CKG, the convolution operation in GGNN ﬁrst computes a parameterized\nweighted average of neighboring node representations and\nthen updates its own representation using a GRU. The nodes\nin CKG are represented via word embeddings. Multi-word\nnodes are represented via averaged word embeddings. The\nCKG representation is learned jointly with the next-turn keyword prediction and the gradients on the CKG are directly\nback-propagated to the word embeddings. Both utterances\nand CKG share the same word embedding layer, which can\neffectively reduce the number of model parameters and enable knowledge transfer on word embeddings.\nKeyword and Concept Representation\nWe extract the\nkeyword and concept representations K ∈ RNk×d2 and C ∈\nRNc×d2 from G, respectively, where Nk = |kn−1| + |kn|\nand Nc denote the number of concepts in xn−1 and xn.\nConcepts are extracted from utterances via string matching\nwith the CKG. We then apply hierarchical pooling where\nwe ﬁrst use mean pooling to aggregate K and C and obtain\nk ∈ Rd2 and c ∈ Rd2, respectively, and then apply max\npooling to combine k and c and obtain the ﬁnal representation kc ∈ Rd2. Essentially, kc represents the CKG-aware\nrepresentation learned from the utterances xn−1 and xn.\nClassiﬁcation\nFinally, we concatenate the utterance representation x ∈ Rd1 and the CKG-aware keyword and concept representation kc ∈ Rd2, and then feed it into a linear transformation layer, followed by a softmax layer. The\nentire model is optimized by minimizing the negative loglikelihoods of all ground-truth next-turn keywords.\nCKG-Guided Keyword Selection Strategy\nAfter obtaining a keyword distribution of the next utterance\nusing the proposed next-turn keyword predictor, we propose\na CKG-guided keyword selection strategy to select the most\nappropriate keyword for subsequent keyword-augmented response retrieval. Speciﬁcally, we select the keyword that is\ncloser to the target than current keywords and has the highest\nprobability. The distance between keywords is measured as\nthe weighted path length between keywords on the CKG,\ncomputed by the Floyd-Warshall algorithm (Floyd 1962).\nNote that the edge weights on ConceptNet correlate positively with concept relatedness. Hence, we apply a reciprocal operation to the weights before computing path lengths.\nEssentially, our proposed strategy allows the agent to chat\nsmoothly (by selecting the most likely next-turn keyword)\nwhile leading the conversation closer to the target keyword\n(by traversing to the target keyword via the most reasonable\npath on the CKG).\nKeyword-Augmented Response Retrieval\nThe last module in our approach is a keyword-augmented response retrieval model, as illustrated in Figure 4. At a highlevel, it is a response retrieval model that selects the best\ncandidate response given the context utterances and the predicted keywords.\nUtterance Representations\nThe context utterance representation X ∈ RNx×d is obtained by the concatenation of\ntwo representations: 1) the ﬂattened GRU encoded contextual representation and 2) the CKG-aware contextual concept representation, where Nx denotes the total number of\ntokens and concepts in the context and d denotes the hidden\nsize of GRU and GGNN. Similarly, the candidate utterance\nrepresentation Y ∈ RNy×d is obtained by: 1) the GRU encoded candidate representation and 2) the CKG-aware candidate concept representation, where Ny denotes the total\nnumber of tokens and concepts in the candidate.\nKeyword\nRepresentations\nBesides\nutterance-based\nmatching, we learn keyword-based matching to allow\nkeyword-augmented response retrieval. To this end, we\naim to select the candidate that best matches the predicted\nnext-turn keywords given contextual utterances. Speciﬁcally, we ﬁrst obtain the top predicted next-turn keywords\nusing a trained next-turn keyword predictor. We then\nobtain the CKG-aware predicted keyword representation\nKx\n∈\nRNkx×d and candidate keyword representation\nKy ∈ RNky ×d from GGNN, where Nkx and Nky denotes\nthe number of predicted keywords and candidate keywords,\nrespectively. In practice, following (Tang et al. 2019), we\nset Nkx = 3, allowing top-3 keywords to be matched with\ncandidate keywords.\nMatching\nWe compute the matching score su ∈ R between context utterance representation X ∈ RNx×d and candidate utterance\nrepresentation Y ∈ RNy×d as follows:\nsu = dot(max(X), max(Y))\n(1)\nwhere max denotes max pooling along the sequence dimension, and dot denotes dot product.\nSimilarly, the matching score sk ∈ R between predicted\nkeyword representation Kx ∈ RNkx×d and candidate keyword representation Ky ∈ RNky ×d is computed as follows:\nsk = dot(max(Kx), max(Ky))\n(2)\nThe ﬁnal matching score s ∈ R is obtained as follows:\ns = su + λksk\n(3)\nwhere λk denotes a hyper-parameter controlling the weight\nfor keyword scores. We optimize the entire response retrieval model by minimizing the negative log-likelihood of\nthe ground-truth response among all candidates.",
        "experimental settings": "In this section, we introduce the datasets, evaluation metrics,\nbaselines and model settings.\nDataset\nSplit\n#Conv.\n#Utter.\n#Key.\nAvg. #Key.\nConvAI2\nTrain\n8950\n132601\n2678\n1.78\nValid\n485\n7244\n2069\n1.79\nTest\n500\n7194\n1571\n1.50\nReddit\nTrain\n112693\n461810\n2931\n2.27\nValid\n6192\n25899\n2851\n2.25\nTest\n5999\n24108\n2846\n2.30\nTable 1: Dataset statistics. #Key. denotes the number of\nunique keywords and Avg. #Key. denotes the average number of keywords per utterance.\nDataset\nWe use the ConvAI2 dataset proposed in (Zhang et al.\n2018a; Dinan et al. 2019a) and preprocessed in (Tang et al.\n2019) in our experiments. Conversations in ConvAI2 are\nopen-domain and cover a broad range of topics. In addition,\nwe collect a large-scale open-domain conversation dataset\nfrom the social media Reddit5. The proposed Reddit dataset\nis collected from casual chats on the CasualConversation6\nand CasualUK7 subreddits, where users chat freely with\neach other in any topic. Reddit is signiﬁcantly larger and\nmore diverse than ConvAI2.\nFollowing (Tang et al. 2019), we use TF-IDF and partof-speech (POS) features to extract keywords from both\ndatasets. We use a maximum of 8 contextual utterances and\neach utterance is truncated to 30 tokens. The number of keywords for each utterance is capped at 10. We limit the vocabulary of both datasets to the most frequent 20K tokens.\nIn the task of next-turn keyword prediction, we remove\nkeyword transitions not covered by our CKG, as discussed\nin Our Approach. In addition, we remove self-loops, i.e.,\na keyword transit to itself, in both training and evaluation\nexamples to prevent the model from predicting keywords\nthat exist in the context, because predicting self-loops would\nnot lead the conversation to the target. After preprocessing, the average number of keyword candidates for ConvAI2 and Reddit are 158 and 201, respectively. The number\nof nodes/edges on CKG are 87K/221K and 97K/273K for\nConvAI2 and Reddit, respectively. The statistics of the two\ndatasets are presented in Table 1.\nEvaluation Metrics\nTurn-Level Evaluation\nFollowing (Tang et al. 2019; Qin\net al. 2020), we use R@k, the recall at position k (=1, 3,\n5) over all neighboring keywords, and P@1, the precision\nat the ﬁrst position, for next-turn keyword prediction. Note\nthat we have a smaller set of candidate keywords than that\nin (Tang et al. 2019) because we only keep neighboring keywords as candidates.\nWe use R@k, the recall at position k (=1, 3, 5) over all 20\ncandidate responses (a ground-truth response and 19 negative candidates), and MRR, the mean reciprocal rank, for\nkeyword-augmented response retrieval.\n5https://www.reddit.com/. We use the Pushshift dataset on\nGoogle BigQuery.\n6https://www.reddit.com/r/CasualConversation\n7https://www.reddit.com/r/CasualUK/\nConvAI2\nReddit\nModel\nR@1\nR@3\nR@5\nP@1\nR@1\nR@3\nR@5\nP@1\nRandom\n1.03±0.09\n2.99±0.12\n4.83±0.04\n1.18±0.12\n0.60±0.06\n1.88±0.24\n3.35±0.34\n0.69±0.04\nPMI\n16.96\n34.15\n46.39\n19.11\n6.90\n16.06\n22.98\n7.79\nNeural\n17.81±0.35\n34.59±0.42\n44.88±0.66\n19.91±0.57\n7.22±0.26\n16.81±0.20\n23.89±0.21\n8.12±0.35\nKernel\n16.23±0.50\n32.07±0.84\n42.62±0.76\n17.57±0.87\n7.38±0.17\n17.10±0.28\n24.81±0.70\n8.24±0.22\nDKRN\n18.03±0.15\n34.60±0.56\n45.06±0.95\n20.09±0.38\n7.11±0.21\n16.47±0.72\n23.42±0.98\n8.08±0.29\nOurs (CKC)\n19.31±0.44\n36.26±0.45\n46.32±0.57\n21.98±0.66\n8.23±0.31\n17.83±0.25\n24.89±0.12\n9.17±0.28\nTable 2: Test results (in %) for next-turn keyword prediction. Results are averaged over 3 random seeds.\nConvAI2\nReddit\nModel\nR@1\nR@3\nR@5\nMRR\nR@1\nR@3\nR@5\nMRR\nPMI\n48.67±0.25\n75.88±0.49\n86.38±0.15\n64.74±0.26\n45.31±0.70\n68.93±0.37\n79.75±0.46\n60.42±0.50\nNeural\n47.93±0.47\n75.53±0.62\n86.36±0.20\n64.25±0.38\n44.96±0.21\n68.75±0.27\n79.59±0.23\n60.18±0.22\nKernel\n48.55±0.51\n75.57±0.32\n86.04±0.04\n64.47±0.37\n44.55±0.33\n68.47±0.24\n79.66±0.38\n59.92±0.30\nDKRN\n48.44±0.34\n75.78±0.20\n86.83±0.16\n64.64±0.17\n44.92±0.45\n68.84±0.45\n79.59±0.65\n60.19±0.44\nOurs (CKC)\n59.90±0.41\n83.03±0.31\n92.15±0.17\n73.50±0.26\n50.02±0.41\n72.94±0.33\n82.87±0.22\n64.33±0.35\nTable 3: Test results (in %) for keyword-augmented response retrieval. Results are averaged over 3 random seeds.\nDialogue-Level Evaluation\nFollowing (Tang et al. 2019),\nwe measure the target success rate (Succ.) and number of\nturns (#Turns) to reach the target for keyword-guided conversation evaluation using self-play simulations. We run\nself-play simulations for 1K conversations between each\nmodel and a base response retrieval model8. In addition,\nwe measure target success rate (Succ.) and conversation\nsmoothness (Smo.) using human evaluations with three annotators on 100 conversations for each model. The smoothness is rated in the [1, 5] scale, higher is better.\nBaselines and Model Settings\nWe compare our model with the following baselines: PMI\n(Tang et al. 2019), Neural (Tang et al. 2019), Kernel (Tang\net al. 2019) and DKRN (Qin et al. 2020). We follow their released implementations9. All baselines are trained and evaluated using the same ﬁltered datasets as our model.\nWe initialize the embedding layer of all models using\nGloVe embedding of size 200 (Pennington, Socher, and\nManning 2014). All hidden sizes in GRU and GGNN are\nset to 200. We use one layer in GGNN and set λk = 0.01.\nWe optimize our model using Adam (Kingma and Ba 2014)\nwith batch size of 32, an initial learning rate of 0.001 and a\ndecay rate of 0.9 for every epoch.",
        "result analysis": "In this section, we present the experimental results, model\nanalysis, case study and limitations.\nNext-Turn Keyword Prediction\nThe results for next-turn keyword prediction are presented\nin Table 2. Among all baselines except Random, the nonparameterized PMI performs worst, and Neural, Kernel and\n8This model respond passively to messages, which is the same\nas the base model used in (Tang et al. 2019).\n9We ﬁxed a bug in DKRN where the keyword transition mask\nis obtained using train+valid+test datasets.\nConvAI2\nReddit\nModel\nSucc. (%)\n#Turns\nSucc. (%)\n#Turns\nPMI\n14.6\n5.83\n5.1\n4.88\nNeural\n18.9\n6.07\n11.1\n5.99\nKernel\n20.7\n5.89\n10.6\n5.83\nDKRN\n25.6\n4.54\n18.4\n4.42\nOurs (CKC)\n28.9\n4.23\n22.7\n4.19\nTable 4: Self-play simulation results.\nConvAI2\nReddit\nModel\nSucc. (%)\nSmo.\nSucc. (%)\nSmo.\nPMI\n16.0\n3.05\n6.3\n2.68\nNeural\n17.3\n2.77\n11.0\n2.85\nKernel\n22.3\n2.88\n12.3\n2.57\nDKRN\n25.0\n3.01\n17.7\n2.81\nOurs (CKC)\n29.3\n3.27\n22.3\n3.08\nTable 5: Human evaluation results. Smo. denotes conversation smoothness.\nDKRN performs comparably on both datasets. Our proposed\nmodel achieves consistent better performance than all baselines across all metrics and datasets, suggesting that incorporating CKG triplets into keyword prediction helps.\nKeyword-Augmented Response Retrieval\nThe results for keyword-augmented response retrieval are\npresented in Table 3. The baselines differ in which nextturn keyword prediction model is used. It is surprising that\nall baselines perform comparably regardless of the nextturn keyword prediction model. This may suggest that the\nbaselines are unable to effectively leverage the predicted\nkeyword information into response retrieval. Our model\nachieves substantially better performance than all baselines\non both datasets. The performance improvement can be primarily attributed to 1) we additionally incorporate utterancerelated CKG triplets into utterance representation learning;\nNext-Turn Keyword Prediction\nModel\nR@1\nOurs (CKC)\n19.31±0.44\n- concepts\n18.56±0.31\nKeyword-Augmented Response Retrieval\nModel\nR@1\nOurs (CKC)\n59.90±0.41\n- concepts\n53.11±0.43\n- keywords\n52.30±0.54\nSelf-Play Simulation\nModel\nSucc. (%)\n#Turns\nOurs (CKC)\n28.9\n4.23\n- CKG-based strategy\n22.3\n4.42\nTable 6: Ablation study (in %) on ConvAI2.\nTarget: music\nA: Hey, how are you doing?\nH: I’m well, thanks. Working on a party I’m planning.\nA: I am sitting here listening to pearl jam, my favorite band.\nH: Super cool! Do you sing? I was just singing in my shower.\nA: No, but I was in jazz band in hs.\nH: Congrats! I love music and playing my guitar and violin.\nA: That’s awesome! However, my favorite is country music.\nTable 7: Case study from self-play simulations on ConvAI2.\nA denotes our model and H denotes the base model.\nand 2) we propose an additional keyword matching module\nto match the predicted keywords with candidate keywords,\nwhereas baselines directly match predicted keywords with\ncandidate utterances.\nKeyword-Guided Conversation\nThe self-play simulation results for keyword-guided conversation are presented in Table 4. DKRN performs best among\nall baselines, which can be primarily attributed to its strategy of selecting keyword-related responses. This strategy requires a pool of conﬁdent candidates to select from. A larger\npool will lead to higher success rate but lower smoothness\nbecause potentially less likely candidates can be selected. In\nall experiments, we set the pool size to 100. Our model also\nleverages this strategy but instead use weighted path lengths\nto measure keyword relatedness. Our model outperforms all\nbaselines in both metrics on both datasets. Note that the success rates on ConvAI2 are consistently larger than that on\nReddit across all models, which can be partially due to the\nhigher next-turn keyword prediction accuracy on ConvAI2.\nThe human evaluation results are presented in Table 5.\nThe results for success rate are similar to that in self-play\nsimulations. Among all baselines, DKRN has slightly more\nrobust performance in smoothness on both datasets. Our\nmodel obtains consistently better performance in both success rate and smoothness on both datasets, suggesting that\nour model can select conﬁdent candidates that are also related to the target keyword.\nModel Analysis\nTable 6 presents the ablation study of our model across multiple tasks on the ConvAI2 test set. In both next-turn keyword prediction and keyword-augmented response retrieval,\nremoving concepts representation from our model leads to\ndegraded performance in R@1, suggesting that CKG triplets\nare helpful in learning the semantic representation of utterances. In keyword-augmented response retrieval, unlike\nother baselines that do not leverage keywords effectively,\nour model performs noticeably worse when keywords are removed, showing that our design of matching keywords separately indeed contribute to the overall matching. Finally,\nwe examine the impact of our CKG-guided keyword selection strategy on self-play simulations. The results in Table 6 show that replacing our CKG-based strategy by the\nembedding-based strategy (Tang et al. 2019; Qin et al. 2020)\nleads to worse performance in both success rate and number\nof turns.\nCase Study\nWe present a case study from our self-play simulations in\nTable 7. Our model can lead the conversation from a starting keyword “party” to the target keyword “music” smoothly\nand fast.\nLimitations\nOne major limitation of existing approaches including ours\nis the mediocre accuracy of retrieving keyword-related responses (this is different from keyword-augmented response\nretrieval where the ground-truth responses do not necessarily correlate with the input keywords), which bottlenecks the\noverall target success rate. In fact, for both DKRN and our\nmodel, the target keyword can be successfully selected most\nof the time during self-play simulations, however, both models can not retrieve the keyword-related responses given the\nselected target keyword accurately. A potential solution to\nthis problem is to train the keyword-augmented response retrieval model on datasets where input keywords and groundtruth responses are correlated, which is left to future work.",
        "conclusion": "We\nstudy\nthe\nproblem\nof\nimposing\nconversational\ngoals/keywords on open-domain conversational agents.\nThe keyword transition module in existing approaches\nsuffer from noisy datasets and unreliable transition strategy.\nIn this paper, we propose to ground keyword transitions\non commonsense and propose two GNN-based models for\nthe tasks of next-turn keyword transition and keywordaugmented\nresponse\nretrieval,\nrespectively.\nExtensive\nexperiments show that our proposed model obtains substantially better performance on these two tasks than\ncompetitive baselines. In addition, the model analysis\nsuggests that CKG triplets and our proposed CKG-guided\nkeyword selection strategy are helpful in learning utterance\nrepresentation and keyword transition, respectively. Finally,\nboth self-play simulations and human evaluations show that\nour model can achieve better success rate, reach the target\nkeyword faster, and produce smoother conversations than\nbaselines.",
        "summary_en": "This paper studies the problem of imposing conversational goals/keywords on open-domain conversational agents, where the agent is required to lead the conversation to a target keyword smoothly and fast. Solving this problem enables the application of conversational agents in many real-world scenarios, e.g., recommendation and psychotherapy. The dominant paradigm for tackling this problem is to 1) train a next-turn keyword classifier, and 2) train a keyword-augmented response retrieval model. However, existing approaches in this paradigm have two limitations: 1) the training and evaluation datasets for next-turn keyword classification are directly extracted from conversations without human annotations, thus, they are noisy and have low correlation with human judgements, and 2) during keyword transition, the agents solely rely on the similarities between word embeddings to move closer to the target keyword, which may not reflect how humans converse. Therefore, this paper assumes that human conversations are grounded on commonsense and proposes a keyword-guided neural conversational model that can leverage external commonsense knowledge graphs (CKG) for both keyword transition and response retrieval. Automatic evaluations suggest that commonsense improves the performance of both next-turn keyword prediction and keyword-augmented response retrieval. In addition, both self-play and human evaluations show that the model produces responses with smoother keyword transition and reaches the target keyword faster than competitive baselines.",
        "summary_zh": "这篇论文介绍了一种关键词引导的神经对话模型。在开放域对话系统中引入会话目标或关键词，有很大的应用价值，但是目前解决这一问题的主流方法存在数据噪音和模型过度依赖词向量相似性的问题，因此，作者提出了一种关键词引导神经对话模型，该模型可利用外部常识知识图谱（CKG）进行关键词转换和响应检索。实验结果表明，使用常识知识图谱能够提升关键词预测和响应检索的性能，并且模型在关键词转换方面比基准更为流畅和快速。"
    },
    {
        "title": "Boundary Enhanced Neural Span Classification for Nested Named Entity Recognition",
        "abstract": "Named entity recognition (NER) is a well-studied task in natural language processing. However, the widely-used sequence labeling framework is usually difﬁcult to detect entities with nested structures. The span-based method that can easily detect nested entities in different subsequences is naturally suitable for the nested NER problem. However, previous span-based methods have two main issues. First, classifying all subsequences is computationally expensive and very inefﬁcient at inference. Second, the span-based methods mainly focus on learning span representations but lack of explicit boundary supervision. To tackle the above two issues, we propose a boundary enhanced neural span classiﬁcation model. In addition to classifying the span, we propose incorporating an additional boundary detection task to predict those words that are boundaries of entities. The two tasks are jointly trained under a multitask learning framework, which enhances the span representation with additional boundary supervision. In addition, the boundary detection model has the ability to generate high-quality candidate spans, which greatly reduces the time complexity during inference. Experiments show that our approach outperforms all existing methods and achieves 85.3, 83.9, and 78.3 scores in terms of F1 on the ACE2004, ACE2005, and GENIA datasets, respectively.",
        "introduction": "Named entity recognition (NER) is a fundamental task in\nthe ﬁeld of natural language processing. It aims to identify text spans to speciﬁc entity types such as Person, Organization, and Location, which beneﬁts many downstream\nNLP applications. Previous works usually treat NER as a sequence labeling task. For example, Lample et al. (2016) propose the LSTM-CRF model, which achieves promising results by combining deep recurrent neural networks (RNNs)\nwith conditional random ﬁelds (CRFs) (Lafferty, McCallum,\nand Pereira 2001). However, Finkel and Manning (2009)\npoint out that named entities are often nested. For example,\n43.27% and 37.35% entities are nested in the ACE2004 and\nACE2005 datasets, respectively. Figure 1 and Figure 2 show\ntwo examples in the ACE2005 and GENIA datasets, respectively. In the ﬁrst example, “Britain” is an entity with the\n\u0002\u0003\u0004\n\u0004\u0005\u0006\n\u0002\u0003\u0004\u0005\u0006\u0007\u0003\u0006\b\t\n\u0006\u000b\b\f\r\u000e\u000e\u0005\u000e \u0005\u000f\b\u0010\u0006\r\u0007\n\r\u000e\b\u0011\u0012\b\u0013\u0014\u0015\b\u000f\r\u000b\u0003\u0016\b\u0007\u0017\r\u0012\b\u0006\u0003\u0004\u0005\u0006\u0007\b\u0018\n\u0005\u0006\u0003\n\u0005\u0006\u0003\nFigure 1: An example in the ACE dataset.\n\u0002\u0003\u0004\n\u0002\u0003\u0004\n\u0005\u0007\b\b\u0006\b\t\n\u0007\n\u0005\u0007\b\b\u0006\u000b\f\r\u0007\n\r\u000e\u000f\u000b\u0007\t\n\n\u0002\u0003\u0004\n\r\u000e\u000f\u000b\u0007\t\n\n\u0002\u0019\b\u0003\u0004\b\u001a \u001b\b\u001c\u001d\u001e\u0005\u001e\u001f\u0006\u001d\b\u0007\u0006\b\u001d\t\u0019\n\b\b\b\u000b\u0006\f\u0006\r\u001f\t \u0006\b\u000e\u000f\b\u001a \r\u0010\t\b\u0011\b!\t\u001f\u0006\b\"\b!\b\r\u001e\u0005\u001c\u0012\u0006\u001f\u0006\u0012#\b\t\u0019\b\r\u001f\t \u0006\n\t\u0019\b\u0013\u0011\u0014\b\u001a \u001f\u001d\b\u0019!\f\u001e\u001d\u0005\u0006\u000b\b\u0011\b\r\u0006\u0012\u0012!\b$\b\"\u0010\t\u0012\u0006\b\t\u001f\b!\u001f\t\u0012\u0012\b\u0010\b\u000b\b\b\r\u001f\t \t\u001f#\b\t\u0019\b\u0015%\u001d\u0016\b\u001f \u0017\b\r\u0006\u0012\u0012!\b&\n\u0005\u0007\b\b\u0006\u000b\f\r\u0007\nFigure 2: An example in the GENIA dataset.\ntype of “GPE”. It is nested in “Britain’s ITN” with the type\nof “ORG”. They are further nested in “Reporter Carl Dinnon\nof Britain ’s ITN” with the type of “PER”. The above nested\nstructure cannot be handled by the predominant sequence\nlabeling models.\nVarious approaches for nested NER have been proposed\nin recent years. One representative direction is based on\nhypergraph-based methods (Lu and Roth 2015; Wang and\nLu 2018) that recognize nested entities by designing expressive tagging schemas. However, the hypergraph-based\nmethod needs a lot of human efforts to carefully design\nthe unambiguous hypergraph. Another direction is based\non span-based methods that recognize nested entities by\nclassifying subsequences of the sentence (Xu, Jiang, and\nWatcharawittayakul 2017; Sohrab and Miwa 2018; Xia et\nal. 2019). The span-based method has its own advantages\nthat we can easily ﬁnd all candidate entities with different\nsubsequences, which is straightforward and does not need\nhuman efforts. We therefore solve the nested NER task with\nthe span-based method in this work. However, the spanbased method still has two main issues. First, classifying all\nsubsequences in the sentence is computationally expensive.\nSecond, the span-based methods mainly focus on learning\nspan representations but lack of explicit boundary supervision. Compared with methods under the sequence labeling\nframework and hypergraph methods, we observe that spanbased methods usually perform worse in detecting boundaries of entities. Span-based methods are usually confused\nby spans with minor difference. For example, as shown in\nFigure 2, “B cells” and “EBV - transformed B cells” are entities, while “transformed B cells” is not, which brings great\ndifﬁculty in learning span representations and usually leads\nto false-positive errors at inference time.\nTo alleviate the above-mentioned issues in the span-based\nmethod, we propose a Boundary Enhanced Neural Span\nClassiﬁcation (BENSC) model. In addition to classifying\nspans into corresponding semantic tags, we propose incorporating an additional boundary detection task to enhance\nthe boundary supervision in learning span representations.\nSpeciﬁcally, given a sentence, we ﬁrst encode the word\nwith the token-level representation, and then jointly train\nthe boundary detection model and the span classiﬁcation\nmodel under a multitask framework. The boundary detection model consists of two token-level classiﬁers predicting\nwhether each word is the ﬁrst or last word of an entity respectively. The span classiﬁcation model is to aggregate the\ninside information of the span to predict its semantic tag.\nDuring inference, we can obtain the boundary conﬁdence\nscores Ps and Pe via the boundary detection model and the\ntag C of the span with the conﬁdence score Psp via the span\nclassiﬁcation model. The three scores will jointly determine\nwhether a span is an entity with the tag C.\nWe conduct experiments on three standard benchmark\ndatasets. Experimental results show that our approach\nachieves 75.3, 75.6, and 75.7 scores in terms of F1 on\nthe ACE2004, ACE2005, and GENIA datasets, respectively.\nWith the pre-trained language model BERT, our approach\nfurther improves the result to 85.3, 83.9, and 78.3 on three\ndatasets, which outperforms all existing methods and our\nspan classiﬁcation baselines. Ablation tests show that jointly\nlearning boundary detection and span classiﬁcation tasks\nbeneﬁts the model with better representation and it improves\nboth two tasks. In addition, the boundary detection model\nhas the ability to generate high-quality candidate spans,\nwhich greatly reduces the number of spans feeding into\nthe span classiﬁcation model and therefore reduces the time\ncomplexity of the whole model. We also show that incorporating the boundary probability can help avoid mistakes by\nthe span classiﬁcation model through case studies.",
        "related work": "It has been a long history of research involving named entity\nrecognition (McCallum and Li 2003). Zhou and Su (2002)\npresent a system for recognizing named entities using an\nHMM-based approach. McDonald and Pereira (2005) apply conditional random ﬁelds to recognize the protein\nand gene entities in biomedical texts. Alex, Haddow, and\nGrover (2007) propose building models on top of linearchain conditional random ﬁelds for recognizing nested entities in biomedical texts. With the development of deep learning methods, LSTM-CRF achieves very promising results\nin recognizing named entities (Huang, Xu, and Yu 2015;\nLample et al. 2016). However, traditional sequential labeling models cannot handle the nested structure because they\ncan only assign one label to each token.\nFinkel and Manning (2009) point out that named entities\nare often nested. The earliest research efforts on nested NER\nare rule-based (Zhang et al. 2004). The authors ﬁrst detect\nthe inner-most mentions and then identify overlapping mentions based on the rule-based post-processing methods. Lu\nand Roth; Katiyar and Cardie; Wang and Lu (2015; 2018;\n2018) propose the hypergraph-based method to solve this\nproblem. They design a hypergraph to represent all possible nested structures, which guarantees that nested entities\ncan be recovered from the hypergraph tags. However, the\nhypergraph needs to be carefully designed to avoid spurious\nstructures and structural ambiguities, and inevitably leads to\nhigher time complexity during both training and inference.\nIn addition, Muis and Lu (2017) develop a gap-based tagging schema to capture nested structures. Wang et al. (2018)\npropose a transition-based method to construct nested mentions via a sequence of specially designed actions. Fisher\nand Vlachos (2019) propose forming nested structures by\nmerging tokens and/or entities into entities for entity representation. Lin et al. (2019) propose a sequence-to-nuggets\narchitecture that ﬁrst identiﬁes anchor words with corresponding semantic types of all entities, and then recognizes\nthe boundaries of the entity for each anchor word.\nAnother strategy for the nested NER problem is the\nspan-based methods. In the span-based method, nested entities can be easily detected because they belong to different subsequences. Recently, Xu, Jiang, and Watcharawittayakul (2017) try to directly classify all subsequences of\na sentence by encoding each subsequence into a ﬁxed-size\nrepresentation. Sohrab and Miwa (2018) also enumerate all\npossible regions or spans as potential entity mentions and\nclassify them with deep neural networks. Xia et al. (2019)\npropose MGNER that consists of a Detector that examines\nall possible spans and a Classiﬁer that categorizes spans into\ncorresponding semantic tags. Luan et al. (2019) propose a\ngeneral framework that leverages the coreference and relation type conﬁdences for better span representations. These\napproaches are straightforward for nested mention detection,\nbut have two main drawbacks. First, classifying all subsequences in the sentence need high computational cost. Second, compared with methods based on the sequence labeling, span-based methods usually show worse performance\nin determining the boundary of an entity because of less\nsupervision in boundary detection. Our work is also under\nthe span classiﬁcation framework. To alleviate the abovementioned issues, we propose incorporating boundary detection into span classiﬁcation, which can help model learn\nbetter representation with boundary supervision and reduce\nthe time complexity by generating high-quality candidates1.",
        "approach": "Following the overview in Figure 3, our approach consists\nof two parts as boundary detection and span classiﬁcation.\nThe boundary detection part aims to predict whether a word\nis the ﬁrst or last word of an entity. The span classiﬁcation\npart aims to classify spans to corresponding semantic tags.\nThe two parts are jointly trained under a multitask learning framework. Speciﬁcally, we ﬁrst apply an encoder to the\n1We observe a contemporaneous work that leverages entity\nboundaries to predict entity categorical labels (Zheng et al. 2019).\n\u0002\u0003\n\u0004\u0003\n\u0002\u0005\n\u0004\u0005\n\u0002\u0006\n\u0004\u0006\n\u0002\u0007\n\u0004\u0007\n\u0002\u0003\n\u0002\u0004\n\u0005\u0006\u0007\n\b\t\n\u000b\f\n\u0002\u0003\u0004\u0005\u0006\u0007\b\n\t\n\u000b\f\r\b\f\u000e\n\t\n\u0002\u0003\u0006\u000e\n\u000b\f\r\b\f\u000f\t\u0005\u0010\u0011\f\u0011\u0005\u0003\u0012\u0013\n\u0002\u0003\u0006\u000f\t\u0005\u0010\u0011\f\u0011\u0005\u0003\u0012\u0014\n\u0015\u0005\u0016\u0003\u0006\r\b\u0017\u000f\u0018\u0007\f\u0007\u0004\f\u0011\u0005\u0003\n\u000b\u0019\r\u0003\u000f\u001a\u001b\r\u0010\u0010\u0011\u001c\u0011\u0004\r\f\u0011\u0005\u0003\n\u0002\u0003\n\u0004\u0003\n\u0002\u0005\n\u0004\u0005\n\u0002\u0006\n\u0004\u0006\n\u0002\u0007\n\u0004\u0007\n\u0002\u0003\n\u0002\u0004\n\u0005\u0006\u0007\n\b\t\n\u000b\f\n\u0002\u0003\u0004\u0005\u0006\u0007\b\n\t\n\u000b\u0019\r\u0003\u000e\n\u0002\u0003\u0004\u0005\u0006 \u0007\b\t\u0004\u0005\u0004\u0005\n\n\t\n\u000b\f\r\b\f\u000e\u000f\u001d \t\n\u0002\u0003\u0006\u000e\u000f\u001d \t\n\u000b\u0019\r\u0003\u000e\n\t\u0002\u001e\n\u001f\t\u0002\n !\u001a\n\u001f\t\u0002\n\u0002\u0003\u0004\u0005\u0005\u0006\u0007\b\t\n\u0003\u000b\f\u0005\r\u000e\u000f\u000e\u0010\u000f\u0011\u0007\t\n\u0002\u0012\u0004\u0005\u0013\u0014\u0003\t\u0005\u0015\u0016\u0003\u0017\u0017\u0011\u0018\u0011\u0010\u0003\u000f\u0011\u0007\t\n\u0019\u0007\u0011\t\u000f\u0005\u001a\t\u0018\u000e\u000b\u000e\t\u0010\u000e\n\"\u0005\u0003\u0007\nFigure 3: An overview of our proposed approach. The whole model consists of (a) boundary detection to predict the span’s\nboundary (b) span classiﬁcation to predict the semantic tag of the span. The two parts are jointly trained under the multitask\nlearning framework, and jointly determine the ﬁnal result.\nsequence for the contextual word representation. This representation will be shared in the downstream boundary detection and span classiﬁcation tasks. The boundary detection\nmodel consists of two token-level classiﬁers that predict the\nprobabilities of a word being the start or end words of an entity respectively. The span classiﬁcation model is to aggregate the span information for the multi-class classiﬁcation.\nDuring inference, we will make the decision by jointly considering the boundary probability and the tag probability.\nEncoder\nConsider a sentence S with words {wi}N\ni=1, we ﬁrst convert the words to their respective word-level embeddings\nand contextual embeddings. In this work, we implement\ntwo kinds of encoders as LSTM (Hochreiter and Schmidhuber 1997) and BERT (Devlin et al. 2019) respectively.\nFor LSTM encoder, we ﬁrst convert the words to their\nrespective word-level embeddings, character-level embeddings, and part-of-speech embeddings. The character-level\nembeddings are generated by taking the ﬁnal hidden states\nof a bi-directional LSTM applied to embeddings of characters in the token. We then use a bi-directional LSTM to\nproduce new representation h1, . . . , hN of all words.\nxi = [wi; chari; posi]\n(1)\n−→\nhi = LSTM(xi, −−→\nhi−1)\n(2)\n←−\nhi = LSTM(xi, ←−−\nhi+1)\n(3)\nhi = [−→\nhi; ←−\nhi]\n(4)\nFor BERT encoder, we ﬁrst tokenize the sentence with the\nwordpiece vocabulary, and then generate the input sequence\n˜\nwi by concatenating a [CLS] token, the tokenized sentence,\nand a [SEP] token. Next, we use a series of L stacked\nTransformer blocks (Vaswani et al. 2017) to project the input\nembeddings into a sequence of contextual vectors.\nhi = Transformer BlockL( ˜\nwi)\n(5)\nWe do not combine the character embedding and part-ofspeech embedding because we assume that they have already been encoded in the BERT representation.\nBoundary Detection\nBoundary detection aims to identify whether a word is the\nﬁrst or last word of an entity. Instead of detecting the boundary via sequence labeling methods, we predict the start and\nend positions with two token-wise classiﬁers.\nSpeciﬁcally, we feed the contextual representation hi into\na multi-layer perceptron (MLP) classiﬁer, and apply a softmax layer to obtain the probability P i\ns of the word wi being\nthe ﬁrst word of an entity.\nP i\ns = softmax(MLPstart(hi))\n(6)\nSimilarity, we can apply a MLP classiﬁer to obtain the probability P i\ne of the word wi being the last word of an entity.\nP i\ne = softmax(MLPend(hi))\n(7)\nDuring training, since each sentence may contain multiple\nentities, we label the span boundaries of all entities as the\nground-truth. Then, we deﬁne the training objective function\nas the sum of two following cross-entropy losses in detecting\nthe start and end boundaries, respectively,\nLs\nbdr = −\nN\n\u0002\ni=1\n[yi\ns log P i\ns + (1 − yi\ns) log(1 − P i\ns)]\n(8)\nLe\nbdr = −\nN\n\u0002\ni=1\n[yi\ne log P i\ne + (1 − yi\ne) log(1 − P i\ne)]\n(9)\nLbdr = Ls\nbdr + Le\nbdr\n(10)\nwhere yi\ns and yi\ne denote the label that whether the word i is\nthe ﬁrst and last word of an entity, respectively.\nSpan Classiﬁcation\nSpan classiﬁcation is a span-wise classiﬁer, which aims to\nclassify spans to corresponding semantic tags. If a span is\nnot an entity, it should be mapped to an additionalNone.\nWe propose to summarize the word representation from\ncontextual word vectors according to its span boundary. For\nthe LSTM encoder, we calculate a summarized vector vsp\nusing the attention mechanism (Bahdanau, Cho, and Bengio\n2014) over tokens in its corresponding boundary (i, j),\nα = softmax(Whi:j)\n(11)\nvsp =\nj\n\u0002\nt=i\nαtht\n(12)\nwhere W is the parameter to be learned.\nFor BERT encoder, we obtain the span representation by\nthe mechanism of self attentions. For the span {wi, . . . , wj},\nwe use Transformer Blocks (Vaswani et al. 2017) to further\nencode words inside the span based on their word representations (hi, . . . , hj),\nh∗\ni:j = Transformer Blocks(hi:j)\n(13)\nWe then use vsp = [h∗\ni , h∗\nj] to represent the span.\nNext, we feed the span representation vsp into a multilayer perceptron (MLP) classiﬁer, and apply a softmax layer\nto obtain the probability Psp to predict its semantic tag.\nPsp = softmax(MLPsp(vsp))\n(14)\nFinally, we minimize the following cross-entropy loss\nfunction,\nLsp = −\nk\n\u0002\nt=1\n(yt\nsp log P t\nsp + (1 − yt\nsp) log(1 − P t\nsp)) (15)\nwhere k is the number of semantic tags, and yt\nsp denotes a\nlabel that whether the span (wi, . . . , wj) is in tag t.\nJoint Training and Inference\nFor training, we jointly minimize the following loss,\nL = wLbdr + (1 − w)Lsp\n(16)\nwhere w is the hyper-parameter to balance two sub-tasks.\nDuring inference, given the instance (wi, . . . , wj), we\nﬁrst obtain the boundary probabilities P i\ns and P j\ne predicted\nby the boundary detection model. We then classify all legal spans where j must be larger than i if P i\ns ∗ P j\ne is\nlarger than the threshold pre-selected on the development\nset. We further feed the span into the span classiﬁcation\nmodel for its semantic tag C with probability Psp. If the\nscore P i\ns ∗ P j\ne ∗ Psp is still larger than the threshold, we recognize the span as an entity with tag C.\nImplementation Details\nFor the LSTM encoder, we use 300-dimensional uncased\npre-trained GloVe embeddings (Pennington, Socher, and\nManning 2014) without update during training. We use zero\nvectors to represent all out-of-vocabulary words. The size\nof character embedding and part-of-speech embedding are\nset to 50. The hidden vector length is set to 150. The model\nis optimized using Adam (Kingma and Ba 2014) with the\nlearning rate of 0.002.\nFor BERT encoder, we use the BERTBASE model (Devlin et al. 2019) to obtain the word representation, and the\nparameter in BERT is also trainable. The hidden vector\nlength is 768 and the number of heads is 12. Detailed model\nsize is referred to Devlin et al. (2019). We use Adam optimizer with the learning rate of 3e-5.\nIn addition, we also apply 0.2 dropout (Srivastava et al.\n2014) between layers. w is set to 0.5 for both the LSTM and\nBERT encoder. To speed up the process of training, we only\nsample part of the negative spans from all subsequence. The\nrule is that the length of the span is less than 6 and negative spans should overlap with positive spans. We observe\nthat training with this negative subset has no performance\ndegradation compared with using full negative spans.",
        "experiments": "We conduct experiments on three standard benchmark\ndatasets as ACE2004, ACE2005, and GENIA respectively.\nResults show that our proposed approach achieves state-ofthe-art performance on all three datasets. The ablation tests\nshow that our multitask learning framework beneﬁts both\nboundary detection and span classiﬁcation tasks. Then, the\nanalysis of the time complexity shows that the boundary detection model can generate high-quality candidates, which\ngreatly reduces the time complexity to linear time. Finally,\nwe analyze how the boundary detection model beneﬁts the\nﬁnal result with two cases.\nDatasets\nWe evaluate our model on the ACE2004, ACE2005 (Doddington et al. 2004), and GENIA (Kim et al. 2003) datasets.\nSpeciﬁcally, there are seven different types of entities as\n‘FAC’, ‘LOC’, ‘ORG’, ‘PER’, ‘WEA’, ‘GPE’, ‘VEH’ in\nthe ACE datasets and ﬁve types of entities as ‘G#DNA’,\n‘G#RNA’, ‘G#protein’, ‘G#cell line’, ‘G#cell type’ in the\nGENIA dataset. The statistics of these datasets are shown\nin Table 1. We observe that the statistics are not strictly consistent with previous works in the ACE datasets due to word\ntokenization and sentence segmentation, but the difference\nis less than 0.35%.\nACE2004\nACE2005\nGENIA\nTrain\nDev\nTest\nTrain\nDev\nTest\nTrain\nDev\nTest\n# sentences\n7,078\n859\n922\n7,194\n969\n1,047\n14,836\n1,855\n1,855\nwith nested entities\n2,691\n290\n377\n2,691\n338\n330\n3,199\n362\n448\n# entities\n22,172\n2,510\n3,024\n24,441\n3,200\n2,993\n46,473\n5,014\n5,600\n# nested entities\n10,080\n1,086\n1,410\n9,389\n1,112\n1,118\n8,337\n903\n1,217\navg length\n20.38\n20.69\n20.96\n19.21\n18.93\n17.19\n30.13\n29.17\n30.48\nTable 1: Statistics of ACE2004, ACE2005, and GENIA datasets.\nACE2004\nACE2005\nGENIA\nModel\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nLSTM-CRF (Lample et al. 2016)\n71.3\n50.5\n58.3\n70.3\n55.7\n62.2\n75.2\n64.6\n69.5\nMulti-CRF\n69.7\n61.3\n65.2\n73.1\n64.9\n68.8\nFOFE(c=6) (Xu et al. 2017)\n68.2\n54.3\n60.5\n76.5\n66.3\n71.0\n75.4\n67.8\n71.4\nFOFE(c=n) (Xu et al. 2017)\n57.3\n46.8\n51.5\n76.9\n62.0\n68.7\n74.0\n65.5\n69.5\nTransition (Wang et al. 2018)\n74.9\n71.8\n73.3\n74.5\n71.5\n73.0\n78.0\n70.2\n73.9\nCascaded-CRF (Ju et al. 2018)\n74.2\n70.3\n72.2\n78.5\n71.3\n74.7\nMH (Lu and Roth 2015)\n70.0\n56.9\n62.8\n66.3\n59.2\n62.5\nLH (Katiyar and Cardie 2018)\n73.6\n71.8\n72.7\n70.6\n70.4\n70.5\n79.8\n68.2\n73.6\nSH(c=6) (Wang and Lu 2018)\n79.1\n67.3\n72.7\n75.9\n70.0\n72.8\n76.8\n71.8\n74.2\nSH(c=n) (Wang and Lu 2018)\n77.7\n72.1\n74.5\n76.8\n72.3\n74.5\n77.0\n73.3\n75.1\nARNs (c=6) (Lin et al. 2019)\n75.2\n72.5\n73.9\n75.2\n73.3\n74.2\nARNs (c=n) (Lin et al. 2019)\n76.2\n73.6\n74.9\n75.8\n73.9\n74.8\nMerge and Label (Fisher and Vlachos 2019)\n75.1\n74.1\n74.6\nBENSC (LSTM)\n78.1\n72.8\n75.3\n77.1\n74.2\n75.6\n78.9\n72.7\n75.7\nwith Pretrained LM\nMGNER (ELMo) (Xia et al. 2019)\n81.7\n77.4\n79.5\n79.0\n77.3\n78.2\nMerge and Label (ELMo)\n79.7\n78.0\n78.9\nMerge and Label (BERT)\n82.7\n82.1\n82.4\nBENSC (BERT)\n85.8\n84.8\n85.3\n83.8\n83.9\n83.9\n79.2\n77.4\n78.3\nTable 2: Overall results on ACE2004, ACE2005, and GENIA datasets.\nBaselines\nWe compare our model with the following baseline models:\nLSTM-CRF is a classical baseline for NER, which cannot\nsolve the problem of nested entities (Lample et al. 2016).\nMulti-CRF is similar to LSTM-CRF but learns one model\nfor each entity type.\nFOFE is a span-based approach that classiﬁes over all subsequences of a sentence by encoding each span with a\nﬁxed-size ordinarily forgetting encoding (Xu, Jiang, and\nWatcharawittayakul 2017).\nTransition is a shift-reduce based system that learns to construct the nested structure in a bottom-up manner through\nan action sequence (Wang et al. 2018).\nCascaded-CRF applies several stacked CRF layers to recognize nested entities at different levels in an inside-out\nmanner (Ju, Miwa, and Ananiadou 2018).\nMH makes use of hypergraphs for recognizing overlapping\nentities (Lu and Roth 2015).\nLH uses an LSTM model to learn features and then decodes\nthem into a hypergraph (Katiyar and Cardie 2018).\nSH improves LH by considering the transition between labels to alleviate labeling ambiguity (Wang and Lu 2018).\nARNs ﬁrst identiﬁes anchor words and then recognizes the\nmention boundaries for each anchor word. They propose\na bag-loss to jointly train the two parts (Lin et al. 2019).\nMGNER ﬁrst applies the Detector to generate possible\nspans as candidates and then applies a Classiﬁer for the\nentity type (Xia et al. 2019).\nMerge and Label ﬁrst merges tokens and/or entities into\nentities forming nested structures, and then labels entities\nto corresponding types (Fisher and Vlachos 2019).\nMain Results\nTable 2 shows the overall results on ACE2004, ACE2005,\nand GENIA datasets. Our BENSC model achieves state-ofthe-art results in both the LSTM and BERT settings. When\nusing the LSTM encoder, our BENSC model achieves 75.3,\n75.6, and 75.7 scores in terms of F1 on the ACE2004,\nACE2005, and GENIA datasets, respectively. Compared\nwith the span-based method FOFE, our BENSC model\nachieves 14.8, 4.6, and 4.3 absolute gains on the ACE2004,\nACE2005, and GENIA datasets, respectively. Our model\nalso outperforms the hypergraph-based methods LH and SH,\nand the other state-of-the-art methods such as the ARNs\nmodel and the Merge and Label model. With the pre-trained\nACE2004\nACE2005\nGENIA\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nBoundary Detection (Start)\nOnly Boundary Detection\n92.0\n92.8\n92.4\n89.7\n93.8\n91.7\n85.9\n84.9\n85.4\nBENSC\n92.8\n92.5\n92.6\n90.0\n94.1\n92.0\n86.5\n85.9\n86.2\nBoundary Detection (End)\nOnly Boundary Detection\n92.3\n92.5\n92.4\n88.8\n93.3\n91.0\n88.8\n87.8\n88.3\nBENSC\n92.1\n93.8\n92.9\n89.2\n94.1\n91.6\n89.0\n89.4\n89.2\nSpan Classiﬁcation\nSpan Classiﬁcation Only\n75.1\n87.4\n80.8\n73.2\n85.7\n79.0\n72.1\n81.4\n76.5\nBENSC\n85.8\n84.8\n85.3\n83.8\n83.9\n83.9\n79.2\n77.4\n78.3\nTable 3: Ablation tests on ACE2004, ACE2005, and GENIA datasets using the BERT encoder.\nlanguage model, our BENSC model with the BERT encoder achieves 85.3, 83.9, and 78.3 scores in terms of F1\non the ACE2004, ACE2005, and GENIA datasets, respectively, which outperforms all existing baselines such as the\nMGNER model and the Merge and Label model.\nAblation Test\nTo analyze the effectiveness of our joint model, we show\nthe result of ablation tests based on the BERT encoder in\nTable 3. We observe that jointly training the boundary detection model and the span classiﬁcation model can improve\nthe result of both two tasks. Firstly, we observe that the result\nof predicting the start and end boundaries achieves a little\nimprovement compared with the isolate boundary detection\nmodel. Then, compared with the original span classiﬁcation\nmethod, incorporating the boundary detection model obtains 4.5%, 4.9%, and 1.8% absolute gains on the ACE2004,\nACE2005, and GENIA datasets, respectively. In addition,\nour BENSC model shows better precision than the original span classiﬁcation method. As we mentioned before,\nthe span-based model is usually confused when the positive and negative instances have many overlapping words,\nwhich may lead to some false-positive errors. However, our\nBENSC model takes the boundary information as additional\nsupervision, which beneﬁts the model to distinguish the confusing cases.\nTime Complexity\nTheoretically, given a sentence consisting of N words, there\nare altogether n(n+1)\n2\npossible candidates. Previous spanbased methods need to classify almost all sentence subsequences into corresponding semantic tags, which leads to\nthe high computational cost with O(mn2) time complexity where m is the number of semantic tags. However, in\nour work, the boundary detection model can help us generate high-quality candidates, which can signiﬁcantly reduce\nthe number of candidates and lead to much lower time complexity. The time complexity of our approach consists of two\nparts. The boundary detection model is a token-wise classiﬁcation model with O(n) time complexity. The span classiﬁcation model needs to classify the span to corresponding\nsemantic tags. Its time complexity is determined by the number of candidates. In our experiment, we prune spans whose\nDataset\n# Entities\n# Candidates\n# Words\nACE2004\n3.28\n4.80\n20.96\nACE2005\n2.86\n4.19\n17.19\nGENIA\n3.02\n3.56\n30.48\nTable 4: The statistics of the average number of entities, candidates, and words in the sentence. We can observe that the\nnumber of our candidates is far less than the length of sentence.\nboundary probability P i\ns ∗P j\ne are lower than the pre-selected\nthreshold after the boundary detection part since they cannot be triggered whatever the result of the span classiﬁcation model is. Although in the worst case, every position\nis marked with both the start and end label, which leads to\nn(n+1)\n2\npossible candidates, we observe that the number of\ncandidate spans is much closer to the number of entities c\nin practice. Ideally, for c entities, the model will detect c\nstart positions and c end positions, which may form c2 candidates. However, as shown in Figure 1, nested entities may\nshare the same start or end positions. The actual number\nof candidates is therefore much less than c2 and closer to\nc. As shown in Table 4, the average number of candidates\nin our experiments is 4.80, 4.19 and 3.56 in the ACE2004,\nACE2005, and GENIA dataset, respectively, which is closed\nto the number of entities and much less than the average\nlength of sequences. For example, on the ACE2005 dataset,\nwe reduce candidates from over 100 thousand subsequences\nof the sentence to about 4.43 thousand spans that is only 1.5\ntimes than the number of entities. Therefore, the total time\ncomplexity of our approach is approximated to O(n + cm)\nwhere c << n. This analysis demonstrates that adding the\nboundary detection model can help us generate high-quality\ncandidates to reduce the time complexity to almost linear\ntime in practice.\nCase Study\nTo demonstrate how each module of our model takes effect\nwhen predicting the ﬁnal answer, we conduct a case study\nin Table 5 with two cases in the GENIA and ACE datasets,\nrespectively. In the ﬁrst example, we can observe that the\ncorrect entity span “nuclear proteins” and “TRE - DNA fragment” obtain high probabilities in all three modules. If only\nSentence 1: First , from different cell lines three or all four of the nuclear proteins were speciﬁcally\ncross-linked by UV irradiation to the radioactively labeled TRE-DNA fragment ..\nCandidate Spans:\nPredsp\nGold\nPi\ns\nPj\ne\nPsp\nOutput\nnuclear proteins\nG#protein\nG#protein\n1.0\n1.0\n1.0\nG#protein\nTRE - DNA fragment\nG#DNA\nG#DNA\n1.0\n1.0\n1.0\nG#DNA\nradioactively labeled TRE - DNA fragment\nG#DNA\nNone\n0.11\n1.0\n0.77\nNone\nSentence 2: reporter : now willie williams the girl ’s father is qharthd attempted murder\nCandidate Spans:\nPredsp\nGold\nPi\ns\nPj\ne\nPsp\nOutput\nwillie williams\nPER\nPER\n1.0\n0.99\n0.99\nPER\nthe girl\nPER\nPER\n1.0\n1.0\n1.0\nPER\nthe girl ’s father\nPER\nPER\n1.0\n1.0\n1.0\nPER\nwillie williams the\nPER\nNone\n1.0\n0.0\n0.58\nNone\nwillie williams the girl\nPER\nNone\n1.0\n1.0\n0.76\nPER\nwilliams the\nPER\nNone\n0.0\n0.0\n0.54\nNone\nwilliams the girl\nPER\nNone\n0.0\n1.0\n0.72\nNone\nwillie williams the girl ’s father\nPER\nNone\n1.0\n1.0\n1.0\nPER\nTable 5: Case study on GENIA and ACE datasets. In practice, if P i\ns ∗ P j\ne is lower than the pre-selected threshold, we will not\nfeed the span into the span classiﬁcation model for its semantic tag. However, to analyze the effect of each module, we show its\nsemantic tag Predsp with corresponding probability Psp in this case study.\nconsidering the span classiﬁcation model, the span “radioactively labeled TRE - DNA fragment” may be misidentiﬁed\nas “G#DNA”, however, since the boundary detection model\ngives a lower P i\ns score, we can prune this span because its\nﬁrst word does not look like the start of an entity. In the second example, correct spans are also correctly recognized,\nand parts of wrong spans such as “willie williams the” and\n“williams the girl” can be correctly pruned. However, the\nspan “willie williams the girl” and “willie williams the girl\n’s father” are false-positive errors. Actually, “willie” is the\nﬁrst word of “willie williams” and “girl” as well as “father”\nare last words of “the girl” as well “the girl ’s father” respectively. Since our boundary detection model independently\npredicts the probability at the word level, it cannot distinguish whether the ﬁrst word and the last word come from\nthe same entity, and therefore make a mistake to recognize\nthis two spans as entities. An alternative solution is to take\nthe span into consideration when determining the start and\nend probabilities, but it will lead to higher time complexity.",
        "conclusion": "In this paper, we tackle the problem of nested NER in which\nentities may be nested with others. We consider that the\nspan-based approach has its advantages as nested entities\ncorrespond to different subsequences. However, the spanbased method has two main drawbacks as the high time complexity and the weak supervision of the boundary. To overcome the two above issues, we propose a boundary enhanced\nneural span classiﬁcation model (BENSC), which incorporates the boundary detection task into the span classiﬁcation\ntask under a multitask learning framework. We ﬁrst apply an\nencoder for the token-level representation. On top of it, we\nimplement a boundary detection model with two token-level\nclassiﬁers to predict whether a word is the ﬁrst or last word\nof an entity, and a span classiﬁcation model to aggregate the\nspan information for the semantic type. During inference,\na span treated as an entity should have high probabilities\nat both the span level and the boundary level. Experiments\nshow that our BENSC model achieves the state-of-the-art results on three standard benchmark datasets and outperforms\nthe span classiﬁcation baselines. Ablation tests demonstrate\nthat the boundary detection beneﬁts our BENSC model with\nbetter representation of the span. In addition, the boundary\ndetection model can generate high-quality candidates, which\ngreatly reduces the time complexity to almost linear time\nduring inference.",
        "summary_en": "Named entity recognition (NER) is a well-studied task in natural language processing. However, the widely-used sequence labeling framework is usually difficult to detect entities with nested structures. The span-based method that can easily detect nested entities in different subsequences is naturally suitable for the nested NER problem. However, previous span-based methods have two main issues. First, classifying all subsequences is computationally expensive and very inefficient at inference. Second, the span-based methods mainly focus on learning span representations but lack of explicit boundary supervision. To tackle the above two issues, this paper proposes a boundary enhanced neural span classification model. In addition to classifying the span, the paper proposes incorporating an additional boundary detection task to predict those words that are boundaries of entities. The two tasks are jointly trained under a multitask learning framework, which enhances the span representation with additional boundary supervision. In addition, the boundary detection model has the ability to generate high-quality candidate spans, which greatly reduces the time complexity during inference. Experiments show that the approach outperforms all existing methods and achieves 85.3, 83.9, and 78.3 scores in terms of F1 on the ACE2004, ACE2005, and GENIA datasets, respectively.",
        "summary_zh": "这篇论文介绍了一种增强边界的神经跨度分类模型，用于嵌套命名实体识别（NER），解决了以往基于跨度的序列标注框架在检测具有嵌套结构的实体时的困难。作者提出的模型结合了跨度分类和边界检测两个任务，通过多任务学习框架共同训练，通过额外的边界监督来增强跨度表示。实验证明，该方法在ACE2004、ACE2005和GENIA数据集上的F1分数分别达到了85.3、83.9和78.3，优于现有方法。"
    },
    {
        "title": "GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition",
        "abstract": "The dominant approaches for named entity recognition (NER) mostly adopt complex recurrent neural networks (RNN), e.g., long-short-term-memory (LSTM). However, RNNs are limited by their recurrent nature in terms of computational efﬁciency. In contrast, convolutional neural networks (CNN) can fully exploit the GPU parallelism with their feedforward architectures. However, little attention has been paid to performing NER with CNNs, mainly owing to their difﬁculties in capturing the long-term context information in a sequence. In this paper, we propose a simple but effective CNN-based network for NER, i.e., gated relation network (GRN), which is more capable than common CNNs in capturing long-term context. Speciﬁcally, in GRN we ﬁrstly employ CNNs to explore the local context features of each word. Then we model the relations between words and use them as gates to fuse local context features into global ones for predicting labels. Without using recurrent layers that process a sentence in a sequential manner, our GRN allows computations to be performed in parallel across the entire sentence. Experiments on two benchmark NER datasets (i.e., CoNLL2003 and Ontonotes 5.0) show that, our proposed GRN can achieve state-of-the-art performance with or without external knowledge. It also enjoys lower time costs to train and test.",
        "introduction": "Named Entity Recognition (NER) is one of the fundamental\ntasks in natural language processing (NLP). It is designed to\nlocate a word or a phrase that references a speciﬁc entity, like\nperson, organization, location, etc., within a text sentence. It\nplays a critical role in NLP systems for question answering,\ninformation retrieval, relation extraction, etc. And thus many\nefforts have been dedicated to the ﬁeld.\nTraditional NER systems mostly adopt machine learning\nmodels, such as Hidden Markov Model (HMM) (Bikel et\nal. 1997) and Conditional Random Field (CRF) (McCallum\nand Li 2003). Although these systems can achieve high performance, they heavily rely on hand-crafted features or taskspeciﬁc resources (Ma and Hovy 2016), which are expensive\nto obtain and hard to adapt to other domains or languages.\nWith the development of deep learning, recurrent neural\nnetwork (RNN) along with its variants have brought great\nsuccess to the NLP ﬁelds, including machine translation,\nsyntactic parsing, relation extraction, etc. RNNs have been\nproved to be powerful in learning from basic components of\ntext sentences, like words and characters (Tran, MacKinlay,\nand Yepes 2017). Therefore, currently the vast majority of\nstate-of-the-art NER systems are based on RNNs, especially\nlong-short-term-memory (LSTM) (Hochreiter and Schmidhuber 1997) and its variant Bi-directional LSTM (BiLSTM).\nFor example, Huang et al. (2015) ﬁrstly used a BiLSTM to\nenhance words’ context information for NER and demonstrated its effectiveness.\nHowever, RNNs process the sentence in a sequential manner, because they typically factor the computation along the\npositions of the input sequence. As a result, the computation\nat the current time step is highly dependent on those at previous time steps. This inherently sequential nature of RNNs\nprecludes them from fully exploiting the GPU parallelism\non training examples, and thus can lead to higher time costs\nto train and test.\nUnlike RNNs, convolutional neural network (CNN) can\ndeal with all words in a feed-forward fashion, rather than\ncomposing representations incrementally over each word in\na sentence. This property enables CNNs to well exploit the\nGPU parallelism. But in the NER community, little attention has been paid to performing NER with CNNs. It is\nmainly due to the fact that CNNs have the capacity of capturing local context information but they are not as powerful\nas LSTMs in capturing the long-term context information.\nAlthough the receptive ﬁeld of CNNs can be expanded by\nmeans of stacking multiple convolutional layers or using dilated convolutional layers, the global context capturing issue still remains, especially for variant-sized text sentences,\nwhich hinders CNNs obtaining comparable performance as\nLSTMs for NER.\nIn this paper, we propose a CNN-based network for NER,\ni.e., Gated Relation Network (GRN), which is more powerful than common CNNs for capturing long-term context information. Different from RNNs that capture the long-term\ndependencies in a recurrent component, our proposed GRN\naims to capture the dependencies within a sentence by modelling the relations between any two words. Modelling word\nrelations permits GRN to compose global context features\nwithout regard to the limited receptive ﬁelds of CNNs, enabling it to capture the global context information. This allows GRN to reach comparable performance in NER versus\nLSTM-based models. Moreover, without any recurrent layers, GRN can be trained by feeding all words concurrently\ninto the neural network at one time, which can generally improve efﬁciency in training and test.\nSpeciﬁcally, the proposed GRN is customized into 4 layers, i.e., the representation layer, the context layer, the relation layer and the CRF layer. In the representation layer, like\nprevious works, a word embedding vector and a characterlevel embedding vector extracted by a CNN are used as\nword features. In the context layer, CNNs with various kernel sizes are employed to transform the word features from\nthe embedding space to the latent space. The various CNNs\ncan capture the local context information at different scales\nfor each word. Then, the relation layer is built on top of the\ncontext layer, which aims to compose a global context feature for a word via modelling its relations with all words in\nthe sentence. Finally, we adopt a CRF layer as the loss function to train GRN in an end-to-end manner.\nTo verify the effectiveness of the proposed GRN,\nwe conduct extensive experiments on two benchmark\nNER\ndatasets,\ni.e.,\nCoNLL-2003\nEnglish\nNER\nand\nOntoNotes 5.0. Experimental results indicate that GRN\ncan achieve state-of-the-art performance on both CoNLL2003 (F1=91.44 without external knowledge and F1=92.34\nwith ELMo (Peters et al. 2018) simply incorporated) and\nOntonotes 5.0 (F1=87.67), meaning that using GRN, CNNbased models can compete with LSTM-based ones for NER.\nMoreover, GRN can also enjoy lower time costs for training\nand test, compared to the most basic LSTM-based model.\nOur contributions are summarized as follows.\n• We propose a CNN-based network, i.e., gated relation\nnetwork (GRN) for NER. GRN is a simple but effective\nCNN-based architecture with a more powerful capacity\nof capturing the global context information in a sequence\nthan common CNNs.\n• We propose an effective approach for GRN to model the\nrelations between words, and then use them as gates to\nfuse local context features into global ones for incorporating long-term context information.\n• With extensive experiments, we demonstrate that the proposed CNN-based GRN can achieve state-of-the-art NER\nperformance comparable to LSTM-based models, while\nenjoying lower training and test time costs.",
        "related work": "Traditional NER systems mostly rely on hand-crafted features and task-speciﬁc knowledge. In recent years, deep neural networks have shown remarkable success in the NER\ntask, as they are powerful in capturing the syntactic dependencies and semantic information for a sentence. They can\nalso be trained in an end-to-end manner without involving\nsubtle hand-crafted features, thus relieving the efforts of feature engineering.\nLSTM-based NER System. Currently, most state-of-theart NER systems employ LSTM to extract the context information for each word. Huang et al. (2015) ﬁrstly proposed\nto apply a BiLSTM for NER and achieved a great success.\nLater Ma and Hovy (2016) and Chiu and Nichols (2016) introduced character-level representation to enhance the feature representation for each word and gained further performance improvement. MacKinlay et al. (2017) proposed to\nstack BiLSTMs with residual connections between different\nlayers of BiLSTM to integrate low-level and high-level features. Liu et al. (2018) further proposed to enhance the NER\nmodel with a task-aware language model.\nThough effective, the inherently recurrent nature of\nRNNs/LSTMs makes them hard to be trained with full parallelization. And thus here we propose a CNN-based network, i.e., gated relation network (GRN), to dispense with\nthe recurrence issue. And we show that the proposed GRN\ncan obtain comparable performance as those state-of-theart LSTM-based NER models while enjoying lower training\nand test time costs.\nLeveraging External Knowledge. It has been shown that\nexternal knowledge can greatly beneﬁt NER models. External knowledge can be obtained by means of external vocabulary resources or pretrained knowledge representation, etc.\nChiu and Nichols (2016) obtained F1=91.62% on CoNLL2003 by integrating gazetteers. Peters et al. (2017) adopted\na character-level language model pretrained on a large external corpus and gained substantial performance improvement. More recently, Peters et al. (2018) proposed ELMo, a\ndeep language model trained with billions of words, to generate dynamic contextual word features, and gained the latest state-of-the-art performance on CoNLL-2003 by incorporating it into a BiLSTM-based model. Our proposed GRN\ncan also incorporate external knowledge. Speciﬁcally, experiments show that, with ELMo incorporated, GRN can obtain\neven slightly superior performance on the same dataset.\nNon-Recurrent Networks in NLP. The efﬁciency issue of RNNs has started to attract attention from the NLP\ncommunity. Several effective models have also been proposed to replace RNNs. Gehring et al. (2017) proposed\na convolutional sequence-to-sequence model and achieved\nsigniﬁcant improvement in both performance and training\nspeed. Vaswani et al. (2017) used a self-attention mechanism for machine translation and obtained remarkable translation performance. Our proposed GRN is also a trial to investigate whether CNNs can get comparable NER performances as LSTM-based models with lower time costs for\ntraining and test. And different from (Gehring et al. 2017;\nVaswani et al. 2017), we do not adopt the attention mechanism here, though GRN is a general model and can be customized into the attention mechanism easily.\nIterated dilated CNN (ID-CNN), proposed by Strubell et\nal. (2017), also aims to improve the parallelization of NER\nmodels by using CNNs, sharing similar ideas to ours. However, although ID-CNN uses dilated CNNs and stacks layers of them, its capacity of modelling the global context information for a variant-sized sentence is still limited, and\nthus its performance is substantially inferior to those of the\nstate-of-the-art LSTM-based models. In contrast, our proposed GRN can enhance the CNNs with much more capacity to capture global context information, which is mainly\nattributed to that the relation modelling approach in GRN allows to model long-term dependencies between words without regard to the limited receptive ﬁelds of CNNs. And thus\nGRN can achieve signiﬁcantly superior performance than\nID-CNN.",
        "proposed model": "In this section, we discuss the overall NER system utilizing the proposed GRN in detail. To ease the explanation, we\norganize our system with 4 speciﬁc layers, i.e., the representation layer, the context layer, the relation layer and the CRF\nlayer. We will elaborate on these layers from bottom to top\nin the following subsections.\nRepresentation Layer\nRepresentation layer aims to provide informative features\nfor the upper layers. The quality of features has great impacts on the system’s performance. Traditionally, features\nare hand-crafted obeying some elaborative rules that may\nnot be applicable to other domains. Therefore, currently\nmany state-of-the-art approaches tend to employ deep neural\nnetworks for automatic feature engineering.\nAs previous works like (Ye and Ling 2018), the representation layer in GRN is comprised of only word-level features and character-level features. In this paper, we use pretrained static word embeddings, i.e., GloVe1 (Pennington,\nSocher, and Manning 2014), as the initialized word-level\nfeature. And during training, they will be ﬁne-tuned. Here\nwe denote the input sentence s as s = {s1, s2, ..., sT },\nwhere si with i = 1, 2, . . . , T denotes the ith word in the\nsentence, and T is the length of the sentence. We also use\ny = {y1, y2, ..., yT } to denote the corresponding entity labels for all words, i.e., yi corresponding to si. With each\nword si represented as a one-hot vector, its word-level feature wi is extracted as below:\nwi = E(si)\n(1)\nwhere E is the word embedding dictionary, initialized by the\nGloVe embeddings and ﬁne-tuned during training.\nFurthermore, we augment the word representation with\nthe character-level feature, which can contribute to ease\nthe out-of-vocabulary problem (Rei, Crichton, and Pyysalo\n2016). Same as (Ma and Hovy 2016), here we adopt a CNN\nto extract the character-level feature for each word si, as illustrated in Figure 1.\nSpeciﬁcally, the j-th character in the word si containing n\ncharacters is ﬁrstly represented as an embedding vector ci\nj in\na similar manner as Eq. 1, except that the character embedding dictionary is initialized randomly. Then we use a convolutional layer to involve the information of neighboring\ncharacters for each character, which is critical to exploiting\nn-gram features. Finally, we perform a max-over-time pooling operation to reduce the convolution results into a single\nembedding vector ci:\n¯ci\nj = conv([ci\nj−k/2, ..., ci\nj, ..., ci\nj+k/2])\nci = pooling([¯ci\n0, ..., ¯ci\nj, ..., ¯ci\nn])\n(2)\nwhere k is the kernel size of the convolutional layer. Here\nwe ﬁx k = 3 as (Ye and Ling 2018).\n1http://nlp.stanford.edu/projects/glove/\nmax \npooling\n𝑐0\n𝑖\n𝑐𝑖\n𝑐𝑛+1\n𝑖\n𝑐1\n𝑖\n𝑐2\n𝑖\n𝑐3\n𝑖\n𝑐𝑛−2\n𝑖\n𝑐𝑛−1\n𝑖\n𝑐𝑛𝑖\nҧ𝑐1\n𝑖\nҧ𝑐3\n𝑖\nҧ𝑐𝑛−2\n𝑖\nҧ𝑐𝑛𝑖\n[ ҧ𝑐1\n𝑖\nҧ𝑐2\n𝑖\nҧ𝑐𝑛−1\n𝑖\nҧ𝑐𝑛𝑖 ]\nCNN filters\nfeatures\nCharacter embedding layer\nConvolutional layer\nMax-over-time pooling\nFigure 1: CNN to extract the character-level feature for a\nword. Best see in color.\nNote that RNNs, especially LSTMs/BiLSTMs are also\nsuitable to model the character-level feature. However, as\nrevealed in (Yang, Liang, and Zhang 2018), CNNs are as\npowerful as RNNs in modelling the character-level feature.\nBesides, CNNs can probably enjoy higher training and test\nspeed than RNNs. Therefore, in this paper we just adopt a\nCNN to model the character-level feature.\nWe regard ci as the character-level feature for the word si.\nthen we concatenate it to the word-level feature wi to derive\nthe ﬁnal word feature zi = [ci, wi].\nContext Layer\nContext layer aims to model the local context information\namong neighboring words for each word. The local context\nis critical for predicting labels, regarding that there could exist strong dependencies among neighboring words in a sentence. For example, a location word often co-occurs with\nprepositions like in, on, at. Therefore, it is of necessity to\ncapture the local context information for each word.\nAnd it is obvious that the local dependencies are not limited within a certain distance. Therefore, we should enable\nthe context layer to be adaptive to different scales of local\ninformation. Here, like InceptionNet (Szegedy et al. 2015),\nwe design the context layer with different branches, each\nbeing comprised of one certain convolutional layer. Figure 2\nshows the computational process of the context layer.\nSpeciﬁcally, we use three convolutional layers with the\nkernel size being 1, 3, 5, respectively. After obtaining the\nword feature Z = {z1, z2, ..., zT } of a sentence s, each\nbranch ﬁrstly extracts the local information ¯zk\ni within a\nwindow-size k for each word si. Then a max-pooling operation is employed to select the strongest channel-wise signals\nfrom all branches. To add the non-linear characteristic, we\nalso apply tanh after each branch.\n¯zk\ni = convk([zi−k/2, ..., zi, ..., ci+k/2])\nxi = pooling([tanh(¯z1\ni ), tanh(¯z3\ni ), tanh(¯z5\ni )])\n(3)\nwhere k ∈ {1, 3, 5} is the kernel size. For each k, we use\nk/2 zero-paddings to ensure that each word can get the corresponding context feature. Here, we consider the output xi\nof the context layer as the context feature for word si.\nAlthough with various kernel sizes, the context layer can\ncapture different kinds of local context information, it still\n𝑧1\n𝑧2\n𝑧3\n𝑧𝑇−2\n𝑧𝑇−1\n𝑧𝑇\nBranches with convolutions\nMax pooling\nWord feature 𝑍\nҧ𝑧1\nҧ𝑧3\nҧ𝑧5\n𝑥1\n𝑥2\n𝑥𝑇\n𝑥𝑇−1\nFigure 2: Branches with various convolutions for extracting\nthe local context feature for words. Best see in colors.\nstruggles to capture the global one. However, we will show\nthat with the gated relation layer described in the following\nsubsection, the global context information can be realized\nby a fusion of the local one, thus tackling the shortcoming\nof the context layer.\nRelation Layer\nIt has been shown that both short-term and long-term context information in a sequence is very critical in sequence\nlearning tasks. LSTMs leverage the memory and the gating\nmechanism (Hochreiter and Schmidhuber 1997) to capture\nboth context information and gain signiﬁcant success. However, conventional CNNs cannot well capture the long-term\ncontext information owing to the limited receptive ﬁelds,\nand thus existing CNN-based NER models cannot achieve\ncomparable performance as LSTM-based ones.\nIn this subsection, we introduce the gated relation layer\nin our proposed GRN, which aims to enhance the conventional CNNs with global context information. Speciﬁcally, it\nmodels the relations between any two words in the sentence.\nThen, with the gating mechanism, it composes a global context feature vector by weighted-summing up the relation\nscores with their corresponding local context feature vectors, as shown in Figure 3. Similar to the attention mechanism, our proposed GRN is effective in modelling long-term\ndependencies without regard to the limited CNN receptive\nﬁelds. And importantly, GRN can allow computations to be\nperformed in parallel across the entire sentence, which can\ngenerally help to reduce the time costs for training and test.\nGiven the local context features x = {x1, x2, ..., xT }\nfrom the context layer for a sentence s, the relation layer\nﬁrstly computes the relation score vector rij between any\ntwo words si and sj, which is of the same dimension as\nany xi. Speciﬁcally, it ﬁrstly concatenates the corresponding context features xi and xj, and then uses a linear function with the weight matrix Wrx and the bias vector brx to\nobtain rij:\nrij = Wrx[xi; xj] + brx\n(4)\nLike (Santoro et al. 2017), we can directly average these\nrelation score vectors as follows:\nri = 1\nT\nT\nX\nj=1\nrij\n(5)\nwhere ri is the fused global context feature vector for the\nword si by the direct feature fusion operation, i.e., averaging\nin Eq. 5. However, considering that non-entity words generally take up the majority of a sentence, this operation may\nintroduce much noise and mislead the label prediction. To\ntackle that, we further introduce the gating mechanism, and\nenable the relation layer to learn to select other dependent\nwords adaptively. Speciﬁcally, for the word si, we ﬁrstly\nnormalize all its relation score vectors rij with a sigmoid\nfunction to reduce their biases. Then we sum up the normalized relation score vectors rij with the corresponding local\ncontext feature vector xj ∈ x = {x1, x2, ..., xT } of any\nother word sj. And similar to Eq. 5, ﬁnally we normalize\nthe sum by the length of the sentence, i.e., T.\nri = 1\nT\nT\nX\nj=1\nσ(rij) ⊙ xj\n(6)\nwhere σ is a gate using sigmoid function, and ⊙ means\nelement-wise multiplication. Note that rij is asymmetrical\nand different from rji, and the relation vector w.r.t si itself,\ni.e., rii, is also incorporated in the equation above. Therefore, with ri consisting of all the information of other words\nin the sentence, it can be seen as the global context feature\nvector for si.\nIn a way, GRN can be seen as a channel-wise attention\nmechanism (Chen et al. 2017). However, instead of using a\nsoftmax function, we leverage the gating mechanism on the\nrelation score vectors to decide how all the words play a part\nin predicting the label for the word si. We can also customize\nEq. 6 to the formula of attention with gating mechanism,\nwhere a gate is used to compute the attention weight for a\nword:\nαij = σ(Wx[xi; xj] + bx)\nri = 1\nT\nT\nX\nj=1\nαij ∗ xj\n(7)\nwhere αij ∈ R1 is an attention weight rather than a vector.\nTo distinguish from the proposed GRN (i.e., Eq. 6), we\nname Eq. 5 as Direct Fusion Network (DFN) and Eq. 7 as\nGated Attention Network (GAttN). We will consider DFN\nand GAttN as two of our baseline models to show the superiority of the proposed GRN.\nHere we also add a non-linear function for ri as follows.\npi = tanh(ri)\n(8)\nAnd we deﬁne pi as the ﬁnal predicting feature for word si.\nCRF Layer\nModelling label dependencies is crucial for NER task (Ma\nand Hovy 2016; Liu et al. 2018). Following (Ma and Hovy\n2016; Huang, Xu, and Yu 2015), we employ a conditional\nrandom ﬁeld (CRF) layer to model the label dependencies\nand calculate the loss for training GRN.\nFormally, for a given sentence s = {s1, s2, ..., sT } and its\ngeneric sequence of labels y = {y1, y2, ..., yT }, we ﬁrstly\nuse Y(s) to denote the set of all possible label sequences for\n𝑥1\n𝑥2\n𝑥3\n𝑥𝑇−2\n𝑥𝑇−1\n𝑥𝑇\n𝑟2\n𝑟3\n𝑟𝑇−2\n𝑟𝑇−1\n𝑟𝑇\n𝑟1\nRelations\nGating\nLocal context feature x\nrelation matrix: {𝑟𝑖𝑗}\n𝐸𝑞. 4\nFigure 3: Gated relation layer in GRN for composing the\nglobal context feature for each word. rij denotes the relation\nscore vector between word si and word sj. Best see in color.\ns. The CRF model deﬁnes a family of conditional probability p(y|s) over all possible label sequences y given s:\np(y|s) =\nQT\ni=1 φi(yi−1, yi, s)\nP\ny′∈Y(s)\nQT\ni=1 φi(y′\ni−1, y′\ni, s)\n(9)\nwhere φi(yi−1, yi, s) = exp(f(si, y′, y)) with f being a\nfunction that maps words into labels:\nf(si, y′, y) = Wypi + by′,y\n(10)\nwhere pi is derived as Eq. 8, Wy is the predicting weights\nw.r.t y and by′,y is the transition weight from y′ to y. Both\nWy and by′,y are parameters to be learned.\nLoss of the CRF layer is formulated as follows.\nL = −\nX\ns\nlog p(y|s)\n(11)\nAnd for decoding, we aim to ﬁnd the label sequence y∗\nwith the highest conditional probability:\ny∗ = arg maxy∈Y(s)p(y|s)\n(12)\nwhich can be efﬁciently derived via Viterbi decoding.",
        "experiments": "To verify the effectiveness of the proposed GRN, we conduct\nextensive experiments on two benchmark NER datasets:\nCoNLL-2003 English NER (Tjong Kim Sang and De Meulder 2003) and OntoNotes 5.0 (Hovy et al. 2006; Pradhan et\nal. 2013).\n• CoNLL-2003 English NER consists of 22,137 sentences\ntotally and is split into 14,987, 3,466 and 3,684 sentences for the training set, the development set and the\ntest set, respectively. It includes annotations for 4 types of\nnamed entities: PERSON, LOCATION, ORGANIZATION\nand MISC.\n• OntoNotes 5.0 consists of much more (76,714) sentences\nfrom a wide variety of sources (telephone conversation,\nnewswire, etc.). Following (Chiu and Nichols 2016), we\nuse the portion of the dataset with gold-standard named\nentity annotations, and thus excluded the New Testaments\nportion. It also contains much more kinds of entities, including CARDINAL, MONEY, LOC, PRODUCT, etc.\ndataset\nTrain\nDev\nTest\nCoNLL-2003\nSentence\n14,987\n3,466\n3,684\nToken\n204,567\n51,578\n46,666\nEntity\n23,499\n5,942\n5,648\nOntoNotes 5.0\nSentence\n59,924\n8,528\n8,262\nToken\n1,088,503\n147,724\n152,728\nEntity\n81,828\n11,066\n11,257\nTable 1: Statistics of CoNLL-2003 and Ontonotes 5.0.\nTable 1 shows some statistics of both datasets. Following (Ma and Hovy 2016), we use the BIOES sequence labelling scheme instead of BIO for both datasets to train models. As for test, we convert the prediction results back to the\nBIO scheme and use the standard CoNLL-2003 evaluation\nscript to measure the NER performance, i.e., F1 scores, etc.\nNetwork Training\nWe implement our proposed GRN with the Pytorch library (Paszke et al. 2017). And we set the parameters below\nfollowing (Ma and Hovy 2016).\nWord Embeddings. The dimension of word embedding\nis set as 100. And as mentioned, we initialize it with Stanford’s publicly available GloVe 100-dimensional embeddings. We include all words of GloVe when building the vocabulary, besides those words appearing at least 3 times in\nthe training set. For words out of the vocabulary (denoted as\nUNK) or those not in GloVe, we initialize their embeddings\nwith kaiming uniform initialization (He et al. 2015).\nCharacter Embeddings. We set the dimension of character embeddings as 30, and also initialize them with kaiming\nuniform initialization.\nWeight Matrices and Bias Vectors. All weight matrices\nin linear functions and CNNs are initialized with kaiming\nuniform initialization, while bias vectors are initialized as 0.\nOptimization. We employ mini-batch stochastic gradient\ndescent with momentum to train the model. The batch size is\nset as 10. The momentum is set as 0.9 and the initial learning\nrate is set as 0.02. We use learning rate decay strategy to update the learning rate during training. Namely, we update the\nlearning rate as\n0.02\n1+ρ∗t at the t-th epoch with ρ = 0.02. We\ntrain each model on training sets with 200 epochs totally, using dropout = 0.5. For evaluation, we select its best version\nwith the highest performance on the development set and\nreport the corresponding performance on the test set. To reduce the model bias, we carry out 5 runs for each model and\nreport the average performance and the standard deviation.\nNetwork Structure. The output channel number of the\nCNN in Eq. 2 and Eq. 3 is set as 30 and 400, respectively.\nPerformance Comparison\nHere we ﬁrst focus on the NER performance comparison\nbetween the proposed GRN and the existing state-of-the-art\napproaches.\nCoNLL-2003. We compare GRN with various state-ofthe-art LSTM-based NER models, including (Liu et al.\n2018; Ye and Ling 2018), etc. We also compare GRN with\nID-CNN (Strubell et al. 2017), which also adopts CNNs\nwithout recurrent layers for NER. Furthermore, considering that some state-of-the-art NER models exploit external\nModel\nMean(±std) F1\nMax F1\nMean P/R\n(Collobert et al. 2011)\n88.67\n(Luo et al. 2015)\n89.9\n90.0 / 89.9\n(Chiu and Nichols 2016)\n90.91 ± 0.20\n90.75 / 91.08\n(Zhuo et al. 2016)\n88.12\n(Rei, Crichton, and Pyysalo 2016)\n84.09\n(Lample et al. 2016)\n90.94\n(Ma and Hovy 2016)\n91.21\n91.35 / 91.06\n(Rei 2017)\n86.26\n(Zukov-Gregoric et al. 2017)\n89.83\n(Liu, Baldwin, and Cohn 2017)\n89.5\n(Peters et al. 2017)\n90.87 ± 0.13\n(Liu et al. 2018)\n91.24 ± 0.12\n91.35\n(Ye and Ling 2018)\n91.38 ± 0.10\n91.53\nID-CNN (Strubell et al. 2017)\n90.54 ± 0.18\nCNN-BiLSTM-CRF\n91.17 ± 0.18\n91.45\n90.98 / 91.35\nGRN\n91.44 ± 0.16\n91.67\n91.31 / 91.57\n(Collobert et al. 2011)*\n89.59\n(Luo et al. 2015)*\n91.2\n91.5 / 91.4\n(Chiu and Nichols 2016)*\n91.62 ± 0.33\n91.39 / 91.85\n(Peters et al. 2017)*\n91.93 ± 0.19\n(Tran, MacKinlay, and Yepes 2017)*\n91.66\n(Yang, Salakhutdinov, and Cohen 2017)*\n91.26\n(Peters et al. 2018)*\n92.22 ± 0.10\nGRN*\n92.34 ± 0.10\n92.45\n92.04 / 92.65\nTable 2: Performance comparison on CoNLL-2003. * indicates models utilizing external knowledge besides the\nCoNLL-2003 training set and pre-trained word embeddings.\nP/R denotes precision and recall.\nknowledge to boost their performance, here we also report\nthe performance of GRN with ELMo (Peters et al. 2018) incorporated as the external knowledge. Note that ELMo is\ntrained on a large corpus of text data and can generate dynamic contextual features for words in a sentence. Here we\nsimply concatenate the output ELMo features to the word\nfeature in GRN. The experimental results are reported in Table 2, which also includes the max F1 scores, mean precision\nand recall values if available. Note that CNN-BiLSTM-CRF\nis our re-implementation of (Ma and Hovy 2016), and we\nobtain comparable performance as that reported in the paper. Therefore, by default we directly compare GRN with the\nreported performance of compared baselines. It should also\nbe noticed that, since the relation layer in GRN can be related to the attention mechanism, here we also include some\nattention-based baselines, i.e.,, (Rei, Crichton, and Pyysalo\n2016) and (Zukov-Gregoric et al. 2017).\nAs shown in Table 2, compared with those LSTM-based\nNER models, the proposed GRN can obtain comparable or\neven slightly superior performance, with or without the external knowledge, which well demonstrates the effectiveness\nof GRN. And compared with ID-CNN, our proposed GRN\ncan defeat it at a great margin in terms of F1 score. We also\ntry to add ELMo to the latest state-of-the-art model of (Ye\nand Ling 2018) based on their published codes, and we ﬁnd\nthat the corresponding F1 score is 91.79 ± 0.08, which is\nsubstantially lower than that of GRN.\nOntoNotes 5.0. On OntoNotes 5.0, we compare the proposed GRN with NER models that also reported performance on it, including (Chiu and Nichols 2016; Shen et al.\n2017; Durrett and Klein 2014), etc. As shown in Table 3,\nGRN can obtain the state-of-the-art NER performance on\nOntoNotes 5.0, which further demonstrates its effectiveness.\nOverall, the comparison results on both CoNLL-2003 and\nOntoNotes 5.0 well indicate that our proposed GRN can\nModel\nMean(±std) F1\nMean P/R\n(Passos, Kumar, and McCallum 2014)\n82.30\n(Durrett and Klein 2014)\n84.04\n85.22 / 82.89\n(Chiu and Nichols 2016)\n86.28 ± 0.26\n86.04 / 86.53\n(Shen et al. 2017)\n86.63 ± 0.49\nID-CNN (Strubell et al. 2017)\n86.84 ± 0.19\nGRN\n87.67 ± 0.17\n87.79 / 87.56\nTable 3: Performance comparison on OntoNotes 5.0. P/R denotes precision and recall.\nModel\nMean(±std) F1\nF1 Drop\ncontext\nGRN w/o context\n88.36 ± 0.21\n3.08\nGRN w/ branch3\n90.88 ± 0.22\n0.56\nrelation\nGRN w/o relation\n90.13 ± 0.28\n1.31\nDFN\n90.72 ± 0.06\n0.72\nGAttN\n87.11 ± 0.25\n4.33\nFull\nGRN\n91.44 ± 0.16\n0\nTable 4: Ablation study on CoNLL-2003.\nModel\nMean(±std) F1\nF1 Drop\ncontext\nGRN w/o context\n82.21 ± 0.23\n5.46\nGRN w/ branch3\n86.66 ± 0.21\n1.01\nrelation\nGRN w/o relation\n85.87 ± 0.16\n1.80\nDFN\n85.81 ± 0.14\n1.86\nGAttN\n79.83 ± 0.37\n7.83\nFull\nGRN\n87.67 ± 0.17\n0\nTable 5: Ablation study on OntoNotes 5.0.\nachieve state-of-the-art NER performance with or without\nexternal knowledge. It demonstrates that, using GRN, CNNbased models can compete with LSTM-based ones for NER.\nAblation Study\nHere we study the impact of each layer on GRN. Firstly,\nwe analyze the context layer by introducing two baseline\nmodels: (1) GRN w/o context: wiping out the context layer\nand building the relation layer on top of the representation\nlayer directly; (2) GRN w/ branch3: removing branches in\nthe context layer, except the one with kernel size = 3. Then\nto analyze the relation layer and the importance of gating\nmechanism in it, we compare GRN with: (1) GRN w/o relation: wiping out the relation layer and directly building the\nCRF layer on top of the context feature; (2) DFN (see Eq. 5);\n(3) GAttN (see Eq. 7). All compared baselines use the same\nexperimental settings as GRN. Table 4 and Table 5 report the\nexperimental results on both datasets, where the last column\nshows the absolute performance drops compared to GRN.\nAs shown in Table 4 and Table 5, when reducing the number of branches in the context layer, GRN w/o context and\nGRN w/ branch3 drop signiﬁcantly, which indicates that\nmodelling different scales of local context information plays\na crucial role for NER.\nCompared with GRN w/o relation, DFN and GAttN, the\nproposed GRN defeats them at a substantial margin in terms\nof F1 score, which demonstrates that the proposed gated relation layer is beneﬁcial to the performance improvement.\nThe comparison also reveals that the channel-wise gating\nmechanism in GRN is more powerful than the gated attenCoNLL-2003\nOntoNotes 5.0\nTraining\n1.16x\n1.15x\nTest\n1.19x\n1.08x\nTable 6: Training/test speedup of GRN compared with\nCNN-BiLSTM-CRF.\ntion approach (i.e., Eq. 7) and the direct fusion approach\n(i.e., Eq. 5) under the same experimental settings for NER.\nTraining/Test Time Comparison\nIn this section, we further compare the training and test time\ncosts of the proposed GRN with those of CNN-BiLSTMCRF, which is the most basic LSTM-based NER model\nachieving high performance. We conduct our experiments\non a physical machine with Ubuntu 16.04, 2 Intel Xeon E52690 v4 CPUs, and a Tesla P100 GPU. For fair comparison,\nwe keep the representation layer and the CRF layer the same\nfor both models, so that the input and output dimensions for\nthe “BiLSTM layer” in CNN-BiLSTM-CRF would be identical to those of the “context layer + relation layer” in GRN.\nWe train both models with random initialization for a total\nof 30 epochs, and after each epoch, we evaluate the learned\nmodel on the test set. For both training and test, batch size\nis set as 10 as before. And here we use the average training time per epoch and the average test time to calculate\nspeedups.\nAs shown in Table 6, GRN can obtain a speedup of more\nthan 1.15 during training and around 1.10 during test on\nboth datasets. The speedup may seem not so signiﬁcant, because the time costs reported here also include those consumed by common representation layer, CRF layer, etc.\nFor reference, the fast ID-CNN with a CRF layer (i.e.,\nID-CNN-CRF) (Strubell et al. 2017) was reported to have\na test-time speedup of 1.28 over the basic BiLSTM-CRF\nmodel on CoNLL-2003. Compared to ID-CNN-CRF, GRN\nsacriﬁces some speedup for better performance, and the\nspeedup gap between both is still reasonable. We can also\nsee that the speedup on CoNLL-2003 is larger than that\non OntoNotes 5.0, which can be attributed to that the average sentence length of CoNLL-2003 (∼ 14) is smaller than\nthat of OntoNotes 5.0 (∼ 18) and thus the relation layer in\nGRN would cost less time for the former. The results above\ndemonstrate that the proposed GRN can generally bring efﬁciency improvement over LSTM-based methods for NER,\nvia fully exploiting the GPU parallelism.\nWord Relation Visualization\nSince the proposed GRN aims to boost the NER performance by modelling the relations between words, especially\nlong-term ones, we can visualize the gating output in the relation layer to illustrate the interpretability of GRN. Specifically, we utilize the L2 norm of rij to indicate the extent of\nrelations between the word si and the word sj. Then we further normalize the values into [0, 1] to build a heat map. Figure 4 shows a visualization sample. We can ﬁnd out that the\nentity words (y-axis) are more related to other entity words\nas well, even though they may be “far away” from each other\nFigure 4: Word relation visualization: the x-axis shows the\nsentence and the y-axis shows the entity words in it. Regions\nwith deeper color means stronger relations between the corresponding pair of words.\nin the sentence, like the 1st word “Sun” and the 8th word\n“Sidek” in the sample. Note that “Sun” and “Sidek” are not\nin an identical receptive ﬁeld of any CNN used in our experiments, but their strong correlation can still be exploited\nwith the relation layer in GRN. That concretely illustrates\nthat, by introducing the gated relation layer, GRN is able to\ncapture the long-term dependency between words.",
        "conclusion": "In this paper, we propose a CNN-based network, i.e.,\ngated relation network (GRN), for named entity recognition (NER). Unlike the dominant LSTM-based NER models which process a sentence in a sequential manner, GRN\ncan process all the words concurrently with one forward\noperation and thus can fully exploit the GPU parallelism\nfor potential efﬁciency improvement. Besides, compared\nwith common CNNs, GRN has a better capacity of capturing long-term context information. Speciﬁcally, GRN introduces a gated relation layer to model the relations between\nany two words, and utilizes gating mechanism to fuse local\ncontext features into global ones for all words. Experiments\non CoNLL-2003 English NER and Ontonotes 5.0 datasets\nshow that, GRN can achieve state-of-the-art NER performance with or without external knowledge, meaning that\nusing GRN, CNN-based models can compete with LSTMbased models for NER. Experimental results also show that\nGRN can generally bring efﬁciency improvement for training and test.",
        "summary_en": "The dominant approaches for named entity recognitionm (NER) mostly adopt complex recurrent neural networks (RNN), e.g., long-short-term-memory (LSTM). However, RNNs are limited by their recurrent nature in terms of computational efficiency. In contrast, convolutional neural networks (CNN) can fully exploit the GPU parallelism with their feedforward architectures. However, little attention has been paid to performing NER with CNNs, mainly owing to their difficulties in capturing the long-term context information in a sequence. Therefore,this paper proposes a simple but effective CNN-based network for NER, i.e., gated relation network (GRN), which is more capable than common CNNs in capturing long-term context. Specifically, in GRN the paper firstly employs CNNs to explore the local context features of each word. Then the paper models the relations between words and use them as gates to fuse local context features into global ones for predicting labels. Without using recurrent layers that process a sentence in a sequential manner, the GRN allows computations to be performed in parallel across the entire sentence. Experiments on two benchmark NER datasets (i.e., CoNLL2003 and Ontonotes 5.0) show that, the proposed GRN can achieve state-of-the-art performance with or without external knowledge. It also enjoys lower time costs to train and test.",
        "summary_zh": "这篇论文介绍了一种简单但有效的基于CNN的NER网络，即门控关系网络（GRN），用于命名实体识别，旨在解决传统RNN在计算效率上的限制以及CNN在捕获长期上下文信息方面的困难。GRN比普通CNN更能捕捉长期上下文，首先利用CNN探索每个词的局部上下文特征，然后建模单词之间的关系，并将它们用作门来将局部上下文特征融合到全局特征中进行标签预测。实验结果表明，GRN在两个基准NER数据集上均取得了最先进的性能，并且在训练和测试时具有更低的时间成本。"
    },
    {
        "title": "Unified Named Entity Recognition as Word-Word Relation Classification",
        "abstract": "So far, named entity recognition (NER) has been involved with three major types, including flat, overlapped (aka. nested), and discontinuous NER, which have mostly been studied individually. Recently, a growing interest has been built for unified NER, tackling the above three jobs concurrently with one single model. Current best-performing methods mainly include span-based and sequence-to-sequence models, where unfortunately the former merely focus on boundary identification and the latter may suffer from exposure bias. In this work, we present a novel alternative by modeling the unified NER as word-word relation classification, namely W2NER. The architecture resolves the kernel bottleneck of unified NER by effectively modeling the neighboring relations between entity words with Next-Neighboring-Word (NNW) and Tail-Head-Word-* (THW-*) relations. Based on the W2NER scheme we develop a neural framework, in which the unified NER is modeled as a 2D grid of word pairs. We then propose multi-granularity 2D convolutions for better refining the grid representations. Finally, a co-predictor is used to sufficiently reason the word-word relations. We perform extensive experiments on 14 widely-used benchmark datasets for flat, overlapped, and discontinuous NER (8 English and 6 Chinese datasets), where our model beats all the current top-performing baselines, pushing the state-of-the-art performances of unified NER.",
        "introduction": "Named entity recognition (NER) has long been a fundamental task in natural language processing (NLP) community,\ndue to its wide variety of knowledge-based applications,\ne.g., relation extraction (Wei et al. 2020; Li et al. 2021b), entity linking (Le and Titov 2018; Hou et al. 2020), etc. Studies\nof NER have gradually evolved initially from the flat NER\n(Lample et al. 2016; Strubell et al. 2017), late to the overlapped NER (Yu et al. 2020; Shen et al. 2021), and recently\nto the discontinuous NER (Dai et al. 2020; Li et al. 2021a).\nSpecifically, flat NER simply detects the mention spans and\ntheir semantic categories from text, while the problems in\nI  am  having  aching  in  legs  and  shoulders \n e1\n e2\n(a)\n(b)\nNNW\nNNW\nNNW\nTHW-S\nTHW-S\nI  am  having  aching  in  legs  and  shoulders \nFigure 1: (a) An example to show three types of NER.\ne1 is a flat entity overlapped with a discontinuous entity e2 at the span “aching in”. (b) We formalize three\nNER subtasks as word-word relation classification, where\nthe Next-Neighboring-Word (NNW) relation indicates that a word pair are successively joint as a segment of\nan entity (e.g., aching→in), and the Tail-Head-Word-*\n(THW-*) relation implies the edges where the tail words\nconnect to the head words (e.g., legs→aching) as an entity\nwith “*” type (e.g., Symptom).\noverlapped and discontinuous NER become more complicated, i.e., overlapped entities contain the same tokens,1 and\ndiscontinuous entities entail non-adjacent spans, as illustrated in Figure 1.\nPrevious methods for multi-type NER can be roughly\ngrouped into four major categories: 1) sequence labeling, 2)\nhypergraph-based methods, 3) sequence-to-sequence methods and 4) span-based methods. A majority of initial work\nformalizes NER as a sequence labeling problem (Lample\net al. 2016; Zheng et al. 2019; Tang et al. 2018; Strakov´a\net al. 2019), assigning a tag to each token. However, it is\ndifficult to design one tagging scheme for all NER subtasks.\nThen hypergraph-based models are proposed (Lu and Roth\n2015; Wang and Lu 2018; Katiyar and Cardie 2018) to represent all entity spans, which however suffer from both the\nspurious structure and structural ambiguity issue during inference. Recently, Yan et al. (2021) propose a sequence-tosequence (Seq2Seq) model to directly generate various entities, which unfortunately potentially suffers from the decoding efficiency problem and certain common shortages of\nSeq2Seq architecture, e.g., exposure bias. Span-based meth1Without losing generality, “nested” can be seen as a special\ncase of “overlapped” (Zeng et al. 2018; Dai 2018; Fei et al. 2020).\nods (Luan et al. 2019; Li et al. 2021a) are another state-ofthe-art (SoTA) approaches for unified NER, enumerating all\npossible spans and conduct span-level classification. Yet the\nspan-based models can be subject to maximal span lengths\nand lead to considerable model complexity due to the enumerating nature. Thus, designing an effective unified NER\nsystem still remains challenging.\nMost of the existing work has paid the major focus on\nhow to accurately identify the entity boundary, i.e., the kernel problem of NER, especially for flat one (Strakov´a et al.\n2019; Fei et al. 2021). However, after carefully rethinking\nthe common characteristics of all three types of NER, we\nfind that the bottleneck of unified NER more lies in the\nmodeling of the neighboring relations between entity words.\nSuch adjacency correlations essentially describe the semantic connectivity between the partial text segments, which especially plays the key role for the overlapping and discontinuous ones. As exemplified in Figure 1(a), it could be effortless to detect the flat mention “aching in legs”, since its constituent words all are naturally adjacent. But, to detect out\nthe discontinuous entity “aching in shoulders”, effectively\ncapturing the semantic relations between the neighboring\nsegments of “aching in” and “shoulders” is indispensable.\nOn the basis of the above observation, we in this paper investigate an alternative unified NER formalism with a\nnovel word-word relation classification architecture, namely\nW2NER. Our method resolves the unified NER by effectively modeling both the entity boundary identification as\nwell as the neighboring relations between entity words.\nSpecifically, W2NER makes predictions for two types\nof relations, including the Next-Neighboring-Word\n(NNW) and the Tail-Head-Word-* (THW-*), as illustrated in Figure 1(b). The NNW relation addresses entity\nword identification, indicating if two argument words are adjacent in an entity (e.g., aching→in), while the THW-* relation accounts for entity boundary and type detection, revealing if two argument words are the tail and head boundaries\nrespectively of “*” entity (e.g., legs→aching, Symptom).\nBased on the W2NER scheme, we further present a neural framework for unified NER (cf. Figure 3). First, BERT\n(Devlin et al. 2019) and BiLSTM (Lample et al. 2016) are\nused to provide contextualized word representations, based\non which we construct a 2-dimensional (2D) grid for word\npairs. Afterwards, we design multi-granularity 2D convolutions to refine the word-pair representations, effectively capturing the interactions between both the close and distant\nword pairs. A co-predictor finally reasons the word-word relations and produces all possible entity mentions, in which\nthe biaffine and the multi-layer perceptron (MLP) classifiers\nare jointly employed for the complementary benefits.\nWe conduct extensive experiments on 14 datasets, ranging\nfrom 2 English and 4 Chinese datasets for flat NER, 3 English and 2 Chinese datasets for overlapped NER, 3 English\ndatasets for discontinuous NER. Compared with 12 baselines for flat NER, 7 baselines for overlapped NER, 7 baselines for discontinuous NER, our model achieves the best\nperformances on all the datasets, becoming the new SoTA\nmethod of unified NER. Our contributions include:\n• We present an innovative method that casts unified NER\nI\nam\nhaving\naching\nin\nlegs\nand\nshoulders\nNNW\nNNW\nNNW\nTHW-S\nTHW-S\nFigure 2: An example to show our relation classification\nmethod for NER. We leverage a word-pair grid to visualize the relations between each word pair. NNW denotes the Next-Neighboring-Word relation and THWS denotes the Tail-Head-Word relation that exists in\na “Symptom” entity. To avoid the sparsity of relation instances, NNW and THW relations are tagged in the upper\nand lower triangular regions.\nas word-word relation classification, where both the relations between boundary-words and inside-words of entities\nare fully considered.\n• We develop a neural framework for unified NER, in\nwhich we newly propose a multi-granularity 2D convolution\nmethod for sufficiently capturing the interactions between\nclose and distant words.\n• Our model pushes current SoTA performances of NER\non total 14 datasets. Our code is available at https://github.\ncom/ljynlp/W2NER.",
        "ner as word-word relation classification": "Flat, overlapped, discontinuous NER can be formalized as\nfollows: given an input sentence consisting of N tokens or\nwords X = {x1, x2, ..., xN}, the task aims to extract the relations R between each token pairs (xi, xj), where R is predefined, including NONE, Next-Neighboring-Word\n(NNW), and Tail-Head-Word-* (THW-*). These relations can be explained as below and we also give an example\nas demonstrated in Figure 2 for better understanding.\n• NONE, indicating that the word pair does not have any\nrelation defined in this paper.\n• Next-Neighboring-Word: the NNW relation indicates that the word pair belongs to an entity mention, and\nthe word in certain row of the grid has a successive word\nin certain column of the grid.\n• Tail-Head-Word-*: the THW relation indicates that\nthe word in certain row of the grid is the tail of an entity\nmention, and the word in certain column of the grid is the\nhead of an entity mention. “*” indicates the entity type.\nWith such design, our framework is able to identify\nflat, overlapped and discontinuous entities simultaneously.\nAs shown in Figure 2, it is effortless to decode out two\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nBERT\nI\nam\nhaving\naching\nin\nlegs\nand\nshoulders\nCLN\nMLP\nMLP\nBiaffine\nDistance Embedding\nRegion Embedding\naching in legs\naching in shoulders\nInput\nEncoder Layer\nConvolution Layer\nCo-Predictor Layer \nDecoding\nWord Embedding\nDilation Rate = 1\nDilation Rate = 2\nDilation Rate = 3\nWord-Word Relation\n Classification\nBERT-Style Grid Representation\nMulti-Granularity Dilated Convolution\nFigure 3: Overall NER architecture. CLN and MLP represent conditional layer normalization and multi-layer perceptron. L\nand N represent element-wise addition and concatenation operations.\nentities “aching in legs” and “aching in shoulders” by\nNNW relations (aching→in), (in→legs), and (in→ shoulders), and THW relations (legs→aching, Symptom) and\n(shoulders→aching, Symptom). Moreover, NNW and THW\nrelations imply other effects for NER. For example, NNW\nrelations associate the segments of the same discontinuous\nentity (e.g., “aching in” and “shoulders”), and they are also\nbeneficial for identifying entity words (neighbouring) and\nnon-entity words (non-neighbouring). THW relations help\nidentify the boundaries of entities, which plays an important\nrole reported in recent NER studies (Zheng et al. 2019; Fei\net al. 2021; Shen et al. 2021).",
        "unified ner framework": "The architecture of our framework is illustrated in Figure\n3, which mainly consists of three components. First, the\nwidely-used pretrained language model, BERT (Devlin et al.\n2019), and bi-directional LSTM (Lample et al. 2016) are\nused as the encoder to yield contextualized word representations from input sentences. Then a convolution layer is used\nto build and refine the representation of the word-pair grid\nfor later word-word relation classification. Afterward, a copredictor layer (Li et al. 2021b) that contains a biaffine classifier and a multi-layer perceptron is leveraged for jointly\nreasoning the relations between all word pairs.\nEncoder Layer\nWe leverage BERT (Devlin et al. 2019) as inputs for our\nmodel since it has been demonstrated to be one of the stateof-the-art models for representation learning in NER (Wang\net al. 2021) and relation classification (Li et al. 2021b).\nGiven an input sentence X = {x1, x2, ..., xN}, we convert each token or word xi into word pieces and then feed\nthem into a pretrained BERT module. After the BERT calculation, each sentential word may involve vectorial representations of several pieces. Here we employ max pooling to\nproduce word representations based on the word piece representations. To further enhance context modeling, we follow\nprior work (Wadden et al. 2019; Li et al. 2021a), adopting a\nbi-directional LSTM (Lample et al. 2016) to generate final\nword representations, i.e., H = {h1, h2, ..., hN} ∈ RN×dh,\nwhere dh denotes the dimension of a word representation.\nConvolution Layer\nWe adopt convolution neural networks (CNNs) as the representation refiner, since CNNs are naturally suitable for 2D convolution on the grid, and also show the very prominence on handling relation determination jobs (Zeng et al.\n2014; Wang et al. 2016). Our convolution layer includes\nthree modules, including a condition layer with normalization (Liu et al. 2021) for generating the representation of the\nword-pair grid, a BERT-style grid representation build-up to\nenrich the representation of the word-pair grid, and a multigranularity dilated convolution for capturing the interactions\nbetween close and distant words.\nConditional Layer Normalization Since the goal of our\nframework is to predict the relations between word pairs, it\nis important to generate a high-quality representation of the\nword-pair grid, which can be regarded as a 3-dimensional\nmatrix, V ∈ RN×N×dh, where Vij denotes the representation of the word pair (xi, xj). Because both NNW and THW\nrelations are directional, i.e., from a word xi in certain row\nto a word xj in certain column as shown in Figure 2 (e.g.,\naching→in and legs→aching), the representation Vij of the\nword pair (xi, xj) can be considered as a combination of the\nrepresentation hi of xi and hj of xj, where the combination\nshould imply that xj is conditioned on xi. Inspired by Liu\net al. (2021), we adopt the Conditional Layer Normalization\n(CLN) mechanism to calculate Vij:\nVij = CLN(hi, hj) = γij ⊙ (hj − µ\nσ\n) + λij ,\n(1)\nwhere hi is the condition to generate the gain parameter\nγij = Wαhi + bα and bias λij = Wβhi + bβ of layer\nnormalization. µ and σ are the mean and standard deviation\nacross the elements of hj, denoted as:\nµ = 1\ndh\ndh\nX\nk=1\nhjk,\nσ =\nv\nu\nu\nt 1\ndh\ndh\nX\nk=1\n(hjk − µ)2 .\n(2)\nwhere hjk denotes the k-th dimension of hj.\nBERT-Style Grid Representation Build-Up As everyone\nknows, the inputs of BERT (Devlin et al. 2019) consist of\nthree parts, namely token embeddings, position embeddings\nand segment embeddings, which model word, position and\nsentential information respectively. Motivated by BERT, we\nenrich the representation of the word-pair grid using a similar idea, where the tensor V ∈ RN×N×dh represents word\ninformation, a tensor Ed ∈ RN×N×dEd represents the relative position information between each pair of words, and\na tensor Et ∈ RN×N×dEt represents the region information for distinguishing lower and upper triangle regions in\nthe grid. We then concatenate three kinds of embeddings and\nadopt a multi-layer perceptron (MLP) to reduce their dimensions and mix these information to get the position-regionaware representation of the grid C ∈ RN×N×dc. The overall\nprocess can be formulated as:\nC = MLP1([V; Ed; Et]) .\n(3)\nMulti-Granularity Dilated Convolution Motivated by\nTextCNN (Kim 2014), we adopt multiple 2-dimensional dilated convolutions (DConv) with different dilation rates l\n(e.g., l ∈ [1, 2, 3]) to capture the interactions between the\nwords with different distances, because our model is to predict the relations between these words. The calculation in\none dilated convolution can be formulated as:\nQl = σ(DConvl(C)) ,\n(4)\nwhere Ql ∈ RN×N×dc denotes the output of the dilation\nconvolution with the dilation rate l, σ is the GELU activation function (Hendrycks and Gimpel 2016). After that,\nwe can obtain the final word-pair grid representation Q =\n[Q1, Q2, Q3] ∈ RN×N×3dc.\nCo-Predictor Layer\nAfter the convolution layer, we obtain the word-pair grid\nrepresentations Q, which are used to predict the relation\nbetween each pair of words using an MLP. However, prior\nwork (Li et al. 2021b) has shown that MLP predictor can be\nenhanced by collaborating with a biaffine predictor for relation classification. We thus take these two predictors concurrently to calculate two separate relation distributions of word\npair (xi, xj), and combine them as the final prediction.\nBiaffine Predictor The input of the biaffine predictor is the\noutput H = {h1, h2, ..., hN} ∈ RN×dh of the encoder\nlayer, which can be considered as a residual connection (He\net al. 2016) that is widely-used in current deep learning research. Given the word representations H, we use two MLPs\nto calculate the subject (xi) and object (xj) word representations, si and oj respectively. Then, a biaffine classifier\n(Dozat and Manning 2017) is used to compute the relation\nscores between a pair of subject and object words (xi, xj):\nsi = MLP2(hi) ,\n(5)\noj = MLP3(hj) ,\n(6)\ny′\nij = si\n⊤Uoj + W[si; oj] + b ,\n(7)\nwhere U, W and b are trainable parameters, si and oj denote the subject and object representations of the i-th and\nj-th word, respectively. Here y′\nij ∈ R|R| is the scores of the\nrelations pre-defined in R.\nMLP Predictor Based on the word-pair grid representation\nA\nB\nD\nE\nA B C D E\n(a) A B C D E\n(a)\nA\nB\nD\nE\nA B C D E\n(a)\nA\nB\nC\nA B C D E\n(b)\nA\nB\nC\nD\nA B C D E\n(c)\nA\nB\nC\nD\nA B C D E\n(c)\nA\nC\nD\nE\nB\nA B C D E\n(d)\nA\nC\nD\nE\nB\nA B C D E\n(d)\nFigure 4: Four decoding cases for the word sequence\n“ABCDE”. (a) “AB” and “DE” are flat entities. (b) The flat\nentity “BC” is nested in “ABC”. (c) The entity “ABC” is\noverlapped with a discontinuous entity “ABD”. (d) Two discontinuous entities “ACD” and “BCE” are overlapped. The\nblue and red arrows indicate NNW and THW relations.\nQ, we adopt an MLP to calculate relations scores for word\npairs (xi, xj) using Qij:\ny′′\nij = MLP(Qij) ,\n(8)\nwhere y′′\nij ∈ R|R| is the scores of the relations pre-defined\nin R. The final relation probabilities yij for the word pair\n(xi, xj) are calculated by combining the scores from the biaffine and MLP predictors:\nyij = Softmax(y′\nij + y′′\nij) .\n(9)\nDecoding\nThe predictions of our model are the words and their relations, which can be considered as a directional word graph.\nThe decoding object is to find certain paths from one word\nto anther word in the graph using NNW relations. Each\npath corresponds to an entity mention. Besides the type and\nboundary identification for NER, THW relations can also be\nused as auxiliary information for disambiguation. Figure 4\nillustrates four cases for decoding from easy to difficult.\n• In the example (a), two paths “A→B” and “D→E” correspond to flat entities, and THW relations indicate their\nboundaries and types.\n• In the example (b), if there is no THW relation, we can\nonly find one path and thus “BC” is missing. In contrast,\nwith the help of THW relations, it is easy to identify that\n“BC” is nested in “ABC”, which demonstrates the necessity of THW relations.\n• The case (c) shows how to identify discontinuous entities. Two paths “A→B→C” and “A→B→D” can be\nfound, and the NNW relation contributes to connecting\nthe discontinuous spans “AB” and “D”.\n• Considering a complex and rare case (d), it is impossible to decode correct entities “ACD” and “BCE” because\nwe can find 4 paths in this ambiguous case using only\nNNW relations. In contrast, only using THW relations\nwill recognize continuous entities (e.g., “ABCD”) rather\nthan correct discontinuous entities (e.g., “ACD”). Therefore, we can obtain correct answers by collaboratively\nusing both relations.\nLearning\nFor each sentence X = {x1, x2, ..., xN}, our training target is to minimize the negative log-likelihood losses with\nCoNLL2003\nOntoNotes 5.0\nP\nR\nF1\nP\nR\nF1\n• Sequence Labeling\nLample et al. (2016)\n90.94\nStrubell et al. (2017)\n90.65\n86.84\n• Span-based\nYu et al. (2020) †\n92.91\n92.13\n92.52\n90.01\n89.77\n89.89\nShen et al. (2021)\n92.13\n93.73\n92.94\n• Hypergraph-based\nWang and Lu (2018)\n90.50\n• Seq2Seq\nStrakov´a et al. (2019)\n92.98\nYan et al. (2021) †\n92.56\n93.56\n93.05\n89.62\n90.92\n90.27\nW2NER (ours)\n92.71\n93.44\n93.07\n90.03\n90.97\n90.50\nTable 1: Results for English flat NER datasets. “†” denotes our re-implementation via their code. We run our model for 5 times\nand report averaged values.3\nOntoNotes 4.0\nMSRA\nResume\nWeibo\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nZhang and Yang (2018)\n76.35\n71.56\n73.88\n93.57\n92.79\n93.18\n94.81\n94.11\n94.46\n53.04\n62.25\n58.79\nYan et al. (2019)\n72.43\n92.74\n95.00\n58.17\nGui et al. (2019)\n76.40\n72.60\n74.45\n94.50\n92.93\n93.71\n95.37\n94.84\n95.11\n57.14\n66.67\n59.92\nLi et al. (2020b)\n81.82\n96.09\n95.86\n68.55\nMa et al. (2020)\n83.41\n82.21\n82.81\n95.75\n95.10\n95.42\n96.08\n96.13\n96.11\n70.94\n67.02\n70.50\nW2NER (ours)\n82.31\n83.36\n83.08\n96.12\n96.08\n96.10\n96.96\n96.35\n96.65\n70.84\n73.87\n72.32\nTable 2: Results for Chinese flat NER datasets. All the baselines are sequence labeling methods or their variations.\nACE2004\nACE2005\nGENIA\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\n• Sequence Labeling\nJu et al. (2018)\n74.20\n70.30\n72.20\n78.50\n71.30\n74.70\n• Span-based\nWang et al. (2020)\n86.08\n86.48\n86.28\n83.95\n85.39\n84.66\n79.45\n78.94\n79.19\nYu et al. (2020)\n87.30\n86.00\n86.70\n85.20\n85.60\n85.40\n81.80\n79.30\n80.50\nShen et al. (2021)\n87.44\n87.38\n87.41\n86.09\n87.27\n86.67\n80.19\n80.89\n80.54\n• Hypergraph-based\nWang and Lu (2018)\n78.00\n72.40\n75.10\n76.80\n72.30\n74.50\n77.00\n73.30\n75.10\n• Seq2Seq\nStrakov´a et al. (2019)\n84.33\n83.42\n78.20\nYan et al. (2021)\n87.27\n86.41\n86.84\n83.16\n86.38\n84.74\n78.87\n79.60\n79.23\nW2NER (ours)\n87.33\n87.71\n87.52\n85.03\n88.62\n86.79\n83.10\n79.76\n81.39\nTable 3: Results for English overlapped NER datasets.\nregards to the corresponding gold labels, formalized as:\nL = − 1\nN 2\nN\nX\ni=1\nN\nX\nj=1\n|R|\nX\nr=1\nˆyr\nijlogyr\nij,\n(10)\nwhere N it the number of words in the sentence, ˆyij is the binary vector that denotes the gold relation labels for the word\npair (xi, xj), and yij are the predicted probability vector. r\nindicates the r-th relation of the pre-defined relation set R.",
        "experimental settings": "Datasets\nTo evaluate our framework for three NER subtasks, we conducted experiments on 14 datasets.\nFlat NER Datasets We adopt CoNLL-2003 (Sang and\nMeulder 2003) and OntoNotes 5.0 (Pradhan et al. 2013b)\nin English, OntoNotes 4.0 (Weischedel et al. 2011), MSRA\n3The results in Table 2-6 are also the averaged values.\n(Levow 2006), Weibo (Peng and Dredze 2015; He and Sun\n2017), and Resume (Zhang and Yang 2018) in Chinese. We\nemploy the same experimental settings in previous work\n(Lample et al. 2016; Yan et al. 2021; Ma et al. 2020; Li et al.\n2020b).\nOverlapped NER Datasets We conduct experiments on\nACE 2004 (Doddington et al. 2004), ACE 2005 (Walker\net al. 2011), GENIA (Kim et al. 2003). For GENIA, we follow Yan et al. (2021) to use five types of entities and split the\ntrain/dev/test as 8.1:0.9:1.0. For ACE 2004 and ACE 2005 in\nEnglish, we use the same data split as Lu and Roth (2015);\nYu et al. (2020). For ACE 2004 and ACE 2005 in Chinese,\nwe split the train/dev/test as 8.0:1.0:1.0.\nDiscontinuous NER Datasets We experiment on three\ndatasets for discontinuous NER, namely CADEC (Karimi\net al. 2015), ShARe13 (Pradhan et al. 2013a) and ShARe14\n(Mowery et al. 2014), all of which are derived from biomedical or clinical domain documents. We use the preprocessing scripts provided by Dai et al. (2020) for data splitting.\nCADEC\nShARe13\nShARe14\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\n• Sequence Labeling\nTang et al. (2018)\n67.80\n64.99\n66.36\n• Span-based\nLi et al. (2021a)\n69.90\n82.50\n• Hypergraph-based\nWang and Lu (2019)\n72.10\n48.40\n58.00\n83.80\n60.40\n70.30\n79.10\n70.70\n74.70\n• Seq2Seq\nYan et al. (2021)\n70.08\n71.21\n70.64\n82.09\n77.42\n79.69\n77.20\n83.75\n80.34\nFei et al. (2021)\n75.50\n71.80\n72.40\n87.90\n77.20\n80.30\n• Others\nDai et al. (2020)\n68.90\n69.00\n69.00\n80.50\n75.00\n77.70\n78.10\n81.20\n79.60\nWang et al. (2021)\n70.50\n72.50\n71.50\n84.30\n78.20\n81.20\n78.20\n84.70\n81.30\nW2NER (ours)\n74.09\n72.35\n73.21\n85.57\n79.68\n82.52\n79.88\n83.71\n81.75\nTable 4: Results for discontinuous NER datasets.4\nACE2004\nACE2005\nYu et al. (2020) ⋆\n87.35\n88.39\nShen et al. (2021) ⋆\n87.47\n88.21\nW2NER (ours)\n88.00\n88.81\nTable 5: F1s for Chinese overlapped NER datasets. Models\nwith “⋆” are adapted to target datasets using their code.\nAround 10% of entities in these datasets are discontinuous.\nBaselines\nTagging-based methods, which assign a tag to every token\nwith different label schemes, such as BIO (Lample et al.\n2016), BIOHD (Tang et al. 2018), and BIEOS (Li et al.\n2020b; Ma et al. 2020). Span-based methods, which enumerate all possible spans and combine them into entities\n(Yu et al. 2020; Li et al. 2021a). Hypergraph-based approaches, which utilize hypergraphs to represent and infer\nentity mentions (Lu and Roth 2015; Wang and Lu 2018;\nKatiyar and Cardie 2018). Seq2Seq methods, which generate entity label sequences (Strubell et al. 2017), index or\nword sequences (Yan et al. 2021; Fei et al. 2021) at the\ndecoder side. Other methods, which is different from the\nmethods above, such as transition-based (Dai et al. 2020)\nand clique-based (Wang et al. 2021) approaches.",
        "experimental results": "Results for Flat NER\nWe evaluate our framework on six datasets. As shown in\nTable 1, Our model achieves the best performances with\n93.07% F1 and 90.50% F1 on CoNLL 2003 and OntoNotes\n5.0 datasets. Especially, our model outperforms another unified NER framework Yan et al. (2021) by 0.23% in terms\nof F1 on OntoNotes 5.0. The results in Chinese datasets\nare shown in Table 2, where baselines are all tagging-based\nmethods. We find that our model outperforms the previous SoTA results by 0.27%, 0.01%, 0.54% and 1.82% on\nOntoNotes 4.0, MSRA, Resume and Weibo.\nResults for Overlapped NER\nTable 3 presents the results for three overlapped NER\ndatasets in English. Our W2NER model outperforms the pre4Note that discontinuous NER datasets include both flat and\noverlapped entities as well.\n55\n60\n65\n70\n(a) Overlapped NER\nF1 (%)\nDai et al. (2020)\nYan et al. (2021)\nWang et al. (2021)\nOurs\n60\n65\n(b) Discontinuous NER\nFigure 5: Results of overlapped (a) and discontinuous mentions (b) on ShARe14.\nvious works, including tagging-based (Ju et al. 2018), spanbased (Wang et al. 2020; Yu et al. 2020; Shen et al. 2021),\nhypergraph-based (Wang and Lu 2018) and sequence-tosequence (Strakov´a et al. 2019; Yan et al. 2021) approaches,\nand achieves the SoTA performances on F1 scores, with\n87.52%, 86.79% and 81.39% on ACE2004, ACE2005 and\nGENIA, respectively. For ACE2004 and ACE2005 corpora\nin Chinese, we reproduce the SoTA models proposed by Yu\net al. (2020) and Shen et al. (2021), and list their results\nin Table 5. Our model can significantly outperform the two\nbaselines by 0.53% and 0.42%.\nResults for Discontinuous NER\nTable 4 presents the comparisons between our model and\nother baselines in three discontinuous NER datasets. As\nseen, our model outperforms previous best model (Fei et al.\n2021; Wang et al. 2021) by 0.81%, 0.02%, and 0.45% in\nF1s in the CADEC, ShARe13 and ShARe14 datasets, respectively, leading to new SoTA results.\nSince the above datasets also include flat entities, we further investigate the performances of our model on recognizing only overlapped or discontinuous entities, as shown in\nFigure 5. We can learn that the clique-based model (Wang\net al. 2021) shows better performances than the Seq2Seq\nmodel (Yan et al. 2021) and transition-based method (Dai\net al. 2020). Most importantly, our system achieves the best\nresults against all other baselines for both overlapped and\ndiscontinuous NER.\nCoNLL2003\nACE2005\nCADEC\nOurs\n93.07\n86.79\n73.21\n- Region Emb.\n92.80 (-0.27)\n86.39 (-0.40)\n72.56 (-0.65)\n- Distance Emb.\n92.89 (-0.18)\n86.47 (-0.32)\n72.66 (-0.55)\n- All DConv\n92.31 (-0.76)\n86.07 (-0.72)\n72.45 (-0.76)\n- DConv(l=1)\n93.05 (-0.02)\n86.64 (-0.15)\n73.12 (-0.09)\n- DConv(l=2)\n92.78 (-0.29)\n86.58 (-0.21)\n72.95 (-0.26)\n- DConv(l=3)\n92.82 (-0.25)\n86.59 (-0.20)\n73.10 (-0.11)\n- Biaffine\n93.02 (-0.05)\n86.30 (-0.49)\n72.71 (-0.50)\n- MLP\n91.87 (-1.20)\n85.66 (-1.13)\n68.04 (-5.17)\n- NNW\n92.65 (-0.42)\n86.23 (-0.56)\n69.01 (-4.20)\nTable 6: Model ablation studies (F1s). DConv(l=1) denots\nthe convolution with the dilation rate 1.\nModel Ablation Studies\nWe ablate each part of our model on the CoNLL2003,\nACE2005 and CADEC datasets, as shown in Table 6. First,\nwithout region and distance embeddings, we observe slight\nperformance drops on the three datasets. By removing all\nconvolutions, the performance also drops obviously, which\nverifies the usefulness of the multi-granularity dilated convolution. Furthermore, after removing convolutions with different dilation rate, the performance also decreases, especially for the convolution with the dilation rate 2.\nComparing the biaffine and MLP in the co-predictor layer,\nwe find that although the MLP plays a leading role, the biaffine also brings about 0.5% gains at most. At last, when the\nNNW relation is removed, the F1s on all datasets drop, especially on the CADEC (4.2%). This is because the CADEC\ndataset also contains discontinuous entities and without the\nNNW relation, discontinuous spans will be incorrectly recognized as continuous ones, as shown in Figure 4(d). Therefore, the results of ablation studies on the NNW relation\ndemonstrate its importance as we argued before.",
        "related work on ner": "Sequence Labeling Approaches NER is usually considered\nas a sequence labeling problem, to assign each token a tag\nfrom a pre-designed tagging scheme (e.g., BIO). Current\nmainstream work combine the CRF (Lafferty et al. 2001;\nFinkel et al. 2005) with neural architecture, such as CNN\n(Collobert et al. 2011; Strubell et al. 2017), bi-directional\nLSTM (Huang et al. 2015; Lample et al. 2016), and Transformer (Yan et al. 2019; Li et al. 2020b). However, these\nmethods fail to directly solve neither overlapped nor discontinuous NER. Ju et al. (2018) propose a neural model for\nnested NER by dynamically stacking flat NER layers. Tang\net al. (2018) extend the BIO label scheme to BIOHD to address the problem of discontinuous mention.\nSpan-based Approaches There have been several studies\nthat cast NER as span-level classification, i.e., enumerating\nall possible spans, and determining if they are valid mentions\nand the types (Xu et al. 2017; Luan et al. 2019; Yamada et al.\n2020). Yu et al. (2020) utilize biaffine attention (Dozat and\nManning 2017) to measure the possibility as a mention of a\ntext span. Li et al. (2020a) reformulate NER as a machine\nreading comprehension (MRC) task and extract entities as\nthe answer spans. Shen et al. (2021) implement a two-stage\nidentifier to generate span proposals through a filter and a\nregressor, and then classify them into the corresponding categories. Li et al. (2021a) convert the discontinuous NER to\nfind complete subgraphs from a span-based entity fragment\ngraph, and achieve competitive results. But, due to the exhaustively enumerating nature, those methods suffer from\nmaximal span lengths and considerable model complexity,\nespecially for long-span entities.\nHypergraph-based Approaches Lu and Roth (2015) first\npropose the hypergraph model for overlapped NER, by exponentially representing possible mentions. The method is\nthen widely explored by follow-up work (Muis and Lu 2016;\nKatiyar and Cardie 2018; Wang and Lu 2018). For instance,\nMuis and Lu (2016) extend the method for discontinuous\nNER, and Wang and Lu (2018) utilize deep neural networks\nto enhance the hypergraph model.\nSequence-to-Sequence Approaches Gillick et al. (2016)\nfirst apply the Seq2Seq model for NER, taking as inputs the\nsentence, and outputting all the entity start positions, span\nlengths and labels. Strakov´a et al. (2019) use the Seq2Seq\narchitecture for overlapped NER with enhanced BILOU\nscheme. Fei et al. (2021) employ Seq2Seq with pointer network for discontinuous NER. The latest attempt in (Yan\net al. 2021) tackles the unified NER via a Seq2Seq model\nwith pointer network based-on BART (Lewis et al. 2020),\ngenerating a sequence of all possible entity start-end indexes\nand types. Seq2Seq architecture unfortunately suffers from\nthe potential decoding efficiency problem as well as the exposure bias issue.\nDifferences between Our Approach and Previous Approaches Most of the existing NER work mainly consider more accurate entity boundary identification. In this\nwork, we explore a different task modeling for unified NER,\ni.e., a formalism as word-word relation classification. Our\nmethod can effectively model the relations between both\nthe boundary-words and inside-words of entities. Also, our\nmethod with 2D grid-tagging can substantially avoid the\ndrawbacks in current best-performing baselines, e.g., spanbased and sequence-to-sequence models.",
        "conclusion": "In this paper, we propose a novel unified NER framework\nbased on word-word relation classification to address unified\nNER concurrently. The relations between word pairs are predefined as next-neighboring-word relations and tail-headword relations. We find that our framework is quite effective\nfor various NER, which achieves SoTA performances for 14\nwidely-used benchmark datasets. Moreover, we propose a\nnovel backbone model that consists of a BERT-BiLSTM encoder layer, a convolution layer for building and refining the\nrepresentation of the word-pair grid, and a co-predictor layer\nfor jointly reasoning relations. Through ablation studies, we\nfind that our convolution-centric model performs well and\nseveral proposed modules such as the co-predictor and grid\nrepresentation enrichment are also effective. Our framework\nand model are easy to follow, which will promote the development of NER research.",
        "summary_en": "So far, named entity recognition (NER) has been involved with three major types, including flat, overlapped (aka. nested), and discontinuous NER, which have mostly been studied individually. Recently, a growing interest has been built for unified NER, tackling the above three jobs concurrently with one single model. Current best-performing methods mainly include span-based and sequence-to-sequence models, where unfortunately the former merely focus on boundary identification and the latter may suffer from exposure bias. Therefore, this paper presents a novel alternative by modeling the unified NER as word-word relation classification, namely W^2NER. The architecture resolves the kernel bottleneck of unified NER by effectively modeling the neighboring relations between entity words with Next-Neighboring-Word (NNW) and Tail-Head-Word-* (THW-*) relations. Based on the W^2NER scheme the paper develops a neural framework, in which the unified NER is modeled as a 2D grid of word pairs. The paper then proposes multi-granularity 2D convolutions for better refining the grid representations. Finally, a co-predictor is used to sufficiently reason the word-word relations. The paper performs extensive experiments on 14 widely-used benchmark datasets for flat, overlapped, and discontinuous NER (8 English and 6 Chinese datasets), where the model beats all the current top-performing baselines, pushing the state-of-the-art performances of unified NER.",
        "summary_zh": "这篇论文介绍了一种将统一命名实体识别（NER）建模为单词-单词关系分类的方法，旨在解决NER中的三种主要类型（平铺型、嵌套型和不连续型）的识别问题，通过一个模型同时处理这三种任务。作者提出的方法名为W^2NER，通过对实体词之间的邻接关系和尾-头-词-*关系进行有效建模，解决了统一NER的核心瓶颈。作者还开发了一个神经框架，将统一NER建模为一个单词对的二维网格，并提出了多粒度的二维卷积来更好地细化网格表示。最后，他们使用协同预测器来充分推理单词之间的关系。在14个广泛使用的平铺型、嵌套型和不连续型NER基准数据集上的实验结果表明，该模型击败了所有当前性能最优的基准模型，推动了统一NER的最先进的水平。"
    },
    {
        "title": "AMOM: Adaptive Masking over Masking for Conditional Masked Language Model",
        "abstract": "Transformer-based autoregressive (AR) methods have achieved appealing performance for varied sequence-tosequence generation tasks, e.g., neural machine translation, summarization, and code generation, but suffer from low inference efficiency. To speed up the inference stage, many non-autoregressive (NAR) strategies have been proposed in the past few years. Among them, the conditional masked language model (CMLM) is one of the most versatile frameworks, as it can support many different sequence generation scenarios and achieve very competitive performance on these tasks. In this paper, we further introduce a simple yet effective adaptive masking over masking strategy to enhance the refinement capability of the decoder and make the encoder optimization easier. Experiments on 3 different tasks (neural machine translation, summarization, and code generation) with 15 datasets in total confirm that our proposed simple method achieves significant performance improvement over the strong CMLM model. Surprisingly, our proposed model yields state-of-the-art performance on neural machine translation (34.62 BLEU on WMT16 EN→RO, 34.82 BLEU on WMT16 RO→EN, and 34.84 BLEU on IWSLT De→En) and even better performance than the AR Transformer on 7 benchmark datasets with at least 2.2× speedup. Our code is available at GitHub1.",
        "introduction": "Transformer-based models (Vaswani et al. 2017) have been\nproven effective for various sequence to sequence generation tasks, such as machine translation (Wu et al. 2019;\nLiang et al. 2021), text summarization (Savelieva, AuYeung, and Ramani 2020; Elsaid et al. 2022), dialogue systems (Zhang et al. 2020; Ma et al. 2020), code generation (Wang et al. 2020), etc. Despite the excellent performance of Transformer-based models, they usually adopt the\nautoregressive (AR) decoding paradigm in which the decoding of a target sequence is decomposed into multi-step predictions in left-to-right order, i.e., the next prediction is conditioned on the previously generated part. Such an attribute\nincreases the inference time cost linearly with the target sequence length, which is time-consuming for long sequences.\nTo alleviate this problem, many recent works explore nonautoregressive (NAR) methods (Gu et al. 2018; Qian et al.\n2021; Xiao et al. 2022) to predict a target sequence in parallel, which can dramatically increase inference speed. As the\ncost of increasing decoding speed, NAR models remove the\ninternal dependency of the target sequence and perform each\ndecoding prediction depending entirely upon the source/input sequence. Inevitably, the generation quality of NAR\nmethods falls behind their AR counterparts without targetside information in decoding (Gu et al. 2018).\nTo achieve a better trade-off between inference speedup\nand generation quality, the conditional masked language\nmodel (CMLM) (Ghazvininejad et al. 2019) has been proposed and has already become one of the most competitive\nand widely-used NAR frameworks, which exploits an iterative mask-predict decoding strategy. In the training stage,\nCMLM leverages a masked language model objective to\ngenerate the masked subset of the target sequence in parallel conditioned on the source input and unmasked part in\ntarget sequence. During inference, CMLM first generates the\nwhole target sequence in parallel (the first iteration) and then\niteratively masks and predicts low-confidence tokens. Based\non CMLM, many recent works have achieved performance\nimprovements with advanced enhancement strategies from\ndifferent perspectives, e.g., improving the inference strategy (Kasai et al. 2020a; Geng, Feng, and Qin 2021), benefiting from the AT counterpart (Hao et al. 2021), training with\nbetter criterion (Marjan et al. 2020; Du, Tu, and Jiang 2021),\nintroducing self-correction mechanism (Huang, Perez, and\nVolkovs 2022) and pre-training (Li et al. 2022b).\nIn this paper, we further introduce a simple yet very effective strategy to enhance the refinement capability of CMLM\nwithout changing the model structure and the inference algorithm, named adaptive masking over masking (AMOM).\nConcretely, we present two adaptive masking operations for\nboth the source and target sequence based on the conventional one-time masking in CMLM. The masking operation\nfor the source sequence can make the encoder optimization\neasier by adaptively masking a proportion of tokens based on\nthe masked target sequence. In contrast, the vanilla CMLM\nconstructs multiple masked target sequences for each source\nsequence in model training, making the encoder difficult to\nconverge (Guo, Xu, and Chen 2020). Another potential merit\nof the source-side masking is to improve the stability of the\nCMLM model against different decoder inputs by preventing the internal co-adaptation (akin to dropout (Hinton et al.\n2012)). Moreover, cooperating it with the masking condition of the target sentence can better improve the ability\nrather than fixed masking. Notice that JM-NAT (Guo, Xu,\nand Chen 2020) also explores the source-side masking operation but has a clear difference from our strategy. It introduces a BERT-like masked language model task on the\nencoder side to enhance the encoder training, whereas our\nadaptive strategy does not introduce any extra task and can\ndynamically capture target-side information. The target-side\nadaptive masking operation is presented to enhance the refinement process of CMLM, motivated by the masking ratio changes of the target sequence in different inference iterations, which cannot be captured by the one-time masking. Simultaneously, unlike the adaptive target-side masking\nstrategy in GLAT (Qian et al. 2021) to achieve curriculum\nlearning, we design the masking strategy specially to encourage the model to perform steadily and conduct refinements effectively. We focus on the promotion of each iteration rather than only enhancing the first iteration in GLAT.\nMore comparisons between our strategy and the counterparts used in GLAT can be found in the experiments part.\nThough AMOM is simple, i.e., only two extra masking\noperations in model training, we find it is surprisingly effective on different sequence generation tasks, including neural machine translation, summarization, and code generation (15 datasets in total). It achieves state-of-the-art performance on multiple datasets based on the vanilla CMLM,\ne.g., 34.62 BLEU score on WMT16 EN→RO, 34.82 BLEU\non WMT16 RO→EN, and 34.84 BLEU on IWSLT De→En.\nAMOM even performs better than the strong autoregressive\nTransformer on 7 datasets with at least 2.2× speedup.",
        "methodology": "Our proposed adaptive masking over masking (AMOM)\nstrategy is a simple yet effective add-on for the conditional\nmasked language model (CMLM) (Ghazvininejad et al.\n2019) training, which comprises two adaptive masking operations for the encoder and decoder, respectively, to enhance the encoder training and the refinement capability of\nCMLM. Specifically, we adopt the same encoder-decoder\narchitecture as the CMLM.\nConditional Masked Language Model\nA conditional masked language model feeds a source sequence X to the encoder and a target sequence in which\npart of the tokens are masked by replacing them with the\n[mask] token to the decoder. The training objective of\nCMLM is to learn to predict the masked tokens Ymask in\nparallel given X and the unmasked tokens Yobs in the rest\npart of the target sequence, based on the assumption that all\ntarget tokens in Ymask are independent of each other, i.e.,\nthe prediction of each Ymask token is merely conditioned on\nX and Yobs. To eliminate the particularity of Ymask, CMLM\nsamples a different number of tokens each time as Ymask\nfrom the uniformly distributed number between one to the\ntarget length during training, rather than a fixed proportion\nof the target sequence. The training objective of CMLM is\nto maximize:\nLCMLM =\nX\nyt∈Ymask\nlog P(yt|Yobs, X; θ),\n(1)\nwhere θ denotes the trainable parameters of CMLM. Unlike\nAR methods that can automatically decide the decoding end\nby generating a special [EOS] (end of a sentence) token,\ntypical NAR methods require learning to predict the target\nlength in advance. CMLM adds a special token [LENGTH]\n(akin to the [cls] token in BERT) into its encoder to predict the target length. During inference, given the input X\nand the predicted target length, CMLM executes k iterations\nof mask-predict operation (Ghazvininejad et al. 2019) to create the final target sequence. At the first iteration, the CMLM\npredicts the entire Y in parallel fully depending on X. In the\nnext k − 1 iterations, CMLM repeatedly masks a specific\nnumber of low-confidence tokens generated from the last iteration and regenerates them in parallel.\nAdaptive X Masking\nBasically, CMLM leverages an encoder-decoder structure to\nachieve sequence to sequence generation, which requires the\nmutual cooperation between encoder and decoder. However,\nduring model training, each X will be paired with multiple\nYmask due to the uniform masking strategy of CMLM, making the encoder optimization much harder than the decoder.\nGuo, Xu, and Chen also empirically prove that the convergence speed of the encoder is significant lower than the decoder. Another drawback of conditioning different Ymask on\nthe same X is the internal co-adaptation of X, i.e., each prediction of Ymask relies on the whole input sequence, making\nthe decoder less focused on the changes of decoder inputs.\nTo enhance the encoder training and address the abovementioned flaws, we propose a simple yet effective adaptive masking for input X. Unlike previous research, our proposed adaptive X masking is included in the sequence to\nsequence generation task, and the number of masked tokens\nis coordinated with the number of masked Y tokens. More\nconcretely, given a training pair (X, Y ) in CMLM, where Y\nwill be divided into Yobs and Ymask, the masking ratio α of\nY can be calculated as\nNmask\nNobs+Nmask . Nobs and Nmask denote\nthe number of tokens in Yobs and Ymask, respectively. Then,\nwe introduce a mapping function φ(·) to decide the masking ratio of X based on the masking ratio in Y , i.e., we will\nrandomly mask φ(α) ∗ LX tokens in the source sequence,\nwhere LX denotes the length of the source sequence. Then\nthe training loss of CMLM with adaptive X masking can be\ncomputed as:\nLcmlm = −\nX\nyt∈Ymask\nlog P(yt|Yobs, ˆX; θ),\n(2)\nwhere ˆX refers to the input sequence with φ(α) ∗ LX tokens being masked. We introduce different variations of φ\nin Table 5 and compare their performance.\nAdaptive Y Masking\nAs mentioned above, the superior performance of CMLMbased methods comes from the iterative refinement process,\ni.e., the previously generated target sequence draft is repeatedly polished by regenerating a specific number of lowconfidence tokens in the subsequent iterations. In seeing the\nself-correction nature of the refinement process, many recent\nworks introduce a correction objective in CMLM training to\nenhance its refinement capability e.g., SMART (Ghazvininejad et al. 2020), CMLMC (Huang, Perez, and Volkovs 2022).\nUnlike these works that introduce extra training objectives\nand optimize the inference process of CMLM, we present\nan ultra-simple yet effective adaptive masking operation for\nY in model training without any change to the CMLM inference2. Our strategy is motivated by the quality improvement\nof predicted tokens along with the refinement iterations, i.e.,\nthe proportion of low-confidence tokens (for regeneration\nin each iteration) from Ymask will gradually decrease along\nwith the refinement iterations, resulting in a varied masking\nratio between Ymask and Yobs in the refinement process.\nTo capture the masking ratio changes in CMLM inference, we add another masking operation (adaptive Y masking) upon the one-time masking in the vanilla CMLM\nmodel. Specifically, for each training pair (X, Y ), Y is divided into Yobs and Ymask. CMLM generates the masked\ntokens based on Yobs and X, where the generated result\nis denoted as ˆYmask to distinguish with Ymask. Then, we\ncompute the correctness ratio of predicted tokens in ˆYmask\nby comparing with target tokens in Ymask, formulated as\nβ = | ˆYmask=Ymask|\nNmask\n. Similar to adaptive X masking, we introduce another mapping function ψ(·) to decide the masking proportion of ˆYmask and Yobs tokens. Different types of\nmapping function ψ(·) are experimented in Analysis, and\nmore details are given in Appendix. We assign a masking\nprobability of 1 − ψ(β) to each token in ˆYmask and a masking probability of ψ(β) to each token in Yobs. As a result, the\nnewly masked tokens in the second time denote Y ′mask, and\nthe rest tokens will serve as a new Y ′\nobs, for the next iteration.\nThe training loss of the new subset Y ′mask is computed the\nsame as the first-time masking in CMLM, formulated as:\nLaday = −\nX\nyt∈Y ′\nmask\nlog P(yt|Y ′\nobs, ˆ\nX′; θ),\n(3)\nwhere\nˆ\nX′ refers to the input sequence with an adaptive\nmasking ratio of Y ′\nmask being masked.\nAMOM Training and Inference\nWe simply adopt two adaptive masking strategies based on\nthe original CMLM training process. The training objective\nof our proposed adaptive masking over masking (AMOM)\nis the simple combination of Lcmlm and Laday mentioned in\nEquation 2 and 3, formulated as:\nLAMOM = Lcmlm + Laday,\n(4)\nAs for inference, we utilize the same decoding strategy with\nCMLM. As mentioned above, we utilize a special token\n[LENGTH] in the encoder to predict the target length in\nadvance. Inevitably, there is a deviation between the predicted length and the ground-truth length. Thus, we also\n2More comparisons are given in Appendix.\nconsider selecting the translation with the highest probability with different target lengths to obtain better results.\nGiven the target length LY and the total number of refinement iterations T, the model performs generation based on\nthe fully masked decoder input (i.e., empty Yobs) at the first\niteration. In the next T − 1 iterations, a specific number of\nlow-confidence tokens will be masked and re-generated. The\nnumber of masked tokens in each iteration can be computed\nas n =\nT −t\nT\n∗ LY , where t denotes the current iteration\nnumber. Given the number of masked tokens, the model will\nselect them based on the output probability of each token,\nwhere tokens with the lowest probability will be masked,\nand their scores will be updated in the next iteration.",
        "experiments": "To evaluate our AMOM method and show its universal impact on various sequence generation tasks, we conduct experiments on natural machine translation, summarization,\nand code generation tasks.\nDatasets\nFor machine translation, we conduct experiments both on\nIWSLT and WMT datasets, which are widely used for\nNMT tasks. The datasets from IWSLT competitions contain 4 language pairs (170k pairs), see details in Table 2.\nFor WMT datasets, we choose two language pairs which\nare widely used in non-autoregressive machine translation\ntask, WMT16 English→Roman (0.6M pairs) and WMT14\nEnglish→German (4.5M pairs) tasks. Following previous\nworks on non-autoregressive machine translation, we apply sequence-level knowledge distillation (Kim and Rush\n2016; Zhou, Gu, and Neubig 2019) for all datasets. For\nWMT datasets, we use the same distilled data as the same as\nCMLM (Ghazvininejad et al. 2019). Then, we amalgamate\nthe raw and distilled data as our final training data, following (Ding et al. 2020). For all IWSLT datasets, we train the\nteacher model with Transformersmall, and use the generated\nresults as the distilled data. Then, we train our AMOM on\ndistilled data. For summarization task, we use the XSUM\ndataset (Narayan, Cohen, and Lapata 2018) which contains\n204,045/11,332/11,334 online articles and single sentence\nsummary pairs from the British Broadcasting Corporation\nfor training/validation/test. We preprocess the dataset, following (Lewis et al. 2020). For code generation task, we use\nPy150 dataset (Raychev, Bielik, and Vechev 2016) and use\nGitHub-Java dataset (Allamanis and Sutton 2013). We use\nthe Python official library tokenizer3 and Javalang4 to split\nthe datasets into lines of codes. Then we use a sliding context window to adopt 10-lines of code tokens as the source\nsentences and the next 4-lines as the target sentences. We\nfollow (Wang et al. 2020) to process the dataset to transform\nsome special tokens as [str] token (without bpe).\nSettings\nAll experiments are done using the Fairseq library (Ott\net al. 2019). Following previous settings (Ghazvininejad\n3https://docs.python.org/3/library/tokenize.html\n4https://github.com/c2nes/javalang\nModel\nIterations\nWMT16\nWMT14\nSpeedup\nEN→RO\nRO→EN\nEN→DE\nDE→EN\nAR Transformer (Vaswani et al. 2017)*\nN\n34.23\n34.28\n28.41\n32.28\n1.0x\nFull NAT\nNAT-FT (Gu et al. 2018)\n1\n27.29\n29.06\n17.69\n21.47\n15.6×\nAXE (Marjan et al. 2020)\n1\n31.54\n30.75\n23.53\n15.3x\nOAXE (Du, Tu, and Jiang 2021)\n1\n33.3\n32.4\n26.1\n15.3x\nGLAT (Qian et al. 2021)\n1\n32.87\n33.51\n26.55\n31.02\n15.3x\nFullyNAT (Gu and Kong 2021)\n1\n33.71\n34.16\n27.20\n31.39\n16.8x\nDSLP (Huang et al. 2022a)\n1\n34.17\n34.60\n27.02\n31.61\n14.8x\nDAT (Huang et al. 2022b)\n1\n27.49\n31.37\n13.9x\nIterative\nRefine-NAT (Lee, Mansimov, and Cho 2018)\n10\n27.11\n30.19\n21.61\n25.48\n1.5x\nLevenshteinNAR (Gu, Wang, and Zhao 2019)\n>7\n33.02\n27.73\n4.0x\nDisCo (Kasai et al. 2020a)\n3.1\n33.25\n33.22\n27.34\n3.5x\nCMLM-Based\nCMLM (Ghazvininejad et al. 2019)*\n10\n33.46\n33.83\n27.21\n31.03\n2.3x\nSMART (Ghazvininejad et al. 2020)\n10\n33.85\n33.53\n27.65\n31.27\n1.7x\nJM-NAT (Guo, Xu, and Chen 2020)\n10\n33.52\n33.72\n27.69\n32.24\nRDP (Ding et al. 2020)\n10\n33.7\n27.8\n1.5x\nLFR (Ding et al. 2021)\n10\n33.9\n27.8\n1.5x\nMvSR-NAT (Xie, Li, and Hu 2021)\n10\n33.38\n33.56\n27.39\n31.18\n3.8x\nCORR (Huang, Perez, and Volkovs 2022)\n10\n34.31\n34.08\n28.19\n31.31\nCMLMC (Huang, Perez, and Volkovs 2022)\n10\n34.57\n34.13\n28.37\n31.41\nOurs AMOM\n10\n34.62\n34.82\n27.57\n31.67\n2.3x\nTable 1: Results on 4 WMT machine translation tasks. “*” denotes the results of our implementations.\nModel\nEn↔De\nEn↔Fr\nEn↔Zh\nEn↔Es\nAvg\nSpeedup\nTransformer\n28.71/34.68\n36.2/37.0\n25.7/18.2\n37.8/39.5\n32.22\n1.0x\nCMLM\n27.77/33.87\n35.2/35.0\n26.0/17.9\n37.1/39.0\n31.48\n2.2x\nAMOM\n28.41/34.84\n35.6/36.3\n26.1/18.4\n38.0/39.8\n32.18\n2.2x\nTable 2: Results on 8 IWSLT datasets. Numbers before and after “/” denote BLEU scores from and to English directions.\net al. 2019), we use the standard Transformerbase configuration on WMT datasets and standard Transformersmall\nconfiguration on IWSLT datasets for both auto-regressive\nand non-autoregressive experiments. During AMOM training, we follow the hyper-parameters in CMLMC (Huang,\nPerez, and Volkovs 2022) for WMT14 En↔De and follow\nthe hyper-parameters of CMLM realization in Fairseq5 for\nthe other datasets. During inference, we average the 5 best\ncheckpoints chosen by validation BLEU scores as our final model and set the length beam as 3/5 for IWSLT/WMT\ndatasets. For XSUM, we choose Transformerbase with embedding dimension 768 and follow the training schedule applied in NMT. During our training, we make a specific modification of the hyper-parameters referring to (Lewis et al.\n2020). During inference we follow the process in (Qi et al.\n2021), where the same consecutive tokens will be merged\nto avoid repeated n-gram tokens. For code generation tasks,\nwe choose Transformerbase with embedding size 512 and\nfollow the original training schedule. We make a specific\nmodification of the hyper-parameters referring to (Liu et al.\n2022). For all datasets, we set the limits ratio of adaptive X\n5https://github.com/facebookresearch/fairseq/tree/main/\nexamples/nonautoregressive translation\nfrom 10%-30% and adaptive Y from 20%-80%, and select a\nlinear mapping function to decide the masking ratios. More\ndetails about training are presented in Appendix.\nMain Results\nNatural Machine Translation. Following previous works,\nwe evaluate the performance with BLEU (Papineni et al.\n2002) for WMT datasets and IWSLT En↔De dataset, and\nfor the other IWSLT datasets, we use SacreBLEU 6 (Post\n2018; Liang et al. 2021). Speedup is measured by LGPU\n1\nfollowing the previous work (Kasai et al. 2020b; Gu and Kong\n2021; Helcl, Haddow, and Birch 2022). Table 2 presents\nthe results on 8 IWSLT datasets, we compare our AMOM\nwith original CMLM and strong Transformer (AR) baseline. First, a significant improvement can be found over the\noriginal CMLM on all datasets, with about 0.7 BLEU on\naverage. More excitingly, compared with the strong Transformer (AR) baseline, our AMOM has achieved better performance on five datasets, and only a tiny gap (0.04 BLEU)\nstill exists on average. We show our results in Table 1 for\nWMT datasets, we compare our approach with various iterative NAR models, including two popular fully NAR models.\n6https://github.com/mjpost/sacrebleu\nModel\nROUGE-1\nROUGE-2\nROUGE-L\nTransformer\n30.66\n10.80\n24.48\nWithout pretrain\nvanilla NAT\n24.04\n3.88\n20.32\nInsertNAR\n17.65\n5.18\n16.05\nLevenshitein\n25.33\n7.40\n21.48\nDisco\n26.85\n6.86\n21.72\nPOSPD\n27.39\n7.26\n22.15\nCMLM*\n25.80\n6.31\n20.45\nAMOM*\n31.59\n9.30\n24.98\nWith pretrain\nBANG\n34.71\n11.71\n29.16\nMIST\n34.63\n11.29\n28.70\nELMER\n37.30\n13.17\n29.92\nTable 3: Results on XSUM for the text summarization task.\n“*” denotes the results of our implementations.\nModel\nPython\nJAVA\nIter.\nBLEU\nES\nIter.\nBLEU\nES\nCMLM\n4\n49.61\n69.58\n4\n60.54\n76.68\n10\n53.44\n70.42\n10\n62.82\n77.24\nAMOM\n4\n50.57\n70.22\n4\n62.86\n76.61\n10\n56.50\n71.38\n10\n65.43\n77.17\nTable 4: Results on Py150 and Github-Java dataset.\nWe re-run the experiments of CMLM with the same settings\nin AMOM to avoid inconsistency. After applying our simple yet effective methods to the traditional CMLM framework, we achieved state-of-the-art (SOTA) BLEU score on\nWMT16 En→Ro (34.62) and Ro→En (34.82) with 10 iterations. For the WMT14 En↔De dataset, AMOM also outperforms most of the baselines on De→En (31.67). On the\nEn→De dataset, AMOM only gains 0.36 BLEU improvement compared with CMLM and a comparable score compared with strong CMLM-Based baselines. This might be\nbecause our adaptive X strategy hurts the performance in\nthe first iteration to some extent. Note that AMOM is complementary to other effective tricks applied in CMLM, and\nstronger results can be expected by combining our adaptive\nmasking strategies with their methods.\nSummarization. See Table 3, the performance is evaluated\nby ROUGE F1 score (Lin and Hovy 2002). Specifically,\nwe report the unigram ROUGE-1 and bigram ROUGE-2\noverlap to assess the informativeness, and the longest common subsequence ROUGE-L score to assess the fluency.\nWe compare our AMOM with the original CMLM and several NAR baseline models, including vanilla NAT (Gu et al.\n2018), InsertNAR (Stern et al. 2019), Levenshitein (Gu,\nWang, and Zhao 2019), Disco (Kasai et al. 2020a),\nPOSPD (Yang et al. 2021), CMLM (Ghazvininejad et al.\n2019), BANG (Qi et al. 2021), MIST (Jiang et al. 2021),\nELMER (Li et al. 2022a). Results show that AMOM outperforms all other NAR models without pre-training. Since pretraining always benefits summarization task a lot, models\nwith pre-training achieve significant performance improvements. Notice that AMOM can also be applied to the pretraining and finetune stage, we believe it also works to improve the performance.\nCode Generation. The performance is evaluated by BLEU\nand ES (Wang et al. 2020), which measure character-level\nedit similarity and n-gram level precision between the target\ncodes and generated codes, respectively. We also report the\nresults of different iterations in Table 4. Our AMOM outperforms the original CMLM with different iterations and gains\nbetter improvements during refinements.\nAnalysis\nThe Mapping Function of Adaptive X Masking. In this\nsubsection, we exhibit exhaustive experiments to explore encoder masking strategies and how to affect the model performance. In particular, we analyse the effects of different mapping functions, these strategies can utilize decoder masking\nratio αdec to obtain encoder masking ratio αenc:\n• φlinear: αenc = (b − a)αdec + a;\n• φconvex: αenc = (b − a)α2\ndec + b;\n• φconcvae: αenc = (a − b)α2\ndec + 2(b − a)αdec + b;\n• φladder: αenc = a − ⌈\nαdec\na−b+0.1⌉,\nwhere a and b are two hyper-parameters controlling the\nmasking limits, and the specific curves corresponding to the\nabove mapping function are presented in Appendix. The\nresults are shown in Table 5, and it is worth noting that\nthe above experiments are based on the CMLM model and\nIWSLT14 De→En dataset for clear contrast. Early experiments show that encoder masking can boost the model performance, and at αenc = 0.2, the encoder masked model\nperforms best when using the fixed masking strategy, results\nare shown in Appendix. That is why we design the mapping\nfunction to limit the masking raio around 0.2. Firstly, we\ntake linear mapping functions as our priority. Fortunately,\nlinear mapping has been proved by comprehensive experiments that it is indeed one of the most effective implementations to boost the performance. Besides, the results\nare consistent with our intuition that the more tokens in\nY are masked, the few tokens in X should be masked to\nkeep the masking ratio balanced. We also have briefly tried\na few alternative implementations beyond linear mappings,\nbut without achieving further performance improvement.\nThe Effect of Adaptive X Masking. We also compare our\nadaptive X masking strategy with several related works to\nfurther show its effectiveness. Since JM-NAT (Guo, Xu, and\nChen 2020) also introduces masking operation in X, we also\nconduct experiments to compare AMOM and their bert-like\nmasking. Also, they introduce an auxiliary MLM training\nobjective to improve the encoder, we further verify if this\ncan combine with AMOM, see Table 6. Notice that we keep\nthe decoder-side the same as vanilla CMLM (without adaptive Y masking in AMOM and n-gram loss in JM-NAT)\nto make a fair comparison of encoder-side. Results show\nthat this MLM training objective can also improve AMOM\nslightly, but seems less related to our assumption and purpose. Besides, we can find adaptive X outperforms the bertlike masking for CMLM. Also, we find that the adaptive X\nStrategy\nαenc\nBLEU\nLinear\nφlinear(αdec, 0.25, 0.15)\n34.20\nφlinear(αdec, 0.3, 0.1)\n34.48\nφlinear(αdec, 0.35, 0.15)\n34.30\nφlinear(αdec, 0.4, 0.1)\n34.40\nφlinear(αdec, 0.1, 0.3)\n33.64\nφlinear(αdec, 0.1, 0.4)\n33.76\nConvex\nψconvex(αdec, 0.3, 0.1)\n33.55\nConcave\nψconcave(αdec, 0.3, 0.1)\n33.96\nLadder\nψladder(αdec, 0.3, 0.1)\n34.17\nTable 5: The BLEU scores of adaptive X masking strategy.\nMethod\nBLEU\nMethod\nBLEU\nCMLM\n33.87\nCMLM\n33.87\n+ adax\n34.48\n+ mix cutoff\n33.96\n+ adax+mlmloss\n34.57\n+ span cutoff\n33.93\n+ jm-nat\n34.13\n+ random replace\n34.13\n+ jm-nat+mlmloss\n34.21\n+ random delete\n33.95\nTable 6: Comparison between adaptive X masking and related methods.\nmasking operation is similar to a data augmentation strategy\n(such as cutoff (Shen et al. 2020)), and specially designed\nto improve the refinements ability of CMLM. To better analyze them, we also compare adaptive X masking with several common data augmentation strategies (including cutoff). Since fixed masking is similar to token cutoff, we conduct experiments with span cutoff and mix cutoff. We also\ncompare with some other strategies (such as random delete,\nrandom replace). Results show that adaptive X masking outperforms all other operations on X, while various traditional\nstrategies can boost vanilla CMLM to some extent.\nThe Mapping Function of Adaptive Y Masking. We\nalso experiment with different masking strategies when applied to the decoder side in a two-step training scheme.\nWe try same adaptive mapping function and denoted as\nψlinear, ψconvex, ψconcvae, and ψladder to obtain masking\nratio αdec. Specifically, we can calculate αdec based on randomly sampled variable β which is correctness ratio predicted by first step training as mentioned above : αdec =\nψlinear(β, a, b) = (b − a)β + a. Unlike the encoder masking mapping function, we choose a large masking ratio range\nbecause there exist various conditions of masking ratios and\ntokens confidence during inference. The schedule curves are\nalso shown in Appendix. Table 7 lists the results of several\nadaptive decoder masking strategies. Notice that we achieve\nall results here with a linear mapping φlinear(αdec, 0.3, 0.1)\nfor source-side masking. The simple linear mapping function achieves the best performance, and the large masking\nratio range seems better. Besides, a high correctness ratio always indicates high token confidence, and then fewer tokens\nin ˆYmask will be masked in the next iteration. Our adaptive\nY masking strategy matches the inference strategy of the\noriginal CMLM.\nStrategy\nαdec\nBLEU\nLinear\nψlinear(β, 0.1, 0.9)\n34.65\nψlinear(β, 0.2, 0.8)\n34.84\nψlinear(β, 0.3, 0.7)\n34.79\nψlinear(β, 0.2, 0.5)\n34.62\nψlinear(β, 0.5, 0.8)\n34.77\nψlinear(β, 0.8, 0.2)\n34.61\nConvex\nψconvex(β, 0.2, 0.8)\n34.80\nConcave\nψconcave(β, 0.2, 0.8)\n34.59\nLadder\nψladder(β, 0.2, 0.8)\n34.75\nTable 7: The BLEU scores of adaptive Y masking strategy.\nMasking Strategy\nBLEU\nMasking Strategy\nBLEU\nAdaptive (Ours)\n34.84\nUniform\n34.53\n+ same ratio\n34.65\nGlancing\n34.68\n+ 3 step\n34.50\nGlat\n33.72\n+ exposure bias\n34.79\nGlat + fix-x (0.1)\n33.64\n+ confidence-based\n33.85\nGlat + ada-x\n33.35\nTable 8: Comparison of adaptive Y masking with different\nconstraints and related methods.\nThe Effect of Adaptive Y Masking. To better understand\nthe two-step training scheme and how to guide model training, we analyze the effect of different masking and training\nsettings, and notice that we all keep the uniform masking\nstrategy in the first step as the original CMLM. First, we\nuse uniform sampling to replace adaptive ψ sampling in the\nsecond masking step. Then we also keep the masking ratio αdec = β to verify whether the masking ratio is critical\nfor model training. Besides, we use an adaptive strategy to\ntrain three steps which simulate the multi-step inference scenarios. We also test the impact of whether recover ground\ntruth tokens or keep the predicted token in the second training step. Since the masking tokens are chosen by prediction confidence during inference, we also apply confidencebased masking during training to further verify our adaptive Y masking. Moreover, we also compare our adaptive\nY masking with the glancing masking strategy proposed in\nGLAT to improve the one-pass decoding. The results are\nshown in Table 8. We can observe that adaptive masking\noutperforms uniform masking in the second-step training,\nand the uniform masking seems to bring little improvements\ncompared with adaptive X masking (34.48). This also indicates that although AMOM may expand training expenses,\nadaptive Y masking is truly valuable, and the performance\nimprovements do not come from more updates. Moreover,\nresults also reflect that two-step refinements are enough for\nmodel training without the necessity for more steps. Besides,\nusing model prediction instead of ground truth can effectively reduce the problem of exposure bias, and introducing a confidence-based masking strategy does not bring improvements. Compared with GLAT, adopting the glancing\nmasking as the second step masking strategy also performs\nbetter than uniform masking but is inferior to our adaptive\n[0,10) [10,20) [20,30) [30,40) >= 40\nLENGTH\n20\n25\n30\n35\nBLEU\n[0,10) [10,20) [20,30) [30,40) >= 40\nLENGTH\n20\n25\n30\n35\nBLEU\nCMLM :   Iter.1\nAMOM :   Iter.1\nIter.10\nIter.10\nIter.N\nIter.N\nFigure 1: Comparison between different source language\nsentence length and decoding iterations.\nY masking. Besides, if we directly adopt glancing masking and one-step training the same as GLAT (Glat), the performance declines, and further combining it with encoder\nmasking even harms the performance. This indicates that our\nmethods play a different role compared with GLAT.\nMore Iterations for Long Sequence. For long source input\nsentences, it is almost impossible to obtain a fluent and relatively correct result for non-autoregressive machine translation models. It often requires multiple iterations to refine the\ntranslation results. Therefore, the ability to refine is a crucial evaluation criterion for a model. First, we compare the\nBLEU scores of AMOM and CMLM in different iterations\nsteps, as shown in Appendix. We can see that the AMOM\noutperforms the CMLM model when the iterations step increases, which proves that an adaptive masking strategy can\nenhance refinement ability. In addition, we make a comparison of results with different source sentence length N and\ndifferent decoding iterations T on two two datasets (IWSLT\nDE→EN and WMT EN→RO). We split each dataset into\nfive segments according to sentence length and run inference three times according to different steps N ∈ [1, 10, N].\nIn Figure 1, we present the improvements of more decoding steps with different colours. Results show that AMOM\nexhibit significant gain than vanilla CMLM with more steps,\ne.g., although the performance of AMOM in Iter.1 is inferior\nthan CMLM, it all outperforms CMLM in Iter.10, especially\nfor long sentences. We can also find that long sentences often require more decoding steps, and AMOM perform better.",
        "related work": "Iterative-based Non-autoregressive Sequence Generation. Non-autoregressive models have attracted an increasing attention in recent years due to their efficient decoding, but the improvements in decoding speed come at the\nexpense of generation quality. Thus, iterative-based nonautoregressive (NAR) models (Lee, Mansimov, and Cho\n2018; Gu, Wang, and Zhao 2019; Saharia et al. 2020; Geng,\nFeng, and Qin 2021; Lu, Meng, and Peng 2022) are proposed to achieve a better trade-off between the inference\nspeedup and generation quality. Lee, Mansimov, and Cho\nfirst propose the iterative model which aims refine the noised\ntarget sequence. Later, insertion and deletion operations are\nintroduced in each decoding iteration to create the final\ntranslation. Among these iterative NAR methods, the conditional masked language model (CMLM) (Ghazvininejad\net al. 2019) is widely-used owing to its promising performance when using the mask-predict strategy. In particular,\nCMLM leverages the masked language model objective to\nguide model training and iteratively masks and predicts tokens during inference. Many recently works have achieved\nperformance improvements based on CMLM (Guo, Xu, and\nChen 2020; Huang, Perez, and Volkovs 2022). Recently,\nSavinov et al. proposed step-unrolled denoising autoencoder\nwhich adopts denoising operation in each iteration.\nMasked Language Model. The masked language model\n(MLM) first introduced by BERT (Devlin et al. 2018) has\nbecome the essential component of various popular pretraining methods (Song et al. 2019; Liu et al. 2019; Dong\net al. 2019; Joshi et al. 2020; Li et al. 2022b; Xu, Van Durme,\nand Murray 2021). Its standard paradigm is to select some\ntokens in the source sequence by different strategies and then\nreplace them with a [mask] token, and then the model\nis trained to predict the masked tokens. Since the masking\nstrategy is significantly essential for these model, different\nmasking strategies are served as different learning methods.\nAs BERT is served as a single Transformer encoder and a\nmonolingual framework, there are limitations in various applications, such as machine translation. Then much progress\nhas been made to extend the applications of masked language modeling strategy (Guo et al. 2020; Zhu et al. 2020;\nLi et al. 2022b). The CMLM-based non-autoregressive models can also benefit from it by introducing a uniform masking strategy in training and a mask-predict decoding strategy during inference (Ghazvininejad et al. 2019). However,\nonly few improvements on masking strategies are explored\nfor CMLM. In this work, we further design a simple yet effective adaptive masking over masking method on both the\nencoder and decoder sides to enhance the CMLM training\nfor better refinement capability during inference.",
        "conclusion": "In this paper, we present an adaptive masking over masking\n(AMOM) strategy to enhance the conditional masked language model (CMLM) for non-autoregressive sequence generation. Our AMOM only contains two masking operations\nin model training without modifying the model structure\nor changing the inference schedule. Extensive experiments\non different sequence generation tasks indicate our proposed AMOM can yield significant performance improvement over the original CMLM model and even outperform\nthe strong autoregressive (Transformer) counterpart on 7\nNMT benchmark datasets and achieves SOTA performance\non WMT16 EN→RO, 34.82 BLEU on WMT16 RO→EN,\nand 34.84 BLEU on IWSLT De→En. Due to the limitation\nof computational resources, we only test our AMOM for the\nCMLM model. In the near future, we will design more elegant AMOM strategies and explore their effectiveness on\ndifferent NAR frameworks. We also will extend our AMOM\nto other types of masked language models, both in the pretraining and fine-tuning stages.",
        "summary_en": "Transformer-based autoregressive (AR) methods have achieved appealing performance for varied sequence-to-sequence generation tasks, e.g., neural machine translation, summarization, and code generation, but suffer from low inference efficiency. To speed up the inference stage, many non-autoregressive (NAR) strategies have been proposed in the past few years. Among them, the conditional masked language model (CMLM) is one of the most versatile frameworks, as it can support many different sequence generation scenarios and achieve very competitive performance on these tasks. Therefore, this paper further introduces a simple yet effective adaptive masking over masking strategy to enhance the refinement capability of the decoder and make the encoder optimization easier. Experiments on 3 different tasks (neural machine translation, summarization, and code generation) with 15 datasets in total confirm that the proposed simple method achieves significant performance improvement over the strong CMLM model. Surprisingly, the proposed model yields state-of-the-art performance on neural machine translation (34.62 BLEU on WMT16 EN to RO, 34.82 BLEU on WMT16 RO to EN, and 34.84 BLEU on IWSLT De to En) and even better performance than the AR Transformer on 7 benchmark datasets with at least 2.2x speedup.",
        "summary_zh": "这篇论文介绍了一种简单而有效的自适应掩码策略（AMOM）用于条件掩码语言模型（CMLM），以增强解码器的精细化能力并使编码器的优化更容易。在神经机器翻译、摘要和代码生成任务上的实验结果表明，其显著提高了CMLM模型的性能。特别是在神经机器翻译任务中，该方法取得了最先进的性能，在 7 个基准数据集上甚至比AR Transformer性能更好，且速度至少提高了2.2倍。"
    },
    {
        "title": "Language Model Pre-training on True Negatives",
        "abstract": "Discriminative pre-trained language models (PLMs) learn to predict original texts from intentionally corrupted ones. Taking the former text as positive and the latter as negative samples, the PLM can be trained effectively for contextualized representation. However, the training of such a type of PLMs highly relies on the quality of the automatically constructed samples. Existing PLMs simply treat all corrupted texts as equal negative without any examination, which actually lets the resulting model inevitably suffer from the false negative issue where training is carried out on pseudo-negative data and leads to less efficiency and less robustness in the resulting PLMs. In this work, on the basis of defining the false negative issue in discriminative PLMs that has been ignored for a long time, we design enhanced pre-training methods to counteract false negative predictions and encourage pre-training language models on true negatives by correcting the harmful gradient updates subject to false negative predictions. Experimental results on GLUE and SQuAD benchmarks show that our counter-false-negative pre-training methods indeed bring about better performance together with stronger robustness.",
        "introduction": "Large-scale pre-trained language (PLM) models are playing\nan important role in a wide variety of NLP tasks with their\nimpressive empirical performance (Radford et al. 2018; Peters et al. 2018; Devlin et al. 2019; Yang et al. 2019; Lan\net al. 2020; Clark et al. 2020). So far, there comes two major categories of PLMs with regards to the output style, the\ngenerative like GPT (Radford et al. 2018), which employ\na decoder for learning to predict a full sequence, and the\ndiscriminative like BERT style of PLMs which learn to reconstruct the original uncorrupted text from the intentionally\ncorrupted ones (Raffel et al. 2020; Lewis et al. 2020). In this\nwork, we focus on the latter category of PLMs, typically\nwith denoising objectives (also known as masked language\nmodeling, MLM) (Liu et al. 2019; Joshi et al. 2020; Sun\net al. 2019). In a denoising objective, a certain percentage of\ntokens in the input sentence are masked out, and the model\nshould predict those corrupted tokens during the pre-training\n(Peters et al. 2018; Sun et al. 2019; Levine et al. 2021; Li and\nZhao 2021).1\nAlthough existing studies have made progress in designing effective masking strategies (Sun et al. 2019; Joshi et al.\n2020; Levine et al. 2021) and auxiliary objectives (Lan et al.\n2020; Wang et al. 2020) for language model pre-training,\nthere is still a lack of attention on the quality of training\ndata. Discriminative PLM can be regarded as a kind of auto\ndenoising encoder on automatically corrupted texts. Thus, it\nis critical to ensure the auto-constructed data is true enough.\nIntuitively, a discriminative PLM learns to distinguish two\ntypes of samples, positive (already existing original ones)\nand negative (the corrupted ones from the auto constructing). Taking MLM as an example, a proportion of tokens in\nsentences are corrupted, e.g., replaced with mask symbols,\nwhich would affect the sentence structures, leading to the\nloss of semantics and increasing the uncertainty of predictions. In extreme cases, such corrupted texts may be linguistically correct. However, the current PLMs simply consider\nall corrupted texts as negative samples, so that the resulting\nPLM has to be trained on such pseudo-negative data with\nless efficiency and less robustness – suffers from the wasted\ntraining time on meaningless data and the trained PLM may\nbe vulnerable to adversarial attacks like diversity distraction\nand synonym substitution (Wang et al. 2021).\nFor each training instance, MLM only calculates labelwise matching between the prediction and the gold tokens\nin the training process, thus inevitably suffering from the\nissue of false negatives where the prediction is meaningful\nbut regarded as wrong cases, as examples shown in Table\n1. We obverse that such cases appear in more than 7% of\nthe training examples (more details in Section 2). The issue is also observed in sequence generation tasks, which is\ntied to the standard training criterion of maximum likelihood\nestimation (MLE) that treats all incorrect predictions as being equally incorrect (Wieting et al. 2019; Li et al. 2020).\n1There are different classification standards for PLMs, i.e., output style and model architecture. For simplicity, our taxonomy follows Wang, Liu, and Zhang (2021), which is based on the output style. Note that PLMs can also be classified into three types\nbased on the model architecture: encoder-only, decoder-only and\nencoder-decoder.\nExample\nGround-truth\nPrediction\nMLM\nCorrection\ni am trying to copy [MASK] onto my ipod good\nyou\nhappy\n✗\nan adaptive immune system whose [MASK] function ...\nprimary\nmain\n✗\n✓\nTable 1: Examples of true negative (the first line) and false negative (the second line). The standard MLM will treat all the\npredictions as incorrect ones. However, the last false negative predictions can be corrected to ensure more accurate pre-training.\nMore examples are in Figure 3.\nPositive\nPseudo-Negative\nPositive\nTrue Negative\nFalse Negative\na) Existing Work\nb) Our Work\nFigure 1: Overview of our study. Existing PLMs were\ntrained by distinguishing positive from pseudo-negative\ndata. In contrast, our work aims to encourage pre-training\nlanguage models on true negatives by detecting and counteracting false negative predictions.\nInstead of measuring negative diversity via diversity scores\nbetween the different incorrect model outputs, our method is\ndedicated to mediating the training process by detecting the\nalternative predictions as opposed to the gold one, to steer\nmodel training on true negatives, which benefits the resulting language modeling in general. The comparison with existing work is illustrated in Figure 1.\nThough the false negatives may potentially hurt the pretraining in both efficiency and robustness to a great extent,\nit is surprising that this problem is kept out of the research\nscope of PLMs until this work to our best knowledge. To\naddress the issue of misconceived false negative predictions\nand encourage pre-training language models on true negatives or more true negatives, we present an enhanced pretraining approach to counteract misconceived negatives. In\ndetail, we investigate two enhanced pre-training objectives:\n1) hard correction to shield the gradient propagation of the\nfalse negative samples to avoid training with false negative predictions; 2) soft regularization by minimizing the\nsemantic distances between the prediction and the original\none to smooth the rough cross-entropy. Experimental results\non widely-used down-streaming benchmark tasks, including GLUE (Wang et al. 2019) and SQuAD (Rajpurkar et al.\n2016), show that our approach boosts the baseline performance by a large margin, which verifies the effectiveness of\nour proposed methods and the importance of training on true\nnegatives. Case studies show that our method keeps simplicity and also improves robustness.\nBase Model\nLarge Model\nCheck.\nIter.\nPred.\nCheck.\nIter.\nPred.\n6.25\n6.90\n1.31\n6.25\n7.46\n1.50\n12.5\n6.96\n1.34\n12.5\n7.58\n1.55\n25.0\n6.97\n1.36\n25.0\n7.31\n1.49\n50.0\n7.05\n1.36\n50.0\n7.46\n1.56\n80.0\n7.06\n1.40\n80.0\n7.38\n1.57\n100.0\n7.07\n1.41\n100.0\n7.44\n1.60\nTable 2: Statistics (%) of the hard corrections under base\nand large settings on the wikitext-2-raw-v1 corpus. Checkpoint means the checkpoint saved at the specific training\nsteps (%).",
        "preliminaries: the false negative issue": "Definition\nOur concerned false negatives in MLM are the\nreasonable predictions but discriminated as wrong predictions because such predictions do not match the single gold\ntoken for each training case. For example, many tokens are\nreasonable but written in different forms or are synonyms of\nthe expected gold token.\nSeverity\nFor simplicity, we focus on the subset of false\nnegatives from WordNet (Miller 1992) – the predictions\nwhich are the synonyms of the ground-truth tokens. To\nhave an intuition about the severity of false negative predictions during pre-training, we collect the statistics from\ntwo perspectives: 1) prediction-level: the proportion of corrected predictions when they mismatch the gold labels; 2)\niteration-level: the proportion of iterations (sequences) when\nthe correction happens.2 We use the wikitext-2-raw-v1 corpus (Merity et al. 2017) for validation. We use the pretrained checkpoints of the BERT-base and BERT-large models described in Section 4.1 for the analysis.3\nAccording to Table 2, we observe that the ratio of detected false negatives is around 6.0%-7.0% in iteration-level\nand 1.0%-2.0% in token-level.4 As training goes on, the correction ratio increases, indicating that our method gradually\n2The rationale is that training on false negatives tends to learn\nincorrect semantics of the whole sequence.\n3The MLM process is the same as our experiments on BERT\nmodels in the subsequent sections.\n4It is hard to collect the statistics of false negatives automatically. For simplicity, we only calculate the subset related to synonyms. Therefore, the issue is expected to occur more frequently\nthan counted.\nplays a more important role as the training proceeds, which\nsupports our hypothesis.\nInfluence\nPre-training on false negatives would possibly\nbring harm in terms of training efficiency, model effectiveness, and robustness against adversarial attacks (detailed discussions in Section 5). As the saying goes, ”the rotten apple\ninjures its neighbors”, training on random examples would\nbring training bias from meaningless data, so it needs to be\ncorrected with more data and results in more cost of resources and time. In addition, the inaccurate pre-training\nmay affect the model robustness, as the PLM may fail to\ncapture the similarity of tokens or sentences in different expressions.",
        "methodology": "3.1\nMasked LM\nMasked LM (MLM) is a denoising language model technique used by BERT (Devlin et al. 2019) to take advantage of both the left and right contexts. Given a sentence s = {w1, w2, . . . , wn}, where a certain proportion\nof tokens are randomly replaced with a special mask symbol. The input is fed into the multi-head attention layer\nto obtain the contextual representations, which is defined\nas H = FFN(MultiHead(K, Q, V )), where K, Q, V are\npacked from the input sequence representation s. Then, the\nmodel is trained to predict the masked token based on the\ncontext.\nDenote Y as the set of masked positions using the mask\nsymbol, and the masked tokens are represented as wk, k ∈\nY. The objective of MLM is to maximize the following objective:\nLmlm(wk, s) = E\n \n−\nX\nk∈Y\nlog pθ(wk | s)\n!\n.\n(1)\n3.2\nPre-training on True Negatives\nA natural solution to encourage the language model pretraining on true negatives is to identify and counteract the\nfalse negative issue in language model pre-training. To this\nend, it is possible to correct or prune the harmful gradient\nupdate after detecting the false negative predictions. In detail, we investigate two enhanced pre-training objectives, including 1) hard correction (HC), which shields the gradient\npropagation of the false negative samples to avoid training\nwith false negative predictions; 2) soft regularization (SR),\nwhich measures the distribution similarity between the predicted token and the original one, to smooth the tough crossentropy by minimizing the semantic distances. Figure 2 illustrates our pre-training scheme.\nHard Correction\nThe criteria of hard correction is to\nprune the gradient when the model suffers from confusion\nabout whether the prediction is correct or not. For each prediction, we check if the predicted token rk is highly related\nto the ground-truth token wk based on a short lookup table\nV in which each wk is mapped to a list of synonyms V[wk].\nThe training objective is:\nLhc = E\n \n−\nX\nk∈Y,rk /∈V[wk]\nlog pθ(wk | s)\n!\n.\n(2)\nIn our implementation, the lookup table is built by retrieving the synonym alternatives for each word in the model\nvocabulary, e.g., from WordNet (Miller 1992) or Word2Vec\nembedding (Mikolov et al. 2013). Therefore, there will be no\nextra computation overhead for the construction of lookup\ntable during training and the cost of retrieving synonyms\nis imperceptible. For the synonym source, we use WordNet\nsynonyms by default (Section 5 will compare retrieving synonyms from WordNet and Word2Vec embedding). For each\ntraining iteration, if the predicted token is found in the synonym list for the gold token, then the correction is activated\nand the loss calculation for the k-th token will be neglected.5\nSuch a prediction will be judged as correct by HC in crossentropy — the correction can be applied by simply ignoring this prediction before feeding to the cross-entropy loss\nfunction. As a post-processing technique, the hard correction technique will not bring any false positives.\nSoft Regularization\nThe hard correction method above\nrelies on external tools, which may affect the coverage of\ncorrections due to the restricted size of the lookup table. In\npursuit of more general usage, we are interested in finding a\nsofter way to minimize the harm of false negatives. A natural way is to leverage semantic distance between the original\nand predicted tokens as regularization.\nFor wk and rk, we fetch their token representations from\nthe model’s embedding layer, denoted as ek and e′\nk, respectively. We leverage cosine similarity as the regularization\nbased on the intuition that the semantic distance between\nthe prediction and gold tokens should be minimized:\nLsr =\n1\nNm\nNm\nX\nk=1\n(1 −\nek · e′\nk\n∥ek∥ · ∥e′\nk∥),\n(3)\nwhere Nm is the number of masked tokens to predict.\nSR is based on the hypothesis that the predicted tokens\nshould have a semantic relationship with the gold ones in the\nsame embedding space to some extent, which is supported\nby various existing studies (Bordes et al. 2013; Zhang and\nZhao 2021; Chen et al. 2021; Li et al. 2020).6 We choose\nto apply SR to the embedding layer because the embedding\n5Words might be tokenized into several pieces before feeding\nPLMs, in which cases the correction will not be applied because\nthose cases do not violate our criteria. We found that there are\n62.51% tokens in the model vocabulary that have synonyms found\nin WordNet after removing all the stopwords.\n6A possible concern of SR is that it may encourage the model\nto minimize the cosine similarity between unrelated tokens in early\nstages when the model is not well-trained. However, since MLM\ndominates the training, especially in the early stages (e.g., the loss\nfrom around 30 to 10 until convergence in ELECTRA-small). In\ncontrast, the value of SR is usually 0-1. As such, SR would not\nharm the training in the early stages. As a regularization method,\nit further enhances the model performance beyond MLM when the\ntraining proceeds.\nantibodies are protein components of \nan adaptive immune system whose  \n[MASK]  function is to bind antigen ##s , \nor other substances in the body ...\nPrLM \nθ\nprimary\nmain\n+\nθ\nLookup\nmatch?\nretrieval\n~~\nEmbed.\nRetrieval\ns’\nIk\nLhc\nLsr\nFigure 2: Illustration of our pre-training scheme.\nlayer is the most fundamental and stable layer as it is far\nfrom the output layer to reflect on the returned gradients during training. Optimizing the embedding layer would possibly lead to a more severe influence on the model training\nand help the model learn semantics between words better, as\nindicated by (Jiang et al. 2020). Just calculating token-wise\ndistance neglects the context of the whole sequence. In Section 5, we will discuss the pros and cons of token-level and\nsentence-level SR variants.",
        "experiments": "4.1\nSetup\nPre-training\nIn this part, we will introduce the model\narchitecture, hyper-parameter setting, and corpus for pretraining our models. Our methods are applicable to general\nMLM-style language models. Considering the training efficiency, we employ ELECTRA small and base as our default\nbackbone models and implement our pre-training objectives\non top of them. We follow the model configurations in (Clark\net al. 2020) for fair comparisons. For hyper-parameters, the\nbatch size is 128 for the base models in our work instead of\n256 as in the original setting due to limited resources. The\nmask ratio is 15%. We set a maximum number of tokens as\n128 for small models and 512 for base models.7 The small\nmodels are pre-trained from scratch for 1000k steps. To save\ncomputation, like previous studies (Dong et al. 2019), we\ncontinue training base models for 200k steps using the pretrained weights as initialization. The learning rates for small\nand base models are 5e-4, and 5e-5, respectively. We use\nOpenWebText (Radford et al. 2019) to train small models,\nand Wikipedia and BooksCorpus (Zhu et al. 2015) for training base models following (Clark et al. 2020). The baselines\nand our models are trained to the same steps for a fair comparison.\nTo verify the generality of our methods on other PLMs,\nwe also implemented them on BERTbase and BERTlarge\nbackbones (Devlin et al. 2019) according to the same implementation for ELECTRAbase. Specifically, we pre-train our\nmethods based on BERTbase and BERTlarge checkpoints for\n200k steps on the Wikipedia and BooksCorpus. For a fair\ncomparison, we also train the baseline models to the same\nsteps. Please note that it is inadequate to pursue absolute\n7For evaluation of the reading comprehension tasks, we also\npre-train the variants with the length of sentences in each batch as\nup to 512 tokens.\ngains for large models by using single-machine NVIDIA\nV100 GPUs (e.g., slower convergence speed with much\nsmaller batch sizes), compared with TPUs for training large\nmodels in public releases (Devlin et al. 2019). Therefore,\nwe focus on the relevant improvements between our methods and the baselines under the same training steps.\nFine-tuning\nFor evaluation, we fine-tune the pre-trained\nmodels on GLUE (General Language Understanding Evaluation) (Wang et al. 2019) and SQuAD v1.1 (Rajpurkar et al.\n2016) to evaluate the performance of the pre-trained models.\nGLUE include two single-sentence tasks (CoLA (Warstadt,\nSingh, and Bowman 2019), SST-2 (Socher et al. 2013)),\nthree similarity and paraphrase tasks (MRPC (Dolan and\nBrockett 2005), STS-B (Cer et al. 2017), QQP (Chen et al.\n2018) ), three inference tasks (MNLI (Nangia et al. 2017),\nQNLI (Rajpurkar et al. 2016), RTE (Bentivogli et al. 2009).\nWe follow ELECTRA hyper-parameters for single-task finetuning. We did not use any training strategies like starting\nfrom MNLI, to avoid extra distractors and focus on the fair\ncomparison in the single-model and single-task settings.\n4.2\nMain Results\nWe evaluate the performance of our pre-training enhancement compared with the baselines in small and base sizes\non GLUE and SQuAD benchmarks in Tables 3-4. From the\nresults, we have the following observations:\n1) The models with our enhanced pre-training objectives outperform the BERT and ELECTRA baselines in all\nthe subtasks. In particular, with the same configuration and\npre-training data, for both the small-size and the base-size,\nour methods outperform the strong ELECTRA baselines by\n+1.5(dev)/+1.4(test) and +0.7(dev)/+1.3(test) on average, respectively. The results demonstrate that our proposed methods improve the pre-training of ELECTRA substantially and\ndisclose that mediating the training with true negatives is\nquite beneficial for improving language model pre-training.\n2) Our methods outperform the baselines on both the base\nand large models,8 which indicates that the false negative\nissue may be independent of the model size, and the training\nremains insufficient in training language models on different\nscales.\n8Since larger models obtain better baseline results, we also calculate error-reduction ratio (ERR) for comparison, e.g., the ERR\nof BERTHC\nbase and BERTHC\nlarge is 3.6% and 2.3%, respectively. The\nstatistics also indicate consistent strength in varied model sizes.\nModel\nCoLA\nSST\nMRPC\nSTS\nQQP\nMNLI\nQNLI\nRTE\nAverage\n∆\nBERTbase\n61.1\n93.0\n86.8\n87.1\n90.8\n84.7\n91.4\n67.9\n82.9\nBERTHC\nbase\n62.9\n93.2\n87.5\n87.4\n90.9\n84.9\n91.5\n69.3\n83.5\n↑0.7\nBERTSR\nbase\n61.2\n93.5\n89.0\n87.5\n90.9\n84.8\n91.6\n68.6\n83.4\n↑0.6\nBERTLarge\n61.7\n93.7\n88.5\n90.1\n91.3\n86.7\n92.4\n72.9\n84.7\nBERTHC\nlarge\n62.3\n93.4\n89.0\n90.5\n91.5\n87.0\n93.0\n73.7\n85.1\n↑0.4\nBERTSR\nlarge\n62.3\n94.2\n89.2\n90.1\n91.4\n87.0\n92.8\n74.0\n85.1\n↑0.4\nELECTRAsmall\n56.8\n88.3\n87.4\n86.8\n88.3\n78.9\n87.9\n68.5\n80.4\nELECTRAHC\nsmall\n62.0\n89.8\n87.0\n86.7\n89.0\n80.4\n88.0\n67.9\n81.4\n↑1.0\nELECTRASR\nsmall\n61.1\n90.1\n89.5\n87.0\n89.4\n80.8\n88.8\n68.6\n81.9\n↑1.5\nELECTRAbase\n68.3\n95.3\n90.9\n91.3\n91.7\n88.5\n93.0\n82.3\n87.7\nELECTRAHC\nbase\n70.9\n95.6\n91.2\n91.3\n92.0\n88.7\n93.6\n83.8\n88.4\n↑0.7\nELECTRASR\nbase\n70.4\n95.4\n90.4\n91.2\n91.9\n89.1\n93.4\n84.8\n88.3\n↑0.6\nTable 3: Comparisons between our proposed methods and the baseline pre-trained models on the dev set of GLUE tasks. STS\nis reported by Spearman correlation, CoLA is reported by Matthew’s correlation, and other tasks are reported by accuracy.\nModel\nEM\n∆EM\nF1\n∆F1\nELECTRAsmall\n75.8\n83.9\nELECTRAHC\nsmall\n77.7\n↑1.9\n85.6\n↑1.7\nELECTRASR\nsmall\n76.0\n↑0.2\n84.2\n↑0.3\nELECTRAbase\n85.1\n91.6\nELECTRAHC\nbase\n85.7\n↑0.6\n92.1\n↑0.5\nELECTRASR\nbase\n85.6\n↑0.5\n92.0\n↑0.4\nTable 4: Results on the SQuAD dev set. EM and F1 are short\nfor the exact match and F1 scores (Rajpurkar et al. 2016).\n3) Both SR and HC pre-training strategies help the resulting model surpass the baselines. Note that our proposed\nmethod is model-agnostic so that the convenient usability\nof its backbone precursor can be kept without architecture\nmodifications. In comparison, SR is more generalizable as it\ndoes not require extra resources, while HC has the advantage\nof interpretation via explicit correction.\n4) Our enhanced pre-training objectives show considerable performance improvements on linguistics-related tasks\nsuch as CoLA and MRPC. These tasks are about linguistic acceptability and paraphrase/semantic equivalence relationship. Besides, our methods also achieve obvious gains\nin tasks requiring complex semantic understanding and reasoning, such as MNLI and SQuAD, showing that they may\nhelp capture semantics to some extent.",
        "analysis": "Robustness Evaluation\nIntuitively, our method would be\nhelpful for improving the robustness of PLMs because the\napproaches may indicate lexical semantics and representation diversity during the correction or regularization operations. To verify the hypothesis, we use a robustness evaluation platform TextFlint (Wang et al. 2021) on SQuAD, from\nwhich two standard transformation methods are adapted: 1)\nAddSentenceDiverse generates distractors with altered questions and fake answers; 2) SwapSynWordNet transforms an\ninput by replacing its words with synonyms provided by\nWordNet.\nTable 5 shows the robustness evaluation results. We observe that both kinds of attacks induce a significant performance drop of the baseline system, by 54.95% and 6.0% on\nthe EM metrics, respectively, indicating that the system is\nsensitive to distractors with similar meanings. In contrast,\nboth of our models can effectively resist those attacks with\nless performance degradation. Specifically, the HC method\nworks stably in the SwapSynWordNet attack. We speculate\nthe reason is that the hard correction strategy captures the\nsynonym information during pre-training, which would take\nadvantage of lexical semantics. The other variant, the soft\nregularization objective, achieves much better performance\nin the AddSentenceDiverse. The most plausible reason might\nbe the advantage of acquiring semantic diversity by regularizing the semantic distance in the SR objective.\nLookup Table from WordNet vs. Word2Vec\nFor the\nhard correction approach, the candidate synonyms for detecting false negative predictions can be derived from WordNet (Miller 1992) or Word2Vec embedding space (Mikolov\net al. 2013) as described in Section 3.2.9 To verify the impact\nof different sources, we compare the results as shown in the\nsecond block of Table 6. We see that ELECTRAHC\nWordNet outperforms ELECTRAHC\nEmbedding by a large margin. The most\nplausible reason would be that the retrieved list of synonyms\nfrom ELECTRAHC\nWordNet would have higher quality than that\nfrom ELECTRAHC\nEmbedding. Although the embedding-based\nmethod may benefit from semantic matching, but would\nalso bring noise as it is hard to set the threshold to ensure the top-ranked words are accurate synonyms. Therefore, ELECTRAHC\nWordNet turns out to be better suitable for\n9We use the public GloVe.6B.50d vectors for embedding\nretrieval. Since the embedding method returns a ranked list by calculating the similarity score with the whole vocabulary, we only\ntake the top 10 most similar words for each retrieval.\nModel\nAddSentenceDiverse (Ori.→Trans.)\nSwapSynWordNet (Ori.→Trans.)\nExact Match\n∆EM\nF1 Score\n∆F1\nExact Match\n∆EM\nF1 Score\n∆F1\nELECTRAsmall\n80.55→25.60\n↓54.95\n85.10→26.43\n↓58.67\n80.67→74.67\n↓6.00\n85.38→80.43\n↓4.95\nELECTRAHC\nsmall\n82.59→34.13\n↓48.46\n86.78→36.60\n↓50.18\n82.33→79.67\n↓2.66\n86.68→83.65\n↓3.03\nELECTRASR\nsmall\n78.84→37.20\n↓41.64\n80.84→38.29\n↓42.55\n78.67→75.67\n↓3.00\n80.88→78.51\n↓2.37\nTable 5: Robustness evaluation on the SQuAD dataset. Ori. represents the results of original dataset for robustness evaluation\nderived from the SQuAD 1.1 dev set by TextFlint (Wang et al. 2021) while Trans. indicates the transformed one. The assessed\nmodels are the small models from Table 4.\nModel\nCoLA\nSST\nMRPC\nSTS\nQQP\nMNLI\nQNLI\nRTE\nAverage\n∆\nELECTRAsmall\n56.8\n88.3\n87.4\n86.8\n88.3\n78.9\n87.9\n68.5\n80.4\nELECTRAHC\nWordNet\n62.0\n89.8\n87.0\n86.7\n89.0\n80.4\n88.0\n67.9\n81.4\n↑1.0\nELECTRAHC\nEmbedding\n59.0\n88.5\n87.0\n86.4\n88.8\n79.6\n87.9\n67.1\n80.6\n↑0.2\nELECTRASR\nWord\n61.1\n90.1\n89.5\n87.0\n89.4\n80.8\n88.8\n68.6\n81.9\n↑1.5\nELECTRASR\nSent\n59.5\n89.6\n90.0\n86.7\n89.1\n80.4\n90.0\n68.2\n81.6\n↑1.2\nTable 6: Comparative studies of variants on GLUE dev sets on small models. The first block shows our baseline. The second\nblock presents the results of HC methods based on WordNet and Word2Vec embedding. The third block compares the wordlevel regularization and sentence-level regularization.\nantibodies are protein components of an adaptive immune system whose  [MASK]  function is to bind antigen ##s , or other substances in the body ...\nGold:   primary\nPred:    main\n['primary_winding', 'principal', 'master', 'elementary', 'chief', 'primary', 'elemental', 'basal', 'primary_quill',              \n'main',  'primary_election', 'primary_feather', 'primary_coil']\nthey could not have been in good shape after fighting into the next day in intense moderate heat and having to  [MASK]  in position overnight , far from helpless and harassed by the infantry ...\nGold:   stay\nPred:    remain\n['continue', 'halt', 'stop', 'check', 'delay', 'quell', 'arrest', 'remain', 'bide', 'detain', 'persist', 'stick_around', \n'ride_out', 'last_out',  'stay',  'abide', 'stoppage', ..., ]\n(a) \n(b) \n{\n{\nFigure 3: Interpretation of the hard correction process. The orange box contain the input sentence, the purple buttons indicate\nthe gold and predicted tokens, and the blue box shows the WordNet synonyms for the gold token.\nour task.10\nFrom Word-level to Sentence-level Regularization\nThe\nsoft regularization approach measures the semantic distance\nbetween the predicted one and the ground truth, which\nmay neglect the sentence-level context. We are interested\nin whether measuring the sentence-level similarity would\nachieve better results. To verify the hypothesis, we fill the\nmasked sentence s with the predicted tokens rk to have the\npredicted sentence sp. Then, sp and s are fed to the Transformer encoder to have the contextualized representation\nHp and Hs, respectively. To guide the probability distribution of model predictions Hp to match the expected probability distribution Hs, we adopt Kullback–Leibler (KL) di10Since the HC method relies on synonym retrieval from external sources, it is possible to bring false positive corrections theoretically. However, we seldom detect such cases in our preliminary\nexperiments, so we leave the open question for interested readers\nto avoid deviating from the focus of this paper.\nvergence: Lkl = KL(Hp ∥ Hs), where Lkl is applied as\nthe degree of sentence-level semantic mismatch. In detail,\nwe first apply softmax on the two hidden representations\nto obtain two distributions, and then the KL divergence is\ncalculated between those two distributions. The loss function is then written as: L′ = Ldlm + Lkl. For clarity,\nwe denote the original ELECTRASR\nsmall method described in\nEq. 3 as ELECTRASR\nWord and the sentence-level variant as\nELECTRASR\nSent.\nThe comparative results are reported in the third block\nof Table 6, which indicates that using sentence-level regularization (ELECTRASR\nSent) also outperforms the baseline\nand nearly reaches the performance of word-level one\n(ELECTRASR\nWord) on average, with slightly better results on\nMRPC and MNLI. Although ELECTRASR\nSent still keeps the\nsame parameter size with baseline, it leads to more computation resources because it requires the extra calculation for\nthe predicted sequence Hp. Therefore, considering the balance between effectiveness and efficiency, ELECTRASR\nWord\ncan serve as the first preferred choice for practical applications, and ELECTRASR\nSent can be employed when computation resources are sufficient.11\nCase Studies\nTo interpret how our method works, we\nrandomly select some hard correction examples as shown\nin Figure 3 by taking the ELECTRAsmall as the baseline\nmodel. We find that the baseline model produces reasonable\npredictions such as main and remain, as opposed to the golds\nones, primary and stay. Those predictions will be determined as wrong and possibly harm pre-training. Fortunately,\nsuch cases can be easily solved by our proposed method.\nThough the synonym list may contain irrelevant words, our\ncorrection will not bring false positives because it only cares\nabout whether the predicted word is in the shortlist or not.\nBeing a detected synonym is a sufficient condition, though it\nis not a necessary condition as those predictions make up the\nsubset of false negatives. Therefore, inappropriate options in\nthe list would not bring side effects.",
        "related work": "Self-supervised learning is one of the major topics in training pre-trained models (Peters et al. 2018; Radford et al.\n2018; Devlin et al. 2019; Zhu et al. 2022), which decides\nhow the model captures knowledge from large-scale unlabeled data. Recent studies have investigated denoising patterns (Raffel et al. 2020; Lewis et al. 2020), MLM alternatives (Yang et al. 2019), and auxiliary objectives (Lan\net al. 2020; Joshi et al. 2020) to improve the power of pretraining. However, studies show that the current models still\nsuffer from under-fitting issues, and it remains challenging\nto find efficient training strategies (Rogers, Kovaleva, and\nRumshisky 2020).\nDenoising Patterns\nMLM has been widely used for pretraining (Devlin et al. 2019; Lan et al. 2020; Clark et al.\n2020; Song et al. 2020), in which the fundamental part is\nhow to construct high-quality masked examples (Raffel et al.\n2020). The current studies commonly define specific patterns for mask corruption. For example, some are motivated\nfrom the language modeling units, such as subword masking\n(Devlin et al. 2019), span masking (Joshi et al. 2020), and ngram masking (Levine et al. 2021; Li and Zhao 2021). Some\nemploy edit operations like insertion, deletion, replacement,\nand retrieval (Lewis et al. 2020; Guu et al. 2020). Others\nseek for external knowledge annotations, such as named entities (Sun et al. 2019), semantics (Zhou et al. 2020; Zhang\net al. 2020b), and syntax (Zhang et al. 2020c; Xu et al.\n2021). To provide more diversity of mask tokens, RoBERTa\napplied dynamic masks in different training iterations (Liu\net al. 2019). These prior studies either employ pre-defined\nmask construction patterns or improve the diversity of mask\ntokens to help capture knowledge from pre-training.\n11Besides the methods we discussed in this work, there are alternative ways to achieve the regularization effects, e.g., using a\nsoftmax temperature with the standard loss.\nMLM Alternatives\nTo alleviate the task mismatch between the pre-training and the fine-tuning tasks, XLNet\n(Yang et al. 2019) proposed an autoregressive objective for\nlanguage modeling through token permutation, which further adopts a more complex model architecture. Instead\nof corrupting sentences with the mask symbol that never\nappears in the fine-tuning stage, MacBERT (Cui et al.\n2020) proposes to use similar words for the masking purpose. Yamaguchi et al. (2021) also investigates simple pretraining objectives based on token-level classification tasks\nas replacements of MLM, which are often computationally\ncheaper and result in comparable performance to MLM.\nIn addition, training sequence-to-sequence (Seq2Seq) language models has also aroused continuous interests (Dong\net al. 2019; Lewis et al. 2020; Raffel et al. 2020).\nAuxiliary Objectives\nAnother research line is auxiliary\nobjectives in conjunction with MLM, such as next sentence prediction (Devlin et al. 2019), span-boundary objective (Joshi et al. 2020), and sentence-order prediction (Lan\net al. 2020). Such line of research emerges as hot topics, especially in domain-specific pre-training, such as dialogueoriented language models (Zhang et al. 2020a; Wu et al.\n2020; Zhang and Zhao 2021).\nAs the major difference from the existing studies, our\nwork devotes itself to mediating misconceived negatives as\nthe essential drawback of MLM during the MLE estimation\nand aiming to guide language models to learn from true negatives through our newly proposed regularization and correction methods. Besides the heuristic pre-trained patterns\nlike masking strategies during data construction, we stress\nthat there are potential post-processing strategies to guide\nthe MLM training: correction and pruning. Those strategies are considered to deal with the false negative issue during MLM training, where the model would yield reasonable\npredictions but discriminated as wrong predictions because\nsuch predictions do not match the single gold token for each\ntraining case. For example, many tokens are reasonable but\nwritten in different forms or are the synonyms of the expected gold token. We could directly drop the uncertain predictions or correct the training with soft regularization. Promoting our view to sentence level, the similarity between\nthe predicted sentence and the original sentence can also be\ntaken into account to measure the sentence-level confidence\nthat indicates how hard the task is, which would be beneficial to provide more fine-grained signals and thus improve\nthe training quality.",
        "conclusion": "The work identifies the false negative issue in language\nmodel pre-training and proposes methods to counteract it.\nThough discriminative PLMs may quite straightforwardly\nsuffer from the false negative issue according to our exploration in this work, it has been completely ignored for a long\ntime, and it is a bit surprising that maybe this work is the first\none that formally considers such a big pre-training leak. Our\nwork indicates that mediating false negatives is so important\nthat counter-false-negative pre-training can synchronously\nimprove the effectiveness and robustness of PLMs.",
        "summary_en": "Discriminative pre-trained language models (PrLMs) learn to predict original texts from intentionally corrupted ones. Taking the former text as positive and the latter as negative samples, the PrLM can be trained effectively for contextualized representation. However, the training of such a type of PrLMs highly relies on the quality of the automatically constructed samples. Existing PrLMs simply treat all corrupted texts as equal negative without any examination, which actually lets the resulting model inevitably suffer from the false negative issue where training is carried out on pseudo-negative data and leads to less efficiency and less robustness in the resulting PrLMs. Therefore, on the basis of defining the false negative issue in discriminative PrLMs that has been ignored for a long time, this paper designs enhanced pre-training methods to counteract false negative predictions and encourage pre-training language models on true negatives by correcting the harmful gradient updates subject to false negative predictions. Experimental results on GLUE and SQuAD benchmarks show that the counter-false-negative pre-training methods indeed bring about better performance together with stronger robustness.",
        "summary_zh": "这篇论文研究了如何解决判别式预训练语言模型中存在的虚假负样本问题。判别性预训练语言模型的训练高度依赖于自动构建样本的质量，而现有的预训练方法简单地将所有损坏的文本视为等价的负样本，而没有对其进行任何检查，这实际上让生成的模型不可避免地受到虚假负样本问题的困扰，从而导致了生成模型的效率和鲁棒性较低。为了解决这个问题，作者设计了增强的预训练方法来对抗虚假负样本预测，并通过校正受虚假负样本预测影响的有害梯度更新来促使语言模型在真实负样本上进行预训练。实验结果表明，对抗虚假负样本的预训练方法带来了更好的性能和更强的鲁棒性。"
    },
    {
        "title": "On Grounded Planning for Embodied Tasks with Language Models",
        "abstract": "Language models (LMs) have demonstrated their capability in possessing commonsense knowledge of the physical world, a crucial aspect of performing tasks in everyday life. However, it remains unclear whether they have the capacity to generate grounded, executable plans for embodied tasks. This is a challenging task as LMs lack the ability to perceive the environment through vision and feedback from the physical environment. In this paper, we address this important research question and present the first investigation into the topic. Our novel problem formulation, named G-PlanET, inputs a highlevel goal and a data table about objects in a specific environment, and then outputs a step-by-step actionable plan for a robotic agent to follow. To facilitate the study, we establish an evaluation protocol and design a dedicated metric, KAS, to assess the quality of the plans. Our experiments demonstrate that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs’ ability in grounded planning. Our analysis also reveals interesting and non-trivial findings. 1",
        "introduction": "Pre-trained language models (LMs) demonstrate exceptional\nproficiency in a wide range of natural language processing\n(NLP) tasks such as question answering, machine translation, and summarization. They indeed capture some commonsense knowledge about our physical world such as\n“birds can fly”. However, the question of whether LMs can\nexhibit reasoning abilities within a grounded, realistic setting remains an open issue. This is because LMs lack the\nsensory experiences and physical interactions with the environment that enable human beings to grasp the nuances of\nreal-life situations and plan for completing tasks.\nEmbodied robotics learning is a growing field that seeks\nto create artificial intelligence agents capable of navigating and performing tasks within real-world environments, typically simulated through physical engines such\nas AI2THOR (Kolve et al. 2017). The ALFRED benchmark (Shridhar et al. 2020) represents one of the pioneering\nGrounded planning\nØ Turn right and walk to the stove.\nØ Pick up the tea pot on the left side of the stove.\nØ Turn left and walk to the shelves on the right.\nØ Place the tea pot on the middle shelf to the left \nof the glass container.\nLMs\nTask: Move a teapot \nfrom the stove to a shelf.\nFigure 1: The task of grounded planning for embodied tasks\n(G-PlanET). The input to the LMs is a goal with a specific\nenvironment, and the output is a step-by-step plan that can\nguide a robot to complete the task.\ndatasets that bridges the gap between NLP and robotics, providing a platform for investigating language-directed agents.\nThe objective of these studies is to design and test agents\nthat can translate language instructions into sequences of\nlow-level actions that enable the agent to manipulate objects\nwithin an environment and achieve a desired outcome (e.g.,\ncleaning an object and placing it elsewhere).\nHowever, the primary emphasis of the ALFRED benchmark and related datasets is on the comprehension of preestablished plans, rather than the ability to reason and independently plan within a realistic environment. Prior research\nfocuses on the capacity of agents to comprehend and execute step-by-step plans, but not on their capacity for decomposing tasks and generating such plans, which represents a\nmore advanced skill. Additionally, the role of LMs has received limited examination in the context of these benchmarks, where they are mainly used as encoders for embedding token sequences, rather than for planning or reasoning.\nPrior studies have explored the planning capability of\nLMs, with Huang et al. (2022) demonstrating that GPT3 and similar models are capable of generating general\nplans for executing everyday tasks. However, these plans\nlack grounding in a realistic environment, as LMs are not\nenvironment-specific. As a result, these plans are not necessarily executable by agents. For instance, in the context of an\nALFRED task to “move a teapot from the stove to a shelf,”\nembodied agents require knowledge of the location of the\nteapot and the path to reach it. Humans, on the other hand,\ncan readily observe the location of the teapot on the stove\nand their current position in the kitchen, allowing them to\nformulate a grounded plan that starts with “turn right and\nwalk to the stove.” This highlights the need for generating\ndetailed, step-by-step action sequences for robotic agents to\nuse in their execution processes.\nCan LMs also learn grounded planning ability? How\nshould we evaluate and improve LMs for grounded planning? In this paper, we address the question of whether\nLMs can also learn grounded planning abilities. To this end,\nwe propose a study on the ability of language models for\ngrounded planning for embodied tasks (G-PlanET). Our approach involves providing LMs with two inputs: a highlevel task description and a realistic environment in the form\nof an object table. The output is a plan consisting of executable, step-by-step actions. We formulate G-PlanET as a\nlanguage generation task and focus on encoder-decoder language models such as BART (Lewis et al. 2020).\nIn order to establish a dataset and evaluation protocol for\nG-PlanET, we leveraged the ALFRED data by developing a\nsuite of data conversion programs. They extract the object\ninformation from the environment and format it into data tables, thereby enabling models to access observations from\nrealistic scenarios. Additionally, we formulated a new evaluation metric, referred to as KAS, that is more appropriate\nfor the task than existing ones for text generation. As regards the methodology of G-PlanET, we suggest flattening\nan object table into a sequence of tokens and appending it\nto the task description as input to the model. The base LMs\nare then fine-tuned with these seq2seq data to learn to generate plans. Furthermore, we propose a simple yet effective\ndecoding strategy that iteratively generates subsequent steps\nby incorporating the previous generation into the input. Our\nempirical results and analysis indicate that incorporating object tables into inputs and the proposed iterative decoding\nstrategies are both crucial for enhancing the performance of\nlanguage models in G-PlanET.\nTo summarize, our main contributions are:\n• The task of G-PlanET: To the best of our knowledge,\nthis is one of the first studies to investigate the ability\nof LMs for embodied planning in realistic environments.\nG-PlanET is crucial for advancing the grounded generalization of large LMs and bridging the gap between NLP\nand embodied intelligence. (Sec. 2)\n• A comprehensive evaluation protocol: We put significant\neffort to convert the ALFRED and AI2THOR data into\ndata tables to support the evaluation of G-PlanET. We\nalso created a new evaluation metric, KAS, to effectively\nassess the plans generated by the LMs.\n• Improving LMs for G-PlanET: We present two simple\nbut effective components for enhancing the grounded\nplanning ability of LMs - flattening object tables and an\niterative decoding strategy. Our experiments show that\nthese components lead to notable performance gains.\n(Sec. 3) Also, through extensive experimentation and indepth analysis, we have gained a deeper understanding of\nthe behavior of LMs for G-PlanET and present a series of\nnon-trivial findings in our study.",
        "problem formulation": "Here we present the background knowledge, the problem\nformulation and the data sources for G-PlanET.\n2.1\nBackground Knowledge\nEmbodied tasks.\nThe ALFRED benchmark (Shridhar\net al. 2020) is among the first benchmarks focusing on embodied tasks in realistic environments, although most of the\nexamples are household tasks. It aims to test the ability of\nagents to execute embodied tasks in real-world scenarios.\nSpecifically, the agents need to understand language-based\ninstructions and output a sequence of actions to interact with\nan engine named AI2-THOR (Kolve et al. 2017), such that\nthe given tasks can be achieved.\nLanguage instructions.\nLanguage instructions play an\nimportant role in the ALFRED benchmark. The embodied\ntasks are annotated with a high-level goal and a low-level\nplan (i.e., a sequence of executable actions for robots) in\nnatural language, which are both inputs to the agents. The\nagents need to understand such language instructions and\nparse them into action templates. Note that the agents do not\nneed to plan for the task, as they already have the step-bystep instructions to follow.\nTask planning.\nPrior works show that large pre-trained\nlanguage models (LMs) such as GPT-3 (Brown et al. 2020)\ncan generate general procedures for completing a task. However, such plans are not aligned with the particular environment in which we are interested. This is because these\nmethods never encode the environment as part of the inputs to LMs for grounding the plans to the given environment. Therefore, such non-grounded plans are hardly useful\nin guiding agents to work in real-world situations.\n2.2\nG-PlanET with LMs\nAs discussed in Sec. 2.1, the ALFRED benchmark does\nnot explicitly test the planning ability, while prior works on\nplanning with LMs have not considered grounding to a specific environment. In this work, we focus on evaluating and\nimproving the ability to generate grounded plans for embodied tasks with LMs, which we dub as G-PlanET. It has been\nan underexplored open problem for both the robotics and\nNLP communities.\nTask formulation.\nThe task we aim to study in this paper is essentially a language generation problem. Specifically, the input is two-fold: 1) a high-level goal G and 2)\nG +𝑆! + … + 𝑆\" + E\n𝑆\"#!\nObject table for the env. (E)\nRealistic env.\nG + E\n𝑆!,𝑆%, … , 𝑆&\nTask (G): Move a teapot from the stove to a shelf.\nFigure 2: The overall workflow of the proposed methods. First, we extract the object table from the realistic environment. Then\nwe flatten the table into a sequence of tokens E (Sec. 3.2). We provide two learning methods for generating plans: 1) generate\nthe whole plan S1, S2, ⋯, ST and 2) iteratively decode the St+1 (Sec. 3.3).\na specific environment E that the agents need to ground\nto. The expected output is a sequence of actionable plans\nS = {S1, S2, ⋯} to solve the given goal in the specific environment step-by-step. The goal G and the plan S are in the\nform of natural language, while the environment E can be\nviewed as a data table consisting of the object information\nin a room. Figure 2 shows an illustrative example and we\nwill discuss more details in Section 3.2.\n2.3\nData for G-PlanET\nTo build a large-scale dataset for studying the G-PlanET\ntask, we re-use the goals and the plans of ALFRED and extract object information from AI2THOR for the aligned environment. The ALFRED dataset uses the AI2THOR engine\nto provide an interactive environment for agents with an egocentric vision to perform actions. However, the dataset does\nnot contain explicit data about objects in the environment\n(e.g., the coordination, rotation, and spatial relationship with\neach other).\nWe develop a suite of conversion programs for using\nAI2THOR to re-purpose the ALFRED benchmark for evaluating the methods shown in Section 3. We managed to get\na structured data table to describe the environment of each\ntask in the ALFRED dataset. We explore the AI2THOR engine and write conversion programs such that we can get\nfull observations of all objects: properties (movable, openable, etc.), positions (3D coordinates & rotation), sizes, and\nspatial relationships (e.g., object A is on the top of object B).\nWe believe our variant of the ALFRED data will be a great\nresource for the community to study G-PlanET and future\ndirections in grounded reasoning.",
        "methods": "Herein, we introduce the methods that we adopt or propose\nto address the G-PlanET problem. First of all, we present\nthe base language models that are encoder-decoder architectures. Then, we show in detail how we encode the environment data and integrate them with the seq2seq learning frameworks. Finally, we propose an interactive decoding\nstrategy that significantly improves performance.\n3.1\nBase Language Models\nPretrained encoder-decoder language models, such as\nBART (Lewis et al. 2020) and T5 (Raffel et al. 2019), have\nachieved promising performance in many well-known language generation tasks such as summarization and question\nanswering. They also show great potential for general commonsense reasoning tasks such as CommonsenseQA (Talmor et al. 2019), suggesting that these large LMs have common sense to some extent. As the G-PlanET can be also\nviewed as a text generation problem, we use these LMs as\nthe backbone for developing further planning methods, hoping that their common sense can be grounded in real-world\nsituations for embodied tasks.\nVanilla baseline methods.\nAs shown in many papers,\nBART and T5, when sizes are similar, show comparable performance in many generation tasks. Thus, we use BARTbase and BART-large as two selected LMs for evaluation.\nThe simplest and most straightforward baseline method of\nusing such LMs to solve G-PlanET is to ignore the environment and only use the goal as the sole input. Then, we\nfine-tune the base LMs with the training data and expect\nthey can directly output the whole plan as a single sequence\nof tokens (including special separator tokens). This simple\nmethod does not allow the LMs to perceive the environment,\nalthough training from the large-scale data can still teach the\nLMs some general strategies for planning. Therefore, we see\nthis as an important baseline method to analyze.\n3.2\nEncoding Realistic Environments\nTo enable the LMs to perceive an environment, we need\nto encode the object tables described in Sec. 2.2. Following prior works in table-based NLP tasks (Chen et al. 2020;\nLiu et al. 2022b), we flatten a table into token sequences\nrow by row, thus creating a linearized version of an object\ntable. Then, we append the flattened table after the goal to\nform a complete input sequence. Thus, the input side of the\nencoder-decoder finally has the environment information for\ngenerating a grounded plan.\nConsidering the max sequence limit, we only choose to\nencode objects by their type, position, rotation, and the receptacle parent. The object type does not only tell what an\nobject is but also implies commonsense affordance (e.g., a\nmicrowave can heat up something, a knife can slice something) which is very important for planning. The position\ninformation is essential for agents to navigate and find objects, thus playing an important part in planning. The rotation is also useful for some objects that can only be used with\na certain orientation (e.g., a refrigerator can only be opened\nwhen the agent is in front of it). The receptacle of an object\nand itself has a close spatial connection (e.g., a pen is on\na desk; an apple is in a fridge). Every object has a unique\nidentifier such that objects of the same type can be referred\nto precisely when they are receptacles of others. In addition,\nthe agent is represented as a special object.\n3.3\nIterative Decoding Strategy\nAdding the flattened table of object information to the input\nsequences indeed improves the LMs in terms of their perception of the realistic environments, which forms the foundation of grounded planning. However, the thinking process\nis still limited by the conventional seq2seq learning framework, which assumes LMs should output a complete plan by\na single pass of decoding. We argue that a thoughtful planning process should carefully handle the coherence of each\nstep, otherwise errors accumulate and cause a failed plan.\nTherefore, we propose a simple yet effective decoding\nstrategy that learns to iteratively generate a plan step by\nstep. Specifically, we append previously generated steps until the current step t to the input sequence (i.e., Input =\n[G + S1 + ⋯ + St(+E)]) for generating the next step (i.e.,\nOutput = St+1). This iterative decoding process will end\nuntil the LM generates the special token END. In the training stage, we use the ground-truth references for S≤t; in the\ninference stage, we do not have such references, so we use\nthe model predictions as S≤t.\nNotably, in contrast to the conventional seq2seq learning process, the iterative decoding strategy needs to run the\nencoder-decoder model N + 1 times to generate a plan with\nN steps. The additional computation cost for re-encoding is\nworthy. Imagine when we humans are planning a task in a\nroom. It is natural for us to come up with the plans step by\nstep, and it is very likely that the most useful information to\ngenerate different steps is about different objects. Therefore,\na temporally dynamic attention mechanism is favorable in\nplanning with LMs. Our iterative decoding strategy encourages the encoder-decoder architectures to learn such ability.\n3.4\nOther Methods\nPretrained table encoders.\nSince we use environmental\ninformation in a tabular format and BART has not been pretrained in the tabular form of input, BART may not be able\nto use this part of information well. Therefore, we employ\nTAPEX (Liu et al. 2022b), the state-of-the-art pre-trained\nlanguage model on tabular data. Using SQL execution as the\nonly pre-training task, TAPEX achieves better tabular reasoning capability than BART, and thus we expect TAPEX\ncan make full use of the environmental information represented by the table in our task.\nIn-context few-shot learning with GPT-J.\nFinally, to explore whether large-scale language models can master the\ntask with few-shot examples, we also experimented with\nfew-shot performance on a larger language model GPT-J 6B.",
        "evaluation": "How do we evaluate a method for G-PlanET? Due to the\nnovelty of the problem setup, it is challenging to evaluate\nand analyze the methods. In this section, we present a general evaluation protocol and a complementary metric to measure the quality of generated plans. We report the main experimental results with the proposed evaluation protocol. We\nleave the analysis in Sec. 5.\n4.1\nMetrics\nStep-wise evaluation.\nConventional evaluation metrics\nsuch as BLEU (Papineni et al. 2002) and ROUGE (Lin 2004)\nmeasure the similarity between generated text and truth references as a whole, which is suitable for translation and summarization. However, the output text of planning tasks such\nas our G-PlanET is highly structured. A plan naturally can\nbe split into a sequence of step-by-step actions. Using the\nconventional way to evaluate plans inevitably breaks such\ninternal structures and will lead to inaccurate measurement.\nFor example, if the first step of the generated plan is the same\nas the last step of the reference plan, the conventional evaluation will still assign a high score to such a generated plan,\neven though it is not useful at all. Therefore, we argue that it\nis much more reasonable to evaluate the similarity of a pair\nof plans step by step. Specifically, we first align the generations and the truths and compute the scores of every step2 by\nmultiple metrics. Then, we aggregate the final score by taking the average of all steps. We also consider other temporal\nweighting aggregation for more analysis in Sec. 5.\nMeasuring grounded plans.\nIt is a unique challenge for\nevaluating G-PlanET to consider the grounding nature of\nplans. Metrics, such as BLEU, METEOR, and ROUGE, do\nnot give a suitable penalty when a plan is similar to the reference in terms of word usage, yet leading to totally different\n2The ALFRED authors ensure that the references consist of\natomic action steps and all references share the same length. Therefore, we consider the length of truth plans as the standard: when the\ngenerated plan has more steps than the truth plans, we cut off them;\nwhen the generation has fewer steps than the references, we duplicate the last step to make them even for step-wise evaluation.\nData Split →\nUnseen Room Layouts\nSeen Room Layouts\nMethods ↓ Metrics →\nCIDEr\nSPICE\nKAS\nCIDEr\nSPICE\nKAS\nBART-base (vanilla)\n0.9417\n0.1378\n0.2455\n0.8231\n0.1277\n0.2197\nBART-large (vanilla)\n1.4632\n0.3168\n0.4069\n1.4414\n0.3161\n0.3900\nGPT-J-6B\n1.1968\n0.2655\n0.3622\n1.1047\n0.2509\n0.3370\nBART-base w/table\n1.6706\n0.3692\n0.4584\n1.6230\n0.3595\n0.4339\nBART-large w/table\n1.6630\n0.3491\n0.4411\n1.5865\n0.3393\n0.4204\nBART-large (TAPEX)\n2.8824\n0.5054\n0.6373\n2.7432\n0.4944\n0.6045\nBART-base w/table + iterative decoding\n2.9147\n0.5107\n0.6334\n2.8582\n0.5118\n0.6124\nBART-large w/table + iterative decoding\n2.8580\n0.5194\n0.6518\n2.8799\n0.5096\n0.6326\nBART-large (TAPEX) + iterative decoding\n2.8440\n0.5210\n0.6313\n2.6959\n0.5036\n0.6074\nTable 1: Experimental results for the G-PlanET by different base LMs. The methods are grouped by model types and whether\nencoding the environment; by decoding strategies.\nstates in an interactive environment for embodied tasks. For\nexample, it is only a one-word difference between “turn to\nthe left” vs “turn to the right”, but the agents that faithfully\nfollow these instructions can arrive at very different places.\nThe LM-based metrics, e.g., BERTScore (Zhang et al.\n2020), are not suitable either because the neural embeddings of “left” and “right” are also very similar. Plus, the\ngrounded plans for G-PlanET are object-centric in a context and very similar to the captions of a sequence of events\nby visual perception, for which these metrics are not specifically designed. Considering these limitations, we use two\ntypical metrics that are widely used for captions and devise\na new metric for complementary measurement.\nThe first two metrics are CIDEr (Vedantam, Zitnick, and\nParikh 2015) and SPICE (Anderson et al. 2016), which are\nboth widely used for tasks where the outputs are highly contextualized and describe natural scenarios in everyday life,\ne.g., VaTex (Wang et al. 2019) and CommonGen (Lin et al.\n2020). In particular, SPICE parses both the generation and\nreferences to scene graphs, a graph-based semantic representation. Then, it calculates the edge-based F1 score to\nmeasure the similarity between each step. Note that SPICE\ncomputation has a special focus on the propositions. This is\nparticularly favorable for evaluating G-PlanET since there\nare many actions in the grounded plans, where propositions\ncan be seen as atomic units for evaluation.\nKeyActionScore (KAS).\nInspired by SPICE, a step in\na plan can be deconstructed into several propositions that\nare represented as edges. However, not all propositions in\nSPICE are necessarily important in evaluating plans for GPlanET. Not to mention that SPICE relies on an external\nparser that is expensive to run yet sometimes contains noisy\noutputs. Also, most of the truth plans in the ALFRED annotations are overly specific, and it is not necessary for a plan\nto cover all details. Therefore, we devise a metric that focuses on the key actions of the generated plans and checks if\nthey are part of references, named Key Action Score (KAS).\nSpecifically, we extract a set of key action phrases from\neach step in the generated plan ˆSi and the truth reference Si\nrespectively. We denote this two sets as ˆSi = {ˆa1, ˆa2, ⋯}\nsplit →\ntrain\nvalid\ntest\naspect ↓\nseen\nunseen\nseen\nunseen\n# tasks\n21,025\n820\n821\n705\n694\navg. ∣G∣\n9.26\n9.32\n9.26\n10.3\n9.95\navg. # O\n73.71\n74.21\n77.91\n75.31\n73.9\navg. # T\n6.72\n6.79\n6.26\n6.95\n6.63\navg. ∣Si∣\n11.24\n11.13\n11.49\n9.84\n10.19\nTable 2: The avg. ∣G∣ means the average length of goal and\nthe avg. ∣Si∣ means the average length of each step. The avg.\n# O is the average number of objects in each room and the\navg. # T is the average number of steps.\nand Si = {a1, a2, ⋯}. Then, we check how many action\nphrases in ˆSi are covered by the truth set Si, the precision\nthen becomes the KAS score for the i-th step in the plan. To\nincrease the matching quality, we curate a set of rules and a\ndictionary to map the actions that share the same behaviors.\nFor example, “turn to the left” and “turn left” are counted\nas a single match; “go straight” and “walk straight” can be\nmatched too. In addition, we break the compound nouns\nsuch that we allow partial scores to match for a smoother\nscoring (e.g., “xxx on the table” vs “xxx on the coffee table”). Simply put, the KAS metric looks at the key actions\nextracted from the plans and checks if these important elements can be (fuzzy) matched to count as a valid step.\n4.2\nExperimental Setup\nData statistics.\nTable\n2 shows some statistics of our\ndataset that we described in Sec. 2.2. We follow the data\nsplit in ALFRED to split the train, valid, and test dataset. The\ndata split is based on whether the room layout has been seen\nin the training tasks. It is usually easier for robotic agents\nto map instructions to low-level actions in seen rooms than\nin unseen rooms. However, for the planning ability that we\nwant to study with G-PlanET in this paper, the two splits\ndo not differ very much. We keep using this split to make\nthe results consistent and convenient for people who want to\nconnect our results with the ALFRED results.\n0 0.2 0.4 0.6 0.8 1\n0.35\n0.45\n0.55\n0.65\n0.75\nP\nPerformance\nVanilla\nTable\nTAPEX\nTable Iter\nTAPEX Iter\n0 0.2 0.4 0.6 0.8 1\n0.20\n0.30\n0.40\n0.50\n0.60\nP\nPerformance\nFigure 3: The step-wise reweighting results of KAS (Left)\nand SPICE (Right).The x-axis indicates the parameter p in\nthe geometric distribution and also the importance of the\npreceding step, and the y-axis indicates the weighted result\nof each step. A larger coefficient means that the previous\nstep is more important.\nImplementation\ndetails.\nIn\nsingle-pass\ndecoding,\nwe\nformat\nthe\noutput\nsequences\nas\nfollows:\n“Step 1:[S1] ∣ Step 2: [S2] ∣ ⋯ ∣ END”. When\nappending the flattened table of objects, we format input\nwith “[G] Env: [row 1] [SEP] [row 2] ⋯”, where the\n[row i] is a sequence of the i-th object including its id,\ntype, coordinates, rotation, parent receptacles, etc. Due to\nthe page limit, we leave the details of the data, methods,\nand hyper-parameters in the Appendix that are linked to our\nproject website.\n4.3\nMain Results\nWe report the main results in Table 1, and leave the deeper\nanalysis in the next section. To sum up, we find that encoding the object table as part of the inputs will significantly\nimprove the performance, and pre-training on other tablerelated tasks can benefit G-PlanET a lot. The iterative decoding strategy is also an important component that can further\nimprove the results to some extent.\nCase Study of Table Effect\nAlthough we have added environment information E to the input, it is still a problem\nwhether the model effectively uses this information. To verify this, we present a case study here. In a number of instances, we have demonstrated that the introduction of environmental information can be helpful. Here is one example:\n• truth: Close the laptop that is on the table.\n• vanilla: Close the laptop and pick it up from the bed\n• w/ table: Pick up the laptop on the coffee table.\nAs shown in the example, the model successfully identified\nthe location of a laptop with the help of the object table.\nEffect of model sizes.\nTable 1 shows that small models\ncan perform as well or even better than large models in\nsome cases. This is mainly due to the following reasons.\n1) The sentences in plans are relatively simpler than other\nNLG tasks, with a smaller vocabulary and shorter length.\nThis leaves the power of large models in terms of generation\nunexpressed, 2) G-PlanET is a task to examine the ability\nto plan rather than write. Whether this ability changes with\n(−∞, −5](−5, −3] (−3, −1]\n0\n(0, +∞)\n0\n100\n200\n300\n400\n500\nStep Range\nCount\nVanilla\nTable\nTAPEX\nTable Iter\nTAPEX Iter\nFigure 4: The result of error statistics for # of step.\nmodel size remains to be explored. 3) For scenarios with\nthe table, the form of the task is not the same as the traditional generation task, so the training phase will have a\ngreater impact. Models with fewer parameters are more sufficiently tuned with limited data.",
        "analysis": "In this section, we deeply analyze the performance of the\nmethods in Table 1 from multiple aspects and provide nontrivial findings that can help future research. For a fair comparison, all analytical experiments were performed in the\nBART-large model on the unseen split of the test data.\n5.1\nTemporal Re-weighting of Scores\nWhen we computed the overall score of a plan with a metric,\nwe use the average score to aggregate the score for each step.\nHowever, in a realistic environment, there are causality constraints for an agent to complete the steps – i.e., some tasks\ncan only be done when their prerequisite steps are finished.\nFor example, only when the agent arrives at the microwave\ncan it heat the bread in its hands.\nTherefore, the earlier steps in a plan should be of higher\nimportance, while our previous evaluation is based on a uniform distribution of the weights across steps. To this end,\nwe adopt geometric distribution to re-weight the step-wise\nimportance for weighted aggregation. The geometric distribution can be used to model the number of failures before\nthe first success in repeated mutually independent Bernoulli\ntrials, each with a probability of success p.\nf(x) = p(1 − p)x\n(0 < p < 1)\nThis suits our setup well because when the first step is\nincorrect, the whole task can hardly be completed and executed in a generated plan for ALFRED. The range of p in the\noriginal setting of the geometric distribution is restricted to\nbetween 0 and 1. When p = 0, each step has the same weight\n(uniform importance), which is exactly what we have done\nin Tab. 1. When p = 1, the first step is the only thing we look\nat for evaluation, meaning that the other steps will be given\nzero weights for aggregation.\nFigure 3 shows the results on unseen subset which is more\nrealistic. The performance of the iterative and non-iterative\napproaches is very close in the case of the first step. This is\nmainly because iterative methods are similar to non-iterative\nmethods when generating the first step, and differ only after\n(0, 5]\n(5, 8]\n(8, 10] (10, 13)\n0.0\n0.15\n0.30\n0.45\n0.60\nStep Range\nPerformance\nVanilla\nTable\nTAPEX\nTable Iter\nTAPEX Iter\nFigure 5: The result of KAS of tasks with a different number of steps. Due to the large variance caused by the small\nnumber of samples of certain lengths, we use the statistics\nby dividing the intervals.\nthe second step. At the same time, it can be seen that there\nis an overall downward trend in performance as the focus\nmoves to the early step. The main reason is that the later\nthe subtask is, the closer it is to the high-level instruction.\nFor example, if the task goal is to place the sponge in the\nsink, the final step must be to place the sponge in the sink.\nThis feature makes the last step of subtask generation very\nsimple, resulting in high performance. We also see that the\nperformance of the non-iterative method rises and then falls\nin KAS, and the change in a downward trend in SPICE. The\nmain reason is an error in the number of steps in the noniterative method, which will be explained next.\n5.2\nError Analysis on the Lengths of Plans\nWe found a huge gap in the prediction of the number of task\nsteps between iterative and non-iterative methods, which\nmay be an important reason for the final performance difference. As shown in Figure 4, iterative methods have a higher\nprobability of predicting the number of steps for the correct task, while non-iterative methods do underestimate the\nnumber of steps. In our evaluation framework, the missing\nfollow-up steps of non-iterative methods are often generated by copying. This might be reason for the poor performance of non-iterative methods and the performance of noniterative methods increases first in the reweight step process.\n5.3\nImpact of Task Length on Performance\nAlthough all the tasks in the dataset are part of daily life\ntasks, they differ in difficulty. A simple metric to evaluate the\ndifficulty of a task is the number of steps they require. Figure 5 illustrates the decrease in the quality of the generated\nsteps as the number of task steps increases. The figure also\nreflects the relatively small difference in the performance of\nthe different methods on shorter tasks. And the performance\nof all methods degrades rapidly on the longest tasks. The iterative approach has more significant performance benefits\non longer tasks. This may be because this approach makes\nbetter use of the state changes due to intermediate steps and\nfixes some previous errors.",
        "related work": "Grounded commonsense reasoning.\nALFWorld (Shridhar et al. 2021) also uses LM to generate the next step in\na text game which is based on ALFRED. SciWorld (Wang\net al. 2022) designed a text game to find whether the LMs\nhave learned to reason about commonsense. SayCan (Ahn\net al. 2022) also uses LM to find the potential next step in\nthe real world. Both these three works only expect to learn\nthe next step in a text game. Our methods share similar motivation with decision transformer (Chen et al. 2021) and Behavior Cloning (Farag and Saleh 2018), but we work on very\ndifferent applications.\nTable-based NLP.\nOur work is closely related to two lines\nof tabular data usage in NLP: the approach to modeling tabular representations and the application of a table as an intermediate representation. For the first line of work, there is\nrich literature focusing on modeling tabular representations,\nincluding TabNet (Arik and Pfister 2019), TAPAS (Herzig\net al. 2020), TaBERT (Yin et al. 2020) and TAPEX (Liu\net al. 2022b). We have explored the impact of state-of-theart table representation models (e.g., TAPEX) on our task in\nexperiments. As for the second line of work, previous work\nhas explored to use of tables in several downstream tasks,\nincluding visual question answering (Yi et al. 2018), code\nmodeling (Pashakhanloo et al. 2022), and numerical reasoning (Pi et al. 2022; Yoran, Talmor, and Berant 2022). Different from them, our work is the first to explore the use of\ntabular representations in embodied tasks.\nALFRED Agents.\nSome previous research has been published on embodied tasks in realistic environments since\nthe appearance of ALFRED. E.T. (Pashevich, Schmid, and\nSun 2021) first encoded the history with a transformer to\nsolve compositional tasks and proved that pretraining and\njoint training with synthetic instructions can improve performance. FILM (Min et al. 2021) proposed an explicit spatial\nmemory and a semantic search policy to provide a more effective representation for state tracking and guidance. LEBP\n(Liu et al. 2022a), the currently published SOTA method,\ngenerated a sequence of sub-steps by understanding the language instruction and used the predefined actual actions\ntemplate to complete the sub-steps. We also try to use these\nmethods to evaluate our generated low-level instructions.\nHowever, due to the limited importance of the low-level instructions, there is no gap with conspicuousness between our\ngenerated instructions and the ones in ALFRED.",
        "conclusion": "In this work, we present the first investigation into grounded\nplanning for embodied tasks using language models. The\nG-PlanET problem is of utmost significance for advancing\nthe embodied intelligence of LMs and constitutes a critical\nstep towards artificial general intelligence. To evaluate the\nperformance of encoder-decoder LMs in solving G-PlanET,\nwe developed a benchmark as well as a specialized evaluation metric named KAS to assess the quality of generated\nplans. Furthermore, we propose two methods for improving\nLMs’ ability in G-PlanET - flattening object tables and an\niterative decoding strategy. Our experiments and analyses\ndemonstrate their effectiveness and yield non-trivial findings. This study is expected to encourage further research\ninto G-PlanET and pave the way for integrating LMs and\nembodied tasks in realistic environments.",
        "summary_en": "Language models (LMs) have demonstrated their capability in possessing commonsense knowledge of the physical world, a crucial aspect of performing tasks in everyday life. However, it remains unclear whether they have the capacity to generate grounded, executable plans for embodied tasks. This is a challenging task as LMs lack the ability to perceive the environment through vision and feedback from the physical environment. Therefore, this paper addresses this important research question and present the first investigation into the topic. The novel problem formulation, named G-PlanET, inputs a high-level goal and a data table about objects in a specific environment, and then outputs a step-by-step actionable plan for a robotic agent to follow. To facilitate the study, the paper establishes an evaluation protocol and design a dedicated metric, KAS, to assess the quality of the plans. The paper's experiments demonstrate that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning. The paper's analysis also reveals interesting and non-trivial findings.",
        "summary_zh": "这篇论文研究了语言模型是否具备生成基于具体场景的可执行计划的能力。本文提出了一个名为G-PlanET的新问题形式，即输入一个高级目标和一个关于特定环境中对象的数据表，然后输出一个逐步可执行的计划，供机器人代理执行。为了促进研究，作者建立了一个评估协议，并设计了一个专门指标KAS来评估计划的质量。实验结果表明，使用表格来编码环境并采用迭代解码策略可以显著提升语言模型在基于场景的规划中的能力。最后的分析也揭示了一些有趣且非平凡的发现。"
    },
    {
        "title": "A Unified Multi-Task Learning Framework for Joint Extraction of Entities and Relations",
        "abstract": "Joint extraction of entities and relations has achieved great success in recent year by task decomposition and multi-task learning. Previous works effectively perform the task through different extraction order, such as relation-last, relation-ﬁrst and relation-middle manner. However, these methods still suffer from the template-dependency, non-entity detection and non-predeﬁned relation prediction problem. To overcome these challenges, in this paper, we propose a uniﬁed multitask learning framework, which decomposes the task into three interacted sub-tasks. Speciﬁcally, we ﬁrst introduce the type-attentional method for subject extraction to provide prior type information explicitly. Then, the subject-aware relation prediction is presented to select useful relations based on the combination of global and local semantics. Third, we propose a question generation based QA method for object extraction to obtain diverse queries automatically. Notably, our method detects subjects or objects without relying on NER models and thus it is capable of dealing with the non-entity scenario. Finally, three sub-tasks are integrated into a uniﬁed model through parameter sharing. Extensive experiments demonstrate that the proposed framework outperforms all the baseline methods on four benchmark datasets, and further achieves excellent performance for non-predeﬁned relations.",
        "introduction": "Entity-relation extraction aims to recognize entity spans\nfrom a sentence and detect the relations of entity pairs. Generally, it is formed as extracting an entity-relation triplet\n(e1, r, e2), which denotes that the relation r holds between\nthe subject e1 and the object e2, i.e., (Jack, Live-In, New\nYork). It has attracted increasing attention in recent years.\nEarly works mainly focus on pipelined methods, which\ndivide the task into two independent sub-tasks, named entity recognition (NER) and relation extraction (RE) (Miwa\net al. 2009; Chan and Roth 2011; Lin et al. 2016). However, the pipelined approaches neglect the inherent correlations between NER and RE tasks, which leads to the error\npropagation problem.\nTo leverage the interactions between NER and RE tasks,\nsome recent works propose to extract entities and relations jointly. The joint methods tend to decompose the\nSentence:\nSo far U.S. soldiers have \ndiscovered nearly $600 million \nhidden around Baghdad.\nSub\nTask \n1\n: \nType\nattentional Subject Extraction\n•\nperson; So far U.S. soldiers have discovered nearly $600 \nmillion hidden around Baghdad\n•\ngeopolitical entity; So far U.S. soldiers have discovered \nnearly $600 million hidden around Baghdad \n•\norganization; So far U.S. soldiers have discovered nearly \n$600 million hidden around Baghdad \n•\n……\nsoldiers\nU.S.\nBaghdad\nNone\n……\nGolden Relation Triplets:\n(soldiers, ORG-AFF, U.S.) \n(soldiers, PHYS, Baghdad)\nSub\nTask \n2\n: \nSubject\naware Relation Prediction\n•\nRelation Subset: {ORG-AFF, PHYS}\nSub\nTask\n3\n: \nQA\nbased Object Extraction\nSeed Question: Find geopolitical entities that soldiers is employed ?\nAuto Generated Question:\n•\nWhat country soldiers is actually employed ? \n•\nWhich geopolitical entities employ soldiers ?\n•\nWhere is soldiers in ?\nAnswer: U.S.\nEntities:\nsoldiers\n(PER)\nU.S.   (GPE)\nBaghdad\n(GPE)\nFigure 1: The extraction process based on the proposed\nmulti-task learning framework\ntask into several fundamental procedures or solve the problem through multi-task learning framework. According to\nthe extraction order of the triple elements, these models\nfall into three categories: relation-last, relation-ﬁrst and\nrelation-middle. The relation-last method can be formed\nas (e1, e2) → r, which indentiﬁes all entities in the sentence ﬁrst using named entity recognition (NER) techniques,\nthen conducts relation classiﬁcation for any two entities\n(Katiyar and Cardie 2017; Eberts and Ulges 2019; Sun\net al. 2019). However, the method requires to enumerate\nall pairs of entities, resulting in a heavy computational burden. And the redundant pairs cause negative inﬂuences on\nthe relation classiﬁer. The relation-ﬁrst method is formed\nas r → (e1, e2). In this manner, the relation is generated\ninitially by the Seq2Seq framework, and then subjects and\nobjects are selected respectively based on the copy mechanism (Zeng et al. 2018; Zeng, Zhang, and Liu 2020).\nBy predicting relations ﬁrst, this kind of approaches ﬁlter\nout irrelevant relationships, which mitigate negative effects\ncaused by useless relations and greatly avoid the data imbalance issue. Therefore, they have an overall higher computation efﬁciency. More recently, the relation-middle method,\ndenoted as e1 → r → e2, has shown promising performance in relation extraction. Typical works extract the\nentity-relation triplet by multi-turn question answering (QA)\nstructure (Levy et al. 2017; Li et al. 2019; Zhao et al.\n2020). In this form, the subject and object are characterized by template-based queries. For example, by answering queries like Which [person] is mentioned\nin the text? and Where was [person] born?\nin turn, a triplet as ([person], born in, [location])\ncan be detected from the text. The QA-based structure exihbits advantages as follows: 1) The query question explicitly\nprovides prior signals about the type information. 2) It enhances the interactions between the query and the text based\non the QA structure. 3) It presents a natural way to deal with\noverlapping entities and relations(Zeng et al. 2018).\nDespite the progresses made by these efforts, several major problems still remain to be solved. First, the QA-based\napproaches rely heavily on manually designed templates,\nmaking the model hard to transfer. Particularly, the study\n(Levy et al. 2017) proves that using diverse queries can improve model performance. In that case, designing multiple\ntemplates for every entity and relation type would largely\nincrease the workforce. Second, present mainstreams have\ndifﬁculties in identifying non-entity subjects or objects due\nto the heavy reliance of NER. In fact, relations can exist between any subject and object, including non-entity ones like\ntext spans, time expressions, simple statements, and so on,\nespecially in actual scenes. Third, existing systems mainly\nfocus on predeﬁned relations, so they can not deal with nonpredeﬁned relations that are not speciﬁed in advance. All\nmentioned above challenge the previous RE models.\nTo address the aforementioned problems, in this paper, we\npropose a comprehensive framework for joint entity and relation extraction. We follow the relation-middle based extraction order and decompose the task into three interrelated sub-tasks: the type-attentional subject extraction, the\nsubject-aware relation prediction (SRP) and the QA-based\nobject extraction. Figure 1 illustrates the process of our\nmulti-task learning framework. Speciﬁcally, to alleviate the\nproblem of relying on templates, we ﬁrst present the typeattentional method to provide entity type information explicitly for the subject extraction task. Then, we propose a question generation (QG) strategy to obtain diverse queries automatically for the object extraction task. These two subtasks\nselect text spans from the sentence based on the prior type\ninformation and query, instead of relying on NER. Hence,\nthey also handle the non-entity problem effectively. Moreover, we design a distinctive way of fuzzy question answering to solve the non-predeﬁned relation detection problem.\nThirdly, we introduce the subject-aware relation prediction\ntask to obtain a relation subset for the given subject using\nboth global and local semantics. Finally, the three subtasks\nare integrated into a multi-task learning framework by sharing the feature encoder module. Empirical experiments show\nthat the model better utilizes the inherent interactions among\nthe sub-tasks and boosts the overall performance.\nTo summarize, the contributions of this work are:\n• We deﬁne the entity-relation extraction into three interacted sub-tasks: the type-attentional subject extraction,\nthe subject-aware relation prediction, and the QA-based\nobject extraction, which effectively address the templatedependency, non-entity detection and non-predeﬁned relation prediction issues.\n• We present a multi-task learning framework to integrate\nthe correlated sub-tasks and enhance the interactions\namong them.\n• Extensive experiments show that our framework outperforms all the baseline models by a large margin. Detailed\nanalyses further study the impact of extraction order for\nthe relation extraction task.",
        "problem deﬁnition": "In this section, we deﬁne the problem formally. Denote\nE and R as the sets of predeﬁned entity types and relation categories, respectively. For an input sentence s =\n{s1, s2, . . . , sNs} with Ns tokens, the entity-relation extraction task aims to extract all relational triplets from the sentence. The relational triplet is formed as (ei, rij, ej), where\nei ∈ E, ej ∈ E, rij ∈ R, denoting that the relation rij\nholds between the subject ei and the object ej, e.g., the\ntriplet (Jack, Live-In, New York). Especially, the entity\npair (ei, ej) can associate with multiple relations.",
        "methodology": "In this section, we elaborate on the structure of the proposed\nframework. To fully incorporate the inherent interactions\namong the related tasks, we employ the multi-task learning architecture to improve the overall performance. Based\non the relation-middle extraction order, the framework consists of three interrelated tasks. 1) Type-attentional subject\nextraction, which provides type information explicitly to detect the subject entity from the sentence. 2) Subject-aware\nrelation prediction, which is a multiple classiﬁcation problem to select possible relations relevant to the given subject.\n3) QA-based object extraction, which is a question answering problem to select the object entity from the sentence using auto-generated questions. Figure 2 shows an overview of\nthe framework. In the following subsections, we ﬁrst introduce the shared feature encoder. After that, we describe the\nstructure of each task in detail.\nShared Feature Encoder\nThe shared feature encoder focuses on mapping input tokens to distributed semantic representations, consisting of a\nBERT layer and a contextual fusion layer. The learned feature is shared by the three downstream sub-tasks.\nBERT Layer\nTo better capture and generalize the semantics of the given sentence, we adopt BERT (Devlin et al.\n2019) as the shared feature encoder, which is known as a pretrained language model based on bidirectional transformer\nstructure and has achieved state-of-the-art performance on\na wide range of NLP tasks. As illustrated in Figure 2, the\ninput of BERT consists of three parts: the input sentence,\nthe task-speciﬁc information and special tokens. Note that\nfor the type-attentional subject extraction, the task-speciﬁc\ninformation is the entity type to be detected. While for the\nType-attentional \nSubject\nExtraction\nQA-based\nObject\nExtraction\nSubject-aware \nRelation\nPrediction\n··\n·\n··\n·\nCLS\nSEP\nSEP\nSentence\nSo far U.S. soldiers have discovered nearly \n$600 million hidden around Bagdad.\nTask Specific Information\n• For Sub-Task 1: Subject Type (e.g., person)\n• For Sub-Task 3: Auto Generated Questions \n(e.g., Where does soldiers employed? )\n···\n···\nsoldiers\nORG-AFF, PHYS\nU.S\n…\nO\n…\nS\nO\n…\nS\n…\nO\nO\nGenerate Questions for Object Extraction\nShared Feature Encoder\nBERT Layer\nHighway ( x 2)\nBiGRU ( x 1)\nContext Fusion Layer\nSub-Task 1\nSub-Task 2\nSub-Task 3\nFigure 2: An overview of the multi-task learning framework. It consists of three interrelated tasks including the\ntype-attentional subject extraction, the subject-aware relation prediction, and the QA-based object extraction. The\nthree tasks are built on a shared feature encoder and have\na task-speciﬁc output decoder, respectively. Here, we take\nthe extraction procedure of the triplet (soldiers, ORG-AFF,\nU.S.) from ACE05 as an example.\nQA-based object extraction, it becomes the auto-generated\nquestions that query about the candidate object. Examples\nare shown in Figure 1. Formally, considering the sentence\ns = {s1, s2, . . . , sNs}, the input of BERT is the concatenation as:\nx = [[CLS], Task-speciﬁc Information, [SEP],\ns1, s2, . . . , sNs, [SEP]],\n(1)\nwhere [CLS] is the token will be used for subject-aware\nrelation prediction, and [SEP] is a special separator. Then,\nthe BERT yeilds the semantic representations for each token\nhb = {hb\n1, hb\n2, . . . , hb\nNs} and the hidden state of the [CLS]\ntoken hCLS, where hi ∈ Rdh with dh as the dimension of the\nhidden state of BERT.\nContext Fusion Layer\nTo enhance the contextual information within the sequence efﬁciently, we apply a contextual fusion layer to further encode the output of BERT. The\nhidden states of BERT hb = {hb\n1, hb\n2, . . . , hb\nNs} is ﬁrstly\nfed into a two-layer highway network (Srivastava, Greff, and\nSchmidhuber 2015):\nˆhb\ni = H(hb\ni) ∗ T(hb\ni) + hb\ni ∗ (1 − T(hb\ni))\nT(hb\ni) = σ(W thb\ni + bt),\n(2)\nwhere W t ∈ Rdh×dt, bt ∈ Rdt are learned parameters with\nht as the hidden size of the highway network. Then, a onelayer bidirectional gate recurrent unit (BiGRU) (Cho et al.\n2014) is followed by the highway network:\nhi = [−−→\nGRU( ˆhb\ni), ←−−\nGRU( ˆhb\ni)].\n(3)\nThe output hi is the concatenation of the last hidden states\nfor the forward GRU and the backward GRU. To this end, we\nobtain hi as the ﬁnal output of the shared feature encoder.\nType-attentional Subject Extraction\nThe type-attentional subject extraction task aims at detecting the subject entity from the sentence. Different from the\nprevious works that need to design templated questions for\neach entity type, we simplify it into only using the text of\nentity type (e.g., person, location, organization, etc.). The\nmost obvious advantages of the modiﬁcation are: 1) without\nusing any redundant words, it provides explicit entity type\nindicator for the self-attention structure; 2) it does not rely\non any hand-crafted template and is both simple and effective. Speciﬁcally, the input of BERT is formed as:\nxsubject = [[CLS], t, [SEP], s1, s2, . . . , sNs, [SEP]],\n(4)\nwhere t ∈ E is an entity type (i.e., person) with E as the set\nof entity types.\nTo detect the subject entity from the sentence, some previous works predict the starting and ending position of the\nsentence. However, this span-based method is limited to\nmulti-answer scenario. Therefore, to tackle this problem, we\npredict a BIOES (Begin, Inside, Outside, Ending, Single)\nboundary for each token in the sentence. Formally, we employ a softmax layer to the contextual representations h and\ncalculate the possibilities of all boundary tags for every token as:\nPr(tag = y⋆|xi) = softmax(W nhi + bn),\n(5)\nwhere W n ∈ Rdh×dn, bn ∈ Rdn with dn as the number of\nboundary tags, i.e., dn = 5. If all tags are predicted as O,\nthen it means the sentence does not contain entities of the\ncurrent type. Therefore, a set of subject entities with type t\ncan be detected.\nSubject-aware Relation Prediction\nThe subject-aware relation prediction (SRP) task focuses on\npredicting the relations that are relevant to the given subject,\nand thus the redundant relations can be ﬁltered out. We predict the relations from both local and global features.\nLocal Relation Prediction (LRP)\nWe ﬁrst identify the\nprobable relations for every given subject. For the subject\nei, the input to LRP is the concatenation of the local hidden represetation hi and the learned entity type embedding\nxe\ni ∈ Rde with de as the dimention of xe\ni:\nzi = [hi, xe\ni], i = 1, . . . , Ns,\n(6)\nwhere zi ∈ R(dh+de). During training, we use the golden\nentity type. Then, the relations for ei is calculated through a\nmulti-sigmoid layer:\nPr(relation = r1, . . . , rk|ei) = σ(W l · zi + bl),\n(7)\nwhere W l ∈ R(dh+de)×|R|, bl ∈ R|R|, |R| is the number of\nthe relation types and σ(·) is the sigmoid funtion. Only the\nrelation type with higher score than the threshold δ is kept\nas the candidate.\nGlobal Relation Prediction (GRP)\nThe previous work\n(Zhao et al. 2020) only uses the local information for relation prediction and neglects the semantic of the overall sentence. To address this problem, we introduce a global relation prediction to revise the learning of local relation classiﬁer. Speciﬁcally, the hidden state corresponding to the ﬁrst\ninput token ([CLS]) can be considered as the aggregate representation of the whole sequence. Therefore, after obtaining the hidden output of [CLS] hCLS, we predict the possible\nrelations that is involved with the sentence s as:\nPr(relation = r|s) = σ(W ghCLS + bg),\n(8)\nwhere r ⊆ R, W g ∈ Rdh×|R|, br ∈ R|R|.\nParticularly, the local and global classiﬁers are integrated\nto calculate the loss during training. While for inference, we\nonly use the relations obtained by the local predicter.\nQA-based Object Extraction\nAfter detecting the subject and possible relations, the QAbased object extraction task selects the object entity from\nthe input sentence. In the following subsections, we introduce the procedure of QA-based object extraction and the\nautomatic question generation in turn.\nObject Extraction Process\nFormally, T questions Q =\n{q1, q2, . . . , qT } are generated for object extraction. Each\nquestion qt where t ∈ 1, . . . , T is concatencated with the\ninput sentence s following Equation 1 as:\nxobject = [[CLS], qi, [SEP], s1, s2, . . . , sNs, [SEP]].\n(9)\nThen, xobject is fed into the shared feature encoder. In the\nsame way as Equation 5, the answer is obtained as at =\n{at1, at2, . . . , atNs}, t ∈ 1, . . . , T.\nAdditionally, to select the ﬁnal answer from T answers,\nwe employ a weighted voting strategy for answer selection\nfollowing (Zhao et al. 2020). A weight wt is deﬁned for qt,\ndenoting the quality of the question. We calculate the F1\nscore ft for qt on the development set at the end of every\ntraining epoch. The weight wt is updated as:\nwt = σ(ft) ∗ T,\n(10)\nwhere σ(·) is the sigmoid function. And the ﬁnal answer a⋆\ni\nfor position i, i ∈ 1, . . . , Ns is selected by weighted voting\non the T tokens:\na⋆\ni = arg max\nX\nt\nwt · ati.\n(11)\nTherefore,\nthe\nﬁnal\nanswer\nis\nobtained\nas\na⋆\n=\n{a⋆\n1, a⋆\n2, . . . , a⋆\nNs}.\nAutomatic Question Generation\nDifferent from the\ntemplate-based methods, we propose to generate questions\nautomatically based on a Seq2eq model. The advantages of\nthe generation-based extraction manner are: 1) it saves a lot\nof manpower; 2) it can quickly generate any number of diverse questions to improve the model performance. In the\nfollowing, we ﬁrst introduce the structure of Seq2Seq model,\nand then describe the strategy for question generation.\nTransformer\nTransformer\nTransformer\n……\nTransformer\nTransformer\nTransformer\n……\nPretrain\nFinetune\nFind geopolitical entities that soldiers is employed ?\n•\nWhat country soldiers is actually employed ? \n•\nWhich geopolitical entities employ soldiers ?\n•\nWhere is soldiers in ? \nSeed Question:\nGenerate\nd\nQuestions:\nEncoder\nDecoder\nFigure 3: The architecture of the transformer-based Seq2Seq\nmodel for question generation. Here, we take the NLquestion-driven generation as an example. The encoder\ntakes a natural language question as input, and the decoder\ngenerates multiple paraphrased questions.\n• Transformer-based Seq2Seq. As shown in Figure 3, we\nadopt a Seq2Seq structure with encoder and decoder both\ncomposed of Transformer layers (Rothe, Narayan, and\nSeveryn 2019). The objective of the Seq2Seq is to predict\nmultiple paraphrases based on the input seed question. For\nthe encoder, we use the 12 transformer layers the same\nas BERT and initialize it with the pre-trained checkpoint.\nThe decoder shares the same structure as the encoder but\nwith all weights initialized randomly. The encoder and decoder use the identical embedding matrix initialized from\nthe checkpoint. During training, we ﬁne-tune the Seq2Seq\non a question pairs task dataset, i.e., Quora 1.\n• Question\nGeneration\nStrategy.\nWe\npropose\ntwo\nstrategies for question generation. The ﬁrst one is the\npseudo-question-driven\ngeneration,\nwhich\nuses\na\nsimple pseudo-question as the seed question. The pseudo\nseed question is the combination of the subject text, the\nrelation text, and the object type, such as sodliers;\norganization affiliated; geographic\npolitical entity. The strongest point of this\nformalization is that we can obtain any number of\nquestions without human labor. However, considering\nthe inner syntactic structure of questions, we further\npropose the NL-question-driven generation, which\nuses a natural language question as the seed question.\nIt is obtained based on a general template as Find\n[object type] that [subject text] is\n[relation type]. Compared with the ﬁrst one, this\nform can generate multiple grammatical questions, which\nmay contain more syntactic information. Figure 3 shows\nan example of the NL-question-driven generation.\n1https://www.kaggle.com/quora/question-pairs-dataset.\nData set\nALL\n\f\fR\n\f\f\n\f\fET\n\f\f\nTriplet\nTrain/Test\nCoNLL04\n910 / 288\n5\n4\n1.37 / 1.41\nACE05\n2600 / 583\n6\n7\n1.81 / 1.92\nDuIE2.0\n171293 / 20674\n55\n17\n1.88 / 1.90\nTravel20\n- / 200\n8\n1\n- / 2.97\nTable 1: Statistics of four datasets. Among them, R and ET\ndenote the number of relation set and the number of entity\ntype. Triplet is the average number of triplets that contained\nin each sentence.\nNon-predeﬁned Relation Detection\nSpeciﬁcally, to deal with non-predeﬁned relations that are\nnot observed beforehand, we propose a fuzzy question answering strategy. We ﬁrst extract the subject through the\naforementioned type-attentional approach. Then, we design\ndefault fuzzy questions to traverse the non-predeﬁned relation and ﬁnd the object in triplets. For example, the fuzzy\nquestion can be\nWhat is the [relation type]\nof the [subject text]? By ﬁlling the relation type\nand subject text, we ﬁnally extract the corresponding object through the QA-based method. In addition, we can customize the fuzzy questions of the non-predeﬁned relations to\nmake them more consistent with natural language questioning. Beneﬁt from this strategy, we make obvious improvements in non-predeﬁned relation detection.\nJoint Training Objective\nOverall, the input sentence is encoded by the shared BERT.\nThe contextual outputs are used to calculate three tasks:\ntype-attentional subject extraction, subject-aware relation\nprediction and QA-based Object Extraction. Therefore, we\njointly optimize the integrated loss during training as:\nL = LSubject + LGlobal + LLocal + LObject,\n(12)\nwhere LSubject denotes the ﬁnal loss function. LGlobal and\nLLocal denote the binary cross entropy for global and local relation classiﬁcation, respectively. LObject denotes the\ncross entropy loss for QA-based object extraction.",
        "experiment": "In this section, we conduct extensive experiments to evaluate\nthe effectiveness of the proposed multi-task framework.\nDataset\nCoNLL04 (Roth and Yih 2004) and ACE05 (Doddington\net al. 2004) are two widely used English benchmark datasets.\nWe use the data split by (Gupta, Sch¨utze, and Andrassy\n2016) and (Miwa and Bansal 2016), respectively. DuIE2.02\nand Travel20 are used for detailed analyses. DuIE2.0 is the\nlargest Chinese RE dataset in the industry. Travel20 is an introductory data about attractions that includes only 200 texts\nand we use it for testing non-predeﬁned relation extraction.\nThe statistics of the datasets are listed in Table 1.\n2https:/github.com/PaddlePaddle/Research/tree/master/KG/Du\nIE Baseline.\nImplemental Details\nFor all experiments, precision, recall, and micro-F1 score are\nadopted as our evaluation metrics for both entity and relation\nextraction. We initialize the BERT encoder layer using the\npre-trained BERT-Base-Cased checkpoint 3 and therefore\nhas 12 layers, a hidden size of 768. We use Adam optimizer\nwith an initial learning rate of 5×10−5. During training, we\ndo warm-up startup ﬁrst and employ a linearly decrease with\n0.05 as the decay rate. For the model structure, we generate\n5 questions for QA-based object extraction. The hidden dimension for highway and BiGRU are set as 400. The size of\nentity type embedding dl is set as 50. The conﬁdence threshold for relation classiﬁcation δ is set as 0.3. For the question\ngeneration model, we adopt the BERT-Large-Cased checkpoint which has 24 layers, a hidden size of 1024. The training process is warm-started with 40K steps and is ﬁne-tuned\nusing Adam with a learning rate of 0.05.\nPerformance Comparison\nBaselines\nWe make comparisons with both non-QA-based\nmethods and QA-based methods. Miwa et al. (2016) use the\nTree-BiLSTM to learn context features using a dependency\nparser. Zhang et al. (2017) transfer the relation extraction\ntask as a table-ﬁlling problem build a globally optimized\nmodel for end-to-end extraction. Models (Adel and Sch¨utze\n2017) utilizes the attentional LSTMs and normalized CNNs\nto capture features of entity pairs. Bekoulis et al. (2018)\npropose a multi-head selection framework to extract entities\nand relations simultaneously. Eberts and Ulges (2019) develop a span-based approach for joint entity-relation extraction based on BERT. (Sun et al. 2019) employs the graph\nconvolutional network on an entity-relation bipartite graph\nstructure. (Luan et al. 2019) adopts multi-task learning to\nidentify coreferences, entities, relations using dynamic span\ngraph. Li et al. (2019) cast the entity-relation extraction into\na multi-turn question answering problem and tackle it with\nthe QA-based method.\nMain Results\nTable 2 shows the testing performance on\nboth CoNLL04 and ACE05 datasets. As we can see, our\nuniﬁed multi-task framework outperforms all the baselines\non CoNLL04 dataset, achieving the state-of-the-art performance. We also conduct a T-test to test statistical signiﬁcance for the results, which shows the p-values are below\nthe signiﬁcance level p < 0.05, indicating the improvement\nis signiﬁcant. Comparing with the QA-based model, UMT\nw/ NLGQ and UMT w/ PseudoGQ consistently surpass (Li\net al. 2019) by 3.5% and 1.2% on the two datasets, showing that we obtain obvious improvement without using templated questions. Meanwhile, with the type-attentional subject extraction, we obtain strong competitive and even better\nperformance than (Li et al. 2019). It reveals that the entity\ntype is a useful indicator and can guide the self-attentive\nstructure to detect the target tokens. To notice that, (Luan\net al. 2019) uses external tools to ease the coreference problem. Differently, we still achieve considerable performance\n3https://github.com/google-research/bert.\nModel\nEntity\nRelation\nPrecision\nRecall\nF1 score\nPrecision\nRecall\nF1 score\nCoNLL04\n(Adel and Sch¨utze 2017)\n82.1\n62.5\n(Zhang, Zhang, and Fu 2017)\n85.6\n67.8\n(Bekoulis et al. 2018)\n83.8\n84.1\n83.9\n63.8\n60.4\n62.0\n(Eberts and Ulges 2019)\n88.3\n89.6\n88.9\n73.0\n70.0\n71.5\n(Li et al. 2019) w/ NLQ\n89.0\n86.6\n87.8\n69.2\n68.2\n68.9\n(Li et al. 2019) w/ PseudoQ\n87.4\n86.4\n86.9\n68.2\n67.4\n67.8\nUMT w/ NLGQ\n88.7\n88.8\n88.8\n72.9\n71.6\n72.2\nUMT w/ PseudoGQ\n88.8\n89.0\n88.9\n73.2\n71.6\n72.4\nACE05\n(Miwa and Bansal 2016)\n82.9\n83.9\n83.4\n57.2\n54.0\n55.6\n(Zhang, Zhang, and Fu 2017)\n83.5\n57.5\n(Sun et al. 2019)\n83.9\n83.2\n83.6\n64.9\n55.1\n59.6\n(Luan et al. 2019)\n88.4\n63.2\n(Li et al. 2019) w/ NLQ\n84.7\n84.9\n84.8\n64.8\n56.2\n60.2\n(Li et al. 2019) w/ PseudoQ\n83.6\n84.7\n84.2\n60.4\n55.9\n58.1\nUMT w/ NLGQ\n86.2\n85.1\n85.7\n61.2\n61.7\n61.4\nUMT w/ PseudoGQ\n86.2\n85.1\n85.7\n61.0\n61.3\n61.2\nTable 2: Performance comparisons on CoNLL04 and ACE05. UMT is the proposed method based on the multi-task learning\nframework. Here, w/ NLGQ denote models using questions generated by NL-question-driven strategy and w/ PseudoGQ denote\nmodels using questions generated by pseudo-question-driven strategy. w/ NLQ denotes models using one templated NL question\nand w/ PseudoQ denotes models using one pseudo question.\nwithout using any NLP tools. All these above demonstrate\nthe effectiveness of the proposed framework.\nAblation Study\nIn this subsection, we conduct ablation studies to discuss\nthe effects of the multi-task combination and the impacts of\nseveral components. We adopt the UMT w/ PseudoGQ as\nthe full model for comparison. Model with − GRP denotes\nthe model dicard the global semantics and only use local\ninformation for relation classiﬁcation. Model with − GRP\n− LRP denotes the model ablates the subject-aware relation\nprediction process and enumerates all relation types instead.\nModel with − Context Fusion denotes the model excluding\nthe highway and BiGRU network. NER + SRP + MHE is a\nmulti-task variant that replaces the QA-based object extraction with the multi-head selection (Bekoulis et al. 2018).\nModel\nEntity\nRelation\nF1(∆)\nF1(∆)\nCoNLL04\nUMT w/ PseudoGQ\n88.9\n72.4\n− GRP\n88.8 (-0.1)\n72.2 (-0.2)\n− GRP − LRP\n88.7 (-0.2)\n71.8 (-0.6)\n− Context Fusion\n87.9 (-1.0)\n71.3 (-1.1)\nNER + SRP + MHE\n86.3 (-2.6)\n64.7 (-7.5)\nACE05\nUMT w/ PseudoGQ\n85.7\n61.2\n− GRP\n85.6 (-0.1)\n61.1 (-0.1)\n− GRP − LRP\n85.5 (-0.2)\n60.3 (-0.9)\n− Context Fusion\n85.5 (-0.2)\n60.7 (-0.5)\nNER + SRP + MHE\n84.5 (-1.2)\n59.4 (-1.8)\nTable 3: Ablation study results. GRP and LRP denote global\nrelation prediction and loacal relation classiﬁcation, respectively. SRP denotes subject-aware relation prediction which\ncontains both GRP and LRP.\nTable 3 shows the comparison results. We observe that,\nglobal relation prediction gives beneﬁts to both datasets,\nproving that global information can guide the learning of\nlocal features. Meanwhile, without the subject-aware relation prediction, both entity and relation F1 drop signiﬁcantly\non two datasets. It demonstrates that ﬁltering irrelevant relations is necessary for QA-based object extraction. Moreover, the context fusion layer (i.e., highway and BiGRU)\ncontributes obviously by 0.9% and 0.5% to the performance,\nrevealing that the combination of highway and BiGRU can\nfurther encode the features of BERT output. Additionally,\nour multi-task combination is much superior to NER + SRP\n+ MHE. It further explains the rationality of our framework.\nAnalyses on Extraction Order and Non-predeﬁned\nRelations\nImpact of Extraction Order\nNote that, the proposed\nUMT follows the relation-middle extraction order. Therefore, we take the relation-ﬁrst model (UMTRF ) for comparison and conduct experiments on CONLL04, ACE05 and\nDuIE2.0. Speciﬁcally, UMTRF conducts relation prediction initially to narrow the relation set. Then, it follows by\nthe same subject extractor and the pseudo generated question based object extractor. Table 4 shows the experimental result. For clarity, we rename the UMT w/ PseudoGQ as\nUMTRM to highlight its relation-middle extraction manner.\nWe can observe that, the UMTRM outperforms the UMTRF\non CONLL04 and ACE05 by a large margin. While UMTRF\nbeats UMT w/ PseudoGQ obviously on DuIE2.0. The results\ndemonstrate that the relation-middle method (UMT) is more\nqualiﬁed for concise datasets with fewer entity and relation\ntypes. Differently, the number of relation type on DuIE2.0\nis up to 55. Such a large relation set greatly increases the\ncomputational burden of UMTRM and makes it difﬁcult to\nachieve good results. Hence, for much complicated datasets,\nDataset\nUMTRM\nUMTRF\nP / R / F1\nP / R / F1\nCoNLL04\n73.2 / 71.6 / 72.4\n63.4 / 73.4 / 68.0\nACE05\n61.0 / 61.3 / 62.1\n59.5 / 61.9 / 60.7\nDuIE2.0\n66.9 / 63.0 / 64.9\n76.1 / 74.9 / 75.5\nTable 4: Comparisons of relation-middle model against\nrelation-ﬁrst model. For clarity, we rename UMT w/ PseudoGQ as UMTRM.\nAction\nPrecision\nRecall\nF1-score\nBERT\n47.82\n49.61\n48.7\nUMT w/ PseudoGQ\n85.5\n75.4\n80.1\nUMT w/ NLGQ\n90.3\n86.2\n88.2\nTable 5: Performane comparisons of non-predeﬁned relation\nextraction on Travel20 dataset.\npredicting relation ﬁrstly may lead to better performance.\nPerformance on Non-predeﬁned Relations\nTo evaluate\nthe performance of UMT on non-predeﬁned relations, we\ntest the model trained with ACE05 on a new Travel20\ndataset, whose relation types are totally different from\nACE05. As shown in Table 5, UMT still achieves considerable results on the new dataset. Meanwhile, UMT w/\nNLGQ gains better performance that UMT w/ PseudoGQ.\nWhile existing methods fail to solve such a problem. The\nresults demonstrate that our UMT also works for the nonpredeﬁned relations, and the syntactic information of natural\nlanguage helps to further improvements.\nImpact of the Number of Questions\nIn this subsection, we discuss the impact of the number of\ngenerated questions. We perform evaluations by varying the\nquestion numbers T as 1, 3, 5, respectively. The results of\nthe relation F1 are shown in Figure 4. Notably, asking diverse questions obviously improves the model performance\non the two datasets. By using 5 questions, we can further\ngain a performance boost. These well verify the effectiveness of diverse question answering mechanism. Note that,\nthe ACE05 is more sensitive to NL generated questions. We\nconsider that the relation types in ACE05 are more confusing, as one relation can further be divided into several subtypes. Therefore, the NL questions can provide structural information to help the semantic understanding.",
        "related work": "Extracting entities and their relations from the unstructured\ntext is an essential task for natural language understanding.\nEarly pipelined works suffer from error propagation problems. Recently, many joint models have been proposed. A\nmajority of joint learning strategies have been well studied,\nsuch as parameter-sharing strategy (Miwa and Bansal 2016;\nKatiyar and Cardie 2017), joint decoding algorithms (Yang\nand Cardie 2013; Katiyar and Cardie 2016), and global normalization (Zhang, Zhang, and Fu 2017; Ren et al. 2017).\n59.3\n60.7\n61.2\n59.9\n61\n61.4\n58\n59\n60\n61\n62\n1\n3\n5\nRelaƟon F1 Score\nNumber of Generated QuesƟons\nPseudoGQ\nNLGQ\n70.3\n72.2\n72.4\n69.9\n71\n72.2\n68\n69\n70\n71\n72\n73\n1\n3\n5\nRelaƟon F1 Score\nNumber of Generated QuesƟons\nPseudoGQ\nNLGQ\nFigure 4: Relation F1 for the number of questions T =\n1, 3, 5 on CONLL04 (left ﬁgure) and ACE05 (right ﬁgure).\nMost works detect entities by sequence labeling approaches.\nMethods for relation extraction can be fell into two categories. The ﬁrst category treats the relation extraction as\na classiﬁcation problem. For example, Miwa et al. (2016)\nadopt the Tree-BiLSTM based on a dependency analysis.\nEberts and Ulges (2019) propose a span-based method with\npre-trained transformers. Another category cast the relation\nextraction as a question asking problem. Levy et al. (2017)\nﬁrstly propose to reduce the relation extraction to answering simple machine reading comprehension questions. Then,\nLi et al. (2019) introduce a multi-turn question answering framework for entity-relation extraction. More recently,\nZhao et al. (2020) further improve it by asking diverse questions and achieving a signiﬁcant performance boost.\nOur work is different from previous works (Zhao et al.\n2020) and enjoys the following new keypoints. First, we propose the type-attentional subject extraction and generation\nbased object extraction without relying on the hand-crafted\ntemplates. Second, they only use the local context feature for\nrelation classiﬁcation, while we further introduce the global\ninformation to guide the feature learning process. Finally,\nwe divide the entity-relation extraction into three sub-tasks\nand integrate them into a multi-learning framework, which\ncan better capture the correlations among the sub-tasks.",
        "conclusion": "This paper proposes a uniﬁed multi-task learning framework for entity- relation extraction. we ﬁrst introduce a\ntype-attentional subject extraction task for subject detection.\nThen, we present a subject-aware relation prediction task to\nﬁlter out irrelevant relations for subject entities. After that,\nwe propose a question generation based method for object\nextraction. Finally, these interacted tasks are integrated into\na uniﬁed multi-task learning framework. Extensive experiments on benchmark datasets verify the effectiveness the\nproposed framework.",
        "summary_en": "Joint extraction of entities and relations focuses on detecting entity pairs and their relations simultaneously with a unified model. Based on the extraction order, previous works mainly solve this task through relation-last, relation-first and relation-middle manner. However, these methods still suffer from the template-dependency, non-entity detection and non-predefined relation prediction problem. To overcome these challenges, this paper proposes a unified multi-task learning framework to divide the task into three interacted sub-tasks. Specifically, the paper first introduces the type-attentional method for subject extraction to provide prior type information explicitly. Then, the subject-aware relation prediction is presented to select useful relations based on the combination of global and local semantics. Third, the paper proposes a question generation based QA method for object extraction to obtain diverse queries automatically. Notably, the method detects subjects or objects without relying on NER models and thus it is capable of dealing with the non-entity scenario. Finally, three sub-tasks are integrated into a unified model through parameter sharing. Extensive experiments demonstrate that the proposed framework outperforms all the baseline methods on two benchmark datasets, and further achieve excellent performance for non-predefined relations.",
        "summary_zh": "这篇论文介绍了一种统一的多任务学习框架，用于实体和关系的联合提取任务。旨在解决以前的方法解决该任务时，会存在模板依赖性、非实体检测和非预定义关系预测等问题。该框架将任务分解为三个交互子任务。具体来说，作者首先引入了类型注意力方法用于主体提取，以明确提供先验类型信息。然后，提出了主体感知关系预测，基于全局和局部语义的组合选择有用的关系。第三，提出了基于问题生成的QA方法用于客体提取，以自动获取多样的查询。最后，通过参数共享将三个子任务集成到一个统一模型中。广泛的实验表明，所提出的框架在两个基准数据集上优于所有基准方法，并进一步在非预定义关系方面取得了优异的性能。"
    },
    {
        "title": "CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning",
        "abstract": "Joint extraction of entities and relations has received significant attention due to its potential of providing higher performance for both tasks. Among existing methods, CopyRE is effective and novel, which uses a sequence-to-sequence framework and copy mechanism to directly generate the relation triplets. However, it suffers from two fatal problems. The model is extremely weak at differing the head and tail entity, resulting in inaccurate entity extraction. It also cannot predict multi-token entities (e.g. Steven Jobs). To address these problems, we give a detailed analysis of the reasons behind the inaccurate entity extraction problem, and then propose a simple but extremely effective model structure to solve this problem. In addition, we propose a multi-task learning framework equipped with copy mechanism, called CopyMTL, to allow the model to predict multi-token entities. Experiments reveal the problems of CopyRE and show that our model achieves signiﬁcant improvement over the current state-of-the-art method by 9% in NYT and 16% in WebNLG (F1 score). Our code is available at https://github. com/WindChimeRan/CopyMTL",
        "introduction": "As a key technology for automatic Knowledge Graph (KG)\nconstruction, relation extraction has received widespread attention in recent years. Relation extraction aims to automatically learn triplets (relation, head, tail) from the unstructured text without human intervention.\nEarly studies use pipeline models (Nadeau and Sekine\n2007; Chan and Roth 2011), where they cast the relation\nextraction problem into two separate tasks, i.e. Named Entity Recognition (NER) to extract the entities and Relation\nClassiﬁcation. They ﬁrst recognize the entities and then predict the relations between entities. However, pipeline models\nsuffer from obvious drawbacks (Roth and Yih 2007). Each\ncomponent limits the performance because of the error cascading effect and there is no chance for the model to correct mistakes. In addition, such pipeline models cannot capture the explicit relation between the two subtasks (Li and Ji\n7H[W\n6WHYHQ\u0003-REV\u0003ZDV\u0003ERUQ\u0003RQ\u0003)HEUXDU\\\u0003\u0014\u001c\u0018\u0018 LQ\u00036DQ\u0003)UDQFLVFR\u000f\u0003&DOLIRUQLD\n&RS\\5( BirthDay\u000bJobs, 1955\f\u0003\nBirthPlace\u000bJobs\u000f\u0003Francisco\f\n*ROG\nBirthDay\u000bSteven Jobs, February 19\u0018\u0018\f\u0003 BirthPlace\u000bSteven Jobs, San Francisco\f\nFigure 1: CopyRE predicts the entity pointer refers to the\nword position in the source sentence. The colored tokens\nshow the limitation of CopyRE which cannot predict multiple tokens.\n2014), where joint models can beneﬁt from such interdependencies.\nRecent studies on joint models of entity and relation extraction have three major research lines: Table Filling, Tagging, and Sequence-to-Sequence (Seq2Seq). Among these\napproaches, the table ﬁlling method (Gupta, Sch¨utze, and\nAndrassy 2016; Adel and Sch¨utze 2017) requires the model\nto enumerate over all possible entity pairs, which leads to a\nheavy computational burden. The tagging method (Zheng et\nal. 2017) suffers from the overlapping relation problem that\nthe model cannot assign different relation tags to one token.\nTo solve this problem, the followers (Takanobu et al. 2018;\nDai et al. 2019) run tagging on a sentence for multiple\nturns, which is akin to the table ﬁlling method together with\nthe heavy computational burden. Relatively speaking, the\nSeq2Seq method is neither plagued with overlapping relations nor with excessive computations. Seq2Seq model receives the unstructured text as input and directly decodes the\nentity-relation triples as a sequential output. This concise approach also matches with the human annotation process, that\nthe annotators ﬁrst read the sentences, understand the meaning and then point out the entity-relation pairs sequentially.\nCurrently, CopyRE (Zeng et al. 2018) is the most powerful Seq2Seq based joint extraction method which expands a\nSeq2Seq framework with copy mechanism in the decoder.\nThe copying mechanism allows the model to avoid the outof-vocabulary (OOV) problem. Despite their promising result, the model still suffers from two major drawbacks.\nFirst, the entity copying in CopyRE is unstable and it depends on an unnatural mask to differ the head (h) and tail (t)\nentities. Experimental results show that CopyRE nearly randomly predicts the head-tail order of the two entities. The\nmodel also needs an unnatural mask that masks the probability of h while predicting t. Without this mask, when\npredicting t, the model would choose the same token as h,\nand the accuracy drops to zero. After analysis, we prove\nthat CopyRE actually uses the same distribution to model h\nand t, chooses the highest probability as h, and the secondhighest would be chosen as t after masking the highest probability, so without this mask, it cannot differ h and t. Modeling the h and t distribution in such manner can cause various\nproblems, the model not only is extremely weak at differing\nh and t, but also cannot get information about h while predicting t.\nSecond, CopyRE cannot extract entities that have multiple tokens. The copy-based decoder always points to the\nlast token of any entities, which limits the applicability of\nthe model. For example, in Fig. 1 we show that CopyRE\nonly predicts “Jobs” rather than the whole entity “Steven\nJobs” when the entity has two tokens. In real-word scene\nmulti-token entities are common, so this can greatly drag\nthe model performance.\nTo address these two problems mentioned above, we propose CopyMTL, which is a multi-task learning based model\nwith a new architecture for entity copying. We ﬁrst provide a\ndetailed analysis of why CopyRE is unstable during copying\nand propose a new model architecture to improve the shortcomings. Our new model architecture merely adds one more\nnon-linear fully connection layer so that the model predicts\nseparate distributions for the head and tail entity, and the\ntail prediction receives information from the head prediction. This architecture no longer needs the unnatural mask\nand increases the accuracy of entity copying, resulting in the\noverall improvements over the state-of-the-art model.\nThen we propose a multi-task learning based Seq2Seq\nmodel to predict multi-token entities. A sequence labeling\nlayer is added at the encoding stage to assist the entity recognition process. We use multi-task learning of NER to predict\nthe start token of each entity while the decoder points at the\nlast token while decoding. During training, we optimize the\nmulti-task loss function jointly.\nIn conclusion, the contribution of this work is as follow:\n1. We analyze the reasons for the unstable performance of\nentity copying in CopyRE and propose a simple but effective\narchitecture to address this problem.\n2. We propose a multi-task framework to enhance the capability of handling with multi-token entities.\n3. Experimental results show that our model achieves\nstate-of-the-art results and outperforms previous approaches\nby a large margin.",
        "background": "In this section, we ﬁrst introduce the CopyRE model that is\nbased on the Seq2Seq framework. Then, we give a detailed\ndescription of the two existing problems. As shown in Fig.\n2, CopyRE consists two parts: an encoder and a decoder.\nGiven a sentence s = {x1, x2...xn}, the encoder transforms the input s into a vector representation. The decoder\npredicts the relation-entity triplets (r, h, t) each three time\nsteps. Inspired by CopyNet (Gu et al. 2016), the ﬁrst step\nuses Generate-Mode to predict a relation. Then, the model\nswitches to Copy-Mode and selects the head and tail entities\none by one in two different time steps.\nEncoder\nTo model the semantics of the input sentence better, CopyRE\nadopts Bidirectional LSTM (BiLSTM) (Schuster and Paliwal 1997) as the encoder, which has shown great strength in\nmany areas of NLP. Given a sentence of word embeddings\n{eE\n1 , ..., eE\nn } as input, the hidden states from two directions\nare computed:\n−→\nhi =\n−−−−−−→\nLSTM E(eE\ni , hi−1)\n←−\nhi =\n←−−−−−−\nLSTM E(eE\ni , hi+1)\nhE\ni = (−→\nhi + ←−\nhi)/2\n(1)\nwhere hidden states −→\nhi and ←−\nhi from two directions are\naveraged1 into one vector hE\ni as output.\nDecoder\nThe decoder uses a one direction LSTM to predict the outputs from left to right. The last hidden state of the encoder\nis used to initialize the decoder hidden state. The attention\nscore is assigned to each hidden state of the encoder and\nthen summed up to obtain an attentive sum. Then the sum\nis combined with the decoded hidden states at the last time\nstep to be fed into the decoder LSTM:\nct = Attention(hD\nt−1, hE\n1:n)\nut = [eD\nt ; ct] · W u\nhD\nt = LSTM D(ut, hD\nt−1)\n(2)\nwhere Attention calculates the attentive sum of all encoder\nhidden states hE\n1:n = {hE\n1 , ...hE\nn } according to the last decoder hidden state hD\nt−1. [·; ·] is the concatenation operator,\net is the embedding of the decoder output in the last time\nstep, W u ∈ R(de+dc)×de is the parameter of linear transformation. All biases are omitted for convenience.\nEvery three time steps form a loop in which the decoder\npredicts relation, last token of head and then last token of\ntail to form a triplet, respectively. The conﬁdence qi\nt for each\ntoken at position i to be copied as an entity is calculated by:\nqi\nt = [hD\nt ; hE\ni ] · W e\n(3)\nwhere W e ∈ R2do×1.\nThen, the decoder computes the logits according to the\ntime step t (we count the time step from 1):\nlogitt =\n⎧\n⎨\n⎩\n[hD\nt · W r; qNA],\nif t%3 = 1;\n[qt; qNA],\nif t%3 = 2;\n[M ⊗ qt; qNA],\nif t%3 = 0.\n(4)\n1The original paper uses concatenation, but actually they use\naverage in the released code.\nFigure 2: The overview of CopyMTL model for joint extraction of relation and entity. The CopyRE model does not contain the\nCopyMTL-Tagging part, i.e., the sequence-labeling part in the ﬁgure.\nwhere W r ∈ Rdo×rel, rel is the cardinality of relations, qt\nis the concatenation of all qi\nt, M is the mask which records\nthe predicted head entity and prevents the decoder predicting\nit at the t%3 = 0 time step. This is based on the fact that an\nentity cannot be both the head and the tail in the same triplet\nat the same time. But the mask makes no contributions for\nminimizing the cross entropy loss we will describe below.\nThrough the unnormalized logit, we can obtain the probability of output entity or relation by softmax:\np(yt|y<t, s) =\nelogitj\nt\n\u0005\ni elogiti\nt\n(5)\nAt time step t%3 = 1 when the model should predict relation, the softmax score is calculated over all relations types;\nwhen the model should predict the entity, the softmax score\nis calculated over all positions in the source sentence. Then,\nthe model can be trained via minimizing the cross entropy\nloss, which measures the difference between the output yt\nand the label y∗\nt .\nLD = −\n\u0006\nt\nlog(p(y∗\nt |y<t, s))\n(6)\nCopyRE also use padding triplets (NA, NA, NA) during training, which do not have any valid relations and entities. The\nconﬁdence qNA of NA-relation and NA-position of the corresponding entity is calculated through a shared parameter:\nqNA = hD\nt · W NA\n(7)\nwhere W NA ∈ Rdo×1.\nProblems of CopyRE\nAs mentioned in the introduction, we found that CopyRE\nhas two problems. First, the prediction of the entity is unstable. In detailed experiments, we observed that CopyRE\ncannot even ﬁt the training set well, in which the F1 scores\nare approximately 0.75 and 0.40 on two datasets (see Fig.\n4). In addition, if we remove the mask M in Eq. (4), the F1\nscore will turn to zero immediately. To ﬁnd out the reason\nbehind it, we evaluate CopyRE for the predicted relations\nand entities in the triplets separately. The experiments show\nthat CopyRE can gain 0.84 F1 score for relations, while the\nF1 score for entities dramatically drops to 0.64 (see Table 4).\nIn addition, when we inspected the prediction errors, we ﬁnd\nthat CopyRE is prone to mix up the order of head and tail.\nThus, we can conclude that entity copying is the bottleneck\nof the model, which causes the performance decline.\nSecond, since CopyRE only predicts the last token of the\nentity, when the target entity contains multiple tokens, the\noutputs are incomplete. There are straightforward ways to\nsolve this problem. For example, we can extend the predicted triplets to quintuple by adding the length of entities.\nHowever, such methods indirectly use or simply ignore the\ninteractions between relation extraction and entity recognition. We propose a multi-task manner method to solve this\nproblem and give detailed comparisons in experiments.",
        "our method": "As described in the last section, CopyRE is suffering the entity copying and the multi-token entity problems. We propose a model named CopyMTL (Fig. 2) to address these two\nproblems. CopyMTL is based on a new model structure and\nuses a multi-task framework which adds a sequence labeling\ntask to CopyRE encoder. In this section, we ﬁrst reveal the\nreasons behind the entity copying problem, then propose a\nsimple but reasonable solution. After that, we introduce an\nadditional tagging layer of the encoder and the multi-task\ntraining procedure.\nNew Structure for Entity Copying\nStrangely, in CopyRE entity copying highly depends on the\nentity mask M, and the predicted distributions of head and\ntail entities are identical. Through our analysis, the main culprit is found in Eq. (3), who calculates the concatenation of\nhD\nt and hE\ni , then passes it to a linear transformation. Eq. (3)\ncan be expanded and simpliﬁed to get the following form:\nqt\ni = [hD\nt ; hE\ni ] · W e\n= [hD\nt ; hE\ni ] · [W e\n1 ; W e\n2 ]\n= hD\nt · W e\n1 + hE\ni · W e\n2\n(8)\nwhere W e\n1 , W e\n2 ∈ Rdo×1. Note that this is a summation of\ntwo scalars and the ﬁrst term is independent of i. If we omit\nthe qNA, the probability of entity copying is calculated by\nsoftmax:\n濃\n濃濁濃濈\n濃濁濄\n濃濁濄濈\n濃濁濅\n濃濁濅濈\n濃濁濆\n澷濣濤濭澔澼濙濕濘\n濣瀅瀂濵濴濵濼濿濼瀇瀌\n濶瀂瀃瀌\n瀁瀂瀇澳濶瀂瀃瀌\n濃\n濃濁濃濈\n濃濁濄\n濃濁濄濈\n濃濁濅\n濃濁濅濈\n濃濁濆\n澷濣濤濭澔濈濕濝濠\n濣瀅瀂濵濴濵濼濿濼瀇瀌\n濶瀂瀃瀌\n瀁瀂瀇澳濶瀂瀃瀌\n瀀濴瀆濾濸濷\nFigure 3: The problematic entity copying of CopyRE. After\npredicting relation BirthPlace, the model will copy the head\nentity Jobs, then mask the predicted head and copy the tail\nFrancisco.\np(yt|y<t, s) =\neqt\ni\n\u0005\nj eqt\nj =\nehD\nt ·W e\n1 · ehE\ni ·W e\n2\nehD\nt ·W e\n1 · \u0005\nj ehE\nj ·W e\n2\n=\nehE\ni ·W e\n2\n\u0005\nj ehE\nj ·W e\n2\n(9)\nAbnormal dependency to the mask:\nIn Eq. (9), we can\nsee that probt\ni does not rely on the time step t. In other\nwords, the output distribution of entity copying at t%3 = 1\nand t%3 = 2 are identical, which causes the dependency on\nthe mask. We visualize the output distribution of the entity\ncopying in Fig. 3. In the ﬁgure, the model ﬁrst copies the token with the highest probability, Jobs. Then, in the next time\nstep, as the pointed token Jobs is masked, the model copies\nthe token with the second highest probability, Francisco.\nUnstable entity copying:\nBecause the distributions of two\ntime steps are the same and the mask is only used in evaluation rather than optimization, the entity copying, especially\nfor the head entity, becomes unstable. In the training stage,\nCopyRE maximizes the likelihood for the head at t%3 = 2\nand for the tail at t%3 = 0, while the likelihood at each\ntime step is identical. However, as the mask is not used for\noptimization, there is no explicit constraint to ensure that the\nhead has the highest probability and tail has the second highest probability. In fact, CopyRE tries to maximize both the\nhead and the tail. Thus, which one would be the highest and\nbe predicted at t%3 = 2 is random.\nTo ﬁx the problem in Eq. (9), we simply map hD\nt and hE\ni\nto a fused feature space via one additional non-linear layer:\nqt\ni = σ([hD\nt ; hE\ni ] · W f) · W o\n(10)\nwhere σ is the selu(·) activation function (Klambauer et al.\n2017), W f ∈ R2do×dW f and W o ∈ RdW f ×1.\nDue to the non-linearity of the activation function, the reduction of Eq. (9) does not hold true. Now, the entity copying depends on both i and t and there is only one target output to maximize instead of that in Fig. 3. Thus, by replacing\nEq. (3) with Eq. (10), the decoder no longer needs to struggle with ranking head and tail at t%3 = 2, and the mask\nis no longer urgently needed2. Therefore, the entity copying\nbecomes stable with our new structure.\nSequence Labeling Layer\nCopyRE only copies the last token of the entity. To predict\nentities with multiple tokens, we cast the problem into a sequence labeling problem and use the NER results to calibrate the entities with multiple tokens. As shown in Fig. 2,\nwe ﬁrst derive the emission potential from the encoder output. Then, an additional Conditional Random Field (CRF)\nlayer (Lafferty, McCallum, and Pereira 2001) is employed\nto calculate the most probable tag for each token. We use\nthe BIO scheme (Begin, Inside, Outside) to recognize all of\nthe entities in the sentence. The predicted tags are used to\npost-process the extracted entities.\nThe conditional probability of target tags∗ given sentence\ns are computed by path probability:\np(tags∗|s) =\nescore(s,tags∗)\n\u0005\ntags′ escore(s,tags′)\n(11)\nwhere the denominator is computed via dynamic programming. The unnormalized path score is deﬁned as:\nscore(s, tags) =\n\u0006\ni\nφi,tagi + btagi−1→tagi\n(12)\nwhere btagi−1→tagi is the transition score from tagi−1 to\ntagi. φi,tagi is the score of the tagi for the i-th input token, which is comes from the hidden state of the Bi-LSTM\nat timestep i.\nThe loss function of the sequence labeling is:\nLE = −log(p(tag∗|s))\n(13)\nIn the inference stage, we use the NER results to postprocess the decoded entities. Since we use BIO tagging\nscheme, there are three circumstances for the decoded last\ntoken of entities:\n• ’B’. a single token entity.\n• ’I’, an entity with multiple tokens, it will look for the token before the current token until it ﬁnds ’B’.\n• ’O’, a single token entity.\nTraining\nOverall, the input sentence is fed into the encoder part. All\nof the hidden states of the encoder are used to label the input\nsequence and calculate the attention of the decoder. Initialized by the last hidden state of the encoder, the decoder generates triplets each three time steps. Thus, the loss function\n2In experiments, we found that adding the mask to our method\nbrings no enhancement.\nModel\nNYT\nWebNLG\nPrec\nRec\nF1\nPrec\nRec\nF1\nNovelTagging\n.642\n.317\n.420\n.525\n.193\n.283\nCopyRE-One (ours)\n.612\n.530\n.571\n.312\n.272\n.291\nCopyRE-Mul (ours)\n.610\n.566\n.587\n.319\n.273\n.294\nGraphRel-1p\n.629\n.573\n.600\n.423\n.392\n.407\nGraphRel-2p\n.639\n.600\n.619\n.447\n.411\n.429\nCopyMTL-One\n.727\n.692\n.709\n.578\n.601\n.589\nCopyMTL-Mul\n.757\n.687\n.720\n.580\n.549\n.564\nTable 1: Results of the compared models on NYT and WebNLG, in which CopyRE uses less strict evaluation.\nDataset\nNYT\nWebNLG\nRelation types\n24\n246\nDictionary size\n90760\n5928\nTrain sentence\n56195\n5019\nTest sentence\n5000\n703\nTable 2: Statistics of the datasets.\ncontains two parts: the encoder part introduces an additional\nCRF loss, and in the decoder part the cross entropy loss is\nused to measure the difference between the output triplets\nand the gold triplets.\nWe deﬁne the loss function as the weighted summation of\nencoder loss and decoder loss:\nL = λ · LE + LD\n(14)\nwhere λ is the weight of the tagging loss.\nThe loss is calculated as the average over shufﬂed minibatch, and the derivatives of each parameter can be computed via back-propagation.",
        "experiments": "Datasets and Setting\nWe evaluated models on two datasets: New York Times\n(NYT) (Riedel, Yao, and McCallum 2010) and WebNLG\n(Gardent et al. 2017). NYT comes from the distant supervised relation extraction task (DSRE), which aims to leverage the strength of the knowledge base to generate a largescale dataset (Mintz et al. 2009). To make joint extraction\nmore challenging than DSRE experiment setting, Zeng et\nal. (2018) additionally modiﬁed the data to include more\noverlapping relations. WebNLG is originally used for natural language generation, in which all of the sentences are\nwritten by annotators. To avoid that the model only remembers the entity linking instead of the relation pattern, we only\nuse the ﬁrst sentence for each instance, which is the same as\nother baselines. The data statistics of both datasets are shown\nin Table 2.\nOur experiments settings also followed most of the settings of CopyRE. The hidden number of LSTM was set to\n1000. The max number of decoded triplets was 5. This was\nbecause the average triplet number in both dataset is about\n2. We did not use “end-of-sentence” token to stop decoding,\nbut to decode all padding triples (NA, NA, NA). The embedding dimension was 100, and we used the same pretrained\nembeddings3. Adam (Kingma and Ba 2014) was used to optimize the neural networks and the learning rate was 0.001.\nThe weight of LE, λ, was set to 1.\nBaselines and Evaluation Metrics\nWe compare CopyMTL with CopyRE (Zeng et al. 2018),\nNovelTagging (Zheng et al. 2017) and GraphRel (Fu, Li,\nand Ma 2019). NovelTagging uses sequence labeling to assign one label to each word, which contains both entity and\nrelation information. GraphRel is the state-of-the-art model,\nwhich uses a post-editing method to revise the triplets phase\nby phase. For Seq2Seq model, CopyRE and our CopyMTL,\nwe give a more detailed comparison to show the advantages\nof our new structure. We also evaluate the OneDecoder and\nMultiDecoder trick for the Seq2Seq models (denoted as One and -Mul). The main difference between the two decoders is the parameter sharing strategy. OneDecoder uses\nshared parameters for predicting all triplets and is exactly\nwhat we described in the background section. MultiDecoder\nuses unshared decoders, each decoder predicts one triplet.\nWe use precision, recall, and micro-F1 score to evaluate\nthe models. The evaluation metrics we use are stricter than\nthat of the original CopyRE. That is to say, instead of leaving out the incomplete entity problem, the outputs of our\nexperiments are regarded as correct only if both the relation\ntypes and all entity tokens are correct. This stricter metric\nmeets real-world usage and the comparison is fairer to NovelTagging and GraphRel because they are not haunted by the\nmulti-token problem. For an intuitive comparison, we also\nlist the result of CopyRE in the table, although their evaluation is not so strict.\nComparison of Baselines\nTo evaluate the performance of the proposed method, we\ncompare CopyMTL with the baseline methods. The results4\nare shown in Table 1.\nAs shown, CopyMTL is the best model in WebNLG and\nNYT. Both the precision and the recall are signiﬁcantly improved. In the NYT dataset, compared with the state-of-theart model, GraphRel-2p, CopyMTL-One outperforms it by\n3https://github.com/xiangrongzeng/copy re\n4As NovelTagging is signiﬁcantly better than previous works,\nwe do not add more comparisons.\n(a)\n(b)\nFigure 4: The training curves of CopyRE and CopyRE’ on NYT and WebNLG.\nDataset\nModel\nPrec\nRec\nF1\nNYT\nCopyRE\n.612\n.530\n.571\nCopyRE’\n.747\n.700\n.722\nWebNLG\nCopyRE\n.312\n.272\n.291\nCopyRE’\n.583\n.629\n.605\nTable 3: Results of CopyRE and CopyRE’ on NYT and\nWebNLG. These models do not consider entities with multiple tokens and use less strict evaluation that ignores entity\nwith multiple tokens.\n8.8% for precision and 9.2% for recall. In the WebNLG\ndataset, the effect is more signiﬁcant. The improvements are\n13.1% in precision and 19% in the recall. These observations verify the effectiveness of our proposed method. NovelTagging is characterized by a low recall, which is caused\nby its deﬁciency in overlapping relations. CopyRE has already solved this problem well, with 8% and 19% absolute\nF1 improvement in WebNLG and NYT. Our method further\nbrings 33% and 19% F1 enhancement compared to CopyRE,\nwhich shows great potentials of Seq2Seq methods.\nCopyRE argued that MultiDecoder is better than OneDecoder, which is validated by our reproduction experiment.\nHowever, with our novel CopyMTL, MultiDecoder is better than OneDecoder in NYT but worse in WebNLG. This\nis probably because NYT is a bigger dataset, in which MultiDecoder with more parameters works better. In practice,\nwhich decoder to use should be determined by the size of\nthe dataset and we cannot conclude that one is better than\nanother in every situation. For simplicity, we only discuss\nOneDecoder in the following sections.\nEffects of the Revised Entity Copying Method\nAlthough CopyMTL outperforms baselines by a huge margin, it is still unclear which component in CopyMTL plays\nthe pivot role. To reveal the strength of the new model architecture, we compare CopyRE with the modiﬁed model,\ncalled CopyRE’, which only substitutes Eq. (3) for Eq. (10).\nThe comparison is in Table 3, from which we can observe\nthat Eq. (10) is extremely effective. CopyRE’ model gains\nDataset\nModel\nRelation\nEntity\nNYT\nCopyRE\n.846\n.647\nCopyRE’\n.869\n.756\nWebNLG\nCopyRE\n.767\n.595\nCopyRE’\n.797\n.782\nTable 4: F1 scores on subtasks.\n13% F1 boost in NYT dataset and 31% F1 boost in WebNLG\ndataset.\nNote that the new model architecture only considers\nthe entity copying while the F1 score computed considers\nthe whole triplet. In order to uncover the performance of\nCopyRE’ in relation classiﬁcation and entity recognition, we\ncalculate the F1 scores for the two subtasks in Table 4. For\nthe entity recognition subtask, the F1 score of CopyRE’ is\n10% higher in NYT and 19% higher in WebNLG. This is\nthe main contribution of the new model architecture. For the\nrelation classiﬁcation subtask, the F1 score of CopyRE’ is\nmarginally higher (less than 3%) than that of CopyRE. This\nimplies the better entity recognition helps relation classiﬁcation learning, which conﬁrms the argument that the interactions between two task are beneﬁcial to each other. In the\ndecoding stage, a more precise prediction of the entity is fed\ninto the decoder, which aids the relation classiﬁcation in the\nnext time step.\nExcept for the ﬁnal result, the learning processes of the\ntwo models are also different. We plot the overall F1 score\nvarying with the training epochs in Fig. 4. The curve shows\nthat CopyRE does not ﬁt the training set well and the model\nsaturates at epoch 20, where the F1 score of NYT is 75%\nand the F1 score of WebNLG is 40%. By contrast, CopyRE’\ngains 97% F1 score in the NYT training set and 97% F1\nscore in the WebNLG training set. In addition, the performance of CopyRE’ continues increasing until epoch 40 on\nboth datasets. The fact that the model gains lower training\nerror which also generalizes to the test set may explain the\neffectiveness of CopyRE’.\nDataset\nModel\nPrec\nRec\nF1\nNYT\nGraphRel-2p\n.639\n.600\n.619\nCopyRE’5\n.680\n.663\n.671\nCopyMTL\n.727\n.692\n.709\nWebNLG\nGraphRel-2p\n.447\n.411\n.429\nCopyRE’5\n.572\n.536\n.553\nCopyMTL\n.578\n.601\n.589\nTable 5: Results of different multi-token models on NYT\nand WebNLG\nEffects of Multi-Task Learning\nCopyMTL aims to solve the multi-token problem. In addition to the multi-task learning used by CopyMTL, there\ncan be other straightforward methods. For example, the\ndecoder of CopyRE’ can predict the length of the entity\nwhen it copies the entities, which forms quintuples, called\nCopyRE’5. This is similar to predicting both the begin and\nthe end of the entities and should work the same.\nWe compare the models in Table 5, from which we can\nsee that CopyRE’5 is worse than CopyMTL in all evaluations, but both outperform GraphRel. We conjecture that the\nthree tasks in CopyRE’5, include relation classiﬁcation, entity recognition, and entity length prediction, varying in their\ndegree of difﬁculty. The entity length prediction task may\ninterfere with the learning of other tasks, as this easier task\nprolongs the dependence distance of harder tasks.\nIn addition, we also evaluate how precisely does the encoder of CopyMTL completes the whole entities. It gains\n99% F1 score in NYT and 96% F1 score in WebNLG. This\nevaluation is less strict than conventional NER tasks as we\nconsider neither the types of the entities nor the entities out\nof relations. We can conclude that NER in joint extraction is\npowerful enough for triplet extraction and the main difﬁculty\nin joint extraction is to make a better prediction for both the\nrelations and the positions of the corresponding entities.",
        "related work": "Extraction of entities and relations is of signiﬁcance to many\nNLP tasks. In recent years, there have been four mainstream\nmethods.\nPipeline methods: Previous works mainly use pipeline\nmethods (Nadeau and Sekine 2007), a.k.a extract entities\nﬁrst then classify the relations. Most of the recent neural\nmodels also focus on pipeline methods, include (1) FullySupervised Relation Classiﬁcation (Hendrickx et al. 2009)\n(2) Distant Supervised Relation Extraction (Mintz et al.\n2009). In spite of the recent progress of neural models (Cai,\nZhang, and Wang 2016; Zeng et al. 2014; Christopoulou,\nMiwa, and Ananiadou 2018; Qin, Xu, and Wang 2018), the\npipeline methods introduce error propagation problem (Li\nand Ji 2014), which does harm to the overall performance.\nTable ﬁlling: The joint extraction task is formalized as a\ntable constituted by the Cartesian product of the input sentence to itself. The table blanks, except for that on the diagonal, are to be predicted as relations. The models include\nhistory-based searching (Miwa and Sasaki 2014), neuralbased prediction (Gupta, Sch¨utze, and Andrassy 2016) and\nglobal normalization (Adel and Sch¨utze 2017). The stateof-the-art model, GraphRel, also belongs to this genre. This\nmodel innovative takes the interaction between entities and\nrelations into account via 2-phase GCN. The main problem\nof table ﬁlling is the over redundant computation for the permutation of all word pairs in a sentence. As a result, most of\nthe blanks in the table are empty and it is the sparsity that\nhinders the learning of the models.\nTagging: The tagging models originally solved the tasks\nseparately through a shared parameter: the model tags the\nentities ﬁrst, then predicts the relations. SPTree (Miwa and\nBansal 2016) used a structural neural model with the help\nof linguistic features. This model was promoted by an\nattention-based model (Katiyar and Cardie 2017). Besides,\nNovelTagging (Zheng et al. 2017) proposed a new tagging\nscheme, by which the model can predict a single tag for each\nword, containing both the entities and relations. However,\nthis tagging scheme cannot handle overlapping relations because it cannot assign one token with multiple labels. To\nsolve it, multi-pass tagging training, HRL5, has been purposed (Takanobu et al. 2018), based on the reinforcement\nlearning framework and (Dai et al. 2019) leverage attention\nmechanism. Although these methods solve the overlapping\nrelation problem, their nature and complexities are akin to\ntable ﬁlling.\nSeq2Seq: CopyRE (Zeng et al. 2018) is another method\nfor solving the overlapping relation problem, which extracts\ntriplets by a Seq2Seq framework (Sutskever, Vinyals, and\nLe 2014) with copy mechanism (Gu et al. 2016). But it cannot predict the entire entities. In addition, the weak performance hinders it from real-world usage. Our work resolves\nthe problems and boosts the performance to a new level.",
        "conclusion": "In this paper, we revisit the CopyRE model, which jointly\nextracts entities and relations by a Seq2Seq model. We ﬁnd\nthat there are two problems in the model: the performance\nof the model is limited by the inaccurate entity copying and\nthe generated entities are not complete. We give a theoretical\nanalysis to reveal the reason behind the ﬁrst problem, then\npropose a new model architecture to solve it. For the second problem, we propose a multi-task learning framework to\ncomplete the entities. Detailed experiments show the effectiveness of our method, which also outperforms the current\nstate-of-the-art model by a huge margin.\nFor future work, CopyMTL still has much potential, for\nexample, the current model can only extract a ﬁxed number\nof triplets. We would also like to extend CopyMTL to extract any number of triplets. CopyMTL can build a strong\nbaseline for future studies.",
        "summary_en": "Joint extraction of entities and relations has received significant attention due to its potential of providing higher performance for both tasks. Among existing methods, CopyRE is effective and novel, which uses a sequence-to-sequence framework and copy mechanism to directly generate the relation triplets. However, it suffers from two fatal problems. The model is extremely weak at differing the head and tail entity, resulting in inaccurate entity extraction. It also cannot predict multi-token entities (e.g. Steven Jobs). To address these problems, this paper gives a detailed analysis of the reasons behind the inaccurate entity extraction problem, and then proposes a simple but extremely effective model structure to solve this problem. In addition, the paper proposes a multi-task learning framework equipped with copy mechanism, called CopyMTL, to allow the model to predict multi-token entities. Experiments reveal the problems of CopyRE and show that eh model achieves significant improvement over the current state-of-the-art method by 9% in NYT and 16% in WebNLG (F1 score).",
        "summary_zh": "这篇论文介绍了一种简单有效的模型结构和一个配备复制机制的多任务学习框架CopyMTL，用于实体和关系的联合提取，旨在解决现有方法CopyRE存在的两个严重问题：对实体的提取不准确以及无法预测多词实体。实验揭示了CopyRE存在的问题，并表明CopyMTL模型比目前最先进的方法在NYT和WebNLG（F1分数）上分别提高了9%和16%"
    },
    {
        "title": "Progressive Multi-task Learning with Controlled Information Flow for Joint Entity and Relation Extraction",
        "abstract": "Multitask learning has shown promising performance in learning multiple related tasks simultaneously, and variants of model architectures have been proposed, especially for supervised classiﬁcation problems. One goal of multitask learning is to extract a good representation that sufﬁciently captures the relevant part of the input about the output for each learning task. To achieve this objective, in this paper we design a multitask learning architecture based on the observation that correlations exist between outputs of some related tasks (e.g. entity recognition and relation extraction tasks), and they reﬂect the relevant features that need to be extracted from the input. As outputs are unobserved, our proposed model exploits task predictions in lower layers of the neural model, also referred to as early predictions in this work. But we control the injection of early predictions to ensure that we extract good task-speciﬁc representations for classiﬁcation. We refer to this model as a Progressive Multitask learning model with Explicit Interactions (PMEI). Extensive experiments on multiple benchmark datasets produce state-of-the-art results on the joint entity and relation extraction task.",
        "introduction": "Multitask learning (MTL) is an important methodology that\nsimultaneously co-models multiple related tasks in a single model. One of the earliest proposed MTL architectures\nlearns a shared representation for multiple tasks, where this\nshared representation is utilized by task-speciﬁc structures\nindependently to learn task-speciﬁc representations for supervision (Collobert and Weston 2008). Thus, it induces an\ninductive bias that enhances the model’s ability to generalize\nwell on new inputs. This is a basic MTL structure which has\nbeen used successfully in several natural language processing (NLP) tasks (Liu et al. 2019b; Hu et al. 2019).\nAlthough impressive performance has been achieved by\nconsidering the basic MTL architecture, some models have\nconsidered incorporating early predictions (predictions of\nthe input in lower layers of the neural network) of the\ntask-speciﬁc structures to improve the task-speciﬁc representations (He, Lee, and and‘ Daniel Dahlmeier 2019; Zhao\net al. 2019). However, their approach applies a deterministic\nmodel (e.g. multilayer perceptron) on both the early predictions and shared representations to output the task-speciﬁc\nrepresentations. Obviously, deterministic models do not take\ninto account the randomness of the early predictions, as the\nquality of these predictions is heavily dependent on the quality of early classiﬁers. A more natural approach is to consider a stochastic model that possesses some inherent randomness. Besides, it is important to additionally control the\nﬂow of information from the early predictions since they\nare just an approximation to the ground truth, and not the\nground truth itself. That way we are sure to reduce the noise\nthat comes with early predictions to extract more expressive\ntask-speciﬁc representations.\nMoreover, the previous MTL models (Liu et al. 2019b;\nHu et al. 2019) only exploit the implicit interaction that is\ncaptured by the shared representation. Our understanding of\ntask relatedness informs us that correlations actually exist\nbetween the outputs of some related tasks, e.g. the entity\nrecognition and relation classiﬁcation tasks. Since we can\nobtain access to early predictions, we can model such correlations explicitly to improve task-speciﬁc representations.\nMotivated by these ﬁndings, the goal of this paper is\nto develop a multi-task learning architecture that incorporates early predictions of task-speciﬁc structures to improve\nthe learning of task-speciﬁc representations. Speciﬁcally,\nwe follow an approach used to extract the minimal sufﬁcient statistics of an input about an output using neural networks (Alemi et al. 2017), and develop stochastic maps that\nconsider the shared representations and the interaction of\nearly predictions of task-speciﬁc networks to extract good\ntask-speciﬁc representations for supervision.\nTo verify our proposed multitask learning approach, we\nchoose the joint entity and relation extraction as our target\ntask due to its popularity in NLP. So far, many works (Miwa\nand Sasaki 2014; Gupta, Sch¨utze, and Andrassy 2016; Fu,\nLi, and Ma 2019) have focused on leveraging multitask\nlearning to solve this joint task by taking entity recognition\n(ER) as one task, and relation classiﬁcation (RC) as another\ntask. The ER aims to extract all entities in the sentence, and\nthe RC aims to classify the relations between all word pairs\nin the sentence. However, most of the previous works do not\nconsider leveraging the early prediction to improve the taskspeciﬁc representation nor do they explicitly model the interactions between the two tasks. We show that our proposed\napproach can improve the performance for this task.\nOur contribution is summarized as follows:\n• We propose a progressive multi-task learning model\n(PMEI) which leverages interactions of early predictions\nto improve the task-speciﬁc representation.\n• Our model employs stochastic maps to encode both the\nshared representations between tasks and the early predictions from tasks. In particular, we ensure the information ﬂow from early predictions is controlled to reduce the\nnoise that comes with it.\n• We take the joint entity and relation extraction as a concrete example and apply our proposed method on this\njoint task. Extensive experiments on several benchmark\ndatasets show the effectiveness of the proposed method.",
        "related work": "Multitask Learning\nThe existing multitask learning architectures proposed so far\ncan be categorized according to their topological structure.\nWe have those with a ﬂat structure, a graph structure, and a\nhierarchical structure (Sun et al. 2020). A suitable structure\ndepends on the relatedness of the tasks. The most commonly\nused multitask learning architecture is the ﬂat structure. In\nthis architecture, task-speciﬁc networks are fed with a shared\nrepresentation, and then each task-speciﬁc network learns in\nisolation without interaction. This structure has been successfully applied in a variety of tasks, including relation extraction (Fu, Li, and Ma 2019) and natural language understanding (Liu et al. 2019b). However, as pointed by (Liu,\nQiu, and Huang 2017), the shared representation exploited\nby one task may contain noise brought by other tasks, contaminating the task-speciﬁc representations. To address this\nweakness, the hierarchical structure places different tasks at\ndifferent layers of the network according to the complexity\nof the tasks. The graph structure model interactions dynamically among the tasks to learn task-speciﬁc representations.\nOur proposed multitask learning architecture can be categorized under the graph structure.\nJoint Entity and Relation Extraction\nTraditional approaches proposed to solve the entity and\nrelation extraction task use a two-step pipeline-based approach (Zelenko, Aone, and Richardella 2003). However,\nthese approaches are faced with error propagation from the\nentity recognition task to the relation classiﬁcation task, and\ncannot leverage the interactions between the two tasks. Recent approaches consider to treat the tasks jointly. Among\nthese works, we have those that consider a sequence-tosequence approach (Zeng et al. 2018, 2019; Zeng, Zhang,\nand Liu 2020), but these approaches fail to effectively deal\nwith the overlapping relation problem (Wei et al. 2020).\nOther works have considered a sequence labelling approach\nto address the problem (Zheng et al. 2017b; Dai et al. 2019;\nTakanobu et al. 2019; Wei et al. 2020).\nMore recently, multitask learning methods have been proposed to address the joint task due to its ability to exploit\ninteractions among related tasks to learn good task-speciﬁc\nX\nBiLSTM\nH\nC\nˆY\n(A) Basic Model\nX\nBiLSTM\nH\nC′\nY ′\nMLP\nT\nˆY\nC\n(B) Progressive Model\nX\nBiLSTM\nH\nC′\nY ′\nSM\nT\nˆY\nC\nI(H; T|Y ′)\n(C) Progressive Model with Information Controlled\nFigure 1: Our models for single task learning.\nrepresentations (Miwa and Sasaki 2014; Miwa and Bansal\n2016; Gupta, Sch¨utze, and Andrassy 2016; Fu, Li, and Ma\n2019; Zeng, Zhang, and Liu 2020). Although the aforementioned MTL methods show satisfactory performance, they\nexploit only the implicit interactions that is captured in the\nshared representations of the related tasks. Besides, they do\nnot exploit early predictions of the ER and RC tasks as seen\nin other NLP tasks (He, Lee, and and‘ Daniel Dahlmeier\n2019; Zhao et al. 2019). Without modeling such information, these methods are limited in performance.",
        "method": "In this section, we gradually present our model architecture. We ﬁrst introduce a progressive classiﬁcation model\non a single task to show the rationale behind our proposed approach. Then we introduce the progressive classiﬁcation model in the context of multitask learning. Finally,\nwe demonstrate the application of the proposed multitask\nlearning methods on the joint ER and RC tasks.\nProgressive Classiﬁcation on Single Task Learning\nIn this section, we describe our models for single-task learning. The overview of our models are shown in Figure 1.\nConsider the basic model in Figure 1(A). Let X ∈ X\nbe an input random variable (e.g, a sentence), and Y ∈ Y\nbe an output random variable (e.g. class label). We employ a\nbidirectional LSTM (BiLSTM) to extract a contextual representation H ∈ H from X. A classiﬁcation model is deﬁned\nas the map C : H → p(Y). The function C takes H as input\nand outputs the probability distribution p( ˆY ) = C(H) over\nthe output space. This is a basic model and requires that the\nMarkov relation Y → X → H is satisﬁed. Thus, for the\njoint distribution p(x, y, h) which factors as follows:\np(x, y, h) = p(h|x, y)p(x, y),\n(1)\nit assumes that the conditional distribution p(h|x, y) =\np(h|x) under the Markov constraint. This means that H is\na function of X and it is deﬁned by X exclusively. In other\nwords, H cannot provide any new information about Y , except for what is contained in X. Suppose H has access to Y ,\nthe classiﬁcation task becomes easy, but this is impossible\nsince Y is unobserved. This observation leads us to design a\nmodel where some knowledge of Y , denoted as Y ′, is used\nas additional information to improve the representation of\nH.\nTo this end, we progressively improve the representation\nof H as shown in Figure 1(B). Speciﬁcally, we employ a\nclassiﬁer C′ which takes H as input and produces the prediction Y ′. Here, Y ′ can be interpreted as an early prediction for the task, and it is likely to approximate the output\nY . Thus, these early predictions provide some information\nabout Y which can be used as additional information to H to\nextract a more expressive representation T. In this model architecture, a multilayer perceptron (MLP) is applied on both\nH and Y ′ to learn the representation T.\nAlthough employing early predictions have proved to be\nuseful in several tasks, previous works (He, Lee, and and‘\nDaniel Dahlmeier 2019; Zhao et al. 2019) merely pass Y ′\nand H into an MLP to extract T. A key observation from\nthese works show that Y ′ can indeed improve the representation of H. However, these approaches ignore the fact that\nY ′ may not necessarily be the ground truth, so not all the information contained in Y ′ may be beneﬁcial to the model\nperformance. Thus, we argue that it is necessary to control the information ﬂow of Y ′. Speciﬁcally, we construct\na stochastic map (SM) to model the mutual information between H and T conditioned on Y ′, denoted as I(H; T|Y ′).\nIn this way, we can control the information ﬂow of Y ′ by\ncontrolling I(H; T|Y ′) during optimization. A small value\nof I(H; T|Y ′) means T is largely determined by Y ′, while a\nlarge value means T is largely determined by H. The mutual\ninformation (MI) I(H; T|Y ′) is given by\nI(H; T|Y ′) =\nZ\ndh dt dy′ p(h, t, y′) log p(t|h, y′)\np(t|y′)\n≤\nZ\ndh dt dy′ p(h, t, y′) log p(t|h, y′)\nr(t|y′) ,\n(2)\nwhere r(t|y′) is a variational approximation to p(t|y′), inducing an upper bound on I(H; T|Y ′). Minimizing the upper bound of I(H; T|Y ′) is the same as minimizing the KLdivergence between p(t|h, y′) and r(t|y′).\nI(H; T|Y ′) ≤ KL(p(t|h, y′)||r(t|y′))\n(3)\nAs the KL-divergence approaches zero, p(t|h, y′) approximates r(t|y′). And in this case, t is determined by y′ to\na great extent. Thus, by controlling the KL-divergence between p(t|h, y′) and r(t|y′), we can control the injection of\ny′ into t.\nProgressive Classiﬁcation on Multitask Learning\nWe design a multitask learning architecture based on our\nproposed single-task learning method which considers the\ncontrol of information ﬂow. Figure 2 shows the architecture\nof our models.\nSuppose YA and YB are the output spaces of two different\nbut related tasks A and B. The two tasks have different outputs, but share the same input in our setting. Let H be the\nshared representation of both tasks modeled by BiLSTM.\nWe employ classiﬁers C′\nA and C′\nB corresponding to tasks\nX\nBiLSTM\nH\nC′\nA\nY ′\nA\nSMA\nTA\nˆYA\nCA\nI(H; TA|Y ′\nA)\nC′\nB\nY ′\nB\nSMB\nTB\nˆYB\nCB\nI(H; TB|Y ′\nB)\n(A) PMEI w/o interaction\nX\nBiLSTM\nH\nC′\nA\nY ′\nA\nSMA\nTA\nˆYA\nCA\nI(H; TA|Y ′\nA, Y ′\nB)\nC′\nB\nY ′\nB\nSMB\nTB\nˆYB\nCB\nI(H; TB|Y ′\nA, Y ′\nB)\n(B) PMEI\nFigure 2: Our models on the multi-task learning\nA and B to make early predictions Y ′\nA and Y ′\nB. Following\nthe approach proposed in the single-task learning method in\nFigure 1(C), we can control the injection of Y ′\nA (or Y ′\nB) in\nH to extract task-speciﬁc representations TA (or TB).\nIt is possible that correlations exist between the learning\ntasks in the multitask learning architecture. But the model\ndepicted in Figure 2(A) does not model interactions explicitly, but exploit only the implicit interactions in H. Without modeling such explicit interactions, as shown in these\nworks (Lan et al. 2017; He, Lee, and and‘ Daniel Dahlmeier\n2019; Zhao et al. 2019; Dankers et al. 2019; Liu et al.\n2019a), the multitask learning model cannot properly distinguish the relevant features for the individual tasks.\nAs a solution, we consider one observation: correlations exist between outputs of several tasks (He, Lee, and\nand‘ Daniel Dahlmeier 2019; Zhao et al. 2019). Assuming we have early predictions for multiple tasks, we can\nexploit these interactions to improve task-speciﬁc representations. Thus, a natural idea should be that the conditional MI term should be under the condition of both Y ′\nA\nand Y ′\nB, i.e. I(H; TA|Y ′\nA, Y ′\nB) and I(H; TB|Y ′\nA, Y ′\nB), so\nthat the model effectively exploits the interactions between\nthe two tasks. We therefore construct an upper bound over\nI(H; TA|Y ′\nA, Y ′\nB) as\nI(H; TA|Y ′\nA, Y ′\nB) ≤ KL(p(tA|h, y′\nA, y′\nB)||r(tA|y′\nA, y′\nB))\n(4)\nAn upper bound over I(H; TB|Y ′\nA, Y ′\nB) will follow the\nsame formulation as I(H; TA|Y ′\nA, Y ′\nB).\nOur Model for the Joint ER and RC Tasks\nIn this section, we demonstrate the application of the proposed multitask learning method in Figure 2(B) for the joint\nextraction of entities and relations.\nGiven a sentence s = {w1, w2, · · · , wn} consisting of\nn words, and a set of l pre-deﬁned relation types R =\n{ρ1, · · · , ρl}, the joint task aims to extract all relational facts\nin sentence s. In this paper, a relational triple is represented\nin the form\n\n\nei, ρ, ej\u000b\n, where ei, ej are entity words (i.e, entities written as a single word) or heads of multi-token entities corresponding to wi, wj ∈ s, and the relation ρ ∈ R.\nGiven a word pair (wi, wj), the goal is to predict the probability ˆy(i,j) that the relational triple\n\n\nwi, ρ, wj\u000b\nis factual.\nBesides, the entity recognition task which takes each word\nwi ∈ s and predicts a probability ˆyi over BIOES labels (Fu,\nLi, and Ma 2019) can be used to identify the head and\ntail words of multi-token entities for the extracted relational\ntriple.\nLearning a shared representation\nWhen addressing this\nproblem, we ﬁrst map the word sequence s to a set of vectors x = {x1, x2, . . . , xn}, where xi ∈ Rd is a word embedding (Pennington, Socher, and Manning 2014) with a dimension size of d. Denote X, a random variable corresponding\nto the initial vectors of sentence s. We construct a shared\nrepresentation H for the ER and RC tasks by means of a\nBiLSTM.\nLearning task-speciﬁc representations\nIn our model for\nthe joint task, the tasks A and B correspond to the ER and\nRC tasks. Let C′\ne and C′\nr be classiﬁcation models for the\nER and RC tasks which takes H as input and produce early\npredictions Y ′\ne and Y ′\nr. In fact, there are correlations between\nthe outputs of the ER and RC tasks. For example, the output\nof the ER task can provide information on whether the words\nwi, wj in a relational triple\n\n\nwi, ρ, wj\u000b\nare entity words or\nmulti-tokens. Moreover, it provides information on which\npairs of words to focus on in the RC task, since not all words\nin the sentence are entity words or involved in multi-token\nentities. Meanwhile, the relation r of the extracted relational\ntriple in the RC task provides information on the entity type\nfor wi, wj (typically when wi, wj are entity words).\nAs already mentioned, the correlations between the outputs of the ER and RC tasks, as well as its ability to increasingly improve predictions makes it necessary to exploit\nH, Y ′\ne and Y ′\nr for the extraction of task-speciﬁc representations Te and Tr. Employing stochastic maps for the respective tasks, we control the information ﬂow to Te and Tr\nby minimizing the mutual information I(H; Te|Y ′\ne, Y ′\nr) and\nI(H; Tr|Y ′\ne, Y ′\nr),\nI(H; Te|Y ′\ne, Y ′\nr) ≤ KL(p(te|h, y′\ne, y′\nr)||r(te|y′\ne, y′\nr))\n(5)\nI(H; Tr|Y ′\ne, Y ′\nr) ≤ KL(p(tr|h, y′\ne, y′\nr)||r(tr|y′\ne, y′\nr))\n(6)\nBoth (5) and (6) are solved similarly. We therefore focus\non how we solve (5). To ﬁnd solutions to the distributions\np(te|h, y′\ne, y′\nr) and r(te|y′\ne, y′\nr), we follow an approach proposed in (Alemi et al. 2017). Speciﬁcally, each of the distribution is modeled by two neural networks f µ(·) and f σ(·),\nwhich are respectively used to compute the mean µ and standard deviation σ of te. The representation te is then sampled\nfrom a Gaussian distribution N(µ, σ2). At this point, it is\nimportant to note that y′\ne ∈ Rn×5 is a probability distribution\nover BIEOS labels (Fu, Li, and Ma 2019), and y′\nr ∈ Rn×n×l\nis a probability distribution over the distinct relations. As a\nconsequence, the dimension of y′\ne and y′\nr are unequal, and\nthis must be considered when integrating both information\nin a neural network.\nNow for r(te|y′\ne, y′\nr), we employ two multilayer perceptrons (MLPs) f µ and f σ. Both take in as input a concatenation y′ = [y′\ne; f t(y′\nr)] to compute a mean and a standard\ndeviation for te. In this case, the function f t is a max pool\nfunction to transform y′\nr to the dimension space Rn×l to ease\nthe concatenation.\nWe now discuss how we model p(te|h, y′\ne, y′\nr). Here, instead of employing simple MLPs, we consider gated recurrent unit cells (GRUCells) to fully model the interactions between the two tasks. GRUCellµ is to model the mean of te,\nand GRUCellσ is to model the standard deviation of te. Both\nGRUCells have similar network structures. The operation of\na GRUCellµ is as follows:\nz = σ (Wz(h ⊕ y′))\nu = σ (Wu(h ⊕ y′))\nˇh = tanh (Wo((u ∗ h) ⊕ y′))\nµ = (1 − z) ∗ h + z ∗ ˇh\n(7)\nwhere y′ is the concatenation of f t(y′\nr) and y′\ne, ⊕ is a\nconcatenation operator, and Wz, Wu, Wo are learnable parameters.\nTask-speciﬁc classiﬁcation\nLet Ce and Cr be classiﬁcation models that take the respective inputs Te ∈ Te and Tr ∈\nTr, modeled by the mutual information I(H; Te|Y ′\ne, Y ′\nr) and\nI(H; Tr|Y ′\ne, Y ′\nr), and outputs the corresponding probability distributions Ce(Te) = p( ˆYe) ∈ p(Ye) and Cr(Tr) =\np( ˆYr) ∈ p(Yr). We can deﬁne the classiﬁcation model for\nthe ER task as the map\nCe : Te → p(Ye),\n(8)\nand the classiﬁcation model for the RC task as the map\nCr : Tr → p(Yr),\n(9)\nWe take Ce and Cr as neural networks with its own set\nof parameters. Now let te = {t1\ne, t2\ne, ..., tn\ne } be an instance\nof the random variable Te, and tr = {t1\nr, t2\nr, ..., tn\nr } be an\ninsstance of the random variable Tr. Ce takes as input the\nfeatures te and makes a prediction Ce(te) over BIEOS labels for each ti\ne ∈ te. Speciﬁcally, for the feature vector ti\ne\ncorresponding to the i-th word in the sentence, the probability distribution ˆyi ∈ Ce(te) is computed as follows:\nˆyi = softmax(Weti\ne + be),\n(10)\nwhere θE = {We, be} are trainable model parameters.\nHence the set of predictions Ce(te) = {ˆyi|ti\ne ∈ te}. Also,\nthe classiﬁcation model Cr takes as input the features tr and\nmakes a prediction Cr(tr) for each pair of feature vectors in\ntr. More speciﬁcally, given ti\nr, tj\nr ∈ tr, where ti\nr ̸= tj\nr, the\nprediction ˆy(i,j) ∈ Cr(tr) is deﬁned as follows:\nm = φ\n\u0000\nWm(ti\nr ⊕ tj\nr)\n\u0001\nˆy(i,j) = σ (Wrm + br)\n(11)\nwhere ⊕ is a concatenation operator, φ(·) is the ReLU\nactivation function, σ(·) is the sigmoid activation function.\nθR = {Wm, Wr, br} are learnable model parameters. Hence\nthe set of predictions Cr(tr) =: {ˆy(i,j)|ti\nr, tj\nr ∈ tr, ti\nr ̸= tj\nr}.\nTraining Objective\nThe ﬁnal training objective of our model is in three parts:\n(1) the supervision loss of Ce and Cr. (2) the supervision\nloss of C′\ne and C′\nr. (3) the loss produced by the MI terms\nI(H; Te|Y ′\ne, Y ′\nr) and I(H; Tr|Y ′\ne, Y ′\nr).\nThe supervision loss of Ce and Cr is computed as follows:\nLe(wi) = CrossEntropy (yi, ˆyi)\nLr(\n\n\nwi, ρ, wj\u000b\n) = CrossEntropy\n\u0000\ny(i,j), ˆy(i,j)\n\u0001\n(12)\nwhere yi and y(i,j) are the respective ground truth values of\nword w and relational triple\n\n\nwi, ρ, wj\u000b\n, and ˆyi and ˆy(i,j)\nare the predictions from the Ce and Cr.\nThe supervision loss L of Ce and Cr over all words and\nrelational triples for all sentences is then calculated as follows.\nL=\nX\ns\n\nX\nwi∈s\nLe(wi) +\nX\nwi,wj∈s,ρ∈R\nLr(\n\n\nwi, ρ, wj\u000b\n)\n\n\n(13)\nThe supervision loss L′ of C′\ne and C′\nr has a similar computation as L. The loss LICe of I(H; Te|Y ′\ne, Y ′\nr) is computed as\nLICe =\nX\ns\nKL[p(te|hs, y′\ne, y′\nr), r(te|y′\ne, y′\nr)]\n(14)\nThe loss of I(H; Tr|Y ′\ne, Y ′\nr) denoted as LICr has a similar\ncomputation as LICe. Hence the total loss is given by\nLtotal = βe LICe + βr LICr + α L′ + L\n(15)\nwhere βe, βr and α are positive parameters to control the\nweight of loss.",
        "experiment": "Datasets\nRecent works (Zeng, Zhang, and Liu 2020; Wei et al.\n2020) on the joint entity and relation extraction task\nmainly evaluate on NYT (Riedel, Yao, and McCallum\n2010), WebNLG (Gardent et al. 2017), NYT10 (Riedel,\nYao, and McCallum 2010) and NYT11 (Hoffmann et al.\n2011) datasets. We directly use the preprocessed NYT\nand WebNLG datasets released by (Zeng et al. 2018),\nand the preprocessed NYT10 and NYT11 datasets released\nby (Takanobu et al. 2019). Note that NYT and WebNLG\ndatasets mark only the tail word of an entity. We take a further step to tag entities with the conventional BIOES tagging\nscheme. Table 1 and 2 show the statistics of the datasets.\nDataset\nTrain\nDev\nTest\nNYT\n56195\n5000\n5000\nWebNLG\n5019\n500\n703\nNYT10\n70339\n4006\nNYT11\n62648\n369\nTable 1: Statistics of the datasets.\nNYT\nWebNLG\nDataset\nTrain\nTest\nTrain\nTest\nMulti-token entities\n39.1%\n38.9%\n64.2%\n63.8%\nSingle-token entities\n60.9%\n61.1%\n35.9%\n36.2%\nRelations\n24\n24\n170\n170\nTable 2: Percentages of multi-token entities and single-token\nentities, and the number of relations on NYT and WebNLG.\nEvaluation Protocols\nWe follow the evaluation protocols of previous works (Zeng,\nZhang, and Liu 2020; Wei et al. 2020) and report the Precision, Recall and micro-F1 performance of our models on\nthe datasets. Each reported result is the average performance\nover three runs using different random seeds. Best performance in boldfaced. Evaluation is performed on the partial\nmatch task and the exact match task. The partial match task\nrequires the relation, and the heads of both the entities in the\nextracted relational triple to be correct. The exact match task\nstrictly requires the relation, the head and tail of both entities\nin the extracted relational triple to be correct.\nImplementation Details\nWe initialize word embeddings with either Glove (Pennington, Socher, and Manning 2014) or BERT (Devlin et al.\n2019). BERT-based models directly use BERT embeddings\nas H. We use a batch size of 50 for Glove models, and a\nmini-batch of 6 for BERT models. We use Adam optimizer\nwith an initial learning rate 1e−3 for Glove models, and\n1e−5 for BERT models. We empirically ﬁne-tune the hyperparameters of the model on the development set. Since\nNYT10 and NYT11 have no development set, we randomly\nselect 10% of samples from the training set as the development set. Due to the space limit, we list the hyperparameter\nvalues used for all models on the datasets. We search the\nword embedding size in [100, 300], BiLSTM embeddings\nin [100, 200], dropout for word embeddings in [0.1, 0.2,\n... , 0.8], βe and βr in [e−3, e−4, ... , e−9], and α in [1.0,\ne−1, ... , e−6]. We implement our model using PyTorch on a\nLinux machine with a GPU device NVIDIA V100 NVLINK\n32GB. The code is available in our Github repository.1\nPerformance Comparison\nWe compare our models with recent works including the\nsequence-to-sequence (seq2seq) models such as OneDecoder (Zeng et al. 2018), MultiDecoder (Zeng et al. 2018),\nOrderRL (Zeng et al. 2019), and the sequence labeling models such as NovelTagging (Zheng et al. 2017b), ReHession (Liu et al. 2017), LSTM-CRF (Zheng et al. 2017a),\n1https://github.com/BDBC-KG-NLP/Progressive AAAI2021\nNYT\nWebNLG\nModel\nP\nR\nF1\nP\nR\nF1\nOneDecoder\n59.4\n53.1\n56.0\n32.2\n28.9\n30.5\nMultiDecoder\n61.0\n56.6\n58.7\n37.7\n36.4\n37.1\nOrderRL\n77.9\n67.2\n72.1\n63.3\n59.9\n61.6\nCASREL\n84.2\n83.0\n83.6\n86.9\n80.6\n83.7\nCASRELBERT\n89.7\n89.5\n89.6\n93.4\n90.1\n91.8\nMTL\n83.9\n83.1\n83.5\n84.9\n86.3\n85.6\nPMEI\n88.7\n86.8\n87.8\n88.7\n87.6\n88.1\nMTLBERT\n89.4\n89.9\n89.7\n89.4\n92.0\n90.7\nPMEIBERT\n90.5\n89.8\n90.1\n91.0\n92.9\n92.0\nNovelTagging\n62.4\n31.7\n42.0\n52.5\n19.3\n28.3\nGraphRel1p\n62.9\n57.3\n60.0\n42.3\n39.2\n40.7\nGraphRel2p\n63.9\n60.0\n61.9\n44.7\n41.1\n42.9\nCopyMTLOne\n72.7\n69.2\n70.9\n57.8\n60.1\n58.9\nCopyMTLMul\n75.7\n68.7\n72.0\n58.0\n54.9\n56.4\nMTL\n77.4\n76.4\n76.9\n76.7\n74.8\n75.7\nPMEI\n84.5\n84.0\n84.2\n78.8\n77.7\n78.2\nMTLBERT\n87.0\n88.7\n87.8\n80.9\n82.0\n81.4\nPMEIBERT\n88.4\n88.9\n88.7\n80.8\n82.8\n81.8\nTable 3: Results on NYT and WebNLG with partial match\n(top) and exact match (bottom).\nPA-LSTM-CRF (Dai et al. 2019), HRL (Takanobu et al.\n2019), CASREL (Wei et al. 2020), as well as the multitask\nlearning models such as SPTree (Miwa and Bansal 2016),\nGraphRel (Fu, Li, and Ma 2019), CopyMTL (Zeng, Zhang,\nand Liu 2020). As a baseline, we include a basic MTL model\n(MTL) which directly pass H into the classiﬁers Ce and\nCr for classiﬁcation. Table 3 shows the results on NYT and\nWebNLG datasets. Table 4 shows the results on NYT10 and\nNYT11.\nGlove embedding results\nWe compare our Glove models (MTL, PMEI) with recent works. Although MTL has\na simple architecture, it signiﬁcantly outperforms some recent seq2seq models including OrderRL, CopyMTL-one\nand CopyMTL-Mul. As noted in (Wei et al. 2020), seq2seq\narchitectures may not be ideal to address the joint task, especially for the overlapping relation problem. The low performance of the seq2seq models when compared to our\nMTL is consistent with the ﬁndings by (Wei et al. 2020).\nWe also ﬁnd that PMEI signiﬁcantly outperforms CASREL\non NYT, WebNLG, and even outperforms CASRELBERT\non NYT11, while showing competitive performance with\nCASRELBERT on NYT10. Additionally, we realize that the\nF1 performance of PMEI signiﬁcantly drops on the exact\nmatch task on WebNLG as compared to the partial match\ntask. Note that 60% of entities in WebNLG are multi-token\nentities, therefore the exact match task is particularly difﬁcult on this dataset.\nBERT embedding results\nWe see an improvement when\nusing BERT embeddings on the exact and partial match\ntasks, suggesting that incorporating prior knowledge induced by BERT in the joint ER and RC tasks is an effective\napproach. Comparing our BERT models with other recent\nBERT models, we ﬁnd that our model PMEIBERT outperNYT10\nNYT11\nModel\nP\nR\nF1\nP\nR\nF1\nMultiDecoder\n56.9\n45.2\n50.4\n34.7\n53.4\n42.1\nCASRELBERT\n77.7\n68.8\n73.0\n50.1\n58.4\n53.9\nMTL\n75.0\n65.9\n70.2\n51.9\n57.0\n54.3\nPMEI\n79.1\n67.2\n72.6\n56.0\n58.6\n57.2\nMTLBERT\n77.9\n69.9\n73.7\n54.3\n59.7\n56.9\nPMEIBERT\n79.1\n70.4\n74.5\n55.8\n59.7\n57.7\nNovelTagging\n59.3\n38.1\n46.4\n46.9\n48.9\n47.9\nMultiDecoder\n56.9\n45.2\n50.4\n34.7\n53.4\n42.1\nReHession\n41.2\n57.3\n48.0\nLSTM-CRF\n69.3\n31.0\n42.8\nSPTree\n49.2\n55.7\n52.2\n52.2\n54.1\n53.1\nPA-LSTMCRF\n49.4\n59.1\n53.8\nHRL\n71.4\n58.6\n64.4\n53.8\n53.8\n53.8\nMTL\n72.0\n59.0\n64.8\n50.7\n55.4\n53.0\nPMEI\n75.4\n65.8\n70.2\n55.3\n57.8\n56.5\nMTLBERT\n77.9\n67.8\n72.5\n55.1\n57.3\n56.2\nPMEIBERT\n77.3\n69.7\n73.3\n54.9\n58.9\n56.8\nTable 4: Results on NYT10 and NYT11 with partial match\n(top) and exact match (bottom).\nNYT\nNYT10\nModel\nP\nR\nF1\nP\nR\nF1\nMTLBERT\n89.4\n89.9\n89.7\n77.9\n69.9\n73.7\nPMEIBERT\n90.5\n89.8\n90.1\n79.1\n70.4\n74.5\nMTLBERT ∗\n73.6\n67.6\n70.4\n60.1\n46.2\n52.3\nPMEIBERT ∗\n80.3\n68.8\n74.1\n67.2\n48.4\n56.2\nTable 5: Results on NYT and NYT10 with partial match.\nModels with ﬁxed BERT parameters are marked “∗”.\nforms the CASRELBERT on NYT10 and NYT11, and show\ncompetitive performance with CASRELBERT on NYT and\nWebNLG.\nBias of BERT\nWe notice that the PMEI signiﬁcantly\noutperforms the MTL model, while the improvement of\nPMEIBERT over MTLBERT seems to be marginal especially on the NYT and NYT10 datasets. We believe that\nthe inductive bias brought by the pre-trained BERT explains these results. To verify this assumption, we conduct\nan experiment on MTLBERT and PMEIBERT on NYT and\nNYT10 datasets where we freeze the parameters of BERT\nduring training. Table 5 shows the results. We mark models\nwith frozen BERT parameters with the symbol “∗”.\nConsidering the F1 performance in Table 5, we ﬁnd that\nPMEIBERT surpasses MTLBERT only by 0.4% on NYT\nand 0.8% on NYT10. Meanwhile, PMEIBERT ∗ surpasses\nMTLBERT ∗ by a great margin, 3.7% on NYT and 3.9% on\nNYT10. The results suggest that when the pre-trained BERT\nis frozen, PMEIBERT ∗ has a better inductive bias relative to\nMTLBERT ∗ for the learning task. However, when further\ntuning is allowed in the pre-trained BERT, MTLBERT also\nhas an inductive bias appropriate for the task, thus the advantage of PMEIBERT over MTLBERT becomes less signiﬁcant.\nNYT\nWebNLG\nModel\nP\nR\nF1\nP\nR\nF1\nPMEI\n84.5\n84.0\n84.2\n78.8\n77.7\n78.2\nMTL\n77.4\n76.4\n76.9\n76.7\n74.8\n75.7\nMTLSM\n79.6\n76.0\n77.8\n77.5\n76.0\n76.7\nPMEI w/o SM\n83.5\n81.9\n82.7\n78.4\n75.4\n76.9\nPMEI w/o interaction\n79.7\n77.8\n78.7\n78.0\n77.5\n77.7\nPMEI w/o GRU\n83.2\n83.6\n83.4\n78.5\n76.7\n77.5\nTable 6: Performance of ablated model architectures on the\nexact match task.\nAblation Study\nWe perform ablation studies to note the importance of several model components. Ablated models include MTL: directly passes H into the classiﬁers Ce and Cr for classiﬁcation. MTLSM: directly passes H through two independent\nstochastic maps to extract task-speciﬁc representations Te\nand Tr. PMEI w/o SM: uses a deterministic map (i.e., GRUCell) instead of a stochastic map to extract Te and Tr. PMEI\nw/o interaction: disregards explicit interactions between the\ntasks. PMEI w/o GRU: in this model GRUCell is replaced\nby an MLP to model µ and σ of the encoder p(te|h, y′\ne, y′\nr).\nWe observe that PMEI w/o SM signiﬁcantly surpasses\nPMEI w/o interaction on NYT, suggesting that the interaction between tasks on the NYT plays a more important role\nthan the information control. On the other hand, PMEI w/o\ninteraction outperforms PMEI w/o SM on WebNLG. Given\nthe statistics shown in Table 2, it is easy to tell that the exact\nmatch task is indeed difﬁcult on WebNLG as compared to\nNYT. This implies that the quality of Y ′\ne and Y ′\nr in WebNLG\nis signiﬁcantly lower than NYT. Modeling explicit interactions with low quality predictions Y ′\ne and Y ′\nr will hurt performance, especially if we do not control the information ﬂow\nfrom Y ′\ne and Y ′\nr to the task-speciﬁc representations Te and\nTr. We can also see that PMEI w/o interaction outperforms\nMTLSM, suggesting the importance of leveraging early predictions to extract a better task-speciﬁc representation for\nclassiﬁcation. Lastly, we observe that PMEI w/o GRU underperforms PMEI, which suggests that employing the GRU\ncan bring about performance improvement.\nImpact of α, βe, and βr\nTo recall, α is a weight that controls the supervision loss\nof C′\ne and C′\nr, while βe and βr are weights to control the\ninformation ﬂow of Y ′\ne and Y ′\nr. A high value for α means\nC′\ne and C′\nr are likely to overﬁt during training. A high value\nfor βe (or βr) means we increase the ﬂow of Y ′\ne (or Y ′\nr)\nto update the task-speciﬁc representation Te (or Tr). In this\nexperiment, β = βe = βr.\nWe investigate the impact of α and β on the performance\nof PMEI. Note that Y ′\ne and Y ′\nr are predictions of the classiﬁers C′\ne and C′\nr. Hence the quality of these predictions\nis subject to the classiﬁers’ ﬁtness, and may inﬂuence our\nmodel’s performance. By controlling the supervision on C′\ne\nand C′\nr, we can control the ﬁtness. In Figure 3, the sub-ﬁgure\nto the left (Figure 3(A)) shows the performance of our model\nFigure 3: F1 performance curves of our model with different\nα (left) and β (right) values on the NYT dataset. β = βe =\nβr\non varying values of α, where βe and βr are ﬁxed. The subﬁgure to the right (Figure 3(B)) shows the performance of\nour model on varying values of β, where the value of α is\nﬁxed.\nWe ﬁnd that as we decrease the value of α or β, the performance of PMEI improves to a point, afterwards the performance deteriorates in a general sense. Speciﬁcally, in Figure 3(A), we ﬁnd that PMEI achieves the best performance\nat α = e−2 on partial match and α = e−3 on exact match\ntask for a ﬁxed β = e−7. Meanwhile, in Figure 3(B) PMEI\nachieves the best performance at β = e−7 on partial match\nand β = e−6 on exact match task for a ﬁxed α = e−2. In\nparticular, the results with varying values for β shows that it\nis important to control the ﬂow of early predictions.",
        "conclusion": "We build a multitask learning model for the joint entity and\nrelation extraction task. The core of our model is the way\nwe learn representations for the data to be classiﬁed, which\nis typically a core task in every supervised learning framework. In this paper, we acknowledge the correlations that exist between the outputs of the related tasks, and exploit these\ncorrelations through the interaction of early predictions in\nthe individual tasks. Previous works have considered this approach to improve representation learning, but they do so by\npassing these early predictions, as well as the input representation through a deterministic map. In our approach, we\nconsider a stochastic map as a natural way to capture the\ntask-speciﬁc representations. Meanwhile, we control the information ﬂow of early predictions to ensure that good taskspeciﬁc representations can be extracted for supervision. In\nthis way, we progressively make predictions on the individual tasks. Extensive experiments on several benchmark\ndatasets show the effectiveness of our approach.",
        "summary_en": "Multitask learning has shown promising performance in learning multiple related tasks simultaneously, and variants of model architectures have been proposed, especially for supervised classification problems. One goal of multitask learning is to extract a good representation that sufficiently captures the relevant part of the input about the output for each learning task. To achieve this objective, this paper designs a multitask learning architecture based on the observation that correlations exist between outputs of some related tasks (e.g. entity recognition and relation extraction tasks), and they reflect the relevant features that need to be extracted from the input. As outputs are unobserved, the proposed model exploits task predictions in lower layers of the neural model, also referred to as early predictions in this work. But the paper controls the injection of early predictions to ensure that good task-specific representations can be extracted for classification. The paper refers to this model as a Progressive Multitask learning model with Explicit Interactions (PMEI). Extensive experiments on multiple benchmark datasets produce state-of-the-art results on the joint entity and relation extraction task.",
        "summary_zh": "这篇论文介绍了一种名为具有显式交互的渐进多任务学习模型 (PMEI)，旨在实现多任务学习的一个目标，即提取一个良好的表征，充分捕捉每个学习任务中与输出相关的输入部分。作者设计了一种多任务学习架构，以联合实体和关系提取任务为例，其基础是观察到任务输出之间存在相关性，这些相关性反映了需要从输入中提取的相关特征。该模型利用了神经模型低层的任务预测，并控制信息流，来确保为分类提取出良好的任务特定表征。实验结果表明，在多个基准数据集上，该方法在联合实体和关系提取任务上取得了最先进的结果。"
    },
    {
        "title": "CSI: A Coarse Sense Inventory for 85% Word Sense Disambiguation",
        "abstract": "Word Sense Disambiguation (WSD) is the task of associating a word in context with one of its meanings. While many works in the past have focused on raising the state of the art, none has even come close to achieving an F-score in the 80% ballpark when using WordNet as its sense inventory. We contend that one of the main reasons for this failure is the excessively ﬁne granularity of this inventory, resulting in senses that are hard to differentiate between, even for an experienced human annotator. In this paper we cope with this long-standing problem by introducing Coarse Sense Inventory (CSI), obtained by linking WordNet concepts to a new set of 45 labels. The results show that the coarse granularity of CSI leads a WSD model to achieve 85.9% F1, while maintaining a high expressive power. Our set of labels also exhibits ease of use in tagging and a descriptiveness that other coarse inventories lack, as demonstrated in two annotation tasks which we performed. Moreover, a few-shot evaluation proves that the class-based nature of CSI allows the model to generalise over unseen or under-represented words.",
        "introduction": "Word Sense Disambiguation (WSD) is the task of assigning the correct meaning from among a ﬁnite set of possible\nchoices to a word in a context (Navigli 2009). It is a key task\nin Natural Language Processing (Navigli 2018), providing\nsemantic information that is potentially beneﬁcial for downstream applications, such as information extraction (Delli\nBovi, Espinosa Anke, and Navigli 2015) and machine translation (Pu et al. 2018). While much effort has been devoted\nto building new algorithms or data (Pasini and Navigli 2018;\nScarlini, Pasini, and Navigli 2019) for this task, state-ofthe-art systems have yet to break the 80% accuracy ceiling on standard WSD benchmark datasets (Raganato, Delli\nBovi, and Navigli 2017; Bevilacqua and Navigli 2019; Vial,\nLecouteux, and Schwab 2019; Scarlini, Pasini, and Navigli\n2020), showing that the WSD task is far from being solved.\nFollowing the literature in the ﬁeld (Hovy et al. 2006;\nPalmer, Dang, and Fellbaum 2007; Navigli, Litkowski, and\nHargraves 2007), we argue that the reason for this unsatisfactory performance does not lie solely in the complexity of the task but also in the ﬁne granularity of the sense\ninventory adopted, i.e., WordNet (Fellbaum 1998). For example the noun street has separate WordNet senses for the\n‘thoroughfare (usually including sidewalks)’ and ‘the part\nof a thoroughfare between the sidewalks’. Such ﬁne-grained\ndistinctions introduce noise and sparsity for machine learning algorithms in a task where reliable data is very costly\nto produce. Moreover, the inter-annotator agreement with\nWordNet ranges from 0.6 to 0.8 (Navigli 2009), making\nit clear that, unless super-human performance is expected,\nWSD systems will not exceed this ceiling. To overcome\nthese issues, in this paper we present Coarse Sense Inventory\n(CSI), a new organization of concepts based on a large-scale\nmapping of WordNet synsets to domain-based semantic labels. CSI labels are tailored to WSD, with each label shared\nacross different words and part-of-speech (POS) tags. The\ninventory has been developed starting from the categories\nof a general domain thesaurus, i.e., Roget’s (2011), which\nhave been clustered into coarser labels, leading to an inventory whose high-level semantics is domain-based (describing what each label is about) rather than hypernymy-based\n(what the label is a kind of). The experimental results provide evidence that CSI is better suited for WSD than other\nexisting competitors; moreover, CSI leads a supervised neural system to reach an F1 of almost 86% overall and helps the\nmodel to generalise over unseen or under-represented words.\nIn this work, we provide four main contributions:\n1. We introduce CSI, a new coarse-grained sense inventory\nwhere semantic labels are shared across the lexicon.\n2. Our new sense inventory achieves better qualitative results on two manual annotation tasks than its alternatives:\nour labels enable a higher inter-annotator agreement and\nare more descriptive than those of the competitors.\n3. CSI outperforms all the compared coarse-grained sense\ninventories on all-words WSD, attaining a better tradeoff between performance and expressiveness.\n4. CSI paves the way to few-shot learning in WSD, as it\nreaches better performances than its alternative inventories on unseen and under-represented words.",
        "related work": "A sense inventory enumerates the possible meanings that\ncontent words (nouns, verbs, adverbs, adjectives) may assume. Even though enumerative representations of lexical\nsemantics have been the object of some criticism (Kilgarriff 1997), the enumerative lexicon is still the most popular approach in WSD as it deﬁnes a possible ﬁnite ground\ntruth for word meanings. Indeed, WordNet is the de facto\nstandard sense inventory for WSD, with it being the largest\nmanually-crafted and freely available inventory, grouping\n155287 different lemmas (word form-POS pairs) in 117659\nconcepts called synsets, i.e., sets of synonyms (statistics for\nversion 3.0). The main criticism that is made against WordNet is that its ﬁne granularity and subtle distinctions between\nnearly identical senses make it hard to select the most suitable meaning of a given word, even for humans (Edmonds\nand Kilgarriff 2002). To overcome this problem, different\nsense inventories with coarser granularity have been developed. Following Izquierdo, Suarez, and Rigau (2015), we\ngroup inventories into two categories: i) word-based and ii)\nclass-based. We review these two groups in what follows.\nWord-based\nMany works in the past have proposed different approaches for solving the ﬁne-granularity problem\nwith coarser sense inventories created by clustering WordNet senses that are associated with the same lemma (Palmer,\nBabko-Malaya, and Dang 2004; Palmer, Dang, and Fellbaum 2007). Hovy et al. (2006) introduced the OntoNotes\nproject, whose objectives included the release of a manuallybuilt sense inventory. The resource was obtained by iteratively merging senses until 90% inter-annotator agreement\nwas reached, thus encouraging annotators to choose coarser\nsenses to meet the agreement goal. Differently, but pursuing the same objective, the work of Navigli (2006) coarsened the WordNet inventory by clustering and mapping its\nword senses to the Oxford Dictionary of English (ODE).\nThis work was later used as the starting point for introducing the task of coarse-grained all-words WSD in the context\nof SemEval-07 (Navigli, Litkowski, and Hargraves 2007).\nWith the same purpose of reducing the granularity of senses,\nSnow et al. (2007) proposed a supervised approach to predict whether two senses should be merged or not.\nClass-based\nOne of the drawbacks of word-based approaches is that their sense labels are still tied to words,\nthus leaving unsolved the problem of rare senses which\nhave none, or only few, occurrences in an annotated corpus. Class-based approaches, instead, cope with this issue\nby providing labels that are shared among different words,\nenabling a more efﬁcient usage of annotated data, and mitigating the problem of the long tail of infrequent word senses.\nOne of the earliest approaches treats WordNet’s lexicographer ﬁles as coarse classes, which we refer to as SuperSenses: each class includes synsets with the same part of\nspeech and a broad semantic type, like VERB.PERCEPTION.\nWhile each WordNet synset is associated with one label\nfrom a set of 45 available, the 4 labels used for adverbs and\nadjectives are not semantically meaningful, making the resource of limited usefulness for all-words WSD. For example, all the senses of the adjective bright are classiﬁed as\nADJ.ALL, making it impossible to distinguish the intelligent\nmeaning of the word from the light one. Izquierdo, Su´arez,\nand Rigau (2007), instead, exploited WordNet relations to\nautomatically extract a set of fundamental senses, called Basic Level Concepts, to which all the other senses are mapped.\nSimilarly, Vial, Lecouteux, and Schwab (2019) leveraged\nhypernymy to reduce the WordNet granularity, releasing a\n39K-label inventory used for ﬁne-grained WSD. Another\nWordNet-based resource is WordNet Domains (Magnini and\nCavagli`a 2000), a mapping from WordNet synsets to a set\nof 200 labels loosely following the Dewey Decimal Classiﬁcation system (Dewey 1876). The authors took a semisupervised approach where they manually annotated a moderate number of seed synsets and then propagated the labels by exploiting the WordNet structure. Along the lines of\nWordNet Domains, Camacho-Collados and Navigli (2017)\nintroduced BabelDomains, a set of 42 labels grouping in\ncoarser-grained classes the nominal synsets of BabelNet\n(Navigli and Ponzetto 2012), a multilingual knowledge base\ncomprising WordNet, Wikipedia and other resources. BabelDomains employs top-level categories from Wikipedia featured articles, thus making it able to cover a comprehensive\nset of knowledge domains.\nDifferently from the aforementioned class-based inventories, CSI has been manually created from scratch. Moreover,\nin contrast to BabelDomains – which covers only nouns –\nand SuperSenses and Basic Level Concepts – in which only\nnouns and verbs are meaningfully clustered – CSI covers\nall the content-word POS tags. Furthermore, our coarse inventory encompasses semantic areas that are excluded from\nother existing resources, inter alia, the ﬁve senses of perception and the areas of routines and daily activities.",
        "csi: a coarse sense inventory": "In this Section we present our novel class-based Coarse\nSense Inventory (CSI). The main objective of our approach\nis to build a resource that avoids sense distinctions that are\ntoo ﬁne-grained for WSD, while maintaining a granularity\nthat is still meaningful for the task. Our method (see Figure 1) consists of two steps: i) tagset deﬁnition, in which,\nby clustering Roget’s categories, we deﬁne a new coarsegrained sense inventory, and ii) synset mapping, where we\nmap WordNet synsets to one or more CSI labels.\nTagset Deﬁnition\nThe ﬁrst step aims at building a set of\ncoarse labels which covers the largest possible portion of\nthe semantic space. For this purpose we exploit Roget’s thesaurus, a widely-used resource in NLP which provides a\ncategorization for the lexicon of the English language. The\nthesaurus contains 1075 categories, grouped into 15 broader\nclasses, none of which offers the level of granularity we need\nfor the purpose of class-based WSD. In fact, categories under the same class may be either too similar, such as tribunal,\njury, lawyer, or too different, such as lawyer and learning.\nAt the same time, the 15 Roget’s classes are not speciﬁc\nFigure 1: An excerpt of the mapping between Roget’s categories, CSI labels and WordNet synsets.\nenough to be used as labels, since they describe broad domains such as Values and the ideals. For this reason we\nasked three expert linguists to group the Roget’s categories\ninto clusters which could serve as labels for our sense inventory. For each of the Roget’s classes the annotators identiﬁed\nthose categories representing semantically unrelated ﬁelds\nthat could potentially appear in different contexts (e.g., hair,\nsleep and color in the class the body and the senses). When\nthis was the case, they could either create one or more new\nclusters, or assign such categories to an existing cluster. Finally, once the taggers converged on a common set of clusters, they named each of them by considering the shared semantics of the categories and the possible application context they could appear in.\nFor example, the jurisdiction, tribunal, judge, jury and\nlawyer categories were merged into a single cluster named\nLAW&CRIME, while the fragrance and odor categories were\ngrouped together under the OLFACTORY label. Moreover,\nwe also added the semantically-empty label named GENERAL to our inventory to cover the 135 categories that\ncould not be included in any cluster. As a result, CSI\ncovers all the semantic areas expressed by the 1075 Roget’s categories. Note that some categories can belong to\nmore than one cluster, leading to partitions of the semantic space that are not disjoint. For example, the category religious buildings belongs both to the cluster named\nART,ARCHITECTURE&ARCHAEOLOGY and to the cluster\nRELIGION,MYSTICISM&MYTHOLOGY.\nSynset Mapping\nThe second step aims at mapping the\nWordNet synsets to one or more CSI labels. To this end,\nthe annotators iterated over each WordNet synset, associating it with one or more CSI labels, exploiting as context its\ngloss and its occurrences in the sentences of SemCor (Miller\net al. 1993), i.e., a manually-annotated corpus. The annotators mapped a sample of WordNet composed of the most frequent synsets occurring in SemCor, so as to guarantee a large\ncoverage of its instances. The outcome of this step is a mapping of 8217 synsets to one or more coarse labels, covering\n78% of the annotated instances in SemCor. For example, the\nsynsets {plebeian, pleb} and {Marxism} are associated with\nthe CSI label POLITICS,GOVERNMENT&NOBILITY, while\n{treaty, pact, accord} is labelled with both LAW&CRIME\nand POLITICS,GOVERNMENT&NOBILITY.\nTo further increase CSI coverage, we mapped each of\nthe labels of BabelDomains (Camacho-Collados and Navigli 2017) to one or more CSI tags. To ensure the consistency of the labels, an annotator manually validated all the\nCSI labels that did not have a one-to-one correspondence\nwith BabelDomains tags. This mapping guarantees an additional coverage of 78K WordNet synsets. As a result, a total\nof 83K synsets were annotated with at least one coarse label,\nencompassing all open-class parts of speech.",
        "experiments": "We now present a set of experiments aimed at assessing the\nquality of CSI under different perspectives: i) we designed\ntwo annotation tasks to evaluate the reliability (Section 4.1)\nand the descriptiveness (Section 4.2) of the labels in each\ncoarse-grained inventory, ii) we exploited the WSD task to\nevaluate and compare CSI with other class-based inventories, and iii) we tested CSI’s ability to enable zero- and fewshot learning, also in comparison with its alternatives.\nCompetitors\nAs competitors of CSI, we considered a\nword-based and ﬁne-grained inventory, i.e., WordNet, and\nthree class-based and coarse-grained inventories, i.e., BabelDomains (BD), WordNet Domains (WND) and SuperSenses\n(SuS). As regards the class-based ones, we recall from Section 2 that they provide a mapping from the WordNet synsets\nto one or more of their coarse labels. Among our comparisons we did not calculate the improvements brought by CSI\nwith respect to a random clustering, as proposed by Snow\net al. (2007), since we were mainly interested in evaluating\nthe use of our labels in coarse-grained WSD rather than the\nclustering itself, and because the proposed metric can only\nbe applied to disjoint clusters, which was not our case.\n4.1\nLabel Selection\nTo test whether the use of CSI can result in more reliable annotations, we designed a task where three expert\nlinguists - not involved in the creation of the mapping were asked to annotate 200 target words according to the\ninventories under comparison. For each coarse inventory\nwe deﬁned the set of possible labels for a given word as\nthe union of the labels associated with the word’s synsets.\nFor WordNet, we directly considered the glosses of the\nword’s meanings as labels, instead. Then, for each target word, we provided the annotators with a context sentence from SemCor, together with the set of possible labels for that word in each inventory. For example, we presented the following sentence to the annotators: “Madden\nsettled back to read the will” and they had to choose among\nthe CSI, WordNet, SuperSenses and WordNet Domains\nsenses of settled, that are, respectively, {SPACE&TOUCH,\nBUSINESS,ECONOMICS&FINANCE, LAW&CRIME}, {settle\ninto a position;bring to an end; ...; end a legal dispute},\n{VERB.CHANGE,\nVERB.COGNITION},\nand\n{POLITICS,\nFACTOTUM}.\nMeasures\nTo evaluate the inter-annotator agreement we\ncalculated the Kraemer’s κ coefﬁcient (Kraemer 1980). We\npreferred it to the better known Cohen’s κ (Cohen 1960), of\nwhich the former is an extension, since it allows the annotators to provide more than one answer for an item. To compute the Kraemer’s κ we ﬁrst represent the response Aj\ni of\neach annotator j for an item i as a vector rj\ni ∈ R|L|, where L\nis the set of possible labels. Within rj\ni , each dimension corresponds to one of the labels l1, . . . , l|L| ∈ L and can take\none of the following two values: the mean of 1, . . . , |Aj\ni| for\ndimensions corresponding to labels ∈ Aj\ni and the mean of\n|Aj\ni| + 1, . . . , |L| for the others. Formally:\nrj\ni [k] =\n⎧\n⎪\n⎪\n⎪\n⎪\n⎨\n⎪\n⎪\n⎪\n⎪\n⎩\n1\n|Aj\ni |\n|Aj\ni |\n\u0006\nn=1\nn\nif lk ∈ Aj\ni\n1\n|L|−|Aj\ni |\n|L|\n\u0006\nn=|Aj\ni |+1\nn\notherwise\nwhere k ∈ {1, . . . , |L|}. For example, supposing annotator\n2 gave the CSI response labels A2\ni = {BIOLOGY, CHEMISTRY&MINERALOGY} for item i, we calculate a value of\n1\n2\n\u00062\nn=1 n = 1.5 for the labels chosen by the annotator and\na value of\n1\n45\n\u000645\nn=3 n = 23 for all the others1. Assuming\nthat BIOLOGY and CHEMISTRY&MINERALOGY correspond\nto the indices 2 and 4, we build the following rank vector\nr2\ni = (23, 1.5, 23, 1.5, . . . , 23). Once each annotation Aj\ni\nhas its associated rank vector rj\ni , we can proceed to compute their correlation. To do so, we calculate the mean RI\nof Spearman’s correlations between all pairs of rank vectors\nrj\ni and rk\ni for each item i, i.e., RI = N −1 \u0006N\ni=1 ρ(r1\ni , r2\ni ),\nwhere N is the number of annotation items. RI acts as a\nmeasure of observed agreement, therefore we now need to\nquantify the agreement by chance. For this purpose we deﬁne U as the set comprising all the annotations and compute\nthe Spearman’s correlation average between all the annotation pairs in U × U as follows:\nRT =\n1\n|U|2\n\u0007\n(Ai,Aj)∈U×U\nρ(ri, rj)\nwhere Ai and Aj are two annotations in U, ri and rj are their\ncorresponding rank vectors and ρ(ri, rj) is their Spearman\ncorrelation. Finally, Kraemer’s κ is calculated as the ratio of\nthe difference between observed agreement (RI) and chance\nagreement (RT ), and the difference between perfect agreement and chance agreement: κ = (RI − RT )(1 − RT )−1.\nTo interpret κ values we followed Landis and Koch (1977),\nthat deﬁne the (0.4, 0.6] interval as moderate agreement,\n(0.6, 0.8] as substantial agreement and (0.8, 1.0) as almost\nperfect agreement. In computational linguistics, there is a\nconsensus that puts the cutoff above which the annotations\nare considered reliable at 0.67 (Di Eugenio and Glass 2004).\nResults\nIn Table 1 (ﬁrst row) we report the Kraemer’s κ\ncoefﬁcient attained with CSI, WordNet Domains (WND),\nSuperSenses and WordNet. As one can see, when the annotations are carried out with CSI, the annotators tend to agree\nmore than when using other coarse-grained or ﬁne-grained\n1We recall from Section 3 that CSI has 45 labels, i.e., |L| = 45.\nMeasure\nCSI\nWND\nSuperSenses\nWordNet\nIAA\n0.81\n0.74\n0.69\n0.51\nDescriptiveness\n2.23\n1.80\n2.04\nTable 1: Kraemer’s κ agreement (ﬁrst row); average descriptiveness for coarse inventories’ labels (second row).\nSentence\nThe street that is full now of trafﬁc and parked cars drowsed\non an August afternoon in the shade of the curbside trees,\nand silence was a weight [...].\nWord\nCSI\nSuperSenses\nWND\ncars\nTRANSPORT&TRAVEL\nN.ARTIFACT\nTOURISM\nshade\nPHYSICS&ASTRONOMY\nN.STATE\nFACTOTUM\nsilence\nMUSIC, SOUND&DANCING\nN.ATTRIBUTE\nACOUSTICS\nTable 2: An example of how target words from SemCor are\nannotated in CSI, SuperSenses and WordNet Domains.\nsense inventories. Indeed, the agreement achieved when using CSI falls in the almost perfect part of the spectrum of κ\nvalues according to the literature (Landis and Koch 1977),\nwhile, when using SuperSenses and WordNet Domains inventories, the agreement has to be considered substantially\nreliable. As expected, instead, due to their ﬁne granularity,\nWordNet senses allow only a moderate agreement, hence\nconﬁrming the results reported by Palmer, Dang, and Fellbaum (2007). These outcomes show that CSI labels provide\nuseful semantic information that make them easier to use\nthan the labels of the other sense inventories, hence simplifying the task of annotating large amounts of data.\n4.2\nDescriptiveness\nIn this Section we assess the extent to which CSI and the\nother coarse-grained inventories provide labels that are easy\nto understand for humans. Speciﬁcally, we are interested in\nstudying the degree of both pertinence and informativeness\nthat characterise each inventory when using it to tag a text.\nTo this end, we designed an annotation task for 150 words in\nwhich, given a target word in a sentence from SemCor and\nits gold label according to each of the coarse inventories, the\nthree annotators had to rank the labels in increasing order\nof descriptiveness for the given target word. For example, as\nshown in Table 2, we presented the annotators with the four\ntarget words street, cars, shade and silence, together with\ntheir corresponding sentence “the street that is full [ . . . ]”,\nand asked them to rank the three labels provided for each\nword by CSI, SuperSenses and WordNet Domains. As an\nannotation for a given target word, the linguists were asked\nto provide a score ranking ranging from 1 for the labels that\nwere less descriptive to 3 for those that were the most descriptive (ties were allowed).\nMeasures\nTo evaluate the descriptiveness of each inventory under comparison, we calculated the average rank of\nthe labels across all the 150 annotations. Formally,\ndescriptiveness(I) =\n1\n|N||J|\n\u0007\nj∈J\n\u0007\n(t,s)∈N\nrankj(l(t,s)\nI\n)\nCoverage\nSC instances\nSC synsets\nWN synsets\ntotal\n%\ntotal\n%\ntotal\n%\nCSI\n198K\n88.0\n16K\n61.7\n83K\n70.4\nSuperSenses\n226K\n100.0\n26K\n100.0\n118K\n100.0\nWND\n163K\n72.1\n18K\n69.5\n93K\n78.7\nIntersection\n153K\n67.5\n14K\n54.0\n79K\n67.1\nTable 3: Coverage of SemCor (SC) and WordNet (WN) by\nclass-based inventories.\nwhere J is the set of annotators, l(t,s)\nI\nis the label with which\nthe target word t in the sentence s is tagged according to\ninventory I, rankj(x) is the rank given by annotator j to x\nand N is the set of annotations to be carried out.\nResults\nWe now report the scores attained in the descriptiveness task, computed over the responses provided by the\nthree annotators. As can be seen in the second row of Table 1, CSI is the inventory with the highest scores, proving on average to be the one with the most descriptive labels, while those of WordNet Domains and SuperSenses are\nranked lower, meaning that they do not offer an adequate degree of characterization. In fact, considering the example in\nTable 2, the labels provided by both SuperSenses and WND\nare either inappropriate, i.e., the WND TOURISM label associated with car, or too general and hence not informative,\ni.e., the FACTOTUM label of WND for shade and the SuperSenses label ATTRIBUTE for silence. CSI, instead, provides a\nmore precise and detailed information on each word in bold\nin the example. In summary, not only did CSI prove to be\nthe inventory with the highest ease of use compared to its\ncompetitors (see Section 4.1), but it also exhibited a higher\ndegree of descriptiveness in its labels, which can increase\nthe readability of a text, i.e., making it easier for humans to\nunderstand it. It is reasonable to expect, in fact, that very descriptive labels, such as those provided in CSI, might also\nbe useful on their own, e.g., to improve the reading comprehension of a language learner.\n4.3\nAll-Words Word Sense Disambiguation\nIn this Section we compare CSI with the aforementioned\nsense inventories (see the beginning of Section 4), by using\nthem as labels for a WSD system.\nWSD Models\nTo evaluate the performances of our inventory across different learning models, we implemented two\nneural WSD systems. The ﬁrst model was similar to that\ndescribed in Vial, Lecouteux, and Schwab (2019), featuring two bidirectional LSTM layers, that encode the context\nof each token, and an attention mechanism. The two vectors (the attention and LSTM outputs) are concatenated and\nfed into a dense layer for classiﬁcation. As input features,\nwe tried two different pre-trained contextual embeddings,\ni.e., ELMo (Peters et al. 2018) and BERT base (Devlin et\nal. 2019). The second model, instead, had a simpler architecture in which BERT large contextualized embeddings are\nfed directly to a fully-connected layer for classiﬁcation.\nData\nFor training, developing and testing we used the\nWSD evaluation framework made available by Raganato,\nCamacho-Collados, and Navigli (2017). It includes SemCor,\nwhich we used as training set, and the 5 standard all-words\nWSD benchmarks for English, i.e., Senseval-2 (Palmer et\nal. 2001), Senseval-3 (Snyder and Palmer 2004), SemEval07 (Pradhan et al. 2007), SemEval-13 (Navigli, Jurgens, and\nVannella 2013), SemEval-15 (Moro and Navigli 2015). We\nwill use ALL to refer to the concatenation of all the foregoing benchmark datasets except SemEval-07, which, following Raganato, Camacho-Collados, and Navigli (2017), we\nused as development set.\nIn order to evaluate each sense inventory we replaced the\nWordNet sense keys appearing in the training set and in all\ntest sets with their coarse labels in the sense inventory under assessment. Whenever a sense key in the training set occurred that was associated with multiple labels, we replaced\nit with a random tag taken from among the mapped ones.\nFor the evaluation, we considered the predicted label to be\ncorrect if it was included in the set of all the possible labels\nfor the instance. Finally, to set a level playing ﬁeld among all\nthe sense inventories under comparison, we considered only\nthe training and testing instances that were in common, i.e.,\nall those tagged with a synset that was covered by each of\nthe inventories. Therefore, to consider the intersection, we\nrestricted the training data to 153K out of 226K instances,\nas shown in the last row of Table 3, where we also report\nthe coverage of each inventory with respect to the synsets of\nWordNet and to those that appear in SemCor.\nEvaluation Measure\nAs evaluation measure for performance we used F1. However, we note that each inventory\nmakes the task either more or less difﬁcult as there are signiﬁcant differences in the number of labels and in the way\nthe synsets are grouped. Therefore, to estimate the difﬁculty\nof the disambiguation task according to the sense inventory,\nwe computed the perplexity of a random guessing model as\nthe inverse of the probability of choosing the correct answer,\nby randomly sampling one label from those possible for the\ngiven item: PPL =\n1\nm\n\u0006m\ni=1\nLli\nGi , where Lli is the number\nof all the labels that are associated with the lemma li in a\ngiven sense inventory, Gi is the number of correct answers\nfor the instance i and m is the number of instances in the\ndataset. Since the difﬁculty of the task (PPL) and the performance of the model (F1) are inversely correlated, we compute a Geometric Trade-Off (GTO) measure, which is the\nbest choice when averaging values with different magnitude\n(see, e.g., Komninos and Manandhar (2016)).\nHyperparameters\nEvery model variant freezes the embedding layer’s weights during training, and the input to the\nnetwork is fed in batches of 64. In the ﬁrst model, the output of the two Bi-LSTMs layers is set to 512. When using\nELMo, the sentences longer than 30 words are truncated. As\noptimizer we used Adam (Kingma and Ba 2015) with learning rate 10−3 and 10−4 for ELMo and BERT, respectively.\nF1\nPPL\nGTO\nInventory\nSE-2\nSE-3\nSE-07\nSE-13\nSE-15\nALL\nSE-2\nSE-3\nSE-07\nSE-13\nSE-15\nALL\nSE-2\nSE-3\nSE-07\nSE-13\nSE-15\nALL\nELMo\n+\nLSTM\nCSI\n83.5\n81.7\n79.9\n81.9\n77.9\n81.7\n2.62\n3.13\n3.71\n2.28\n2.93\n2.70\n1.48\n1.6\n1.72\n1.37\n1.51\n1.49\nWND\n89.8\n86.5\n91.7\n80.6\n85.0\n85.5\n2.00\n2.33\n2.25\n2.12\n2.01\n2.13\n1.34\n1.42\n1.44\n1.31\n1.31\n1.35\nSuS\n82.3\n78.9\n81.5\n79.8\n80.2\n80.3\n2.25\n2.69\n2.98\n2.15\n2.26\n2.34\n1.36\n1.46\n1.56\n1.31\n1.34\n1.37\nBERT\n+\nLSTM\nCSI\n84.8\n83.4\n75.7\n80.3\n76.8\n81.9\n2.62\n3.13\n3.71\n2.28\n2.93\n2.70\n1.49\n1.62\n1.67\n1.35\n1.50\n1.49\nWND\n87.4\n85.3\n89.1\n82.1\n81.0\n84.4\n2.00\n2.33\n2.25\n2.12\n2.01\n2.13\n1.32\n1.41\n1.42\n1.32\n1.28\n1.34\nSuS\n81.5\n79.1\n79.4\n79.0\n79.6\n79.8\n2.25\n2.69\n2.98\n2.15\n2.26\n2.34\n1.36\n1.46\n1.54\n1.30\n1.34\n1.37\nBERT\n+\nDense\nCSI\n86.0\n84.5\n75.5\n83.3\n79.3\n83.8\n2.62\n3.13\n3.71\n2.28\n2.93\n2.70\n1.50\n1.63\n1.67\n1.38\n1.53\n1.51\nWND\n91.2\n87.9\n89.4\n83.7\n84.7\n87.2\n2.00\n2.33\n2.25\n2.12\n2.01\n2.13\n1.35\n1.43\n1.42\n1.33\n1.30\n1.36\nSuS\n83.2\n81.4\n79.9\n80.5\n82.3\n81.8\n2.25\n2.69\n2.98\n2.15\n2.26\n2.34\n1.37\n1.48\n1.54\n1.32\n1.36\n1.38\nTable 4: Comparison of CSI against WordNet Domains (WND) and SuperSenses (SuS) on all-words WSD tasks from past\nSenseval and SemEval competitions.\nCSI vs. BabelDomains\nModel\nF1\nPerplexity\nGTO\nCSI\nBD\nCSI\nBD\nCSI\nBD\nELMo + Bi-LSTM\n86.9\n86.9\n1.92\n1.95\n1.29\n1.30\nBERT + Bi-LSTM\n89.4\n87.1\n1.92\n1.95\n1.31\n1.30\nBERT + Dense\n91.1\n89.1\n1.92\n1.95\n1.32\n1.32\nTable 5: Comparison of CSI against BabelDomains (BD), on\nall-words WSD. Results are shown for the ALL dataset.\nResults\nWe now report the results of the comparison between CSI and its competitors. For each sense inventory\nwe computed its performance in terms of F1, the perplexity of random guessing with the considered inventory and\ntheir geometric mean. We remark that a higher perplexity\nindicates a higher uncertainty of random guessing, i.e., the\nsense inventory associates on average a higher number of\npossible labels with a given lemma, thus making the disambiguation task harder. Since CSI extends BabelDomains, we\nﬁrst report their comparison in Table 5. As can be seen, the\ntwo inventories reach a GTO score that is almost the same\nacross the WSD models. However, BabelDomains is inherently limited for WSD tasks as it only covers nouns, while\nCSI covers all the open-class parts of speech without losing anything in terms of performance compared to BabelDomains. Since CSI proved to be on a par with BabelDomains while, at the same time, having a wider coverage of\nPOS tags, in what follows we only report the results for CSI.\nWe now move to compare our inventory with the other classbased approaches, i.e., WordNet Domains and SuperSenses.\nAs shown in Table 4, CSI consistently attains better GTO\nscores than the other inventories, proving its better balance\nbetween label granularity and expressiveness, regardless of\nthe underlying neural model. More in detail, we note that,\nwhile WordNet Domains achieves higher F1 scores across\ndatasets, except for SemEval-13 with the ELMo + Bi-LSTM\nmodel, its perplexity is always lower, meaning that the disambiguation task becomes easier due to the lower expressiveness of the inventory. Indeed, more than 18% of the\nWordNet synsets are mapped to the semantically-empty label of WND, i.e., FACTOTUM. CSI, in contrast, resorts to\nGENERAL for less than 1% of the annotated synsets.\nDifferently\nfrom\nWordNet\nDomains,\nSuperSenses\nachieves F1 scores that are lower than CSI, except for\nSemEval-07 and SemEval-15. Moreover, it shows a lower\nperplexity overall, proving to be a less expressive inventory.\nIn fact, it provides only 4 possible classes in total for\nadjectives and adverbs, thus making the task of disambiguation undemanding on these POS tags. Since BERT +\nDense attained overall better results than ELMo and BERT\n+ Bi-LSTM, in what follows we report its performance\nonly. Finally, to better analyse the impact that the two\nsemantically-empty labels of CSI and WordNet Domains\nhave on the results, we compared the precision, recall and F1\nobtained when excluding GENERAL (CSI) and FACTOTUM\n(WND) from the valid answers. As shown in Table 7, CSI\nobtains a higher precision and recall, thus conﬁrming our\nhunch that FACTOTUM highly impacts WordNet Domains\nperformance. In fact, most testing instances were tagged\nwith FACTOTUM by the model and when it was excluded\nfrom the valid answers, this made the recall drop.\nWe note that, when taking full advantage of the CSI labels and let BERT + Dense train on all SemCor instances\ncovered by CSI, we report an 85.9 F1 score on ALL. This,\ntogether with the results of the qualitative analysis (Section\n4.2), highlights that CSI is the most viable candidate to replace or complement ﬁne-grained inventories for WSD.\n4.4\nZero- and Few-Shot Learning\nTo investigate the improvement that CSI can bring to the disambiguation of unseen or under-represented words, we performed a zero- and few-shot learning experiment. We randomly sampled a set of words, and trained the WSD model\nremoving the annotations for those words. Then, we tested\nthe ability of the class-based sense inventories to leverage\nlabels from other words when zero or only few annotated\nexamples for a word are provided.\nExperimental Setup\nWe deﬁne L as the set of all lemmas\nappearing in SemCor and the evaluation datasets. From L\nwe sample a set Lout of 100 words, that we partition in two\ndisjoint subsets, Ltest and Ldev, of size 70 and 30 respectively. We deﬁne DW as the subset of dataset D which contains only instances for the lemmas in a set W. For example,\nSemCorL is the unmodiﬁed training corpus, and Senseval2Ldev is the dataset containing all the instances in Senseval2 for lemmas in Ldev. Starting from SemCor, we build the\ntraining set SemCorL\\Lout, i.e., containing as instances all\nlemmas not in Lout, which we used for training the BERT\n+ Dense WSD model with the hyperparameters deﬁned in\nInventory\nF1\nPPL\nGTO\nMFS\nT0\nT3\nT5\nT0\nT3\nT5\nT0\nT3\nT5\nCSI\n69.0 ± 2 × 10−4\n68.6 ± 8 × 10−5\n77.8 ± 7 × 10−4\n4.88\n1.54\n1.85\n1.84\n1.03\n1.20\n72.1\nWND\n64.3 ± 1 × 10−3\n75.1 ± 2 × 10−4\n76.2 ± 3 × 10−5\n4.41\n1.39\n1.57\n1.68\n1.02\n1.09\n74.9\nSuS\n62.6 ± 3 × 10−4\n67.8 ± 1 × 10−3\n73.0 ± 2 × 10−3\n4.07\n1.51\n1.73\n1.60\n1.01\n1.12\n68.7\nTable 6: Comparison of CSI against WordNet Domains (WND) and SuperSenses (SuS) on zero- and few-shot settings.\nCSI vs. WordNetDomains\nDataset\nPrecision\nRecall\nF1\nCSI\nWND\nCSI\nWND\nCSI\nWND\nSenseval-2\n96.2\n95.5\n86.8\n85.3\n91.2\n90.1\nSenseval-3\n97.0\n91.2\n85.0\n78.6\n90.6\n84.4\nSemEval-07\n96.8\n87.2\n73.4\n65.4\n83.5\n74.7\nSemEval-13\n96.7\n93.9\n84.0\n79.0\n89.9\n85.8\nSemEval-15\n96.8\n96.2\n75.8\n66.7\n85.0\n78.8\nALL\n96.6\n94.0\n84.0\n79.1\n89.9\n85.9\nTable 7: Comparison of CSI against WordNet Domains\n(WND) when discarding semantically-empty predictions.\nSection 4.3; we will refer to this model as M0. To perform\nthe tuning and evaluation of the models described below, we\nuse Senseval-2Ldev as dev set and ALLLtest as test set.\nZero- and Few-Shot Setting\nWe evaluated the performance of M0 on ALLLtest without any further training. Note\nthat the lemmas in Ltest had never been seen tagged during\ntraining, so we called this the zero-shot setting. To evaluate the inventory and the model on the few-shot learning\ntask, instead, we built the training datasets T3, T5 by randomly sampling 3, 5 examples, respectively, for each lemma\nin Ltest, such that T3 ⊂ T5. For each Ti we trained a separate model Mi that we tuned on Senseval-2Ldev as done for\nM0. Finally, we initialized the weights from M0 and backpropagated the gradients only through the dense layer.\nEvaluation Measure\nThe experiment aims at proving\nthat, even if the training set contains only a few tagged examples for a word, the model can still beneﬁt from the classbased nature of CSI in classifying the under-represented\nwords. Therefore, we measure the performance of the models in terms of both F1 and GTO. Each inventory is compared against its most frequent sense (MFS) baseline, which,\ngiven a target word w, is deﬁned as the class that is most frequently used to tag w in SemCor. Since we sample the test\nlemmas, Ltest, at random, we report the average of the results obtained on three random word samples L1\ntest, L2\ntest\nand L3\ntest on their respective instances in the ALL dataset,\ni.e., ALLL1\ntest, ALLL2\ntest and ALLL3\ntest.\nResults\nIn Table 6 we compare CSI, WordNet Domains\nand SuperSenses with their MFS baselines. While all the\nsense inventories manage to beat their MFS, CSI is the one\nthat surpasses it with the greatest gap, i.e., 5.7 F1 points\ncompared to the 1.3 and 4.3 for WordNet Domains and SuperSenses, respectively. This proves that CSI allows the network to better exploit the semantic information carried by\nthe words within the training set, hence enabling a model to\ngeneralise well over under-represented words and mitigating\nthe need for large amounts of annotated data for WSD. This\nis further conﬁrmed when considering the GTO scores in\nTable 6, where CSI reaches the best performance across the\nboard. Therefore, not only does CSI lead the WSD model to\nattain higher results than its competitors as regards the MFS,\nbut it also provides - in this setting as well - a better balance\nbetween polysemy and performances of the model.",
        "conclusion": "In this paper we presented CSI, a new sense inventory for\ncoarse-grained WSD. Our labels proved to be of higher quality than those of alternative inventories, as they exhibited a\ndescriptiveness that was not matched by any of the other inventories, hence making the text annotated with CSI labels\nof easier interpretation for humans. Moreover, we showed\nthat CSI enabled annotators to attain a higher agreement\ncompared to other ﬁne- and coarse-grained inventories that\nare employed for the task. On the quantitative side, we\nshowed that CSI allows a supervised WSD model to achieve\nthe most competitive trade-off between performance and expressiveness, and to attain almost 86 F1 points overall when\nnot restricting the set of training instances to those also covered by other inventories as well. In addition, we showed\nthat, when using CSI labels, a supervised model can better\ngeneralise over rare words, i.e., those that never or seldom\nappear in the training data. In fact, in the few-shot learning task, our inventory was the one that led the underlying model to achieve the highest increment over the MFS\nwhen just ﬁve training examples were provided for the tested\nwords. Foreseeing the potential beneﬁts that CSI can bring\nto coarse-grained WSD, we release to the community the\nfull inventory, covering more than 120K unique words in\nthe English vocabulary, together with its mapping to WordNet synsets and the code to reproduce the experiments at\nhttp://lcl.uniroma1.it/csi.",
        "summary_en": "Word Sense Disambiguation (WSD) is the task of associating a word in context with one of its meanings. While many works in the past have focused on raising the state of the art, none has even come close to achieving an F-score in the 80% ballpark when using WordNet as its sense inventory. This paper contends that one of the main reasons for this failure is the excessively fine granularity of this inventory, resulting in senses that are hard to differentiate between, even for an experienced human annotator. Therefore,the paper copes with this long-standing problem by introducing Coarse Sense Inventory (CSI), obtained by linking WordNet concepts to a new set of 45 labels. The results show that the coarse granularity of CSI leads a WSD model to achieve 85.9% F1, while maintaining a high expressive power. The paper's set of labels also exhibits ease of use in tagging and a descriptiveness that other coarse inventories lack, as demonstrated in two annotation tasks which the paper performed. Moreover, a few-shot evaluation proves that the class-based nature of CSI allows the model to generalise over unseen or under-represented words.",
        "summary_zh": "这篇论文介绍了一种粗义词表（CSI），用于词义消歧任务（WSD）。作者认为当前使用WordNet作为词义库的细粒度问题导致了WSD任务的性能瓶颈，因此提出了CSI，将WordNet概念映射到一组新的45个标签上。实验结果表明，CSI的粗粒度使得WSD模型能够达到85.9%的F1得分，同时保持了较高的表达能力。作者还证明了CSI标签的易用性和描述性，并通过两个标注任务进行了验证。此外，少样本评估显示，CSI的基于类别的特性使模型能够泛化到未见或少见的词汇上。"
    },
    {
        "title": "SensEmBERT: Context-Enhanced Sense Embeddings for Multilingual Word Sense Disambiguation",
        "abstract": "Contextual representations of words derived by neural language models have proven to effectively encode the subtle distinctions that might occur between different meanings of the same word. However, these representations are not tied to a semantic network, hence they leave the word meanings implicit and thereby neglect the information that can be derived from the knowledge base itself. In this paper, we propose SENSEMBERT, a knowledge-based approach that brings together the expressive power of language modelling and the vast amount of knowledge contained in a semantic network to produce high-quality latent semantic representations of word meanings in multiple languages. Our vectors lie in a space comparable with that of contextualized word embeddings, thus allowing a word occurrence to be easily linked to its meaning by applying a simple nearest neighbour approach. We show that, whilst not relying on manual semantic annotations, SENSEMBERT is able to either achieve or surpass state-of-the-art results attained by most of the supervised neural approaches on the English Word Sense Disambiguation task. When scaling to other languages, our representations prove to be equally effective as their English counterpart and outperform the existing state of the art on all the Word Sense Disambiguation multilingual datasets. The embeddings are released in ﬁve different languages at http://sensembert.org.",
        "introduction": "Word Sense Disambiguation (WSD) is the task of associating the occurrence of a word in a text with its correct meaning from a predeﬁned inventory of senses (Navigli 2009).\nOver the years, two distinct lines of research have been developed to tackle this problem: supervised and knowledgebased WSD. On the one hand, supervised models rely on\nsemantically-annotated corpora for training (Raganato, Delli\nBovi, and Navigli 2017; Kumar et al. 2019; Bevilacqua\nand Navigli 2019), while, on the other hand, knowledgebased systems employ graph-based algorithms on semantic networks to ﬁnd the set of meanings that better disambiguate the input words (Moro, Raganato, and Navigli 2014;\nAgirre, de Lacalle, and Soroa 2014). Even though supervised approaches have proved to achieve better performance,\nthey have difﬁculty scaling to different languages due to the\npaucity of multilingual sense-annotated data. Knowledgebased approaches, instead, are more ﬂexible and can be applied to different languages, however at the cost of achieving\nlower performance than their supervised counterpart.\nRecently, language models in different sauces, i.e., ELMo\n(Peters et al. 2018), BERT (Devlin et al. 2019), XLNET\n(Yang et al. 2019), etc., have attracted much interest as they\nhave proved to be beneﬁcial to several downstream tasks in\nNLP (Wang et al. 2018; 2019). In fact, the word representations provided by these models encode several pieces of linguistic information and, differently from static word embeddings (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), they depend directly on the context a word is\nsurrounded by. This has made these vectors especially interesting for the task of WSD, where effective contextual representations can be highly beneﬁcial for solving lexical ambiguity. In fact, thanks to contextual embeddings, simple nearest neighbor algorithms have proved to be more effective and\nprecise than complex supervised and knowledge-based approaches (Loureiro and Jorge 2019). These representations\nallowed sense-annotated corpora to be exploited in a more\nefﬁcient way, as demonstrated by Loureiro and Jorge (2019).\nIn fact, they closed the gap between the meanings in a semantic network and their occurrences in a text by producing\nconcept embeddings that lie in a space that is comparable\nwith that of contextual word embeddings. Nevertheless, this\napproach is still hampered by the need for manual semantic\nannotations in order to construct the concept vectors, which\nlimits its range of action to texts in English only, as almost\nno manually-annotated data are available in other languages.\nIn this paper we present SENSEMBERT, a knowledgebased approach for producing sense embeddings in multiple\nlanguages. We leverage the lexical-semantic information in\na knowledge base, i.e., BabelNet, and an encyclopedic resource like Wikipedia, to relieve the burden of producing\nmanually-tagged corpora. SENSEMBERT, whilst not relying on annotated data, achieves state-of-the-art results on the\nmultilingual WSD tasks and remains competitive with the\nbest supervised models on English. Moreover, when providing supervision to our approach, our embeddings set a new\nstate of the art on all the English WSD test sets for nouns.",
        "related work": "At the core of Natural Language Processing lies the problem\nof Word Sense Disambiguation (WSD), which addresses the\nambiguity of words in a given context. WSD is usually tackled by exploiting two sources of knowledge: semantic networks and sense-annotated corpora. Semantic networks encode a more general knowledge that is not tied to a speciﬁc task and the information enclosed therein is usually\nemployed for WSD by knowledge-based approaches. Senseannotated corpora, instead, are tailored to the WSD task and\nare typically used as training sets for supervised systems.\nKnowledge-based systems\nKnowledge-based approaches\n(Moro, Raganato, and Navigli 2014; Agirre, de Lacalle, and\nSoroa 2014) frame WSD as a graph-based problem, where\nthe structure of a semantic network, such as WordNet (Fellbaum 1998) and BabelNet (Navigli and Ponzetto 2012), is\nused to ﬁnd, for each input word, its correct meaning according to its context. WordNet is the most widespread lexical\nknowledge base, but it is limited to the English lexicon only,\nwhich restricts its applicability to other vocabularies. BabelNet copes with this problem by merging together lexicalsemantic information in multiple languages coming from\ndifferent resources, hence enabling knowledge-based approaches to scale over all the languages it supports. Despite\ntheir ability to scale over different languages, knowledgebased approaches fall behind supervised systems on English\nin terms of accuracy.\nSupervised systems\nSupervised systems have attained\nstate-of-the-art results across all English datasets by exploiting either SVM models (Iacobacci, Pilehvar, and Navigli\n2016), or neural architectures (Melamud, Goldberger, and\nDagan 2016; Raganato, Delli Bovi, and Navigli 2017;\nVial, Lecouteux, and Schwab 2019). Nevertheless, they\nsuffer from the knowledge acquisition bottleneck, which\nhampers the creation of large manually-curated corpora\n(Gale, Church, and Yarowsky 1992), and in turn hinders the ability of these approaches to scale over unseen\nwords and new languages. To overcome the aforementioned shortcomings, coarser sense inventories (Lacerra\net al. 2020) and automatic data augmentation approaches\n(Pasini and Navigli 2017; Pasini, Elia, and Navigli 2018;\nScarlini, Pasini, and Navigli 2019) have been developed to\ncover more words, senses and languages. At the same time,\ndedicated architectures have been built to exploit the deﬁnitional information of a knowledge base (Luo et al. 2018;\nKumar et al. 2019).\nRecently, contextual representations of words (Peters et\nal. 2018; Devlin et al. 2019) have brought a breeze of change\nto WSD, where they have been employed for the creation of\nsense embeddings (Peters et al. 2018; Loureiro and Jorge\n2019). These proved to be of high-quality inasmuch as they\nwere able to surpass complex state-of-the-art models on the\nEnglish WSD tasks when coupled with simple distancebased algorithms, i.e., k-NN. Nevertheless, these approaches\nrely on sense-annotated corpora to gather contextual information for each sense, and hence are limited to languages\nfor which gold annotations are available, i.e., English.\nIn this paper, we present SENSEMBERT which, in dispensing with the need for human-annotated corpora, unleashes the power of sense embeddings to virtually all BabelNet’s languages. By leveraging the mapping between\nsenses and Wikipedia pages, the relations among BabelNet synsets and the expressiveness of contextualized embeddings, we get rid of manual annotations while at the same\ntime providing valuable contexts for the creation of our embeddings for all the nominal senses in BabelNet.",
        "preliminaries": "SENSEMBERT relies on different resources for building\nsense vectors: Wikipedia, a multilingual knowledge base,\ni.e., BabelNet (Navigli and Ponzetto 2012), the NASARI\nlexical vectors (Camacho-Collados, Pilehvar, and Navigli\n2016) and a pre-trained language model for producing contextual representations, i.e., BERT (Devlin et al. 2019).\nWikipedia\nis the largest electronic encyclopedia freely\navailable on the Web, including approximately 300 separate\neditions, each written in a different language. The information contained in Wikipedia is organized into articles, which\nare also referred to as Wikipedia pages. Each page aims at\ndescribing either abstract concepts, e.g., FREEDOM, or real\nworld entities, e.g., MARTIN LUTHER KING.\nBabelNet1\n(Navigli and Ponzetto 2012) is a multilingual semantic network which comprises information coming\nfrom heterogeneous resources, such as WordNet, Wikipedia,\netc. It is organized into synsets, i.e., sets of synonyms that\nexpress a single concept, which, in their turn, are connected to each other by different types of relation. We note\nthat a synset clusters together several senses, each identiﬁed by one of the synset’s lexicalizations. Moreover, thanks\nto cross-lingual mappings, the synsets in BabelNet conﬂate\nlexicalizations coming from distinct languages. For example, the terms glass (English), verre (French), vaso (Spanish), bicchiere (Italian), etc., are grouped together under the\nsame synset expressing the container meaning of glass.\nFor our purposes we are especially interested in the following kinds of information contained in BabelNet:\n• Hypernym and hyponym edges: each concept2 is connected to other concepts by means of hypernym-hyponym\nrelations. For example, the concept computer1\nn (A machine for performing calculations automatically)3 is connected, inter alia, to the concept machine1\nn (Any mechanical or electrical device) via a hypernym relation (i.e.,\ngeneralization), and to home computer1\nn (a computer intended for use in the home) via a hyponym relation (specialization).\n1https://babelnet.org\n2We use synset and concept interchangeably for ease of reading.\n3We use the notation of Navigli (2009), where lk\np denotes the\nk-th meaning of the lemma l with pos p according to WordNet.\n• Semantically-related edges: BabelNet also comprises\nedges expressing a general notion of relatedness between\nconcepts. For example, computer1\nn is connected, among\nothers, to mouse4\nn (a hand-operated electronic device) and\nto keyboard1\nn (device consisting of a set of keys).\n• Mappings to Wikipedia: most of the concepts in the\nknowledge base are linked to one or more Wikipedia\npages. For example, the concept for computer1\nn is linked\nto the Wikipedia page COMPUTER4.\nNASARI lexical vectors5\n(Camacho-Collados, Pilehvar,\nand Navigli 2016) provide explicit representations of BabelNet concepts by means of sparse lexical vectors. Each dimension is a word scored by its lexical speciﬁcity (Lafon\n1980) with respect to the concept it is representing. The lexical speciﬁcity value is computed from the Wikipedia pages\nrelated to the target concept. We note that the words enclosed\nwithin each sense vector are those that most characterize it.\nFor example, the vector for the animal sense of mouse includes, among others, the words rat, rodent, animal, cat, etc.\nBERT6\n(Devlin et al. 2019) is a Transformer-based language model for learning contextual representations of\nwords in a text. Recently, BERT ushered a new era for\nNLP. In fact, its contextual embeddings made it possible\nto achieve high performance in different NLP tasks, such\nas question answering and sentiment classiﬁcation. In this\nwork we take advantage of the BERT large and multilingual7\nmodels for English and the other languages, respectively.",
        "sensembert": "In this Section we present SENSEMBERT, a novel\nknowledge-based approach for creating latent representations of senses in multiple languages. It computes contextaware representations of BabelNet senses by combining the\nsemantic and textual information that can be derived from\nmultilingual resources, i.e., NASARI (Camacho-Collados,\nPilehvar, and Navigli 2016) and BabelNet (Navigli and\nPonzetto 2012), with the representational power of neural\nlanguage models, i.e., BERT (Devlin et al. 2019). Our approach can be divided into the following three steps:\n• Context Retrieval, which collects all the relevant textual\ninformation from Wikipedia for a given concept in the semantic network (Section 4.1).\n• Word Embedding, which, given the contexts retrieved in\nthe previous step, computes the vector representation of\neach relevant word of the target synset (Section 4.2).\n• Sense Embedding, which merges the contextual information computed in the previous step and enriches it with\nadditional knowledge from the semantic network, so as to\nbuild an embedding for the target sense (Section 4.3).\n4https://en.wikipedia.org/wiki/Computer\n5http://lcl.uniroma1.it/nasari/\n6https://github.com/google-research/bert\n7The training of multilingual BERT was performed on the texts\ncoming from Wikipedia in 104 different languages.\n4.1\nContext Retrieval\nIn this step we aim at retrieving all the contexts from\nWikipedia that are suitable for characterizing a given word\nsynset. To this end, similarly to Camacho-Collados, Pilehvar, and Navigli (2016), we exploit the mappings between\nsynsets and Wikipedia pages available in BabelNet, as well\nas its taxonomic structure, to collect textual information that\nis relevant to a target synset s.\nFirst, starting from a synset s, we collect the set of its\nmost related concepts, i.e., all the synsets that are connected\nto s through either a hypernym, hyponym or semanticallyrelated edge (see Section 3). More formally, being s the target synset, we deﬁne its set of related synsets Rs as follows:\nRs = {s′ | (s, s′) ∈ E}\nwhere E is the set of hypernym, hyponym and semanticallyrelated edges in BabelNet.\nIn order to make this set as reliable as possible and reduce\nthe possible error due to the automatic nature of BabelNet,\nwe further reﬁne the set Rs by retaining just those synsets\nthat are strongly related to the target one, i.e., s. To do so, we\nemploy the information coming from the Wikipedia pages\nassociated with s, i.e., ps, and each synset s′ in Rs, i.e., ps′.\nFor each page pi we compute its lexical vector as described\nin Camacho-Collados, Pilehvar, and Navigli (2016), where\nwords correspond to dimensions and are scored by their lexical speciﬁcity value. These lexical representations are then\nemployed to score the similarity between ps and ps′ for each\ns′ ∈ Rs by means of the Weighted Overlap (WO) measure\n(Pilehvar, Jurgens, and Navigli 2013). WO determines the\nsimilarity between two input pages p1 and p2 as follows:\nWO(p1, p2) =\n\u0002 \u0003\nw∈O\n1\nrp1\nw + rp2\nw\n\u0004 ⎛\n⎝\n|O|\n\u0003\ni=1\n1\n2i\n⎞\n⎠\n−1\nwhere O is the set of overlapping dimensions of p1 and p2\nand rpi\nw is the rank of the word w in the lexical vector of pi.\nWe preferred the weighted overlap over the more common\ncosine similarity as it has proven to perform better when\ncomparing sparse vector representations (Pilehvar, Jurgens,\nand Navigli 2013).\nOnce we have scored all the (ps, ps′) pairs, we create\nthree partitions of Rs, each comprising all the senses s′ connected to s with the same relation r, where r can be one\namong: hypernymy, hyponymy and semantic relatedness.\nWe then retain from each partition only the top-k scored\nsenses8 s′\n1, . . . , s′\nk according to WO(ps, ps′\ni). We further reﬁne each ﬁltered partition by solving the conﬂicts that might\narise when a synset s′ is related not only to s but also to\nanother concept s′′ that shares a lexicalization with s. This\nis needed since s and s′′ represent two different meanings\nof the same word and thus require distinct contexts to better\ncharacterize and distinguish them. Therefore, we remove s′\nfrom the set Rs if WO(ps, ps′) < WO(ps′′, ps′), or otherwise from the set Rs′′. Finally, for each synset s, we compute the Bag of Contexts BoCs comprising all the sentences\nof the pages associated with a sense in Rs.\n8We use k = 10 for all our experiments.\nFigure 1: Exempliﬁcation of the sense embedding’s creation\nfor the device sense of mouse.\n4.2\nWord Embedding\nThe aim of the second step is to compute, by means of\nBERT, the representations of those words in the sentences\nof BoCs that best deﬁne the target synset s.\nFirst, we deﬁne as relevant words for a target synset those\nthat appear in its NASARI lexical vector. Formally, given a\nsynset s and its NASARI vector Ns, we deﬁne Ws as the set\ncontaining all the non-zero words in Ns. Then, we use Ws to\nﬁlter out from BoCs all those sentences which do not contain any of the words therein and compute the embedding\nof each word w ∈ Ws by averaging the BERT representations of all its occurrences in BoCs. That is, given a word\nw ∈ Ws, we compute its vector vw as follows:\nvw =\n\t\nc∈BoCw\ns\nBERT(c, w)\n|BoCw\ns |\nwhere BoCw\ns is the set of contexts in BoCs where w appears\nand BERT(c, w) is the vector computed by BERT for the\nword w in the context c.\nAt the end of this step, all the words in Ws for a given\nsynset s are associated with their latent representations,\nwhich depend on the contexts in BoCs where they occur.\n4.3\nSense Embedding\nIn this ﬁnal step, we build a unique representation of each\ntarget synset in the knowledge base. Therefore, to prioritize\nthe information that best characterizes a target synset s, we\nweight each word in Ws, and hence its corresponding embedding, according to its rank in the NASARI vector of s.\nWe then build the ﬁnal synset representation by combining\nthe word vectors we computed previously. Formally, given\na target synset s, the set of its relevant words w1, . . . , wn ∈\nWs and their corresponding vectors vw1, . . . , vwn, we compute the synset embedding of s as follows:\nvs =\n\t\nwi∈Ws\nrank(wi)−1 vwi\n\t\nwi∈Ws\nrank(wi)−1\nwhere rank(wi) is the ranking of the word wi according to\nits score in the NASARI vector of s. After a vector for each\nsynset s has been computed, we still lack a representation at\nthe sense level, i.e., speciﬁc to each lemma of the synset9.\nTherefore, to further enrich and specialize our embeddings\nat the sense level, we follow Loureiro and Jorge (2019)\nand leverage the gloss and each of the synset’s lemmas. In\nmore detail, i) for each synset s, we enhance its gloss by\nprepending to it all the lemmas in s; ii) we differentiate the\nsynset gloss for each sense in s by repeating its lemma at\nthe beginning of the gloss; iii) we compute the sense gloss\nembedding, i.e., the representation of the gloss at the sense\nlevel, as the average of the BERT contextual embeddings\nof the tokens of our enhanced gloss. Lastly, the ﬁnal\nrepresentation of the target sense is given by concatenating\nthe embedding of the synset it belongs to, i.e., vs, with the\nsense gloss embedding we just computed.\nAt the end of the three steps outlined in Sections 4.1-4.3,\neach sense in the knowledge base is associated with a vector\nwhich encodes both its contextual and deﬁnitional semantics coming from the contexts extracted from Wikipedia and\nits gloss, respectively. In Figure 1 we exemplify the procedure for creating the embedding of the device sense of the\nlemma mouse. As one can see, we compute the representation of the target sense by combining the information we extracted for its corresponding synset, i.e., {mouse, computer\nmouse}, from Wikipedia (left side of the ﬁgure) and BabelNet (right side of the ﬁgure). As for the ﬁrst component of\nthe vector, i.e., context, we retrieve all the sentences in the\nWikipedia pages collected for the {mouse, computer mouse}\nsynset (step 1, Section 4.1). Then we compute, by means of\nBERT, the embeddings of the relevant words, i.e., device,\npointer, etc. (underlined in ﬁgure), and average them (step\n2, Section 4.2). As for the second component of the vector, i.e., sense gloss, we consider the gloss of the {mouse,\ncomputer mouse} synset and prepend to it all of its lemmas, i.e., mouse, computer mouse (upper right side of the\nﬁgure). Then we specialize it for the device sense of mouse\nby adding the lemma mouse at the beginning of the text and\naverage the BERT representations of the tokens therein (step\n3, Section 4.3). Thus, the ﬁnal vector for the device sense of\nmouse is the concatenation of the context and sense gloss\nvectors.",
        "experimental setup": "In this Section we report the settings in which we conducted\nthe evaluation of SENSEMBERT when testing it on the English and multilingual WSD tasks. In what follows we introduce the test sets, the system setup along with the reference\nWSD model, a supervised version of our approach and the\ncomparison systems.\nEvaluation Benchmarks\nAs for English, we carried\nout the evaluation on the test sets in the English WSD\nframework in Raganato, Camacho-Collados, and Navigli (2017)10. This includes ﬁve standardized evaluation\nbenchmarks from the past Senseval-SemEval competitions,\n9We recall from Section 3 that a synset contains several senses,\neach associated with a lemma.\n10http://lcl.uniroma1.it/wsdeval/\ni.e., Senseval-2 (Edmonds and Cotton 2001), Senseval-3\n(Snyder and Palmer 2004), SemEval-07 (Pradhan et al.\n2007), SemEval-13 (Navigli, Jurgens, and Vannella 2013),\nSemEval-15 (Moro and Navigli 2015), together with ALL,\nthe concatenation of the ﬁve test sets. As for the multilingual setting, we conducted the experiments on the SemEval13 (Navigli, Jurgens, and Vannella 2013) and SemEval-15\n(Moro and Navigli 2015) multilingual WSD tasks.\nIn all test sets we considered just nouns, as the NASARI\nlexical vectors are currently available for nominal synsets\nonly. All performances are reported in terms of F1-measure,\ni.e., the harmonic mean of precision and recall.\nSENSEMBERT Setup\nWe employed two BERT pretrained models: the English 1024-dimensional and the multilingual 768-dimensional pre-trained cased models for the\nEnglish and multilingual settings, respectively. Among all\nthe conﬁgurations reported by Devlin et al. (2019), we used\nthe sum of the last four hidden layers as contextual embeddings of the words. Moreover, BERT exploits WordPiece tokenization, that is, a token can be further split into several\nsubtokens, e.g., the term “embed” is broken down into two\nsubtokens, namely “em” and “##bed”. Thus, the contextual\nembedding of an input word was computed as the average of\nits subtoken embeddings.\nWSD Model\nWe used a 1-nearest neighbour approach to\ntest SENSEMBERT on the WSD task. For each target word\nw in the test set we computed its contextual embedding by\nmeans of BERT and compared it against the embeddings of\nSENSEMBERT associated with the senses of w. Hence, we\ntook as prediction for the target word the sense corresponding to its nearest neighbour. We note that the embeddings\nproduced by SENSEMBERT are created by concatenating\ntwo BERT representations, i.e., context and sense gloss (see\nSection 4.3), hence we repeated the BERT embedding of the\ntarget instance to match the number of dimensions.\nIn contrast to most supervised systems, this approach does\nnot rely on the Most Frequent Sense (MFS) backoff strategy,\ni.e., predicting the most frequent sense of a lemma in WordNet for instances unseen at training time, as SENSEMBERT\nensures full coverage for the English nominal senses.\nSupervised SENSEMBERT\nIn order to set a level playing\nﬁeld with supervised systems on English, we built a supervised version of SENSEMBERT, i.e., SENSEMBERTsup.\nThis version combined the gloss and contextual information (Section 4.3) with the sense-annotated contexts in SemCor (Miller et al. 1993), a corpus of 40K sentences where\nwords have been manually annotated with a WordNet meaning. We leveraged SemCor for building a representation of\neach sense therein. To this end, we followed Peters et al.\n(2018) and, given a word-sense pair (w, s), we collected\nall the sentences c1, . . . , cn where w appears tagged with s.\nThen, we fed all the retrieved sentences into BERT and extracted the embeddings BERT(c1, w), . . . , BERT(cn, w).\nThe ﬁnal embedding of s was built by concatenating the average of its context and sense gloss vectors (Figure 1) and\nits representation coming from SemCor, i.e., the average of\nBERT(c1, w), . . . , BERT(cn, w). We note that, when a\nsense did not appear in SemCor, we built its embedding by\nreplacing the SemCor part of the vector with its sense gloss\nrepresentation.\nComparison\nSystems\nWe\ncompared\nSENSEMBERT\nagainst the best performing supervised and knowledgebased systems evaluated on the WSD framework for English\nnouns. Among knowledge-based approaches, we took into\naccount the extension of Lesk comprising word embeddings\n(Basile, Caputo, and Semeraro 2014, Leskext+emb),the extended version of UKB with gloss relations (Agirre, de\nLacalle, and Soroa 2014, UKBgloss) and Babelfy (Moro,\nRaganato, and Navigli 2014). As for supervised systems\nwe considered an SVM-based classiﬁer integrated with\nword embeddings (Iacobacci, Pilehvar, and Navigli 2016,\nIMS+emb), the Bi-LSTM with attention and multi-task objective presented in Raganato, Delli Bovi, and Navigli, BiLSTM (2017), and the more recent supervised systems\nleveraging sense deﬁnitions, i.e., HCAN (Luo et al. 2018)\nand EWISE (Kumar et al. 2019). We also performed a comparison with the two LSTM-based architectures of Yuan\net al. (2016, LSTM-LP) and context2vec (Melamud, Goldberger, and Dagan 2016) for learning representations of the\nannotated sentences in the training corpus. Finally, we applied the 1-NN strategy to two other supervised approaches\nfor creating sense embeddings, namely Peters et al.’s method\n(2018) using BERT (BERT k-NN) and LMMS (Loureiro\nand Jorge 2019). All supervised systems, apart from BERT\nk-NN and LMMS11, were trained using SemCor and, with\nthe exception of HCAN, EWISE and LMMS, rely on the\nMFS backoff strategy unless otherwise stated.\nAs regards the multilingual setting, we took into account OneSeC (Scarlini, Pasini, and Navigli 2019), the best\nautomatically-tagged corpus available for non-English languages. Therefore, we compared the embeddings produced\nby SENSEMBERT with the state-of-the-art Bi-LSTM-based\nsupervised model trained on OneSeC presented by Scarlini, Pasini, and Navigli (2019, Bi-LSTMOneSeC). Moreover, we also created a multilingual version of LMMS by\nreplicating their approach on the data provided by OneSeC\n(LMMSOneSeC) and tested it on all the target languages.",
        "results": "In this Section we report the results of the evaluation on the\nWSD task. We ﬁrst show the ablation study we carried out\nto assess the contribution brought by each part of SENSEMBERT’s embeddings. We then demonstrate the effectiveness\nof SENSEMBERT by comparing its results on all standard\nWSD benchmarks with the existing state of the art.\n6.1\nAblation Study\nFirst, we show an ablation study of SENSEMBERT and\nSENSEMBERTsup on the English ALL test set in Raganato,\n11We note that, as for SENSEMBERTsup, BERT k-NN and\nLMMS exploit SemCor only to retrieve the sense-annotated contexts that are to be fed to a neural language model.\nModel\nSenseval-2\nSenseval-3\nSemEval-07\nSemEval-13\nSemEval-15\nALL\nKB\nMFS\n72.1\n72.0\n65.4\n63.0\n66.3\n67.6\nLeskext+emb (2014)\n74.6\n72.7\n66.0\n66.2\n67.8\n69.8\nUKBgloss (2014)\n70.6\n58.4\n56.6\n59.0\n62.3\n62.1\nBabelfy (2014)\n74.0\n66.7\n61.0\n66.4\n69.9\n68.6\nSup\nIMS+emb (2016)\n79.0\n74.6\n71.1\n65.9\n72.1\n71.9\nBi-LSTM (2017)\n78.6\n72.7\n71.1\n66.4\n73.3\n71.6\nHCAN (2018)\n78.3\n73.2\n70.9\n68.5\n73.8\n72.6\nEWISEConvE (2019)\n69.4\n74.0\nSupcontext\ncontext2vec (2016)\n78.0\n73.1\n66.7\n65.6\n71.6\n71.0\nLSTM-LP (2016)\n79.6\n76.3\n71.7\n69.5\n72.8\nBERT k-NN (2019)\n71.7\n73.0\n72.9\n65.6\n68.4\n69.3\nBERT k-NN + MFS (2019)\n81.4\n76.3\n73.6\n71.8\n74.0\n75.5\nLMMS (2019)\n81.7\n78.7\n78.0\n75.1\n78.2\n78.0\nOurs\nSENSEMBERT\n80.6\n70.3\n73.6\n74.8\n80.2\n75.9\nSENSEMBERTsup\n83.7\n79.7\n79.9\n78.7\n80.2\n80.4\nTable 1: Comparison in terms of F1 on the nominal instances of the English WSD test sets in Raganato, Camacho-Collados,\nand Navigli (2017). Approaches are grouped by type: i) knowledge-based systems (KB), ii) supervised models for classiﬁcation\n(Sup), iii) supervised models for learning contextual representations of senses (Supcontext), iv) ours (Ours).\nModel\nF1\nGloss\n63.9\nSEBc\n74.5\nSEBc ⊕ Gloss\n75.3\nSEBc | Gloss\n75.9\nTable 2: Ablation study of SENSEMBERT’s components on\nthe nouns of the ALL test set in terms of F1.\nCamacho-Collados, and Navigli (2017) to see how each of\ntheir components inﬂuences the ﬁnal results.\nAs one can see from Table 2, the sense gloss embeddings\npart alone (Gloss) scores only 63.9 F1 points, meaning that\nthe information encoded therein is not sufﬁcient for providing a clear distinction between senses. However, we show\nthat it is beneﬁcial to SENSEMBERT when used to further\nenrich and specialize its contextual representations. In fact,\nthe sense gloss embeddings and the context-enhanced part\nof SENSEMBERT (SEBc) prove to be complementary by\nachieving the best F1 score of 75.9 when concatenated together (SEBc | Gloss)12, increasing the results over the performance of SEBc alone by 1.4 points.\nSimilarly, in Table 3 we show how the contexts extracted\nwith SENSEMBERT are also valuable when combined with\nthe vectors extracted from SemCor. In fact, simply concatenating the two representations (SEBc | SemCor) leads to an\nincrement of 9.9 points over the performance of SemCor\nalone and 1.2 points over the concatenation of SemCor with\nthe glosses. Moreover, we gain an additional boost in performance when averaging the sense gloss embedding with the\ncontext part, by achieving 80.4 F1 in our best conﬁguration,\ni.e., (SEBc ⊕ Gloss) | SemCor. Therefore, in what follows\nwe report the results of SENSEMBERT, i.e., SEBc | Gloss\n(see Table 2), and SENSEMBERTsup, i.e., (SEBc ⊕ Gloss)\n| SemCor (see Table 3).\n12We use ⊕ to represent the average of two vectors and | for their\nconcatenation.\nModel\nF1\nSemCor\n69.3\nSemCor | Gloss\n78.0\nSEBc | SemCor\n79.2\n(SEBc ⊕ SemCor) | Gloss\n79.6\n(SEBc ⊕ Gloss) | SemCor\n80.4\nTable 3: Ablation study of SENSEMBERTsup’s components\non the nouns of the ALL test set in terms of F1.\n6.2\nEnglish WSD\nWe now proceed to testing SENSEMBERT on the ﬁnegrained English tasks. In Table 1 we report the results of\nSENSEMBERT and SENSEMBERTsup and compare them\nagainst the results attained by other knowledge-based and\nsupervised state-of-the-art approaches on all the nominal\ninstances of the test sets in the framework of Raganato,\nCamacho-Collados, and Navigli (2017).\nAs one can see, SENSEMBERT achieves the best results on ALL when compared to other knowledge-based approaches. These results raise the bar for knowledge-based\nWSD by improving the existing state of the art by 6.1\nF1 points and indicating that SENSEMBERT is competitive with supervised models as well. In fact, SENSEMBERT ranks second only to LMMS and outperforms all\nother supervised systems, which, in contrast, rely on senseannotated data and dedicated WSD architectures. Moreover,\nwe show that we are able to surpass, and hence improve, the\nexisting state of the art by including supervision, i.e., SemCor, in our approach. In fact, SENSEMBERTsup proves to\nbe the best system across the board outperforming its competitors on all datasets with an increment of 2.1 points overall compared to LMMS, which also uses SemCor.\n6.3\nWSD on Rare Words and Senses\nTo investigate further the beneﬁts brought by SENSEMBERT, we carried out the evaluation on only those instances\nof the test sets which are associated with a rare word or\nModel\nALLLF S\nALLLF W\nLMMS\n66.7\n76.3\nSENSEMBERT\n76.9\n78.1\nSENSEMBERTsup\n70.4\n81.1\nTable 4: Comparison in terms of F1 on nouns and nominal\nsenses in the ALL dataset not occurring in SemCor.\nsense. Therefore, starting from the ALL dataset we created\ntwo additional test sets: i) ALLLF S (Least Frequent Senses),\ncontaining the 812 instances in ALL associated with a gold\nsense not occurring in SemCor; ii) ALLLF W (Least Frequent Words), containing the 528 instances in ALL for a\nnon-monosemous lemma not occurring in SemCor.\nIn Table 4 we report the performance of SENSEMBERT,\nSENSEMBERTsup and LMMS on the two newly-created\ntest sets. SENSEMBERT outperforms its direct competitor\non both datasets, providing a signiﬁcant gain of 10.2 F1\npoints on ALLLF S. This implies that our approach is better\nable to generalize over both words and senses as it can provide diversiﬁed contexts, while not being tied to a speciﬁc\nsense-annotated corpus. LMMS, instead, performs more\npoorly when it comes to predicting rare instances. In fact,\neven if it provides full coverage of the senses in WordNet, it\ncomputes the representations of the senses not in SemCor by\naveraging the embeddings of the senses therein. Hence, it is\nbiased towards those representations for which sense annotations are provided. In contrast, SENSEMBERTsup demonstrates that the contexts extracted from Wikipedia aid better\ngeneralization over rare words and senses also when they are\ncoupled with the information from SemCor, thus allowing it\nto outperform LMMS on the ALLLF S datasets by 3.7 F1\npoints and to achieve the best result on ALLLF W .\n6.4\nMultilingual WSD\nWe now test the ability of our approach to build vectors that\nare also effective for languages other than English. We recall\nfrom Section 4 that our method covers all the 104 languages\nin both BabelNet, BERT and Wikipedia.\nIn Table 5 we report the results attained by SENSEMBERT in the multilingual WSD tasks of SemEval-13 and\nSemEval-15. We compare our approach with the existing\nstate of the art (Bi-LSTMOneSeC) and the embeddings\nobtained by replicating the LMMS approach on OneSeC’s\nsilver data (LMMSOneSeC). While our approach also\nproves its consistency on languages other than English,\nLMMS loses ground when no manually-curated corpora\nare available for the target language. In fact, when bootstrapped from a fully-automatic resource such as OneSeC,\nthe performance of LMMS drops heavily on most of the\ntested languages, since it is not bulletproof when it comes\nto the unavoidable noise that can be found in silver data.\nIn contrast, not only does SENSEMBERT beat its direct\ncompetitor (LMMSOneSeC) on all test sets by on average\n6.6 points, it also sets a new state of the art on all the\nlanguages by performing 2.5 points above the best model\noverall.\nModel\nSemEval-13\nSemEval-15\nIT\nES\nFR\nDE\nIT\nES\nBi-LSTMOneSeC\n68.2\n72.0\n74.8\n75.1\n62.5\n62.8\nLMMSOneSeC\n64.6\n67.8\n69.0\n77.1\n62.8\n49.4\nSENSEMBERT\n69.6\n74.6\n78.0\n78.0\n66.0\n64.1\nTable 5: Comparison in terms of F1 on the SemEval-2013\nand SemEval-2015 multilingual WSD tasks.\nThe results across different experiments attest the high\nquality of our embeddings, showing that our approach is\nrobust across languages, and hence enables state-of-theart multilingual WSD while at the same time relieving the\nheavy requirement of sense-annotated corpora.",
        "conclusion": "In this paper we presented SENSEMBERT, a novel approach for creating sense embeddings in multiple languages.\nSENSEMBERT proved to be effective both in the English\nand multilingual WSD tasks. Indeed, to the best of our\nknowledge this is the ﬁrst time, in the neural network era,\nthat a knowledge-based approach, employing a 1-NN strategy has succeeded in surpassing most of its supervised competitors and outperforming the state of the art on rare words\nand senses. SENSEMBERT’s generalization ability is further demonstrated by our multilingual experiments, where\nour approach beats all its alternatives, setting a new state\nof the art in all the tested languages. Moreover, we show\nthat our context-rich representations are also beneﬁcial when\ncoupled with manually-annotated data, hence enabling the\nsupervised version of SENSEMBERT to surpass the bar of\n80% accuracy and leaving all the other approaches behind\nwith a gap of more than 2.0 points. We release sense embeddings in ﬁve different languages for all the WordNet nominal\nsynsets at http://sensembert.org.\nAs future work, we plan to extend our approach to cover\nthe other main POS tags, i.e., verbs, adjectives and adverbs,\nby exploiting other knowledge resources, such as VerbAtlas\n(Di Fabio, Conia, and Navigli 2019) and SyntagNet (Maru\net al. 2019). Moreover, we plan to leverage the sense embeddings provided by SENSEMBERT to create high-quality\nsilver data for WSD in multiple languages.",
        "summary_en": "Contextual representations of words derived by neural language models have proven to effectively encode the subtle distinctions that might occur between different meanings of the same word. However, these representations are not tied to a semantic network, hence they leave the word meanings implicit and thereby neglect the information that can be derived from the knowledge base itself. Therefore,this paper proposes SensEmBERT, a knowledge-based approach that brings together the expressive power of language modelling and the vast amount of knowledge contained in a semantic network to produce high-quality latent semantic representations of word meanings in multiple languages. The vectors lie in a space comparable with that of contextualized word embeddings, thus allowing a word occurrence to be easily linked to its meaning by applying a simple nearest neighbour approach.The paper shows that, whilst not relying on manual semantic annotations, SensEmBERT is able to either achieve or surpass state-of-the-art results attained by most of the supervised neural approaches on the English Word Sense Disambiguation task. When scaling to other languages, the representations prove to be equally effective as their English counterpart and outperform the existing state of the art on all the Word Sense Disambiguation multilingual datasets.",
        "summary_zh": "这篇论文介绍了一种基于知识的方法SensEmBERT，用于多语言词义消歧任务。该方法结合了神经语言模型的表达能力和语义网络中的大量知识，生成多语言词义的高质量潜在语义表示。其生成的向量所处的空间与上下文词嵌入的空间相当，因此只需采用简单的近邻方法，就能轻松地将词的出现与词义联系起来。实验结果表明，在不依赖人工语义注释的情况下，SensEmBERT 能够在英语词义消歧任务中达到或超过大多数有监督神经方法的一流结果。当扩展到其他语言时，同样有效，并且在所有词义消歧多语言数据集上都优于现有技术水平。"
    },
    {
        "title": "XL-WSD: An Extra-Large and Cross-Lingual Evaluation Framework for Word Sense Disambiguation",
        "abstract": "Transformer-based architectures brought a breeze of change to Word Sense Disambiguation (WSD), improving models’ performances by a large margin. The fast development of new approaches has been further encouraged by a well-framed evaluation suite for English, which has allowed their performances to be kept track of and compared fairly. However, other languages have remained largely unexplored, as testing data are available for a few languages only and the evaluation setting is rather matted. In this paper, we untangle this situation by proposing XL-WSD, a cross-lingual evaluation benchmark for the WSD task featuring sense-annotated development and test sets in 18 languages from six different linguistic families, together with language-speciﬁc silver training data. We leverage XL-WSD datasets to conduct an extensive evaluation of neural and knowledge-based approaches, including the most recent multilingual language models. Results show that the zero-shot knowledge transfer across languages is a promising research direction within the WSD ﬁeld, especially when considering low-resourced languages where large pretrained multilingual models still perform poorly. We make the evaluation suite and the code for performing the experiments available at https://sapienzanlp.github.io/xl-wsd/.",
        "introduction": "Word Sense Disambiguation (WSD) is the task of associating\nwords in context with their possible meanings contained in a\npre-deﬁned sense inventory (Navigli 2009). This task is central to the understanding of natural language (Navigli 2018),\nand it has received considerable attention over recent years as\nit can be beneﬁcial for a variety of downstream tasks and applications, such as machine translation (Raganato, Scherrer,\nand Tiedemann 2019), information extraction (Delli Bovi,\nTelesca, and Navigli 2015), and text categorization (Shimura,\nLi, and Fukumoto 2019). The WSD task has been tackled\nwith different approaches, which can be broadly divided into\ntwo main categories: knowledge-based (Moro, Raganato, and\nNavigli 2014; Agirre, de Lacalle, and Soroa 2014; Chaplot\nand Salakhutdinov 2018), which leverage computational lexicons and their structure, and supervised (Bevilacqua and\nNavigli 2020; Blevins and Zettlemoyer 2020; Conia and Navigli 2021), which train machine learning algorithms on senseannotated data. This latter kind of approach attains state-ofthe-art results in English WSD, constantly outperforming\ntheir knowledge-based counterparts (Raganato, CamachoCollados, and Navigli 2017).\nThe evaluation in this ﬁeld is usually carried out with the\nframework proposed by Raganato, Camacho-Collados, and\nNavigli (2017), which has set a level playing ﬁeld among\nEnglish WSD approaches, and has facilitated the fast development of models for this task (Raganato, Delli Bovi, and\nNavigli 2017; Luo et al. 2018; Huang et al. 2019; Bevilacqua and Navigli 2020; Bevilacqua, Maru, and Navigli 2020).\nUnfortunately, the same attention has not been devoted to\nmultilingual WSD, which, in the last few years, has revolved\naround 4 European languages only, i.e., French, German, Italian and Spanish. Even though the research community has\ncreated both automatically sense-annotated corpora for different languages (Pasini 2020) and language-speciﬁc WordNetlike resources (Bond and Paik 2012; Navigli and Ponzetto\n2012), the lack of reliable benchmarks in different languages\nremains the main limitation hampering the advancement of\nresearch in this ﬁeld. Indeed, currently available multilingual gold standards use diverse data formats and outdated,\nor even unavailable, inventories of senses, making it hard\nto perform a fair comparison among systems and to draw\nreliable conclusions.\nIn this paper, we overcome the above problems and release\nwhat is, to the best of our knowledge, the ﬁrst large-scale\nmultilingual evaluation framework for WSD with a uniﬁed\nmultilingual sense inventory covering 18 languages: Basque,\nBulgarian, Catalan, Chinese, Croatian, Danish, Dutch, English, Estonian, French, Galician, German, Hungarian, Italian,\nJapanese, Korean, Slovenian, and Spanish from six families.\nOn the one hand, we provide more than 70K new gold annotations across 13 non-English languages by leveraging\nthe multilingual versions of WordNet. On the other hand,\nwe standardise and unify the datasets available in another\n4 languages from the past multilingual SemEval competitions, as well as the inventory of senses to be used across\nlanguages. This allows large multilingual models to be investigated through the semantics’ lens, hence providing a new\nway of studying pre-trained contextualised word embeddings.\nAs for the English language, XL-WSD includes the original framework of Raganato, Camacho-Collados, and Navigli\n(2017), further extending it, however, with data from: i) the\nﬁne-grained English WSD SemEval 2010 Task 17 (Agirre\net al. 2010) and ii) the coarse-grained English WSD SemEval\n2007 task 7 (Navigli, Litkowski, and Hargraves 2007). Moreover, XL-WSD features training data for the majority of its\nlanguages: SemCor (Miller et al. 1993) and the Princeton\nWordNet Gloss corpus1 for English, and their automatically\ntranslated and annotated versions that we created for most of\nXL-WSD’s other languages.\nIn summary, this paper makes the following novel contributions:\n1. A multilingual WSD test suite in 18 languages from six\nlanguage families, namely, Indo-European, Sino-Tibetan,\nUralic, Japonica and Koreanic plus an isolated language,\ni.e., Basque. Our benchmark comprises 99,450 gold annotations in total, new automatically-produced training\ndata for non-English languages and a uniﬁed multilingual\ninventory of concepts.\n2. An extension of the ﬁne-grained English WSD framework\nof Raganato, Camacho-Collados, and Navigli (2017) by\nincluding new training, development and testing data as\nwell as a coarse-grained evaluation dataset.\n3. Strong baselines based on large pre-trained multilingual language models and the ﬁrst large-scale comparison among contextualised word embedding models and\nknowledge-based approaches on a monolingual and zeroshot cross-lingual setting.",
        "related work": "Word Sense Disambiguation has been tackled using various kinds of approach, from knowledge-based algorithms to\nfully supervised models. Knowledge-based methods (Chaplot and Salakhutdinov 2018; Maru et al. 2019) take advantage of the structural properties of a semantic network such\nas WordNet (Miller 1998), a manually-curated electronic\ndictionary for English, or BabelNet (Navigli and Ponzetto\n2012), a large multilingual encyclopedic dictionary obtained\nby automatically merging various lexical resources (WordNet\nand Wikipedia, among others). While not relying on senseannotated data, and hence being able to scale over different\nlanguages, knowledge-based approaches usually fall behind\ntheir supervised counterparts in terms of performance.\nSupervised models (Vial, Lecouteux, and Schwab 2019;\nHuang et al. 2019; Bevilacqua and Navigli 2020; Scarlini,\nPasini, and Navigli 2020; Blevins and Zettlemoyer 2020; Conia and Navigli 2021), by exploiting SemCor (Miller et al.\n1993) – the largest manually-annotated corpus for English –\nhave consistently attained state-of-the-art results on the English all-words WSD tasks (Raganato, Camacho-Collados,\nand Navigli 2017). However, their main drawback is that\nthey have difﬁculty scaling over different languages, since\nno manually-curated training data is available to them. Automatic methods to produce sense distributions (Pasini, Scozzafava, and Scarlini 2020) or sense-annotated data in languages other than English (Delli Bovi et al. 2017; Scarlini,\nPasini, and Navigli 2019; Pasini and Navigli 2020; Pasini\n1http://wordnetcode.princeton.edu/glosstag.shtml\n2020) have mitigated this limitation, thus allowing supervised approaches to be trained on different languages and to\nenter a ﬁeld that was mainly dominated by knowledge-based\nmethods.\nImportantly, while multilingual word embeddings and,\nmore recently, deep multilingual pre-trained neural language\nmodels have proven to perform zero-shot transfer from one\nlanguage to another effectively, cross-lingual WSD research\nhas been dramatically hampered by the lack of a clear and\nlarge-scale multilingual evaluation suite. Indeed, the evaluation benchmarks proposed over the years in the context of\nSenseval and SemEval competitions have focused mainly on\nEnglish: Senseval-2 (Edmonds and Cotton 2001), Senseval-3\n(Snyder and Palmer 2004), SemEval-07 Task 17 (Pradhan\net al. 2007), SemEval-07 Task 7 (Navigli, Litkowski, and\nHargraves 2007), SemEval-10 Task 17 (Agirre et al. 2010),\nSemEval-13 Task 12 (Navigli, Jurgens, and Vannella 2013)\nand SemEval-15 Task 13 (Moro and Navigli 2015), with only\na few of them providing data for other languages too, i.e.,\nSemEval-10 Task 17, SemEval-13 Task 12 and SemEval-15\nTask 13. While the WSD framework proposed by Raganato,\nCamacho-Collados, and Navigli (2017) systematised and uniﬁed the datasets for the English ﬁne-grained WSD task, it\nfocused on English only and did not include any of the available multilingual datasets. As a result, WSD multilingual\nbenchmarks today are still outdated, featuring old, languagespeciﬁc or even unavailable sense inventories, which limits\ntheir use. This is in marked contrast to other NLP tasks where\nmany efforts have been made to evaluate models across languages (Hu et al. 2020; Lewis et al. 2020; Ponti et al. 2020;\nRaganato et al. 2020; Martelli et al. 2021, XTREME, MLQA,\nXCOPA, XL-WiC, MCL-WiC, respectively),\nTo bridge this gap, we put forward a comprehensive multilingual WSD evaluation framework containing new gold\ndevelopment and test sets, as well as silver training data in 18\nlanguages from 6 distinct language families, which ensures\nan easy and fair evaluation of WSD systems across languages.\nXL-WSD is similar to other multilingual evaluation benchmarks in terms of the number of instances, languages and\nlinguistic families covered. Indeed, it is comparable to tasks\nlike XTREME in terms of instances and covers more families\nthan MLQA and more languages than XCOPA. Moreover,\nour framework also includes and enriches the original English test suite for WSD of Raganato, Camacho-Collados,\nand Navigli (2017), by featuring coarse-grained datasets, and\na larger training set.",
        "xl-wsd": "In this Section, we detail the creation of XL-WSD. First, we\ndeﬁne the uniﬁed multilingual sense inventory and introduce\nthe new multilingual gold standards. Then, we present the\nnew multilingual training data providing relevant statistics.\nSense Inventory\nSense inventories deﬁne the possible meanings for a word,\nand, while the Princeton WordNet (PWN) is the de facto standard sense inventory for English, there is no such convention\nin other languages.\nOver the years, several efforts have been made to create\nWordNet-like resources in multiple languages and to link\nthem to the PWN (Bond and Foster 2013). A superset of\nthese lexical resources is BabelNet2, a comprehensive multilingual encyclopedic dictionary that merges various resources\n(WordNet and Wikipedia, among others) into a uniﬁed multilingual repository. It provides a wide coverage of concepts\nacross languages and several lexicalizations for each meaning, e.g., the machine meaning of the English word computer\nis lexicalised with ordinateur in French, computadora in\nSpanish, calcolatore in Italian, etc.\nTherefore, we draw the sense inventory from BabelNet\n(version 4.0) and deﬁne the list of 117,659 BabelNet synsets\ncontaining at least one sense from the Princeton WordNet\n(version 3.0) as our set of possible meanings S. We constrain\nour synsets to contain at least one PWN sense, so as to allow\ntraining a model in English and testing in other languages\nand to ensure a wide coverage of meanings across many\nlanguages. Indeed, most non-English WordNets are created\nby, either translating PWN synsets into the target language\n(extend mode), or by linking newly created concepts to the\nPWN (merge mode) (Vossen 1998). Once the set S of synsets\nis deﬁned, we extract the set of lemmas speciﬁc to a language\nL by collecting all lexicalisations of any synset in S in that\nlanguage. We then associate each lemma and part-of-speech\n(POS) pair (l, p) with the set of its possible meanings s ∈ S,\ni.e., all those synsets with POS tag p containing l among their\nlexicalisations.\nWe are aware that limiting the conceptualisations of other\nlanguages to the English PWN may not deﬁne a faithful\nequivalent of a dictionary in other languages. However, doing\nso allows us to create a shared multilingual sense inventory,\nenabling a fair evaluation of models in the cross-lingual setting.\nGold Standards\nWordNet datasets.\nThe Princeton English WordNet organizes concepts in synsets, i.e., sets of synonyms, and provides,\nfor each of them, one or more usage examples, i.e., sentences\nin which one of the synset’s lexicalizations is used with that\nmeaning. For example, the slope synset of bank contains the\nexample “They pulled the canoe up on the bank”, while the\nﬁnancial institution synset contains the example “He cashed a\ncheck at the bank”. We leverage this structure that is common\nacross WordNet-like resources and create new evaluation\nbenchmarks from the following language-speciﬁc WordNets:\nBasque (Pociello et al. 2008), Bulgarian (Simov and Osenova\n2010), Catalan (Ben´ıtez et al. 1998), Chinese (Huang et al.\n2010), Croatian (Raffaelli et al. 2008), Danish (Pedersen et al.\n2009), Dutch (Postma et al. 2016), Estonian (Vider and Orav\n2002), Galician (Guinovart 2011), Hungarian (Mih´altz et al.\n2008), Japanese (Isahara et al. 2008), Korean (Yoon et al.\n2009), and Slovenian (Fiˇser, Novak, and Erjavec 2012). The\nGalician, Catalan, and Basque WordNets are taken from the\nMultilingual Central Repository project (Gonzalez-Agirre,\nLaparra, and Rigau 2012), while the Bulgarian, Japanese,\n2https://babelnet.org\nand Slovenian from the Open Multilingual WordNet project\n(Bond and Paik 2012).\nIn detail, given a synset s within a language-speciﬁc WordNet, and one of its usage examples e = w1, . . . , wn, we\nselect as target word the one having the same POS tag of s,\nand, as lemma, one of the lexicalisations of s. For example,\ngiven the nominal synset s for salmon, which contains the\nDanish word laks as one of its lexicalisations and the Danish\nexample “Stjernerne i bornholmernes ﬁskerierhverv er ørred,\nlaks og sild”,3 we mark laks as target word since it has been\nPOS tagged with the same tag as s, and is a lexicalisation of\ns. If we ﬁnd more than one word matching our criterion, we\ndiscard the sentence.\nFinally, we leverage the available mapping from the\nlanguage-speciﬁc WordNet to the English WordNet 3.0 and\nthe mapping from WordNet 3.0 to BabelNet included in BabelNet itself, so as to tag each instance with the corresponding\nBabelNet synset within our sense inventory.\nSemEval datasets.\nWe consider all multilingual gold standards released in the past SemEval competitions, i.e., the\nItalian and Chinese datasets in SemEval-10 Task 174 (Agirre\net al. 2010), French, German, Italian and Spanish datasets in\nSemEval-13 Task 12 (Navigli, Jurgens, and Vannella 2013),\nand Italian and Spanish datasets in SemEval-15 Task 13\n(Moro and Navigli 2015).\nThe SemEval-10 dataset contains documents from the European Center for Nature Conservation and the Worldwide\nWildlife Forum corpora. The SemEval-13 datasets contain\n13 parallel documents from 2010, 2011 and 2012 editions\nof the Workshop on Statistical Machine Translation. French,\nGerman, and Spanish text data come directly from the Workshop, the Italian dataset, instead, was created by manually\ntranslating the English documents. As regards the datasets\nin SemEval-15, they were taken from the EMA (European\nMedicines Agency documents), KDEdoc (KDE manuals) and\nEUbookshop (documents from the EU bookshop) corpora.\nOriginally, the SemEval-15 datasets were built for both allwords WSD and Entity Linking tasks. In this work, we use\nonly the instances in the WSD split.\nAs for English, we consider all datasets in the Raganato,\nCamacho-Collados, and Navigli (2017) framework plus the\nEnglish data from SemEval-10 Task 17 and the coarsegrained dataset from SemEval-07 Task 7 (Navigli, Litkowski,\nand Hargraves 2007, SemEval-07-Coarse). This latter contains documents extracted from the Wall Street Journal corpus, Wikipedia and the Knights of the Art book by Amy\nSteedman and is annotated with clusters of WordNet senses.\nData cleaning.\nMost datasets from the past SemEval competitions use different inventories. Speciﬁcally, Chinese and\nItalian datasets of SemEval-10 are tagged with WordNet 1.6;\nSemEval-07-Coarse is annotated with clusters of WordNet\nsenses from version 2.1; SemEval-13 and SemEval-15, in3The stars of the Bornholm ﬁshing industry are trout, salmon\nand herring.\n4The SemEval-10 Dutch sense inventory is no longer available.\nstead, use different versions of BabelNet as inventory, i.e.,\nBabelNet 1.1.1 and BabelNet 2.5.1, respectively.\nTo standardise WordNet versions, we convert all the annotations from WordNet 1.6 and 2.1 to WordNet 3.0 utilising\nthe automatically-generated mappings of Daude, Padro, and\nRigau (2003), keeping the synsets with the highest conﬁdence\nscore only. As regards the instances tagged with BabelNet\n1.1.1 and 2.5.1, we ﬁrst map each annotation from its original\nBabelNet version to the latest available one (4.0), by using\nthe corresponding BabelNet indices, and, then, retain only\nthe instances tagged with a synset in our inventory. We ﬁnally\nremove all the instances that could not be mapped. All the\nother datasets, instead, have already been mapped to WordNet version 3.0, so we retrieve their corresponding BabelNet\nsynset with the BabelNet API 4.0.1.\nEvaluation split.\nWe group all the datasets in the same\nlanguage and randomly split their instances into two subsets,\none for testing (80% of instances) and one for development\n(the remaining 20% of instances). As for English, instead,\nwe provide 2 distinct test sets: a ﬁne-grained one (English-F)\nincluding Senseval-2, Senseval-3, SemEval-10, SemEval-13\nand SemEval-15, and a coarse-grained one (English-C), i.e.,\nSemEval-07 Task 17. As for development, we follow prior\nwork (Raganato, Delli Bovi, and Navigli 2017; Blevins and\nZettlemoyer 2020) and use SemEval-07 (English-Dev). As\na result, each language has a test and a development set\nin the same data format and tagged with the same uniﬁed\ninventory. Furthermore, we enrich the English benchmark of\nRaganato, Camacho-Collados, and Navigli (2017) with 3K\nmore instances, covering different sense granularities.\nTraining Data\nSemCor (SC).\nIntroduced by Miller et al. (1993), this is the\nmost used corpus for English Word Sense Disambiguation.\nIt contains 37,176 sentences and 226,036 instances tagged\nwith a sense in WordNet.\nPrinceton WordNet Gloss Corpus (WNG).\nA corpus created from the synset deﬁnitions and examples of WordNet.5\nIts annotations were carried out both manually and semiautomatically. By following Bevilacqua and Navigli (2020),\ngiven a gloss g for a sense s, we prepend to g the lemma of\ns and tag it with s so as to provide at least one annotated\nexample for each concept. In total, it consists of 614,435\ninstances tagged with 117,653 different synsets.\nTranslated corpora (T-SC+WNG).\nWe provide silver\ntraining data to train language-speciﬁc baselines for 15 nonEnglish languages of our framework6 by leveraging the machine translation models made available by Tiedemann and\nThottingal (2020, Opus-MT).7 The choice of these models\nis motivated, ﬁrst, by the fact that both the English training\ncorpora (SC and WNG) and the training data for the machine\n5http://wordnetcode.princeton.edu/glosstag.shtml\n6Chinese and Korean have no MT models at the time of writing.\n7https://github.com/Helsinki-NLP/Opus-MT\ntranslation models (the OPUS parallel corpora collection\n(Tiedemann 2012)) are general-domain,8 and, second, by\nconsidering that training several domain-speciﬁc models for\neach target language is resource expensive and beyond the\nscope of this work.\nWe create the language-speciﬁc training corpora by translating the English sentences of SC and WNG into the target\nlanguages, and, then, by transferring the sense annotations\nfrom the original English texts to their translations. In more\ndetail, given an English sentence σEN = w1, . . . , wn, its\ntranslation σT = wT\n1 , . . . , wT\nm and the synset annotation s\nfor the word wi in σEN, we propagate the annotation to the\nword wT\nj in σT that appears as a synonym in s. In the case\nthat multiple annotations are associated with the same word\nwT\nj , we discard all of them. To further reﬁne the quality of the\nprojections, we apply a part-of-speech tagger and a lemmatiser to both source and target languages, keeping only those\nsenses in which both source and target words are tagged with\nthe same part of speech.9\nOur goal is not to create the best possible datasets, but\nrather to enable the training of monolingual baselines which\ncan be used as a term of comparison for future work. Our\napproach, moreover, has the following advantages: i) it allows\nannotations to be automatically spread from one language to\nmany others without human effort, ii) the sense distribution\nis potentially maintained across languages, iii) it produces\nannotations for virtually any word and language covered by\nBabelNet and for which a machine translation model exists.\nStatistics\nWe report the general statistics for each dataset of XL-WSD\nin Table 1. The number of annotated instances in the training data varies across languages, ranging from more than\n800K in English, to less than 25K in Japanese and cover\nfrom roughly 1,000 to 117K different synsets depending on\nthe language. Even though the non-English training data, i.e.,\nT-SC+WNG, are all created starting from the same source,\ni.e., SC+WNG, the number of transferred instances is affected\nby both translation quality and BabelNet’s coverage of each\nspeciﬁc language. As regards the test sets, most languages\ncontain more than 1,000 gold annotations, with Bulgarian\nand Chinese containing even more test instances than English. Additionally, Table 1 shows the number of different\nword types for each language, the number of polysemous\nword types, i.e., words with more than one meaning, and\nthe word-type polysemy measure, i.e., the total number of\ncandidate synsets for each word type divided by the total\nnumber of word types. The word-type polysemy is similar\nacross language-speciﬁc training sets as they all come from\nthe translation of SC+WNG. On the other hand, the polysemy\nvaries substantially across test sets, with Croatian having\nthe least polysemous test set (1.24) and Spanish the most\npolysemous one (4.95).\nIn total, XL-WSD contains more than 99K semanticallytagged gold instances for testing and tuning across 18 differ8More details about the translation models and their translation\nquality are given at https://sapienzanlp.github.io/xl-wsd/.\n9We use Stanza pre-trained neural models (Qi et al. 2020).\nWord Types\nPolysemous Words\nWord-Type Polysemy\nInstances\nUnique Synsets\nLanguage\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nEnglish-F\n106906\n2882\n24658\n2199\n1.458\n3.689\n840471\n8062\n117653\n3469\nEnglish-C\n980\n750\n4.255\n1816\n2190\nBasque\n12503\n771\n5294\n525\n2.331\n3.224\n197309\n1580\n16604\n1423\nBulgarian\n12413\n2450\n2412\n1325\n1.304\n1.670\n148479\n9968\n12600\n2658\nCatalan\n18603\n1276\n8378\n1107\n2.291\n3.940\n331757\n1947\n25624\n1767\nChinese\n1786\n1402\n2.638\n9568\n2687\nCroatian\n6882\n4389\n1161\n1652\n1.268\n1.244\n94575\n6333\n6739\nDanish\n15822\n2623\n3324\n1318\n1.338\n1.722\n234681\n3502\n16707\n2693\nDutch\n28351\n2935\n9121\n2122\n1.711\n2.356\n305692\n4400\n30490\n2716\nEstonian\n10460\n1615\n1768\n917\n1.246\n1.815\n132240\n1999\n10462\n1852\nFrench\n17850\n549\n5978\n339\n1.585\n2.413\n252756\n1160\n21510\n584\nGalician\n8390\n1244\n3799\n773\n2.079\n2.219\n247379\n2561\n11821\n1474\nGerman\n16213\n421\n2332\n166\n1.203\n1.639\n184952\n862\n16437\n417\nHungarian\n13234\n3491\n2908\n1931\n1.367\n1.842\n161119\n4428\n13297\n4285\nItalian\n23773\n985\n9540\n758\n2.021\n3.790\n385248\n2278\n29869\n1212\nJapanese\n1008\n4338\n581\n2390\n2.516\n1.871\n23217\n7602\n1141\n5964\nKorean\n1886\n920\n1.373\n3796\n1452\nSlovenian\n7577\n104\n1296\n93\n1.245\n3.519\n128395\n2032\n7705\n243\nSpanish\n22020\n847\n11784\n696\n2.811\n4.955\n393539\n1851\n32151\n1103\nTable 1: Statistics of XL-WSD training and test sets. “Train” column refers to SC+WNG for English and T-SC+WNG for the other\nlanguages.\nent languages, 3M silver annotations from the T-SC+WNG\ndatasets and more than 100K annotations for English.",
        "experimental setup": "Architecture details.\nWe follow Bevilacqua and Navigli (2020) and employ a Transformer-based text encoder\n(Vaswani et al. 2017) followed by a 2-layer feedforward network with swish activation function and batch-normalization.\nWe stack on top of it an unbiased softmax linear layer for\nclassiﬁcation. We represent each sub-token by summing the\noutputs of the last four layers of the text encoder and each\nword by averaging its sub-token representations. Finally, we\napply a linear transformation and feed the resulting vectors\nto a linear layer for classiﬁcation. As text encoders, we use\nXLMR-B, XLMR-L (Conneau et al. 2020), BERT-L, BERTM10 (Devlin et al. 2019) and the language-speciﬁc versions\nof BERT (LS-BERT) for each language11 available through\nthe Huggingface library (Wolf et al. 2020). We train all neural\nmodels for 50 epochs with Adam optimizer12 and use the set\nof weights with the lowest loss on the development set for\ntesting.\nEvaluation measure.\nAs standard in the literature, we\nadopt the F1 score, i.e., the harmonic mean between Precision and Recall. We note that Precision, Recall and F1\nscore are the same, as our models always provide an answer.\nData.\nAs for training, we use SemCor and WordNet Gloss\n(SC+WNG) for English and their translations, i.e., T-SC+WNG,\n10The base multilingual-cased version of BERT.\n11More details at https://sapienzanlp.github.io/xl-wsd/.\n12Gradient clipping = 1.0; learning rate = 2 · 10−5; patient = 3.\nModel\nEnglish-F\nEnglish-C\nALL\nSemCor\nLMMS ⋄\n75.40\nGlossBERT ⋄\n77.00\nBERTGLU\n74.10\nBEM⋄\n79.00\nXLMR-B\n71.29\n86.01\n73.21\nBERT-M\n69.19\n84.80\n61.54\nXLMR-L\n72.46\n86.12\n74.24\nBERT-L\n72.66\n86.51\n74.33\nSC+WNG\nEWISER\n80.10\nXLMR-B\n74.50\n91.02\n76.24\nBERT-M\n72.40\n89.70\n74.10\nXLMR-L\n76.28\n91.30\n78.11\nBERT-L\n76.77\n91.57\n78.36\nTable 2: Comparison on the English datasets. ◦ indicates that\nthe model is an ensemble, while ⋄ indicates that the model\nleverages raw sense deﬁnitions.\nfor the monolingual experiments in other languages. As a\nterm of comparison, we also report the results attained by\ntraining our baseline models on MULAN13 datasets (Barba\net al. 2020). Differently from our approach, MULAN leverages the multilingual contextualised word representations\nof BERT to pair manually-tagged examples in English with\ntheir most similar sentences in a corpus of raw texts, e.g.,\nWikipedia, and then transfer the sense annotations.\n13https://github.com/SapienzaNLP/mulan\n∅-Shot\n(SC+WNG)\nLanguage-Speciﬁc\n(MULAN)\nLanguage-Speciﬁc\n(T-SC+WNG)\nKnowledge-Based\nDataset\nXLMR-L\nXLMR-B\nBERT-M\nXLMR-L\nLS-BERT\nXLMR-L\nLS-BERT\nSyntagRank\nBabelfy\nMCS\nEnglish-F\n76.28\n74.50\n72.40\n76.28\n76.77\n69.96\n64.09\n63.37\nEnglish-C\n91.30\n91.02\n89.70\n91.30\n91.57\n83.78\n82.54\n80.23\nBasque\n47.15\n43.80\n42.41\n41.96\n43.04\n42.91\n36.65\n32.72\nBulgarian\n72.00\n71.59\n68.78\n58.18\n57.85\n61.10\n60.39\n58.16\nCatalan\n49.97\n47.77\n47.35\n36.00\n36.98\n43.98\n36.65\n27.17\nChinese\n51.62\n49.77\n48.99\n41.23\n34.94\n29.62\nCroatian\n72.29\n72.13\n70.65\n63.15\n62.89\n68.35\n63.75\n62.88\nDanish\n80.61\n79.18\n76.04\n78.67\n76.41\n72.93\n71.33\n64.33\nDutch\n59.20\n58.77\n56.64\n57.27\n56.64\n56.00\n44.27\n44.61\nEstonian\n66.13\n64.82\n64.33\n50.78\n51.23\n56.31\n49.62\n46.87\nFrench\n83.88\n82.33\n81.64\n81.98\n80.78\n71.38\n71.12\n69.57\n67.41\n59.31\nGalician\n66.28\n64.79\n68.07\n56.18\n56.95\n67.56\n64.17\n60.85\nGerman\n83.18\n82.13\n80.63\n83.29\n82.13\n73.78\n73.78\n75.99\n77.84\n75.99\nHungarian\n67.64\n68.38\n65.24\n52.60\n52.17\n57.98\n51.99\n47.29\nItalian\n77.66\n76.73\n76.16\n74.10\n73.88\n77.70\n75.68\n69.57\n64.22\n52.77\nJapanese\n61.87\n61.46\n60.34\n50.55\n50.16\n57.46\n51.91\n48.71\nKorean\n64.20\n63.65\n63.37\n50.29\n51.95\n52.48\nSlovenian\n68.36\n66.34\n62.16\n51.13\n49.66\n52.25\n35.38\n36.71\nSpanish\n75.85\n76.55\n74.66\n73.47\n74.77\n77.26\n74.88\n68.58\n64.07\n55.65\nAVG\n65.66\n64.82\n62.84\n57.68\n52.85\n49.31\nTable 3: F1 scores of supervised and knowledge-based approaches as well as language-speciﬁc BERT models (LS-BERT) and\nthe Most Common Sense (MCS) baseline on the test splits. As for the ∅-Shot columns, models are trained and tuned in English\nonly and tested in all the other languages. As for the Language-Speciﬁc columns, models are trained, tuned and tested on either\nMULAN or T-SC+WNG language-speciﬁc datasets. The AVG row shows the micro F1 across all languages but English.\nAs for development and testing, we use the languagespeciﬁc data that we previously introduced. We also consider the ALL dataset in the Raganato, Camacho-Collados,\nand Navigli (2017) framework, which comprises Senseval-2,\nSenseval-3, SemEval-07, SemEval-13 and SemEval-15 to\ncompare our baselines against the state of the art.",
        "results": "English Benchmark\nAs a preliminary experiment, in Table 2 we compare our\nbaselines with the most recent WSD models in the literature\non the English datasets, to give an idea about how our models\ncompare against the state of the art.\nAs one can see, our baselines perform in the same ballpark\nas most of the other approaches. When using SemCor only for\ntraining, BEM is the best system across the board, however, it\nrequires the ﬁnetuning of two distinct BERT-base models and\nleverages raw WordNet glosses. When using the SC+WNG\ndataset, instead, both BERT-L and XLMR-L perform less\nthan 2 F1 points lower than EWISER, which, however, leverages additional information from sense embeddings and the\ntopology of a knowledge graph.\nTherefore, since our multilingual baselines attain results\nthat are comparable with the current best performing models\nfor WSD, we employ them to carry out the evaluation.\nMultilingual Evaluation\nTable 3 shows the performance on the proposed multilingual benchmark, reporting the results attained by our reference models trained and tuned, i) on English data only,\ni.e., SC+WNG, ii) on the automatically-translated languagespeciﬁc training data, i.e., T-SC+WNG, and iii) on MULAN.\nAdditionally, we consider two knowledge-based approaches:\nBabelfy (Moro, Raganato, and Navigli 2014), which is based\non a densest sub-graph algorithm, and SyntagRank (Scozzafava et al. 2020), which relies on the Personalized PageRank algorithm and leverages the collocational relations in\nSyntagNet (Maru et al. 2019). We also show the results of\nthe Most Common Sense (MCS) baseline, which tags each\nword with its most common sense according to BabelNet.\nZero-shot setting.\nAs one can see from Table 3, XLMR-L\nachieves the best results across the board, with a big gap with\nrespect to knowledge-based systems. Interestingly enough,\nsupervised models trained on English data only (zero-shot\ncolumns) almost always outperform their language-speciﬁc\ncounterparts, i.e., either multilingual models trained on\nlanguage-speciﬁc training sets (Language-Speciﬁc / XLMRL columns) or language-speciﬁc models trained on languagespeciﬁc data (LS-BERT columns). We note the same behaviour for French, German, Italian and Spanish, where MULAN training data are also available.\nThese results are in line with the most recent ﬁndings, i.e.,\nthat large multilingual language models play a key role in\nDataset\nALL\nN\nV\nA\nR\nEnglish-F\n76.28\n77.92\n65.74\n81.47\n86.71\nEnglish-C\n91.30\n92.72\n88.64\n89.55\n91.75\nBasque\n47.15\n47.15\nBulgarian\n72.00\n70.69\n86.04\n74.07\nCatalan\n49.97\n49.28\n54.84\n52.89\nChinese\n51.62\n57.92\n45.47\n47.01\n84.48\nCroatian\n72.29\n71.85\n70.37\n85.03\nDanish\n80.61\n80.32\n79.66\n83.63\nDutch\n59.20\n56.08\n63.56\nEstonian\n66.13\n68.81\n49.66\n74.63\n68.14\nFrench\n83.88\n83.88\nGalician\n66.28\n71.43\n65.97\nGerman\n83.18\n83.18\nHungarian\n67.64\n70.41\n50.41\nItalian\n77.66\n77.91\n71.89\n81.58\n77.27\nJapanese\n61.87\n67.87\n52.72\n56.39\n71.29\nKorean\n64.20\n64.47\n46.43\nSlovenian\n68.36\n68.34\nSpanish\n75.85\n76.72\n66.83\n77.88\n85.00\nTable 4: XLMR-L F1 on the zero-shot setting by POS tags,\ni.e., nouns (N), verbs (V), adjectives (A) and adverbs (R).\nmaking up for the paucity of annotated data in non-English\nlanguages (Conneau et al. 2020), and therefore represent\na promising approach towards mitigating the knowledgeacquisition bottleneck problem in WSD. Furthermore, while\nthe multilingual WSD task has so far usually been addressed\nwith knowledge-based approaches, it is now clear that thanks\nto large multilingual pre-trained language models, neural networks can compete in this task too. Indeed, despite the fact\nthat SyntagRank manages to outperform several languagespeciﬁc models trained on T-SC+WNG, it performs 5 and 7\npoints lower than BERT-M and XLMR-B, respectively, and\nfalls behind XLMR-L trained on English by 8 points on average. These results corroborate previous English-focused\nartiﬁcially large-scale ﬁndings on the robustness of supervised WSD approaches (Pilehvar and Navigli 2014).\nLanguage-speciﬁc setting.\nOverall, pre-trained languagespeciﬁc BERT models perform equal or lower than their\nmultilingual counterparts. This is mainly due to the difference in the model size, indeed, XLMR-L has roughly 200M\nmore parameters than most of the language-speciﬁc models,\nwhich are based on BERT-B. Interestingly, our newly introduced training data, i.e., T-SC+WNG, despite being a baseline,\nproves to lead the neural models to attain higher performance\nthan when trained on MULAN, i.e., high-quality silver data,\nin Italian and Spanish. This is explained by the fact that\nItalian and Spanish datasets contain the highest number of\ntransferred labels, as shown in Table 1.\nDiscussion.\nOverall, XLMR-L is the best performing\nmodel scoring 65.66 on average on all non-English languages.\nIts extensive pre-training and the number of parameters play\na crucial role in achieving such high scores. Nevertheless,\nwe note that it still performs poorly in some languages, i.e.,\nBasque, Catalan, and Chinese. This is due to the fact that a\nlarge portion of the test instances are annotated with synsets\noccurring only a few times in the training data, thus making\nthese datasets particularly challenging. This also highlights\nthat representing the least frequent meanings remains an open\nissue even for large pre-trained language models.\nIn Table 4 we provide further insights by showing the results breakdown on each POS tag of the best performing\nmodel, i.e., XLMR-L. As one can see, verbs represent the\nmost challenging instances in most languages with an average F1 10 points lower than on nouns. Bulgarian, Catalan\nand Dutch are the only languages where the model performs\nbetter on verbs than on nouns. This is because verb instances\nin Bulgarian and Dutch are in general less polysemous than\nnouns. As for Catalan, instead, while verbs are more polysemous than nouns, the test set contains only 31 verbal\ninstances, hence making the test on verbs not signiﬁcant.\nOverall, there is still large room for improvement in multilingual and zero-shot Word Sense Disambiguation. Specifically, our benchmarks show that the gap between English\nand other languages is in general wide, with XLMR-L performing, on average, 10 points lower than on English. This\nhighlights the fact that word meanings are still not well captured by state-of-the-art language models, which struggle\nboth on low-resourced languages, such as Catalan or Basque,\nas well as on resource-rich languages, e.g., Chinese.",
        "conclusion": "In this paper, we presented XL-WSD, a large-scale evaluation benchmark for Word Sense Disambiguation in 18\ndifferent languages. On the one hand, XL-WSD features 34\nnew gold datasets for testing and tuning in 17 non-English\nlanguages and 15 silver datasets for training, which we built\nautomatically by translating manually-annotated data into\nthe target languages. On the other hand, it includes and enriches the previously available standard evaluation framework for English (Raganato, Camacho-Collados, and Navigli\n2017) with the addition of two test sets, i.e., SemEval-10 and\nSemEval-07-Coarse. All datasets share a common format\nand, more importantly, a uniﬁed multilingual sense inventory,\nthus allowing a fair and easy comparison among systems that\nwas previously out of reach. Furthermore, we provided strong\nbaselines for the multilingual WSD task and, for the ﬁrst time,\na large-scale evaluation of different contextualised word embedding models on a task with explicit semantics, comparing their results across languages against those achieved by\nknowledge-based models. XL-WSD stands, therefore, as a\nkey semantic benchmark not only in terms of size (i.e. the\nnumber of test instances), but also in terms of coverage (i.e.\nthe number of covered languages), thereby fostering research\nin multilingual WSD and cross-lingual transfer.\nAs future work, we plan to further extend our framework\nby validating the data, manually annotating datasets in new\nlanguages and providing standard splits for testing a model\non instances tagged with senses having different frequencies.\nXL-WSD code and data are freely available for research\npurposes at https://sapienzanlp.github.io/xl-wsd/.",
        "summary_en": "Transformer-based architectures brought a breeze of change to Word Sense Disambiguation (WSD), improving models' performances by a large margin. The fast development of new approaches has been further encouraged by a well-framed evaluation suite for English, which has allowed their performances to be kept track of and compared fairly. However, other languages have remained largely unexplored, as testing data are available for a few languages only and the evaluation setting is rather matted. Therefore, this paper untangles this situation by proposing XL-WSD, a cross-lingual evaluation benchmark for the WSD task featuring sense-annotated development and test sets in 18 languages from six different linguistic families, together with language-specific silver training data. The paper leverages XL-WSD datasets to conduct an extensive evaluation of neural and knowledge-based approaches, including the most recent multilingual language models. Results show that the zero-shot knowledge transfer across languages is a promising research direction within the WSD field, especially when considering low-resourced languages where large pre-trained multilingual models still perform poorly.",
        "summary_zh": "这篇论文介绍了一个针对词义消歧任务的跨语言评估框架（XL-WSD），解决了其他语言在词义消歧领域缺乏评估数据和标准的问题。该框架提供了18种语言的语义标注开发集和测试集，以及针对每种语言的银标注训练数据。作者利用XL-WSD数据集对基于神经和知识的方法进行了广泛评估，包括最新的多语言语言模型。结果显示，跨语言zero-shot知识迁移是词义消歧领域一个有前景的研究方向，特别是考虑到低资源语言的情况。"
    },
    {
        "title": "Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information",
        "abstract": "Non-autoregressive neural machine translation (NAT) generates each target word in parallel and has achieved promising inference acceleration. However, existing NAT models still have a big gap in translation quality compared to autoregressive neural machine translation models due to the multimodality problem: the target words may come from multiple feasible translations. To address this problem, we propose a novel NAT framework ReorderNAT which explicitly models the reordering information to guide the decoding of NAT. Specially, ReorderNAT utilizes deterministic and nondeterministic decoding strategies that leverage reordering information as a proxy for the ﬁnal translation to encourage the decoder to choose words belonging to the same translation. Experimental results on various widely-used datasets show that our proposed model achieves better performance compared to most existing NAT models, and even achieves comparable translation quality as autoregressive translation models with a signiﬁcant speedup.",
        "introduction": "Neural machine translation (NMT) models with encoderdecoder framework (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2014) signiﬁcantly outperform\nconventional statistical machine translation models (Koehn,\nOch, and Marcu 2003; Koehn et al. 2007). Despite their success, the state-of-the-art NMT models usually suffer from\nthe slow inference speed, which has become a bottleneck to\napply NMT in real-world translation systems. The slow inference speed of NMT models is due to their autoregressive\nproperty, i.e., decoding the target sentence word-by-word\naccording to the translation history.\nRecently, Gu et al. (2018) introduced non-autoregressive\nNMT (NAT) which can simultaneously decode all target\nwords to break the bottleneck of the autoregressive NMT\n(AT) models. To this end, NAT models (Gu et al. 2018;\nWei et al. 2019; Wang et al. 2019; Guo et al. 2019a) usually directly copy the source word representations to the input of the decoder, instead of using previous predicted target word representations. Hence, the inference of different\ntarget words are independent, which enables parallel computation of the decoder in NAT models. NAT models could\nachieve 10-15 times speedup compared to AT models while\nmaintaining considerable translation quality.\nHowever, NAT models still suffer from the multimodality problem (Gu et al. 2018): it discards the dependencies\namong the target words, and therefore the target words may\nbe chosen from multiple feasible translations, resulting in\nduplicate, missing or even wrong words. For example, the\nGerman phrase “Vielen Dank” can be translated as both\n“thank you” and “many thanks”. Unfortunately, as each target word is generated independently, “thank thanks” and\n“many you” may also be assigned high probabilities, resulting in inferior translation quality. In this work, we argue reordering information is essential for NAT models and helpful for alleviating the multimodality problem.\nTo this end, we propose a novel NAT framework named\nReorderNAT in this work, which explicitly models the reordering information to guide the decoding of NAT. To be\nspeciﬁc, as shown in Figure 1, ReorderNAT ﬁrst reorders the\nsource sentence into a pseudo-translation formed by source\nwords but in the target language word order, and then translates the source sentence conditioned on it. We further introduce two guiding decoding strategies which utilize the reordering information (i.e. pseudo-translation) to guide the\nword selection in decoding. The ﬁrst one is deterministic\nguiding decoding which ﬁrst generates a most likely pseudotranslation and then generates the target sentence based on\nit. The second one is non-deterministic guiding decoding\nwhich utilizes the conditional distribution of the pseudotranslation as a latent variable to guide the decoding of target\nsentences.\nIdeally, the pseudo-translation can be viewed as a ﬁnal\ntranslation written in source language. Guiding decoding\nwith it could help to model the conditional dependencies of\nthe target words and encourage the decoder to choose words\nbelonging to the same translation, which naturally reduces\nthe multimodality problem. Moreover, the decoding space of\ngenerating pseudo-translation is limited to the permutation\nof words in the source sentence, which can be well modeled\nby a small model. Therefore, ReorderNAT could effectively\nalleviate the multimodality problem by introducing the reordering information in NAT.\nExperimental results on several widely-used benchmarks\nEncoder Block\nI        want      to      thank    my     friends \nEmb\nEmb\nEmb\nEmb\nEmb\nEmb\n×𝑁\nEmb\nEmb\nEmb\nEmb\nEmb\n①\nDecoder Block\nInter-Attention\n×𝐾\nSoftmax\nI     want     my    friends  thank \nReordering Module\nEmb\nEmb\nEmb\nEmb\nEmb\n②\nDecoder Block\n×𝑁-𝐾\nSoftmax\nIch  möchte meinem Freund danken\nDecoder Module\nEncoder Module\n③\n③\nFigure 1: The architecture of our ReorderNAT model. Different from original NAT models, our model adds a reordering module\nbetween the encoder and decoder modules to explicitly model the reordering information. For original NAT models, the decoder\ninputs are the copied embeddings of source sentence (No.1 dashed arrow), and for our ReorderNAT model, the decoder inputs\nare the embeddings of pseudo-translation generated by reordering module (No. 2 dashed arrow). The encoder and decoder\nblocks are the same as existing NMT models (e.g., Transformer block).\nshow that our proposed ReorderNAT model achieves signiﬁcant and consistent improvements compared to existing NAT\nmodels by explicitly modeling the reordering information to\nguide the decoding. Moreover, by introducing a simple but\neffective AT module to model reordering information, our\nReorderNAT immensely narrows the translation quality gap\nbetween AT and NAT models, while maintaining considerable speedup (nearly six times faster). The source codes are\navailable at https://github.com/ranqiu92/ReorderNAT.",
        "background": "Non-autoregressive neural machine translation (NAT) is ﬁrst\nproposed by Gu et al. (2018) to alleviate the slow decoding issue of autoregressive neural machine translation (AT)\nmodels, which could simultaneously generate target words\nby removing their dependencies. Formally, given a source\nsentence X = {x1, · · · , xn} and a target sentence Y =\n{y1, · · · , ym}, NAT models the translation probability from\nX to Y as a product of conditionally independent target\nword probability:\nP(Y|X) =\nm\nY\ni=1\nP(yi|X).\n(1)\nInstead of utilizing the translation history, NAT models\nusually copy source word representations as the input of the\ndecoder. Hence, when translating a sentence, NAT models\ncould predict all target words with their maximum likelihood individually by breaking the dependency among them,\nand therefore the decoding procedure of NAT models is in\nparallel and has very low translation latency.\nHowever, since NAT models discard the sequential dependencies among words in the target sentence, they suffer from the potential performance degradation due to the\nmultimodality problem. To be speciﬁc, a source sentence\nmay have multiple translations. During decoding, NAT models may choose the target words from different translations,\nresulting in duplicate, missing or even wrong words. Consequently, NAT models cannot effectively learn the intricate translation patterns from source sentences to target sentences, leading to inferior translation quality.",
        "methodology": "In this section, we introduce a novel NAT model named ReorderNAT, which aims to alleviate the multimodality problem in NAT models.\nReorderNAT\nAs shown in Figure 1, ReorderNAT employs a reordering\nmodule to explicitly model the reordering information in the\ndecoding1. Formally, ReorderNAT ﬁrst employs the reordering module to translate the source sentence X into a pseudotranslation Z = {z1, · · · , zm} which reorganizes source\nsentence structure into the target language, and then uses\nthe decoder module to generate target translation Y based\non the pseudo-translation. ReorderNAT models the overall\ntranslation probability as:\nP(Y|X)\n=\nX\nZ\nP(Y|Z, X)P(Z|X),\n(2)\nwhere P(Z|X) is modeled by the reordering module and\nP(Y|Z, X) is modeled by the decoder module. Next, we\nwill introduce the reordering and decoder modules in detail2.\nReordering Module\nThe reordering module determines\nthe source-side information of each target word by learning to translate the source sentence into a pseudo-translation.\n1We do not employ positional attention (Gu et al. 2018) as the\nmechanism may be misguided by target supervision due to the indirect optimization and lead to inferior translation.\n2The encoder module of ReorderNAT is a multi-layer Transformer (Vaswani et al. 2017), which is the same as original NAT\nmodels.\nWe propose two feasible implementations of the reordering\nmodule:\n(1) NAT Reordering Module: Intuitively, the pseudotranslation probability can also be modeled as NAT:\nP(Z|X) =\nm\nY\ni=1\nP(zi|X),\n(3)\nwhere P(zi|X) is calculated by a single-layer Transformer.\nDuring inference, the NAT reordering module needs to determine the length of the pseudo-translation. To this end, we\nuse a length predictor and copy the embeddings of the source\nsentence as the input of the reordering module similar to existing NAT models.\n(2) AT Reordering Module: We ﬁnd that AT models are\nmore suitable for modeling the reordering information compared to NAT models, and even a light AT model with similar decoding speed to a large NAT model could achieve better performance in modeling reordering information. Hence,\nwe also introduce a light AT model to model the pseudotranslation probability as:\nP(Z|X) =\nm\nY\ni=1\nP(zi|z<i, X),\n(4)\nwhere z<i\n=\n{z1, · · · , zi−1} indicates the pseudotranslation history, and P(zi|z<i, X) is calculated by a\nsingle-layer recurrent neural network.\nDecoder Module\nThe decoder module translates the\nsource sentence into the target translation with the guiding\nof pseudo-translation, which regards the translation of each\nword as NAT:\nP(Y|Z, X) =\nm\nY\ni=1\nP(yi|Z, X).\n(5)\nAs shown in Figure 1, the encoder module and the decoder module can be viewed as a seq-to-seq model which\ntranslates the source sentence to target sentence. Different\nfrom original NAT, the input of our decoder module is the\nembeddings of pseudo-translation instead of copied embeddings of source sentence, which is used to guide the word\nselection. Note that the encoder outputs are also fed into the\ndecoder attention module, which can help alleviate the reordering errors.\nTo make the model parameter number comparable with\nthe baseline model, we use K and N − K decoder blocks\nfor the reordering and decoder modules respectively 3.\nGuiding Decoding Strategy\nReorderNAT explicitly models reordering information of\nNAT and aims to utilize it to alleviate the multimodality\nproblem. Now the remaining problem is how to perform decoding with the guide of reordering information. We propose\n3We set K to 1 for an AT module while N − 1 for an NAT\nmodule as it is more difﬁcult for an NAT module to model the reordering information (see Experiments).\nto utilize the pseudo-translation as a bridge to guide the decoding of the target sentence, which can be formulated as:\nY∗\n=\narg max\nY\nP(Y|X)\n=\narg max\nY\nX\nZ\nP(Y|Z, X)P(Z|X).\n(6)\nIt is intractable to obtain an exact solution for maximizing\nEq. 6 due to the high time complexity. Inspired by the preordering works in statistical machine translation, we propose\na deterministic guiding decoding (DGD) strategy and a\nnon-deterministic guiding decoding (NDGD) strategy to\nsolve this problem.\nThe DGD strategy ﬁrst generates the most probable\npseudo-translation of the source sentence, and then translates the source sentence conditioned on it:\nZ∗\n=\narg max\nZ\nP(Z|X),\n(7)\nY∗\n=\narg max\nY\nP(Y|Z∗, X).\n(8)\nThe DGD strategy is simple and effective, but the hard\napproximation may bring in some noises.\nDifferent from the DGD strategy which utilizes a deterministic pseudo-translation, the NDGD strategy regards the\nprobability distribution Q of words to generate the most\nprobable pseudo-translations as a latent variable, and models\nthe translation as generating the target sentence according to\nQ, i.e., Eq. 8 is re-formulated as:\nY∗\n=\narg max\nY\nP(Y|Q, X),\n(9)\nwhere Q is deﬁned as:\nQ(zi) = P(zi|z∗\n<i, X) =\nexp\n\u0000\ns(zi)/T\n\u0001\nP\nz′\ni exp\n\u0000\ns(z′\ni)/T\n\u0001,\n(10)\nwhere s(·) is a score function of pseudo-translation (the input of softmax layer in the decoder) and T is a temperature\ncoefﬁcient. Since Q can be viewed as a non-deterministic\nform of the pseudo-translation, the translation with the\nNDGD strategy is also guided by the pseudo-translation.\nTo be speciﬁc, as shown in Figure 1, the major difference between DGD and NDGD strategy is the inputs of\ndecoder module (No. 2 dashed arrow), where the DGD\nstrategy directly utilizes the word embeddings of generated\npseudo-translation and the NDGD strategy utilizes the word\nembeddings weighted by the word probability of pseudotranslation.\nDiscussion\nIn ReorderNAT, the decoding space of generating pseudotranslation with reordering module is much smaller than\nthat of the whole translation in NAT since the decoding\nvocabulary is limited to the words in the source sentence.\nThe reordering module is more likely to be guided to one\npseudo-translation among multiple alternatives. Therefore,\nReorderNAT could easily capture the reordering information compared to the original NAT by explicitly modeling\nwith pseudo-translation as internal supervision. Besides, the\ncandidates of the i-th word of the ﬁnal translation can be\nnarrowed to the translations of zi to some extent since zi\nis the i-th word in the pseudo-translation which indicates\nthe corresponding source information of yi. In other words,\npseudo-translations could be viewed as a translation written in source language which helps the decoder to capture\nthe dependencies among target words and choose words belonging to the same translation.\nTraining\nIn the training process, for each training sentence pair\n(X, Y) ∈ D, where D is the training set, we ﬁrst generate\nits corresponding pseudo-translation ˆZ. And then ReorderNAT is optimized by maximizing a joint loss:\nL = LR + LT ,\n(11)\nwhere LR and LT indicate the reordering and translation\nlosses respectively. Formally, for both DGD and NDGD, the\nreordering loss LR is deﬁned as4 :\nLR =\nX\n(X,ˆZ,Y)∈D\nlog P(ˆZ|X).\n(12)\nFor the DGD approach, the translation loss is deﬁned\nas an overall maximum likelihood of translating pseudotranslation into the target sentence:\nLT =\nX\n(X,ˆZ,Y)∈D\nlog P(Y|ˆZ, X),\n(13)\nFor the NDGD approach, the translation loss is deﬁned as\nan overall maximum likelihood of decoding the target sentence from the conditional probability of pseudo-translation:\nLT =\nX\n(X,ˆZ,Y)∈D\nlog P(Y|Q, X).\n(14)\nIn particular, we use the trained model for the DGD approach to initialize the model for the NDGD approach since\nif Q is not well trained, LT will converge very slowly.",
        "experiments": "Datasets\nThe main experiments are conducted on three widely-used\nmachine translation tasks: WMT14 En-De (4.5M pairs),\nWMT16 En-Ro (610k pairs) and IWSLT16 En-De (196k\npairs)5. For WMT14 En-De task, we take newstest-2013\nand newstest-2014 as validation and test sets respectively.\nFor WMT16 En-Ro task, we employ newsdev-2016 and\nnewstest-2016 as validation and test sets respectively. For\nIWSLT16 En-De task, we use test2013 for validation.\nWe also conduct our experiments on Chinese-English\ntranslation which differs more in word order. The training\nset consists of 1.25M sentence pairs extracted from the LDC\ncorpora. We use NIST 2002 (MT02) as validation set, and\nNIST 2003 (MT03), 2004 (MT04), 2005 (MT05) as test sets.\n4Note that since Q(Z) = P(Z|X), the reordering loss could\nalso learn Q for the NDGD approach.\n5We use the prepossessed corpus provided by Lee, Mansimov,\nand Cho (2018) at https://github.com/nyu-dl/dl4mt-nonauto/tree/\nmultigpu.\nExperimental Settings\nWe use the fast align tool6 to generate the pseudo-translation\nin our experiments. We follow most of the model hyperparameter settings in (Gu et al. 2018; Lee, Mansimov, and Cho\n2018; Wei et al. 2019) for fair comparison. For IWSLT16\nEn-De, we use a 5-layer Transformer model (dmodel = 278,\ndhidden = 507, nhead = 2, pdropout = 0.1) and anneal the learning rate linearly (from 3 × 10−4 to 10−5) as\nin (Lee, Mansimov, and Cho 2018). For WMT14 En-De,\nWMT16 En-Ro and Chinese-English translation, we use a\n6-layer Transformer model (dmodel = 512, dhidden = 512,\nnhead = 8, pdropout = 0.1) and adopt the warm-up learning\nrate schedule (Vaswani et al. 2017) with twarmup = 4000.\nFor the GRU reordering module, we set it to have the same\nhidden size with the Transformer model in each dataset. We\nemploy label smoothing of value ϵls = 0.15 and utilize the\nsequence-level knowledge distillation (Kim and Rush 2016).\nFor each dataset, we select the optimal guiding decoding\nstrategy according to the model performance on validation\nsets. We also set T in Eq. 10 to 0.2 according to a grid\nsearch on the validation set.We measure the model inference\nspeedup on the validation set of IWSLT16 En-De task with\na NVIDIA P40 GPU and set batch size to 1.\nBaselines\nIn the experiments, we compare ReorderNAT (NAT) and ReorderNAT (AT) which utilize an NAT and an AT reordering\nmodules respectively with several baselines.\nWe select three models as our autoregressive baselines: (1) Transformerfull (Vaswani et al. 2017), which\nis the teacher model used in the knowledge distillation\nand of which the hyperparameters are described in experimental settings. (2) Transformerone, a lighter version of Transformer, of which the decoder layer number\nis 1. (3) Transformergru, which replaces the decoder of\nTransformerfull with a single-layer GRU (Cho et al. 2014).\nAnd we set the beam size to 4 in the experiments.\nBesides, we compare with several typical NAT models,\nwhich also have the ability to alleviate the multimodality\nproblem and are highly relevant to our work: (1) NATIR (Lee, Mansimov, and Cho 2018), which adopts an iterative reﬁnement mechanism enabling the model to perform inference based on surrounding words in the translation; (2) NAT-FS (Shao et al. 2019a), which introduces the\nautoregressive property to the top decoder layer of NAT;\n(3) FlowSeq-base (Ma et al. 2019), which uses generative\nﬂow to help model dependencies within target sentences. For\nfair comparison, We use the “base” version as it has comparable model size with our model; (4) imitate-NAT (Wei\net al. 2019), which imitates the behavior of an AT model. (5)\nCMLM-small (Ghazvininejad et al. 2019), which is built\non a conditional masked language model and also iteratively reﬁnes the translation. We use the “small” version for\nfair comparison; (6) NART-DCRF (Sun et al. 2019), which\nuses CRF to capture the word dependencies; (7) LevT (Gu,\nWang, and Zhao 2019), which models the sequence generation as multi-step insertion and deletion operations.\n6https://github.com/clab/fast align\nModel\nMulti-Step\nWMT14\nWMT16\nIWSLT16\nSpeedup\nEn→De\nDe→En\nEn→Ro\nRo→En\nEn→De\nAutoregressive Models\nTransformerfull\n27.17\n31.95\n32.86\n32.60\n31.18\n1.00×\nTransformerone\n25.52\n29.31\n30.61\n31.23\n29.52\n2.42×\nTransformergru\n26.27\n30.62\n30.41\n31.23\n29.26\n3.10×\nNon-Autoregressive Models\nNAT-IR (iter=1)\n13.91\n16.77\n24.45\n25.73\n22.20\n8.98×\nNAT-IR (iter=10)\n√\n21.61\n25.48\n29.32\n30.19\n27.11\n1.55×\nNAT-FS\n22.27\n27.25\n30.57\n30.83\n27.78\n3.38×\nFlowSeq-base\n21.45\n26.16\n29.34\n30.44\n<1.5×\nFlowSeq-base+NPD (s=30)\n23.48\n28.40\n31.75\n32.49\n<1.5×\nimitate-NAT\n22.44\n25.67\n28.61\n28.90\n28.41\n18.6×\nimitate-NAT+LPD (s=7)\n24.15\n27.28\n31.45\n31.81\n30.68\n9.70×\nCMLM-small (iter=10)\n√\n25.51\n29.47\n31.65\n32.27\n<1.5×\nNART-DCRF\n23.44\n27.22\n10.4×\nNART-DCRF+LPD (s=19)\n26.80\n30.04\n4.39×\nLevT\n√\n27.27\n33.26\n4.01×\nOur Models\nReorderNAT (NAT)\n22.79\n27.28\n29.30\n29.50\n25.29\n16.11×\nReorderNAT (NAT)+LPD (s=7)\n24.74\n29.11\n31.16\n31.44\n27.40\n7.40×\nReorderNAT (AT)\n26.49\n31.13\n31.70\n31.99\n30.26\n5.96×\nTable 1: Overall results of AT and NAT models in BLEU score on the test sets of WMT14 and WMT16, and validation set of\nIWSLT16. “DCRF” denotes a CRF layer with dynamic transition (Sun et al. 2019). “NPD” denotes noisy parallel decoding (Gu\net al. 2018), “LPD” denotes length parallel decoding (Wei et al. 2019), and “s” denotes sample size. “iter” denotes translation\nreﬁnement iterations. Better BLEU scores with low speedup are underlined.\nOverall Results\nWe compare ReorderNAT (NAT) and ReorderNAT (AT) that\nutilize an NAT reordering module and an AT reordering\nmodule respectively with all baseline models. All the results\nare shown in Table 1. From the table, we can ﬁnd that:\n(1) Excluding six better BLEU scores with signiﬁcant low\nspeedup, ReorderNAT (AT) achieves the best performance\non most of the benchmark datasets, which is even close to\nthe AT model with smaller than 1 BLEU gap (26.49 vs.\n27.17 on WMT14 En→De task, 31.99 vs. 32.60 on WMT16\nRo→En task, 30.26 vs. 31.18 on IWSLT16 En→De task).\nIt is also worth mentioning that although ReorderNAT utilizes a small AT model to better capture reordering information, it could still maintain low translation latency (about\n16× speedup for ReorderNAT (NAT) and 6× speedup\nfor ReorderNAT (AT)). Compared to Transformerone and\nTransformergru, ReorderNAT (AT) uses a much smaller vocabulary in the AT reordering module, which is limited to the\nwords in the source sentence and makes it faster. Besides,\nunlike NAT-IR, CMLM-small and LevT, our model can decode all target words in parallel without multiple iterations,\nwhich helps maintain the efﬁciency.\n(2) ReorderNAT (NAT) and ReorderNAT (NAT)+LPD\nalso gain signiﬁcant improvements compared to most existing NAT models. It veriﬁes the reordering information modeled by ReorderNAT could effectively guide its word selection.\n(3) A small AT model with close latency to large NAT\nmodels could perform much better in modeling reordering information7. On all benchmark datasets, ReorderNAT\n(AT) with small AT GRU reordering module achieves much\nbetter translation quality than that with large NAT model\n(2-5 BLEU scores). Moreover, we ﬁnd that the AT model\nTransformerone and Transformergru with a single-layer AT\nTransformer or GRU for decoding could also outperform\nmost existing NAT models while maintaining acceptable latency (2.42× and 3.10× speedup respectively). The reason\nis that a major potential performance degradation of NAT\nmodels compared to AT models comes from the difﬁculty\nof modeling the word order difference between source and\ntarget language, i.e., reordering information, which is neglected by most existing NAT models but can be well modeled by the small AT module8.\nResults on Chinese-English Translation\nTo show the effectiveness of modeling reordering information in NAT, we compare ReorderNAT with baselines on\nChinese-English translation since the word order between\nChinese and English is more different than that between German and English (En-De). From Table 2, we can ﬁnd that\n7The decoding speed of ReorderNAT (AT) is limited by the autoregressive property of the reordering module, which is the main\ndrawback of our model. How to further improve its speed is a future\ndirection we would like to pursue.\n8We conduct experiments and ﬁnd that our model outperforms\nthe AT model by a big margin when replacing the predicted pseudotranslation with the ground-truth ones. This also indicates the main\nmultimodality problem on NAT comes from the difﬁculty of modeling the reordering information.\nModel\nMT02*\nMT03\nMT04\nMT05\nAutoregressive Models\nTransformerfull\n46.11\n43.74\n45.59\n44.11\nTransformerone\n43.60\n41.24\n43.39\n41.62\nTransformergru\n43.68\n40.55\n43.02\n40.73\nNon-Autoregressive Models\nimitate-NAT\n33.77\n32.29\n34.83\n31.96\nReorderNAT (NAT)\n37.99\n36.03\n38.17\n36.07\nReorderNAT (AT)\n45.22\n43.20\n44.89\n43.45\nTable 2: BLEU scores on Chinese-English translation. * indicates the validation set.\n(0, 10]\n(10, 20]\n(20, 30]\n(30, 40]\n(40, 50]\n>50\nInput Sentence Length\n20\n25\n30\n35\n40\nBLEU score\nTransformer\nReorderNAT (AT)\nReorderNAT (NAT)\nNAT\nFigure 2: Translation quality on the IWSLT16 validation set\nover various input sentence lengths.\nin Chinese-English translation, ReorderNAT (AT) achieves\nmuch more improvements (6-7 BLEU points) compared to\nReorderNAT (NAT) and imitate-NAT. The reason is that\nmore different word order in Chinese-English translation\nmakes the decoding search space more complicated, which\ncould be effectively alleviated by ReorderNAT.\nTranslation Quality v.s. Sentence Lengths\nFigure 2 shows the BLEU scores of translations generated\nby the Transformer (AT model), the NAT model (ReorderNAT without the reordering module), ReorderNAT (NAT)\nand ReorderNAT (AT) on the IWSLT16 validation set with\nrespect to input sentence lengths. We can observe that:\n(1) ReorderNAT (NAT) and ReorderNAT (AT) achieve\nsigniﬁcant improvement compared to the NAT model for\nmost lengths and ReorderNAT (AT) achieves nearly comparable performance to Transformer. It veriﬁes the reordering\ninformation modeled by ReorderNAT could effectively help\nword selection and improve the translation quality.\n(2) ReorderNAT (AT) achieves much better translation\nperformance than the NAT model for sentences longer than\n20 words, of which word order tends to be more different. Together with the results on Chinese-English translation (Table 2), we can conclude that NAT is weak on word\nreordering and our model is more effective especially when\nword order is more different.\nModel\nBLEU\nRIBES\nDup\nMis\nTransformerfull\n31.18\n83.74\nNAT\n24.57\n82.21\n50.09\n9.09\nReorderNAT (NAT)\n25.29\n82.35\n37.52\n7.35\nReorderNAT (NAT)+LPD\n27.04\n83.21\n24.31\n5.59\nReorderNAT (AT)\n30.26\n83.55\n2.84\n0.52\nTable 3: Relative increment of duplicate (“Dup”) and missing (“Mis”) token ratios on the IWSLT16 validation set.\nSmaller is better.\nMultimodality Related Error Reduction\nIn this section, we investigate how our reordering module reduces the multimodality errors in NAT. Specially, we\nevaluate the RIBES (Isozaki et al. 2010) score and the reduction of duplicate and missing words (two most typical\nmultimodality related errors). The results are shown in Table 3, where “Dup” and “Mis” denote the relative increment of duplicate and missing token ratios compared with\nthe Transformerfull model respectively9 , and NAT is ReorderNAT without the reordering module. From the table,\nwe can observe that:\n(1) Our three ReorderNAT models achieve higher RIBES\nscores than the NAT model, validating our reordering module can help capture the word order difference between\nsource and target languages. Moreover, the ReorderNAT\n(AT) model performs the best in RIBES, indicating the AT\nreordering module can model reordering information more\neffectively than the NAT reordering module.\n(2) Compared with the NAT model, both Dup and Mis\nare signiﬁcantly better for the three ReorderNAT models,\nindicating ReorderNAT is effective for alleviating the multimodality problem.\nCase Study\nTable 4 shows example translations of the NAT model, ReorderNAT (NAT) and ReorderNAT (AT). We ﬁnd the problem of missing and duplicate words are severe in the translation (both 5 occurrences) of the NAT model, while this\nproblem is effectively alleviated by ReorderNAT. Moreover,\nwe ﬁnd that most of the missing, duplicate or wrong words\nin the translation of our two ReorderNAT models come from\nthe errors in the pseudo-translation, demonstrating that NAT\nmodels could well translate the pseudo-translation which is\nin the the target language word order, and the remaining\nproblem of NAT lies on modeling reordering information.",
        "related work": "Non-Autoregressive Neural Machine Translation\nGu et al. (2018) ﬁrst proposed the non-autoregressive neural\nmachine translation (NAT), which enables parallel decoding\nfor neural machine translation (NMT) and signiﬁcantly accelerates the inference of NMT. However, its performance\n9The formal deﬁnition of metrics “Dup/Mis” can be found in\nRan et al. (2020).\nSource\neventually , after a period of six months of brutal war and a toll rate of almost 50,000 dead , we managed\nto liber ate our country and to t opp le the ty rant\nReference\nschließlich , nach einem Zeitraum von sechs Monaten bru talen Krieges und fast 50.000 Toten , gelang\nes uns , unser Land zu befreien und den Tyran nen zu st¨urzen .\nNAT\nTranslation schließlich , nach einer [ ] von sechs Monaten bru bru [ ] Krieg und einer Z rate fast 50.000 50.000\n[ ] , schafften wir es geschafft , unser Land [ ] befreien befreien und den Ty r ann [ ] ann entgegen entgegen deln .\nReorderNAT\n(NAT)\nPseudoTranslation\neventually , after a period of six [ ] brutal brutal war and a toll of almost 50,000 dead , managed we\nmanaged managed, [ ] country country to liber [ ] and the ty ty rant rant opp opp opp .\nTranslation schließlich , nach einer Zeit von sechs [ ] bru talen Krieges und einer Z von fast 50.000 Toten , schafften\nwir es geschafft , unser Land zu befreien und den Ty r r ten zu zu ieren .\nReorderNAT\n(AT)\nPseudoTranslation\neventually , after a period of six months brutal brutal war and a toll toll rate of almost 50,000 dead ,\nmanaged we managed , our country to liber [ ] and the ty ty rant to liber .\nTranslation schließlich , nach einer Zeitraum von sechs Monaten bru talen Krieg und einer Z oll rate von fast 50.000\nToten , schafften wir es , unser Land zu befreien und den Ty r ann zu reparieren .\nTable 4: Translation examples of NAT baseline and ReorderNAT. We use “[ ]” to denote missing words and underline the wrong\nwords. We use\nto concatenate sub-words.\ndegrades greatly since it discards the sequential dependencies among target words. Recently, a variety of works have\nbeen investigated to improve its performance including (Guo\net al. 2019a; Bao et al. 2019) which enhance the representation of decoder with source information; (Libovick´y and\nHelcl 2018; Shao et al. 2019a,b; Ghazvininejad et al. 2020)\nwhich optimize models with respect to sequence-level loss\nfunctions; (Wang et al. 2019; Li et al. 2020) which attempt\nto solve the multimodality problem using regularization or\nspeical decoding strategies; (Li et al. 2019; Wei et al. 2019;\nGuo et al. 2019b; Liu et al. 2020; Sun and Yang 2020)\nwhich use AT models to guide the learning of NAT models; (Kaiser et al. 2018; Akoury, Krishna, and Iyyer 2019;\nLee, Shu, and Cho 2020) which introduce latent variables\nto guide the decoding process of NAT models; and (Wang,\nZhang, and Chen 2018; Lee, Mansimov, and Cho 2018; Gu,\nWang, and Zhao 2019; Stern et al. 2019; Ghazvininejad et al.\n2019; Ghazvininejad, Levy, and Zettlemoyer 2020; Kasai\net al. 2020; Tu et al. 2020; Guo, Xu, and Chen 2020; Ran\net al. 2020) which extend one-step NAT to multi-step NAT\nand generate translations iteratively. Different from existing works, we propose to explicitly model reordering information in NAT models, which serves as a proxy in capturing target word dependencies and encourages the decoder\nto choose words belonging to the same translation to alleviate the multimodality problem. This work intends to enhance the translation quality of one-step NAT models and\nhas the potential to improve the performance of each iteration of multi-step NAT methods without loss of efﬁciency.\nModelling Reordering Information in Machine\nTranslation\nRe-ordering model is a key component in statistical machine translation (SMT), which handles word order differences between source and target languages. There has been\na number of works focusing on word reordering in SMT, including deterministic reordering methods (Xia and McCord\n2004; Collins, Koehn, and Kuˇcerov´a 2005; Wang, Collins,\nand Koehn 2007; Li et al. 2007), which ﬁnd an optimal\nreordering of source words; non-deterministic reordering\nmethods (Kanthak et al. 2005; Zhang, Zens, and Ney 2007)\nwhich encode multiple alternative reorderings into a word\nlattice and remain the selection of best path in the decoder;\nand target word reordering methods (Bangalore, Haffner,\nand Kanthak 2007) which ﬁrst select target lexicals and\nthen reorder them to form ﬁnal sentence. In neural machine\ntranslation (NMT), it has been shown the attention mechanism (Bahdanau, Cho, and Bengio 2014) could implicitly\ncapture reordering information to some extent. Zhang et al.\n(2017) presented three distortion models to further incorporate reordering knowledge into attention-based NMT models. Chen et al. (2019) proposed to learn reordering embedding of a word based on its contextual information. Except\nfor incorporating reordering knowledge in attention mechanism, researchers also proposed to learn to reorder source\nwords according to target sentence structures with neural\nnetworks (Du and Way 2017; Kawara, Chu, and Arase 2018;\nZhao, Zhang, and Zong 2018). This work empirically justiﬁes reordering information is essential for NAT.",
        "conclusion": "In this work, to address the multimodality problem in NAT,\nwe propose a novel NAT framework named ReorderNAT\nwhich explicitly models the reordering information in the\ndecoding procedure. We further introduce deterministic and\nnon-deterministic guiding decoding strategies to utilize the\nreordering information to encourage the decoder to choose\nwords belonging to the same translation. Experimental results on several widely-used benchmarks show that our ReorderNAT model achieves better performance than most existing NAT models, and even achieves comparable translation quality as AT model with a signiﬁcant speedup. We believe that to well model the reordering information is a potential way towards better NAT.",
        "summary_en": "Non-autoregressive neural machine translation (NAT) generates each target word in parallel and has achieved promising inference acceleration. However, existing NAT models still have a big gap in translation quality compared to autoregressive neural machine translation models due to the multimodality problem: the target words may come from multiple feasible translations. To address this problem, this paper proposes a novel NAT framework ReorderNAT which explicitly models the reordering information to guide the decoding of NAT. Specially, ReorderNAT utilizes deterministic and non-deterministic decoding strategies that leverage reordering information as a proxy for the final translation to encourage the decoder to choose words belonging to the same translation. Experimental results on various widely-used datasets show that the proposed model achieves better performance compared to most existing NAT models, and even achieves comparable translation quality as autoregressive translation models with a significant speedup.",
        "summary_zh": "这篇论文介绍了一种新颖的非自回归神经机器翻译NAT框架ReorderNAT。旨在解决由于多模态问题导致的现有NAT模型在翻译质量上与自回归神经机器翻译模型存在较大差距的问题。ReorderNAT明确地建立了重排序信息模型，以指导NAT的解码。特别是，ReorderNAT利用确定性和非确定性解码策略，利用重新排序信息作为最终翻译的代理，以促使解码器选择属于同一翻译的单词。在各种广泛使用的数据集上的实验结果表明，与大多数现有的NAT 模型相比，该模型实现了更好的性能，甚至达到了与自回归翻译模型相当的翻译质量，而且速度明显加快。"
    },
    {
        "title": "Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference Using a Delta Posterior",
        "abstract": "Although neural machine translation models reached high translation quality, the autoregressive nature makes inference difﬁcult to parallelize and leads to high translation latency. Inspired by recent reﬁnement-based approaches, we propose LaNMT, a latent-variable non-autoregressive model with continuous latent variables and deterministic inference procedure. In contrast to existing approaches, we use a deterministic inference algorithm to ﬁnd the target sequence that maximizes the lowerbound to the log-probability. During inference, the length of translation automatically adapts itself. Our experiments show that the lowerbound can be greatly increased by running the inference algorithm, resulting in signiﬁcantly improved translation quality. Our proposed model closes the performance gap between non-autoregressive and autoregressive approaches on ASPEC Ja-En dataset with 8.6x faster decoding. On WMT’14 En-De dataset, our model narrows the gap with autoregressive baseline to 2.0 BLEU points with 12.5x speedup. By decoding multiple initial latent variables in parallel and rescore using a teacher model, the proposed model further brings the gap down to 1.0 BLEU point on WMT’14 En-De task with 6.8x speedup.",
        "introduction": "The ﬁeld of Neural Machine Translation (NMT) has seen\nsigniﬁcant improvements in recent years (Bahdanau, Cho,\nand Bengio 2015; Wu et al. 2016; Gehring et al. 2017;\nVaswani et al. 2017). Despite impressive improvements in\ntranslation accuracy, the autoregressive nature of NMT models have made it difﬁcult to speed up decoding by utilizing parallel model architecture and hardware accelerators.\nThis has sparked interest in non-autoregressive NMT models, which predict every target tokens in parallel. In addition\nto the obvious decoding efﬁciency, non-autoregressive text\ngeneration is appealing as it does not suffer from exposure\nbias and suboptimal inference.\nInspired by recent work in non-autoregressive NMT using discrete latent variables (Kaiser et al. 2018) and iterative reﬁnement (Lee, Mansimov, and Cho 2018), we introduce a sequence of continuous latent variables to capture\nthe uncertainty in the target sentence. We motivate such a\nlatent variable model by conjecturing that it is easier to reﬁne lower-dimensional continuous variables1 than to reﬁne\nhigh-dimensional discrete variables, as done in Lee, Mansimov, and Cho (2018). Unlike Kaiser et al. (2018), the posterior and the prior can be jointly trained to maximize the\nevidence lowerbound of the log-likelihood log p(y|x).\nIn this work, we propose a deterministic iterative algorithm to reﬁne the approximate posterior over the latent variables and obtain better target predictions. During inference,\nwe ﬁrst obtain the initial posterior from a prior distribution\np(z|x) and the initial guess of the target sentence from the\nconditional distribution p(y|x, z). We then alternate between\nupdating the approximate posterior and target tokens with\nthe help of an approximate posterior q(z|x, y). We avoid\nstochasticity at inference time by introducing a delta posterior over the latent variables. We empirically ﬁnd that this iterative algorithm signiﬁcantly improves the lowerbound and\nresults in better BLEU scores. By reﬁning the latent variables instead of tokens, the length of translation can dynamically adapt throughout this procedure, unlike previous approaches where the target length was ﬁxed throughout the\nreﬁnement process. In other words, even if the initial length\nprediction is incorrect, it can be corrected simultaneously\nwith the target tokens.\nOur models2 outperform the autoregressive baseline on\nASPEC Ja-En dataset with 8.6x decoding speedup and bring\nthe performance gap down to 2.0 BLEU points on WMT’14\nEn-De with 12.5x decoding speedup. By decoding multiple\nlatent variables sampled from the prior and rescore using a\nautoregressive teacher model, the proposed model is able to\nfurther narrow the performance gap on WMT’14 En-De task\ndown to 1.0 BLEU point with 6.8x speedup. The contributions of this work can be summarize as follows:\n1. We\npropose\na\ncontinuous\nlatent-variable\nnonautoregressive NMT model for faster inference. The\nmodel learns identical number of latent vectors as the\ninput tokens. A length transformation mechanism is\ndesigned to adapt the number of latent vectors to match\nthe target length.\n1We use 8-dimensional latent variables in our experiments.\n2Our code can be found in https://github.com/zomux/lanmt .\n2. We demonstrate a principle inference method for this\nkind of model by introducing a deterministic inference\nalgorithm. We show the algorithm converges rapidly in\npractice and is capable of improving the translation quality by around 2.0 BLEU points.",
        "background": "Autoregressive NMT\nIn order to model the joint probability of the target tokens\ny1, · · · , y|y| given the source sentence x, most NMT models\nuse an autoregressive factorization of the joint probability\nwhich has the following form:\nlog p(y|x) =\n|y|\n\u0002\ni=1\nlog p(yi|y<i, x),\n(1)\nwhere y<i denotes the target tokens preceding yi. Here, the\nprobability of emitting each token p(yi|y<i, x) is parameterized with a neural network.\nTo obtain a translation from this model, one could predict target tokens sequentially by greedily taking argmax\nof the token prediction probabilities. The decoding progress\nends when a “</s>” token, which indicates the end of a\nsequence, is selected. In practice, however, this greedy approach yields suboptimal sentences, and beam search is often used to decode better translations by maintaining multiple hypotheses. However, decoding with a large beam size\nsigniﬁcantly decreases translation speed.\nNon-Autoregressive NMT\nAlthough autoregressive models achieve high translation\nquality through recent advances in NMT, the main drawback\nis that autoregressive modeling forbids the decoding algorithm to select tokens in multiple positions simultaneously.\nThis results in inefﬁcient use of computational resource and\nincreased translation latency.\nIn contrast, non-autoregressive NMT models predict target tokens without depending on preceding tokens, depicted\nby the following objective:\nlog p(y|x) =\n|y|\n\u0002\ni=i\nlog p(yi|x).\n(2)\nAs the prediction of each target token yi now depends only\non the source sentence x and its location i in the sequence,\nthe translation process can be easily parallelized. We obtain\na target sequence by applying argmax to all token probabilities.\nThe main challenge of non-autoregressive NMT is on capturing dependencies among target tokens. As the probability\nof each target token does not depend on the surrounding tokens, applying argmax at each position i may easily result in\nan inconsistent sequence, that includes duplicated or missing\nwords. It is thus important for non-autoregressive models\nto apply techniques to ensure the consistency of generated\nwords.\ny\nx\nx\nq(z|x, y)\nlength transform\np(z|x)\nNx\np(y|x, z, ly)\np(ly|z)\nself-attention\nfeed-forward\nself-attention\nfeed-forward\ncross-attention\nNx\nNx\nself-attention\nfeed-forward\nNx\nself-attention\nfeed-forward\ncross-attention\nlinear\nlinear\nlinear\nsoftmax\nlength transform\nlinear\nsoftmax\nmean pool\nFigure 1: Architecture of the proposed non-autogressive\nmodel. The model is composed of four components: prior\np(z|x), approximate posterior q(z|x, y), length predictor\np(ly|z) and decoder p(y|x, z). These components are trained\nend-to-end to maximize the evidence lowerbound.",
        "latent-variable non-autoregressive nmt": "In this work, we propose LaNMT, a latent-variable nonautoregressive NMT model by introducing a sequence of\ncontinuous latent variables to model the uncertainty about\nthe target sentence. These latent variables z are constrained\nto have the same length as the source sequence, that is,\n|z| = |x|. Instead of directly maximizing the objective function in Eq. (2), we maximize a lowerbound to the marginal\nlog-probability log p(y|x) = log\n\u0003\np(y|z, x)p(z|x)dz:\nL(ω, φ, θ) = Ez∼qφ\n\u0004\nlog pθ(y|x, z)\n\u0005\n− KL\n\u0004\nqφ(z|x, y)||pω(z|x)\n\u0005\n,\n(3)\nwhere pω(z|x) is the prior, qφ(z|x, y) is an approximate posterior and pθ(y|x, z) is the decoder. The objective function\nin Eq. (3) is referred to as the evidence lowerbound (ELBO).\nAs shown in the equation, the lowerbound is parameterized\nby three sets of parameters: ω, φ and θ.\nBoth the prior pω and the approximate posterior qφ are\nmodeled as spherical Gaussian distributions. The model\ncan be trained end-to-end with the reparameterization trick\n(Kingma and Welling 2014).\nA Modiﬁed Objective Function with Length\nPrediction\nDuring training, we want the model to maximize the lowerbound in Eq. (3). However, to generate a translation, the target length ly has to be predicted ﬁrst. We let the latent variables model the target length by parameterizing the decoder\nas:\npθ(y|x, z) =\n\u0002\nl\npθ(y, l|x, z)\n= pθ(y, ly|x, z)\n= pθ(y|x, z, ly)pθ(ly|z).\n(4)\nHere ly denotes the length of y. The second step is valid\nas the probability pθ(y, l ̸= ly|x, z) is always zero. Plugging\nin Eq. (4), with the independent assumption on both latent\nvariables and target tokens, the objective has the following\nform:\nEz∼qφ\n\u0004 |y|\n\u0002\ni=1\nlog pθ(yi|x, z, ly) + log pθ(ly|z)\n\u0005\n−\n|x|\n\u0002\nk=1\nKL\n\u0004\nqφ(zk|x, y)||pω(zk|x)\n\u0005\n.\n(5)\nModel Architecture\nAs evident from in Eq. (5), there are four parameterized components in our model: the prior pω(z|x), approximate posterior qφ(z|x, y), decoder pθ(y|x, z, ly) and length\npredictor pθ(ly|z). The architecture of the proposed nonautoregressive model is depicted in Fig. 1, which reuses\nmodules in Transformer (Vaswani et al. 2017) to compute\nthe aforementioned distributions.\nMain Components\nTo compute the prior pω(z|x), we\nuse a multi-layer self-attention encoder which has the same\nstructure as the Transformer encoder. In each layer, a feedforward computation is applied after the self-attention. To\nobtain the probability, we apply a linear transformation to\nreduce the dimensionality and compute the mean and variance vectors.\nFor the approximate posterior qφ(z|x, y), as it is a function of the source x and the target y, we ﬁrst encode y with\na self-attention encoder. Then, the resulting vectors are fed\ninto an attention-based decoder initialized by x embeddings.\nIts architecture is similar to the Transformer decoder except\nthat no causal mask is used. Similar to the prior, we apply a\nlinear layer to obtain the mean and variance vectors.\nTo backpropagate the loss signal of the decoder to qφ, we\napply the reparameterization trick to sample z from qφ with\ng(ϵ, q) = μq + σq ∗ ϵ. Here, ϵ ∼ N(0, 1) is Gaussian noise.\nThe decoder computes the probability pθ(y|x, z, ly) of\noutputting target tokens y given the latent variables sampled\nfrom qφ(z|x, y). The computational graph of the decoder is\nalso similar to the Transformer decoder without using causal\nmask. To combine the information from the source tokens,\nwe reuse the encoder vector representation created when\ncomputing the prior.\nLength Prediction and Transformation\nGiven a latent\nvariable z sampled from the approximate posterior qφ, we\ntrain a length prediction model pθ(ly|z). We train the model\nto predict the length difference between |y| and |x|. In\nour implementation, pθ(ly|z) is modeled as a categorical\nz1 z2 z3\nz4\n¯z1\n¯z2\n¯z3\n¯z4\n¯z5\n+\nz7\nz5\nz6\nw2\ni\nFigure 2: Illustration of the length transformation mechanism.\ndistribution that covers the length difference in the range\n[−50, 50]. The prediction is produced by applying softmax\nafter a linear transformation.\nAs the latent variable z ∼ qφ(z|x, y) has the length |x|,\nwe need to transform the latent variables into ly vectors for\nthe decoder to predict target tokens. We use a monotonic\nlocation-based attention for this purpose, which is illustrated\nin Fig. 2. Let the resulting vectors of length transformation\nbe ¯z1, ..., ¯zly. we produce each vector with\n¯zj =\n|x|\n\u0002\nk=1\nwj\nkzk,\n(6)\nwj\nk =\nexp(aj\nk)\n\u0006|x|\nk′=1 exp(aj\nk′)\n,\n(7)\naj\nk = −\n1\n2σ2 (k − |x|\nly\nj)2,\n(8)\nwhere each transformed vector is a weighted sum of the latent variables. The weight is computed with a softmax over\ndistance-based logits. We give higher weights to the latent\nvariables close to the location |x|\nly j. The scale σ is the only\ntrainable parameter in this monotonic attention mechanism.\nTraining\nIf we train a model with the objective function in Eq. (5),\nthe KL divergence often drops to zero from the beginning.\nThis yields a degenerate model that does not use the latent\nvariables at all. This is a well-known issue in variational\ninference called posterior collapse (Bowman et al. 2015;\nDieng et al. 2018; Razavi et al. 2019). We use two techniques to address this issue. Similarly to Kingma, Salimans,\nand Welling (2016), we give a budget to the KL term as\n|x|\n\u0002\nk=1\nmax(b, KL\n\u0004\nqφ(zk|x, y)||pω(zk|x)\n\u0005\n),\n(9)\nwhere b is the budget of KL divergence for each latent variable. Once the KL value drops below b, it will not be minimized anymore, thereby letting the optimizer focus on the\nreconstruction term in the original objective function. As b is\na critical hyperparameter, it is time-consuming to search for\na good budget value. Here, we use the following annealing\nschedule to gradually lower the budget:\nb =\n\u0007\n1,\nif s < M/2\n(M−s)\nM/2 ,\notherwise\n(10)\ns is the current step in training, and M is the maximum step.\nIn the ﬁrst half of the training, the budget b remains 1. In the\nsecond half of the training, we anneal b until it reaches 0.\nSimilarly to previous work on non-autoregressive NMT,\nwe apply sequence-level knowledge distillation (Kim and\nRush 2016) where we use the output from an autoregressive\nmodel as target for our non-autoregressive model.",
        "inference with a delta posterior": "Once the training has converged, we use an inference algorithm to ﬁnd a translation y that maximizes the lowerbound\nin Eq. (3):\nargmax\ny\nEz∼qφ\n\u0004\nlog pθ(y|x, z)\n\u0005\n− KL\n\u0004\nqφ(z|x, y)||pω(z|x)\n\u0005\nIt is intractable to solve this problem exactly due to the\nintractability of computing the ﬁrst expectation. We avoid\nthis issue in the training time by reparametrization-based\nMonte Carlo approximation. However, it is desirable to\navoid stochasticity at inference time where our goal is to\npresent a single most likely target sentence given a source\nsentence.\nWe tackle this problem by introducing a proxy distribution r(z) deﬁned as\nr(z) =\n\b1,\nif z = μ\n0,\notherwise\nThis is a Dirac measure, and we call it a delta posterior in\nour work. We set this delta posterior to minimize the KL\ndivergence against the approximate posterior qφ, which is\nequivalent to\n∇μ log qφ(μ|x, y) = 0 ⇔ μ = Eqφ [z] .\n(11)\nWe then use this proxy instead of the original approximate\nposterior to obtain a deterministic lowerbound:\nˆL(ω, θ, μ) = log pθ(y|x, z = μ) − log pω(μ|x).\nAs the second term is constant with respect to y, maximizing\nthis lowerbound with respect to y reduces to\nargmax\ny\nlog pθ(y|x, z = μ),\n(12)\nwhich can be approximately solved by beam search when\npθ is an autoregressive sequence model. If pθ factorizes over\nthe sequence y, as in our non-autoregressive model, we can\nsolve it exactly by\nˆyi = argmax\nyi\nlog pθ(yi|x, z = μ).\nWith every estimation of y, the approximate posterior q\nchanges. We thus alternate between ﬁtting the delta posterior in Eq. (11) and ﬁnding the most likely sequence y in\nEq. (12).\nAlgorithm 1 Deterministic Iterative Inference\nInputs:\nx : source sentence\nT : maximum step\nμ0 = Epω(z|x) [z]\ny0 = argmaxy log pθ(y|x, z = μ0)\nfor t ← 1 to T do\nμt = Eqφ(z|x,yt−1) [z]\nyt = argmaxy log pθ(y|x, z = μt)\nif yt = yt−1 then\nbreak\noutput yt\nWe initialize the delta posterior r using the prior distribution:\nμ = Epω(z|x) [z] .\nWith this initialization, the proposed inference algorithm is\nfully deterministic. The complete inference algorithm for\nobtaining the ﬁnal translation is shown in Algorithm 1.",
        "related work": "This work is inspired by a recent line of work in nonautoregressive NMT. Gu et al. (2018) ﬁrst proposed a nonautoregressive framework by modeling word alignment as\na latent variable, which has since then been improved by\nWang et al. (2019). Lee, Mansimov, and Cho (2018) proposed a deterministic iterative reﬁnement algorithm where\na decoder is trained to reﬁne the hypotheses. Our approach is most related to Kaiser et al.; Roy et al. (2018;\n2018). In both works, a discrete autoencoder is ﬁrst trained\non the target sentence, then an autoregressive prior is trained\nto predict the discrete latent variables given the source sentence. Our work is different from them in three ways: (1)\nwe use continuous latent variables and train the approximate\nposterior q(z|x, y) and the prior p(z|x) jointly; (2) we use\na non-autoregressive prior; and (3) the reﬁnement is performed in the latent space, as opposed to discrete output\nspace (as done in most previous works using reﬁnement for\nnon-autoregressive machine translation).\nConcurrently to our work, Ghazvininejad et al. (2019)\nproposed to translate with a masked-prediction language\nmodel by iterative replacing tokens with low conﬁdence. Gu,\nLiu, and Cho; Stern et al.; Welleck et al. (2019; 2019; 2019)\nproposed insertion-based NMT models that insert words to\nthe translations with a speciﬁc strategy. Unlike these works,\nour approach performs reﬁnements in the low-dimensional\nlatent space, rather than in the high-dimensional discrete\nspace.\nSimilarly to our latent-variable model, Zhang, Xiong, and\nSu (2016) proposed a variational NMT, and Shah and Barber\n(2018) and Eikema and Aziz (2018) models the joint distribution of source and target. Both of them use autoregressive\nmodels. Shah and Barber (2018) designed an EM-like algorithm similar to Markov sampling (Arulkumaran, Creswell,\nand Bharath 2017). In contrast, we propose a deterministic\nalgorithm to remove any non-determinism during inference.\nASPEC Ja-En\nWMT’14 En-De\nBLEU(%) speedup wall-clock (std) BLEU(%) speedup wall-clock (std)\nBase Transformer, beam size=3\n27.1\n1x\n415ms (159)\n26.1\n1x\n602ms (274)\nBase Transformer, beam size=1\n24.6\n1.1x\n375ms (150)\n25.6\n1.3x\n461ms (219)\nLatent-Variable NAR Model\n13.3\n17.0x\n24ms (2)\n11.8\n22.2x\n27ms (1)\n+ knowledge distillation\n25.2\n17.0x\n24ms (2)\n22.2\n22.2x\n27ms (1)\n+ deterministic inference\n27.5\n8.6x\n48ms (2)\n24.1\n12.5x\n48ms (8)\n+ latent search\n28.3\n4.8x\n86ms (2)\n25.1\n6.8x\n88ms (8)\nTable 1: Comparison of the proposed non-autoregressive (NAR) models with the autoregressive baselines. Our implementation\nof the Base Transformer is 1.0 BLEU point lower than the original paper (Vaswani et al. 2017) on WMT’14 dataset.",
        "experimental settings": "Data and preprocessing\nWe evaluate our model on two\nmachine translation datasets: ASPEC Ja-En (Nakazawa et al.\n2016) and WMT’14 En-De (Bojar et al. 2014). The ASPEC\ndataset contains 3M sentence pairs, and the WMT’14 dataset\ncontains 4.5M senence pairs.\nTo preprocess the ASPEC dataset, we use Moses\ntoolkit (Koehn et al. 2007) to tokenize the English sentences,\nand Kytea (Neubig, Nakata, and Mori 2011) for Japanese\nsentences. We further apply byte-pair encoding (Sennrich,\nHaddow, and Birch 2016) to segment the training sentences\ninto subwords. The resulting vocabulary has 40K unique\ntokens on each side of the language pair. To preprocess\nthe WMT’14 dataset, we apply sentencepiece (Kudo and\nRichardson 2018) to both languages to segment the corpus\ninto subwords and build a joint vocabulary. The ﬁnal vocabulary size is 32K for each language.\nLearning\nTo train the proposed non-autoregressive models, we adapt the same learning rate annealing schedule as\nthe Base Transformer. Model hyperparameters are selected\nbased on the validation ELBO value.\nThe only new hyperparameter in the proposed model is\nthe dimension of each latent variable. If each latent is a highdimension vector, although it has a higher capacity, the KL\ndivergence in Eq. (3) becomes difﬁcult to minimize. In practice, we found that latent dimensionality values between 4\nand 32 result in similar performance. However, when the dimensionality is signiﬁcantly higher or lower, we observed a\nperformance drop. In all experiments, we set the latent dimensionionality to 8. We use a hidden size of 512 and feedforward ﬁlter size of 2048 for all models in our experiments.\nWe use 6 transformer layers for the prior and the decoder,\nand 3 transformer layers for the approximate posterior.\nEvaluation\nWe evaluate the tokenized BLEU for ASPEC\nJa-En datset. For WMT’14 En-De datset, we use SacreBLEU (Post 2018) to evaluate the translation results. We follow Lee, Mansimov, and Cho (2018) to remove repetitions\nfrom the translation results before evaluating BLEU scores.\nLatent Search\nTo further exploit the parallelizability of\nGPUs, we sample multiple initial latent variables from the\nprior pω(z|x). Then we perform the deterministic inference\non each latent variable to obtain a list of candidate translations. However, we can not afford to evaluate each candidate using Eq. (5), which requires importance sampling\non qφ. Instead, we use the autoregressive baseline model\nto score all the candidates, and pick the candidate with the\nhighest log probability. Following Parmar et al. (2018), we\nreduce the temperature by a factor of 0.5 when sampling latent variables, resulting in better translation quality. To avoid\nstochasticity, we ﬁx the random seed during sampling.",
        "result and analysis": "Quantitative Analysis\nOur quantitative results on both datasets are presented in Table 1. The baseline model in our experiments is a base Transformer. Our implementation of the autoregressive baseline is\n1.0 BLEU points lower than the original paper (Vaswani et\nal. 2017) on WMT’14 En-De datase. We measure the latency\nof decoding each sentence on a single NVIDIA V100 GPU\nfor all models, which is averaged over all test samples.\nAs shown in Table 1, without knowledge distillation, we\nobserve a signiﬁcant gap in translation quality compared to\nthe autoregressive baseline. This observation is in line with\nprevious works on non-autoregressive NMT (Gu et al. 2018;\nLee, Mansimov, and Cho 2018; Wang et al. 2019). The gap\nis signiﬁcantly reduced by using knowledge distillation, as\ntranslation targets provided by the autoregressive model are\neasier to predict.\nWith the proposed deterministic inference algorithm, we\nsigniﬁcantly improve translation quality by 2.3 BLEU points\non ASPEC Ja-En dataset and 1.9 BLEU points on WMT’14\nEn-De dataset. Here, we only run the algorithm for one step.\nWe observe gain on ELBO by running more iterative steps,\nwhich is however not reﬂected by the BLEU scores. As a\nresult, we outperform the autoregressive baseline on ASPEC dataset with a speedup of 8.6x. For WMT’14 dataset,\nalthough the proposed model reaches a speedup of 12.5x,\nthe gap with the autoregressive baseline still remains, at 2.0\nBLEU points. We conjecture that WMT’14 En-De is more\ndifﬁcult for our non-autoregressive model as it contains a\nhigh degree of noise (Ott et al. 2018).\nBy searching over multiple initial latent variables and\nrescoring with the teacher Transformer model, we observe\nan increase in performance by 0.7 ∼ 1.0 BLEU score at\nthe cost of lower translation speed. In our experiments, we\nBLEU(%) speed-up\nTransformer (Vaswani et al. 2017) 27.1\nBaseline (Gu et al. 2018) 23.4\n1x\nNAT (+FT +NPD S=100) 19.1 (-4.3)\n2.3x\nBaseline (Lee, Mansimov, and Cho 2018) 24.5\n1x\nAdaptive NAR Model 21.5 (-3.0)\n1.9x\nBaseline (Kaiser et al. 2018) 23.5\n1x\nLT, Improved Semhash 19.8 (-3.7)\n3.8x\nBaseline (Wang et al. 2019) 27.3\n1x\nNAT-REG, no rescoring 20.6 (-6.7)\n27.6x⋆\nNAT-REG, autoregressive rescoring 24.6 (-2.7)\n15.1x⋆\nBaseline (Ghazvininejad et al. 2019) 27.8\n1x\nCMLM with 4 iterations 26.0 (-1.8)\nCMLM with 10 iterations 26.9 (-0.9)\n2∼3x\nBaseline (Ma et al. 2019) 27.1\nFlowSeq-large (NPD n = 30) 25.3 (-1.8)\nBaseline (Ours) 26.1\n1x\nNAR with deterministic Inference 24.1 (-2.0)\n12.5x\n+ latent search 25.1 (-1.0)\n6.8x\nTable 2: A comparison of non-autoregressive NMT models on WMT’14 En-De dataset in BLEU(%) and decoding\nspeed-up. ⋆ measured on IWSLT’14 DE-EN dataset.\nsample 50 candidate latent variables and decode them in parallel. The slowdown is mainly caused by rescoring. With the\nhelp of rescoring, our ﬁnal model further narrows the performance gap with the autoregressive baseline to 1.0 BLEU\nwith 6.8x speedup on WMT’14 En-De task.\nNon-autoregressive NMT Models\nIn Table 2, we list the results on WMT’14 En-De by existing non-autoregressive NMT approaches. All the models use\nTransformer as their autoregressive baselines. In comparison, our proposed model suffers a drop of 1.0 BLEU points\nover the baseline, which is a relatively small gap among the\nexisting models. Thanks to the rapid convergence of the proposed deterministic inference algorithm, our model achieves\na higher speed-up compared to other reﬁnement-based models and provides a better speed-accuracy tradeoff.\nConcurrently to our work, the mask-prediction language\nmodel (Ghazvininejad et al. 2019) was found to reduce the\nperformance gap down to 0.9 BLEU on WMT’14 En-De\nwhile still maintaining a reasonable speed-up. The main difference is that we update a delta posterior over latent variables instead of target tokens. Both Ghazvininejad et al.\n(2019) and Wang et al. (2019) with autoregressive rescoring decode multiple candidates in batch and choose one ﬁnal\ntranslation from them. FlowSeq (Ma et al. 2019) is an recent\ninteresting work on ﬂow-based prior. With noisy parallel decoding, FlowSeq can be fairly compared to the latent search\nsetting of our model. In Table 2, we can see that our model\nis equivalently or more effective without a ﬂow-based prior.\nIt is intriguing to see a combination with the ﬂow approach.\nAnalysis of Deterministic Inference\nConvergences of ELBO and BLEU\nIn this section, we\nempirically show that the proposed deterministic iterative\nFigure 3: ELBO and BLEU scores measured with the target\npredictions obtained at each inference step for ASPEC Ja-En\nand WMT’14 En-De datasets.\nFigure 4: Trade-off between BLEU scores and speedup on\nWMT’14 En-De task by varying the number of candidates\ncomputed in parallel from 10 to 100.\ninference improves the ELBO in Eq. (3). As the ELBO is\na function of x and y, we measure the ELBO value with the\nnew target prediction after each iteration during inference.\nFor each instance, we sample 20 latent variables to compute\nthe expectation in Eq. (3). The ELBO value is further averaged over data samples.\nIn Fig. 3, we show the ELBO value and the resulting BLEU scores for both datasets. In the initial step,\nthe delta posterior is initialized with the prior distribution\npω(z|x). We see that the ELBO value increases rapidly with\neach reﬁnement step, which means a higher lowerbound to\nlog p(y|x). The improvement is highly correlated with increasing BLEU scores. For around 80% of the data samples,\nthe algorithm converges within three steps. We observe the\nBLEU scores peaked after only one reﬁnement step.\nTrade-off between Quality and Speed\nIn Fig. 4, we show\nthe trade-off between translation quality and the speed gain\non WMT’14 En-De task when considering multiple candidates latent variables in parallel. We vary the number of canExample 1: Sequence modiﬁed without changing length\nSource hyouki gensuiryou hyoujun no kakuritsu wo kokoromita. (Japanese)\nReference the establishment of an optical fiber attenuation standard was attempted .\nInitial Guess an attempt was made establish establish damping attenuation standard ...\nAfter Inference an attempt was to establish the damping attenuation standard ...\nExample 2: One word removed from the sequence\nSource ...‘‘sen bouchou keisu no toriatsukai’’ nitsuite nobeta. (Japanese)\nReference ... handling of linear expansion coefficient .\nInitial Guess ... ‘‘ handling of of linear expansion coefficient ’’ are described .\nAfter Inference ... ‘‘ handling of linear expansion coefficient ’’ are described .\nExample 3: Four words added to the sequence\nSource ... maikuro manipyureshon heto hatten shite kite ori ...(Japanese)\nReference ... with wide application fields so that it has been developed ...\nInitial Guess ... micro micro manipulation and ...\nAfter Inference ... and micro manipulation , and it has been developed , and ...\nTable 3: Ja-En sample translation with the proposed iterative inference algorithm. In the ﬁrst example, the initial guess is reﬁned\nwithout a change in length. In the last two examples, the iterative inference algorithm changes the target length along with its\ncontent. This is more pronounced in the last example, where a whole clause is inserted during reﬁnement.\ndidates from 10 to 100, and report BLEU scores and relative speed gains in the scatter plot. The results are divided\ninto two groups. The ﬁrst group of experiments search over\nmultiple latent variables and rescore with the teacher Transformer. The second group applies the proposed deterministic\ninference before rescoring.\nWe observe that the proposed deterministic inference consistently improves translation quality in all settings. The\nBLEU score peaks at 25.2. As GPUs excel at processing\nmassive computations in parallel, we can see that the translation speed only degrades by a small magnitude.\nQualitative Analysis\nWe present some translation examples to demonstrate the\neffect of the proposed iterative inference in Table 3. In Example 1, the length of the target sequence does not change\nbut only the tokens are replaced over the reﬁnement iterations. The second and third examples show that the algorithm removes or inserts words during the iterative inference\nby adaptively changing the target length. Such a signiﬁcant\nmodiﬁcation to the predicted sequence mostly happens when\ntranslating long sentences.\nFor some test examples, however, we still ﬁnd duplicated\nwords in the ﬁnal translation after applying the proposed\ndeterministic inference. For them, we notice that the quality of the initial guess of translation is considerably worse\nthan average, which typically contains multiple duplicated\nwords. Thus, a high-quality initial guess is crucial for obtaining good translations.",
        "conclusion": "Our work presents the ﬁrst approach to use continuous latent variables for non-autoregressive Neural Machine Translation. The key idea is to introduce a sequence of latent variables to capture the uncertainly in the target sentence. The\nnumber of latent vectors is always identical to the number\nof input tokens. A length transformation mechanism is then\napplied to adapt the latent vectors to match the target length.\nWe train the proposed model by maximizing the lowerbound\nof the log-probability log p(y|x).\nWe then introduce a deterministic inference algorithm that\nuses a delta posterior over the latent variables. The algorithm alternates between updating the delta posterior and the\ntarget tokens. Our experiments show that the algorithm is\nable to improve the evidence lowerbound of predicted target sequence rapidly. In our experiments, the BLEU scores\nconverge in one reﬁnement step.\nOur non-autoregressive NMT model closes the performance gap with autoregressive baseline on ASPEC Ja-En\ntask with a 8.6x speedup. By decoding multiple latent variables sampled from the prior, our model brings down the gap\non En-De task down to 1.0 BLEU with a speedup of 6.8x.",
        "summary_en": "Although neural machine translation models reached high translation quality, the autoregressive nature makes inference difficult to parallelize and leads to high translation latency. Inspired by recent refinement-based approaches, this paper proposes LaNMT, a latent-variable non-autoregressive model with continuous latent variables and deterministic inference procedure. In contrast to existing approaches, the paper uses a deterministic inference algorithm to find the target sequence that maximizes the lowerbound to the log-probability. During inference, the length of translation automatically adapts itself. The paper's experiments show that the lowerbound can be greatly increased by running the inference algorithm, resulting in significantly improved translation quality. The proposed model closes the performance gap between non-autoregressive and autoregressive approaches on ASPEC Ja-En dataset with 8.6x faster decoding. On WMT'14 En-De dataset, the model narrows the gap with autoregressive baseline to 2.0 BLEU points with 12.5x speedup. By decoding multiple initial latent variables in parallel and rescore using a teacher model, the proposed model further brings the gap down to 1.0 BLEU point on WMT'14 En-De task with 6.8x speedup.",
        "summary_zh": "这篇论文介绍了一种具有连续潜变量和确定性推理过程的潜变量非自回归模型LaNMT，用于解决神经机器翻译中自回归性质导致的难以并行化和高翻译延迟的问题。与现有方法不同，LaNMT使用确定性推理算法来寻找最大化对数概率下界的目标序列。在推理过程中，翻译长度自动适应。实验证明，通过运行推理算法，对数概率下界可以大大提高，从而显著改善翻译质量。在ASPEC Ja-En数据集上，该模型缩小了非自回归方法与自回归方法之间的性能差距，解码速度提高8.6倍。在WMT'14 En-De数据集上，该模型将与自回归基线的差距缩小到2.0 BLEU点，速度提高12.5倍。在WMT'14 En-De任务中，通过并行解码多个初始潜变量并使用教师模型重新评分，该模型进一步将差距缩小到1.0 BLEU点，速度提高了6.8倍。"
    },
    {
        "title": "Non-autoregressive Translation with Layer-Wise Prediction and Deep Supervision",
        "abstract": "How do we perform efficient inference while retaining high translation quality? Existing neural machine translation models, such as Transformer, achieve high performance, but they decode words one by one, which is inefficient. Recent non-autoregressive translation models speed up the inference, but their quality is still inferior. In this work, we propose DSLP, a highly efficient and high-performance model for machine translation. The key insight is to train a nonautoregressive Transformer with Deep Supervision and feed additional Layer-wise Predictions. We conducted extensive experiments on four translation tasks (both directions of WMT’14 EN–DE and WMT’16 EN–RO). Results show that our approach consistently improves the BLEU scores compared with respective base models. Specifically, our best variant outperforms the autoregressive model on three translation tasks, while being 14.8 times more efficient in inference.",
        "introduction": "Most state-of-the-art neural machine translation (NMT) systems are autoregressive, where they predict one word after\nanother to generate the target sequence (Bahdanau, Cho, and\nBengio 2015; Vaswani et al. 2017). However, autoregressive\nNMT is slow in inference, which sometimes does not meet\nthe efficiency requirement from the industry.\nGu et al. (2018) propose non-autoregressive neural machine translation (NAT), which is 15.6 times faster than its\nautoregressive counterpart. While NAT achieves fast inference by generating target tokens in parallel, it assumes that\nthe generated words are conditionally independent given the\ninput. Such an independence assumption, however, weakens\nthe power of sequence modeling, and results in worse performance than autoregressive translation.\nIn\nrecent\nstudies,\nresearchers\npropose\npartially\n(non-)autoregressive\nmodels\nby\nprogressively\ngenerating several words at a time (Ghazvininejad et al. 2019) or\niteratively editing with the Levenshtein Transformer (Gu,\nWang, and Zhao 2019). However, their inference is significantly slower than a full NAT model. As shown by Kasai\n0\n3\n6\n9\n12\n15\n6SeeduS\n21\n22\n23\n24\n25\n26\n27\nBLEU\n0\n3\n6\n9\n12\n15\n6SeeGXS\n21\n22\n23\n24\n25\n26\n27\nB/EU\nAXtRUegUeVVLve 7UDnVIRUPeU\nInVeUtLRn 1A7\nD&5F\nAXE\n/evenVhteLn 1A7\nG/A7\nG/A7 w/ D6/3\n9DnLllD 1A7\n9DnLllD 1A7 w/ D6/3\n9DnLllD 1A7 w/ D6/3 & 07\n&7&\n&7& w/ D6/3\n&7& w/ D6/3 & 07\nFigure 1: Quality–efficiency trade-off for state-of-the-art\nNAT models and our DSLP on the WMT’14 EN–DE dataset.\nA cross “×” represents our DSLP variants, and a star “⋆”\nrefers a DSLP model that is enhanced with our mixed training (MT) technique. Its base model is shown in the same\ncolor, and the correspondence is represented by an arrow.\net al. (2021), a comparable-sized autoregressive model with\na shallow decoder is able to outperform these approaches\nwith similar latency. Therefore, these approaches do not\nachieve a desired quality–efficiency trade-off.\nIn this work, we propose a simple yet effective approach\nto non-autoregressive generation with a Deeply Supervised,\nLayer-wise Prediction-aware (DSLP) Transformer. It is noticed that a traditional NAT Transformer (Vaswani et al.\n2017) only makes predictions at the last layer, where all the\nwords are generated in parallel. Thus, the prediction of a\nword is unaware of other time steps, which typically leads\nto inconsistent sentences. For example, the English phrase\nthank you has two translations in German: danke sch¨on and\nvielen dank. Instead of generating either of the phrases, a\nnon-autoregressive model may predict danke dank, which is\nabsurd. Therefore, we propose a layer-wise prediction-aware\nTransformer that predicts the output at every decoding layer.\nThe prediction is fed to the next decoding layer for further\nprocessing. Hence, our decoder model is aware of different\nsteps’ predictions and can calibrate the NAT output through\nits decoding layers. We further introduce deep supervision\nfor training our prediction-aware decoder. This is essential\nto our model, because otherwise the intermediate predictions are not grounded to the target output and the calibration\nwould be less meaningful. We also propose to mix the intermediate predictions with groundtruth tokens during training,\nto further enhance meaningful calibration.\nOur DSLP is a generic framework that can be combined with different base NAT models. In our experiments, we evaluated DSLP on WMT’14 English–German\nand WMT’16 English–Romanian pairs, and considered the\ntranslation of both directions. Results show that our DSLP\nconsistently improves the NAT performance for every translation language pair and every base model, including vanilla\nNAT, CMLM, GLAT, and CTC (see experiments section for\ndetails), demonstrating the generality of our approach. In\naddition, we show that mixing the layer-wise predictions\nwith groundtruth during training further improves the performance. Remarkably, our best model, CTC with DSLP and\nmixed training, achieves better performance compared with\nits autoregressive teacher model on three of the four datasets\nwith a 14.8x speedup. Figure 1 positions DSLP in prior work\nin terms of quality–efficiency trade-off.1",
        "related work": "Non-autoregressive neural machine translation is attracting\nan increasing attention in the community (Sun et al. 2019;\nGu, Wang, and Zhao 2019; Saharia et al. 2020; Qian et al.\n2021; Hao et al. 2021), because it is more efficient in inference than an autoregressive model, and has high potentials in\nindustrial applications. Compared with autoregressive generation, NAT faces two major challenges: 1) determining the\noutput length, and 2) handling multi-modality, i.e., generating a coherent text when multiple translations are plausible.\nGu et al. (2018) propose a token-fertility approach that\npredicts the repetition of source tokens to obtain the target\nlength. Lee, Mansimov, and Cho (2018) predict the length\ndifference between the source and target sequences.\nThe Insertion Transformer (Stern et al. 2019) dynamically\nexpands the canvas size (generation slots) through multiple rounds of inference. The Levenshtein Transformer (Gu,\nWang, and Zhao 2019) generates a sentence by explicitly\npredicting edit actions, such as insertion and deletion. Alternatively, an empty token can be introduced into the vocabulary for generating a sentence of appropriate length (Graves\net al. 2006; Saharia et al. 2020).\nTo address the second challenge, Gu et al. (2018) propose a knowledge distillation (KD) approach that first trains\nan autoregressive translation (AT) model, and then trains the\nNAT model by AT’s output instead of the groundtruth. Zhou,\nGu, and Neubig (2020) show that AT’s output has fewer\nmodes than the original corpus, and thus is easier for training\nNAT models.\nPartially (non-)autoregressive generation is also widely\nused to mitigate the multi-modality issue, such as insertionbased methods (Stern et al. 2019), and the Levenshtein\nTransformer (Gu, Wang, and Zhao 2019). Ghazvininejad\net al. (2019) propose to pretrain a conditional masked language model (CMLM) for NAT, where they progressively\ngenerate several new words conditioned on previous ones.\n1Our code, training/evaluation scripts, and output are available\nat https://github.com/MANGA-UOFA/DSLP.git.\nWang, Zhang, and Chen (2018) model the dependencies\namong the words in a chunk. Sun et al. (2019) propose to apply the linear-chain conditional random field on top of NAT\npredictions to capture token dependency. Their method is\nalso partially autoregressive as beam search is required during inference.\nRecently, Qian et al. (2021) propose the Glancing Transformer (GLAT), which trains NAT predictions in a curriculum learning. Specifically, their method is built upon\nCMLM, and the amount of masked out groundtruth is dependent on the model’s current performance. Thus, the\nmodel learns easy tokens first, and gradually moves on to\nhard ones. This also alleviates the multi-modal problem.\nA similar study to ours is Lee, Mansimov, and Cho\n(2018). They train two decoders: one is for the initial generation, and the other is applied iteratively for refinement.\nThey also propose a deterministic variant of token fertility.\nDifferent from previous work, we perform layer-wise prediction within a decoder. This is an insightful contribution, being the key to efficient inference, as previous evidence shows that decoder-level iterative refinement does not\nhave an advantage over a carefully designed autoregressive\nmodel (Kasai et al. 2021). We also show that our layer-wise\nprediction awareness is agnostic to base NAT models, being\na generic framework.\nOur work requires deep supervision on the Transformer\ndecoder. This is similar to the treatment in the depthadaptive Transformer (DAT, Elbayad et al. 2020), where\nthey apply deep supervision for an autoregressive model to\nallow “early exit,” i.e., using a subset of decoding layers.\nOur NAT work is apparently different from DAT.\nDiscussion on autoregressiveness. At first glance, it\nis doubtful whether our DSLP is non-autoregressive, as\nwe have layer-wise prediction. In statistics, autoregression\nmeans using past predictions for future ones, typically in\na time series (Akaike 1969). Our DSLP differs from partially autoregressive models (Stern et al. 2019; Gu, Wang,\nand Zhao 2019; Ghazvininejad et al. 2019). We predict the\noutput words simultaneously in the last layer, whereas its\nlayer-wise prediction is an internal mechanism, similar to\nTransformer hidden states. Thus, DSLP should be considered as a full NAT model from the perspective of statistics.\nMore importantly, the motivation of NAT research is\nthe inference efficiency for industrial applications. Being\nnon-autoregressiveness itself is not the research goal. As\ndemonstrated in experiments, our work achieves satisfactory\nquality–efficiency trade-off, as it does not cause much extra\ncomputational cost.",
        "methodology": "In this section, we describe our model in detail. We first\nintroduce a generic Transformer-based NAT model. Then,\nwe present our layer-wise prediction-aware approach and\nthe deeply supervised training. We also present an improved\ntraining scheme that takes groundtruth tokens as input and\nmixes them the layer-wise predictions.\n𝒉!\n(#)\nStep 𝑡, Layer 𝑛\nDeep supervision\nConcat for\nprediction\nawareness\n$𝒉!\n(#)\n%𝑦!\n(#)\nFigure 2: Overview of our Deeply Supervised Layer-wise\nPrediction-aware (DSLP) Transformer.\nNon-Autoregressive Transformer\nRecent advances in neural machine translation are built upon\nthe Transformer architecture (Vaswani et al. 2017). It is an\nencoder–decoder neural network, where the encoder is responsible for representing the source sentences, and the decoder is used to generate the target translation.\nFormally, let the source text be x = (x1, · · · , xTx), where\nTx is the length. The encoder first takes x as input and maps\nthese discrete words2 into vector representations by an embedding function, denoted by e(0)\nt\n= emb(xt). Each Transformer layer performs multi-head attentions with a feedforward layer to compute deep contextual representation, denoted by\ne(n)\n1:Tx = Layer(n)\nenc (e(n−1)\n1:Tx )\n(1)\nwhere Layer(n)\nenc is the nth encoder layer. Such processing is\nrepeated for every layer, and we take the last layer as the\nencoder’s hidden representation, denoted by E.\nFor non-autoregressive decoding, the first step is to determine the length Ty of the output sentence. This can be handled in various ways. For example, Gu et al. (2018) train a\nclassifier to predict the length during inference, and in training, the length is given by the groundtruth. In latent alignment models (Saharia et al. 2020), an empty prediction is\nallowed, and Ty is typically set as kTx for a constant k. Our\nDSLP approach can be built upon multiple base models, and\nthe output length is handled accordingly.\nFor the decoder input h(0)\nt , we follow Saharia et al. (2020)\nand feed the embedding of a special token s for every step.\nIn our preliminary experiments, this treatment is on par with\ncopying encoder representations as the decoder input (Gu\net al. 2018; Wei et al. 2019).\nThe decoder processes the information in a similar way to\nthe encoder, except that it has an additional encoder–decoder\nattention to obtain input information. Thus, a decoding layer\ncan be represented by\nh(n)\n1:Ty = Layer(n)\ndec (h(n−1)\n1:Ty , E)\n(2)\nwhere Layer(n)\ndec is the nth decoding layer.\n2We adopt BPE segmentation, and strictly speaking, they\nshould be tokens. For clarity, we use word and token interchangeably in the paper.\nFinally, NAT uses a softmax layer to predict the target\nwords based on the decoder’s last hidden states, given by\np(yt|h(N)\nt\n) = softmax(Wh(N)\nt\n)\n(3)\nwhere N is the number of decoding layers. In this way, the\noutput words y1, · · · , yTy are predicted simultaneously in a\nnon-autoregressive fashion.\nThe training objective is to maximize the log-likelihood\nL = PTy\nt=1 log p(yt|h(N)\nt\n)\n(4)\nLayer-Wise Prediction-Awareness\nAs we observe from (3), a conventional NAT model predicts a word yt without being aware of the other words\ny1, · · · , yt−1, yt+1, · · · , yTy. This is undesired when the target distribution is “multi-modal,” i.e., multiple outputs are\nappropriate given an input.\nTo this end, we propose a layer-wise prediction-aware decoder that predict the output sequence at every layer. Such\nprediction serves as a tentative translation, and is fed to the\nnext layer for further processing.\nConsider the tth step of the nth decoding layer. We perform linear transformation on the conventional Transformer\nhidden state h(n)\nt\n, and use softmax to predict the probability\nof words:\np(y(n)\nt\n|h(n)\nt\n) = softmax(Wh(n)\nt\n)\n(5)\nThen, we obtain the most probable word by\nby(n)\nt\n= argmax p(y(n)\nt\n|h(n)\nt\n)\n(6)\nIts embedding is concatenated with h(n)\nt\nand further processed by a linear layer\neh(n)\nt\n= Wc[h(n)\nt\n; emb(by(n)\nt\n)]\n(7)\nwhere Wc is a weight matrix. eh(n)\nt\nis an updated predictionaware hidden state, fed to the next layer. Notice that the last\nlayer does not require such an update, and the final output is\ndirectly given by (5).\nIn this way, the prediction at the (n + 1)th layer is aware\nof all the predictions at the nth layer, due to the Transformer\nattention. This serves as a calibration mechanism that can revise the tentative generation through multiple decoding layers.\nDeeply Supervised Training\nWe further propose to train our layer-wise prediction-aware\nTransformer with deep supervision.\nTypically, NAT models are trained with the supervision at\nthe last layer. With such training, however, our intermediate\npredictions are not grounded to the desired output sentence,\nand such prediction-awareness becomes less meaningful.\nSpecifically, we apply maximum likelihood estimation to\nthe prediction of every decoding layer, given by\nLD = PN\nn=1\nPTy\nt=1 log p(y(n)\nt\n|eh(n)\nt\n)\n(8)\nThis layer-wise deep supervision ensures that our DSLP\npredicts the target output (although imperfect) at every decoding layer. By feeding back such prediction, the model\nis able to calibration the words of different time steps, alleviating the weakness of NAT models. Figure 2 presents an\noverview of our DSLP.\nMixing Predictions and Groundtruth in Training\nDuring training, the layer-wise predictions can still be of low\nquality despite deep supervision, in which case the next layers may not be well trained. As a remedy, we propose to\npartially feed the groundtruth to intermediate layers.\nAs opposed to (7), we compute the hidden states of the\nnext layer as,\neh(n)\nt\n= Wc[h(n)\nt\n; emb(y(n)\nt\n)]\n(9)\nwhere y(n)\nt\n= styt + (1 − st)by(n)\nt\n, and st ∼ Bernoulli(λ).\nλ is a hyperparameter controlling the amount of mixed\ngroundtruth tokens, referred to as the mixing ratio. Note that\nst is not dependent on the layer number n. Otherwise, we\nmay feed the model with all the groundtruth tokens during\ntraining and having low performance.\nAlthough our DSLP is generic to non-autoregressive models, the proposed mix training scheme needs to be adapted\nto the base NAT model accordingly.\nFor the vanilla NAT (Gu et al. 2018), we mask out the\nobserved groundtruth tokens in the loss, which prevents\nthe model from simply copying tokens. This is similar to\nmasked language models (Devlin et al. 2019; Ghazvininejad\net al. 2019).3\nFor the connectionist temporal classification (CTC,\nGraves et al. 2006) model, the predictions are usually longer\nthan the groundtruth. This is because CTC allows predicting\nextra tokens (including repeated tokens and empty tokens) to\ngenerate sentences of different lengths, and the probability\nof generating the groundtruth sequence y1:Ty is computed by\nmarginalizing all valid alignments.4 The length discrepancy\nprevents us from directly replacing a predicted token with a\ngroundtruth token. Alternatively, we regard the best alignment given by the model as the pseudo-groundtruth. This is\nsimilar to how Gu and Kong (2021) combine CTC with the\nGlancing Transformer (GLAT, Qian et al. 2021).\nThe mixed training does not improve the models that\nhave already used groundtruth as input during training,\nsuch as the conditional masked language model (CMLM,\nGhazvininejad et al. 2019) and GLAT. This is because further feeding groundtruth tokens interferes with their training\nschemes.\nOur mixed training is also similar to the scheduled sampling (Bengio et al. 2015), which is proposed to improve the\ntraining of autoregressive models.\nInference\nThe inference of our DSLP resembles its training process:\nthe predicted output of every layer is fed to the next layer,\n3Empirically, we find that masking improves the BLEU score\nby 0.2 on WMT 14’ EN→DE. However, it is not our main focus,\nand our mixed training with masking improves the vanilla NATbased DSLP model by 1.4 BLEU (see Table 1).\n4An alignment is an expanded sequence that can be reduced to\nthe groundtruth sequence. For example, “ aabb c” is a valid alignment to the sequence “abc”, where “ ” represents an empty token.\nThe marginalization of all valid alignments is computed efficiently\nvia dynamic programming. More details can be found in Section 3\nof Graves et al. (2006).\nand we take the last layer as the final output. It should be\npointed out that our layer-wise prediction does not introduce\nmuch extra computational cost. We will show that, despite\nthe high translation quality of DSLP, the time difference between DSLP and a vanilla NAT model is negligible.",
        "experiments": "Experimental Setup\nDatasets. We evaluated our models on benchmark translation datasets: WMT’14 English–German (4.0M sentence\npairs) and WMT’16 English–Romanian (610K pairs). For\nfair comparison, we obtained the preprocessed corpus (tokenization and vocabulary) released by previous work: Zhou,\nGu, and Neubig (2020) for WMT’14 EN–DE, and Lee,\nMansimov, and Cho (2018) for WMT’16 EN–RO. We consider both translation directions, and in total, we have 4\ntranslation tasks.\nHyperparameters. We mostly followed the standard\nhyperparameters used in NAT research. We used the\nTransformer-base (Vaswani et al. 2017) configuration. To\ntrain the models, we used a batch size of 128K tokens for\nEN–DE and 32K tokens for EN–RO, with a maximum 300K\nupdates. For regularization, we set the dropout rate to 0.1 for\nEN–DE and 0.3 for EN–RO. For the mixed training, we used\na fixed mixing ratio λ and set it to 0.3. More details can be\nfound in our GitHub repository (Footnote 1).\nKnowledge Distillation. It is a common technique to\ntrain NAT by an autoregressive model’s output. Previous\nevidence shows that this largely improves NAT performance (Gu et al. 2018; Lee, Mansimov, and Cho 2018; Stern\net al. 2019). In our study, we also adopted knowledge distillation from an autoregressive Transformer-base model for\nall baselines and our DSLP variants.\nEvaluation Metrics. For translation quality, we computed BLEU (Papineni et al. 2002) scores over tokenized\nsentences. To measure inference latency, we used a single\nNvidia V100 GPU and performed inference with one sentence at a time. This mimics a deployed NMT system in the\nindustry. Our models were implemented and evaluated with\nthe open-source toolkit FairSeq (Ott et al. 2019).\nBase Models. Our DSLP can be applied to various base\nNAT models. To evaluate its generality, we consider the following models: 1) Vanilla NAT (Gu et al. 2018), which is the\nfoundation of NAT models. 2) Partially non-autoregressive\ngeneration based on a conditional masked language model\n(CMLM, Ghazvininejad et al. 2019), which progressively\ngenerates several words of a sentence. CMLMk refers to k iterations. 3) Glancing Transformer (GLAT, Qian et al. 2021),\nwhich progressively trains NAT word predictors in a curriculum learning fashion. 4) Connectionist temporal classification (CTC, Graves et al. 2006), which allows empty tokens\nand performs dynamic programming for marginalization of\nlatent alignment. With these base models, we would be able\nto evaluate DSLP with a wide range of prediction schemas\nand training objectives.\nRow#\nModel\nWMT’14\nWMT’16\nLatency (ms)\nSpeedup\nEN–DE\nDE–EN\nEN–RO\nRO–EN\n1\nTransformer (teacher)\n27.48\n31.21\n33.70\n34.05\n326.80\n1×\n2\nCMLM1\n19.91\n22.69\n27.60\n29.00\n20.91\n15.6×\n3\nw/ DSLP\n21.76\n25.30\n30.29\n30.89\n21.76\n15.0×\n4\nGLAT\n25.02\n29.63\n31.33\n32.43\n20.91\n15.6×\n5\nw/ DSLP\n25.69\n29.90\n32.36\n33.06\n21.73\n14.9×\n6\nVanilla NAT\n21.18\n24.93\n29.15\n29.69\n20.86\n15.7×\n7\nw/ DSLP\n22.72\n25.83\n30.48\n31.46\n22.12\n14.8×\n8\nw/ DSLP & Mixed Training\n24.17\n28.63\n31.49\n32.64\n22.12\n14.8×\n9\nCTC\n25.72\n29.89\n32.89\n33.79\n21.04\n15.5×\n10\nw/ DSLP\n26.85\n31.16\n33.85\n34.24\n22.06\n14.8×\n11\nw/ DSLP & Mixed Training\n27.02\n31.61\n34.17\n34.60\n22.06\n14.8×\nAverage Improvement\n1.34\n1.38\n1.58\n1.28\n–\n–\nTable 1: Applying DSLP to different base NAT models, which shows the generality of our approach. Translation quality is\nevaluated in BLEU. Latency is the processing time (in milliseconds) of a single sentence. Speedup is relative to an autoregressive\nmodel. All results are based on our implementation. CMLMk refers to k iterations of progressive generation. Here, we consider\nk = 1, as more iterations make CMLM closer to autoregressive models.\nMain Results\nGenerality of DSLP. Table 1 shows the performance of our\nDSLP compared with base models: Vanilla NAT, CMLM,\nGLAT, and CTC. Comparing with results published in previous papers (quoted in Table 2), our replications mostly\nmatch the previous work, except that our vanilla NAT outperforms the implementation of Gu et al. (2018). It is probably due to the engineering efforts by the community for NAT\nin the past years. The results on base models implicate that\nour implementation is fair for the study on DSLP.\nWe apply DSLP to all baseline models. As seen, DSLP\nconsistently achieves higher performance for every base\nmodel and every translation task. The average improvement\nis more than 1 BLEU point, which is a considerable improvement over strong baselines for machine translation.\nFor the training of vanilla NAT- and CTC-based DSLP,\nwe mix the layer-wise predictions with groundtruth tokens.\nWe do not apply the mixed training on CMLM and GLAT,\nas they have already used groundtruth tokens in training, and\nfurther feeding groundtruth causes potential conflicts.\nWe show the improvement of the mixed training in Table 1 (Lines 8 & 11). We observe mixed training consistently\nimproves the performance, with an average of 0.97 BLEU\nimprovement compared with respective DSLP variants.\nWhen combined with the weak vanilla NAT model, DSLP\nwith mixed training outperforms the strong baseline model,\nthe Glancing Transformer (Qian et al. 2021), on EN→RO,\nand RO→EN datasets. When combining with the strong\nCTC model, the mixed training scheme further improves the\nperformance consistently, even though the CTC with DSLP\nmodel has already achieved superb results.\nComparing with the State of the Art. We compare our\nbest variant (CTC w/ DSLP & Mixed Training) with previous state-of-the-art NAT models in Table 2. Profoundly, our\napproach achieves close results to the autoregressive (AT)\nteacher model on the WMT 14’ EN→DE dataset, and even\noutperforms AT on other three translation datasets.\nCompared with iterative methods (Line 2–8, Table 2), our\nmodel produces very competitive translation quality while\nbeing approximately 4 times faster in inference. Compared\nwith non-iterative methods (Line 9–19, Table 2), our CTC w/\nDSLP & Mixed Training outperforms all existing systems\non three datasets (DE→EN, EN→RO, and RO→EN), while\nonly costing extra 4–6% latency.\nAnalysis\nIn this part, we present in-depth analysis on DSLP. Our development was mainly conducted on vanilla NAT because\nwe would like to rule out complications brought by various\nNAT models. It is also more efficient (and greener). Therefore, the base model is vanilla NAT for this subsection (unless otherwise stated).\nAblation Study on DSLP. Table 3 presents an ablation\nstudy on the layer-wise prediction (LP) and deep supervision\n(DS). We observe that LP or DS alone shows little improvement over the vanilla NAT. This is understandable since LP\nalone does not guarantee that intermediate predictions are\ngrounded to target, and such awareness is mostly ineffective.\nDS alone does not have a calibration mechanism of different time steps. It nevertheless improves the performance to\na small extent when the model is deep. This is because deep\nTransformers are difficult to train (Huang et al. 2020), as the\nvanilla NAT’s performance drops by 0.59 from 12 layers to\n18 layers; DS may help training a deep structure.\nOur DSLP significantly outperforms LP and DS on all the\nsettings. Compared with DS, our full DSLP shows more improvement when the network is deep, as more layers allow\nlarger calibration capacity of DSLP.\nLayer-Wise Performance. Since our model makes intermediate predictions, we are curious how the performance\nevolves through the decoding layers. We took a fixed, trained\nmodel, and tested the BLEU score of every layer. In Figure 3, we show the performance dynamics of a 6-, 12-, and\n18-decoding-layer models.\nCategory\nRow#\nModel\nWMT’14\nWMT’16\nSpeedup\nEN–DE\nDE–EN\nEN–RO\nRO–EN\nAT\n1\nTransformer\n27.48\n31.21\n33.70\n34.05\n1×\nIterative\n2\nIterative NAT (Lee, Mansimov, and Cho 2018)\n21.61\n25.48\n29.32\n30.19\n2.0×\n3\nBlockwise (Stern, Shazeer, and Uszkoreit 2018)\n27.40\n–\n–\n–\n3.0×\n4\nInsertion NAT (Stern et al. 2019)\n27.41\n–\n–\n–\n4.8×\n5\nLevenshtein NAT (Gu, Wang, and Zhao 2019)\n27.27\n–\n–\n–\n4.0×\n6\nCMLM10 (Ghazvininejad et al. 2019)\n27.03\n30.53\n33.08\n33.08\n2.6×‡\n7\nImputer (Saharia et al. 2020)\n28.2\n31.8\n34.4\n34.1\n3.9×\n8\nDisCO (Kasai et al. 2021)\n27.34\n31.31\n33.22\n33.25\n3.5×†\nNon-iterative\n9\nVanilla NAT (Gu et al. 2018)\n17.69\n21.47\n27.29\n29.06\n15.6×\n10\nDCRF (Sun et al. 2019)\n23.44\n27.22\n–\n–\n10.4×\n11\nCMLM1 (Ghazvininejad et al. 2019)\n18.05\n21.83\n27.32\n28.20\n15.6×‡\n12\nAXE (Ghazvininejad et al. 2020)\n23.53\n27.90\n30.75\n31.54\n15.3×\n13\nCTC (Saharia et al. 2020)\n25.7\n28.1\n32.2\n31.6\n18.6×\n14\nCNAT (Bao et al. 2021)\n25.67\n29.36\n–\n–\n10.4×\n15\nREDER (Zheng et al. 2021)\n26.70\n30.68\n33.10\n33.23\n15.5×\n16\nGLAT (Qian et al. 2021)\n25.21\n29.84\n31.19\n32.04\n15.3×\n17\nCTC + GLAT (Qian et al. 2021)\n26.39\n29.54\n32.79\n33.84\n14.6×\n18\nCTC + VAE (Gu and Kong 2021)\n27.49\n31.10\n33.79\n33.87\n16.5×\n19\nCTC + GLAT (Gu and Kong 2021)\n27.20\n31.39\n33.71\n34.16\n16.8×\nOurs\n20\nCTC w/ DSLP & Mixed Training\n27.02\n31.61\n34.17\n34.60\n14.8×\nTable 2: Comparing our model with state-of-the-art NAT models. Results of prior work are quoted from respective papers.\n†Estimated from the plot in the previous paper. ‡Given by our own implementation, as it is not available in the original paper.\n2\n4\n6\nLayer\n0\n10\n20\nBLEU\n(a)\nNAT\nNAT w/ LP\nNAT w/ DS\nNAT w/ DSLP\n0\n2\n4\n6\n8\n10\n12\nLayer\n0\n10\n20\nBLEU\n(b)\nNAT\nNAT w/ LP\nNAT w/ DS\nNAT w/ DSLP\n0\n3\n6\n9\n12\n15\n18\nLayer\n0\n10\n20\nBLEU\n(c)\nNAT\nNAT w/ LP\nNAT w/ DS\nNAT w/ DSLP\nFigure 3: Layer-wise performance of a fixed, trained vanilla NAT model. Decoding layers: 6 for (a), 12 for (b), and 18 for (c).\n6\n12\n18\nVanilla NAT\n21.18\n22.22\n21.63\nw/ LP\n21.22(+0.04)\n22.24(+0.02)\n22.29(+0.66)\nw/ DS\n21.84(+0.66)\n22.66(+0.38)\n22.99(+1.36)\nw/ DSLP\n22.72(+1.54)\n23.56(+1.34)\n24.22(+2.59)\nTable 3: Ablation study on layer-wise prediction (LP) and\ndeep supervision (DS). We tested models of 6 layers, 12\nlayers, and 18 layers. The number in bracket shows the improvement over vanilla NAT.\nWith only LP, the model achieves zero BLEU for all intermediate layers. This verifies that LP alone does not provide meaningful predictions at intermediate layers, leading\nto lower performance than our full DSLP.\nOn the other hand, DS is able to achieve reasonable BLEU\nscores at every decoding layer. However, their BLEU scores\nare consistently lower than those of DSLP.\nNotably, DS shows no improvement (or sometimes even\ndecreases) at the last few layers. By contrast, the performance of DSLP improves steadily by the calibration over\n2\n4\n6\n8 10 12 14 16 18\nLayer\n0.0\n0.1\n0.2\n0.3\nChange Rate\n(a)\n6-layer model\n12-layer model\n18-layer model\n2\n4\n6\n8 10 12 14 16 18\nLayer\n0.0\n0.1\n0.2\n0.3\nRepetition Rate\n(b)\n6-layer model\n12-layer model\n18-layer model\nFigure 4: (a) Change rate for vanilla NAT w/ DSLP. The first\nlayer is excluded as it does not perform editing. (b) Word\nrepetition rate in each layer of vanilla NAT w/ DSLP.\nmultiple layers.\nChange Rate. Since our DSLP performs layer-wise calibration, we investigate how much word changing is performed in every layer. Figure 4a shows the percentage of\nchanged words, where we also consider three variants: 6layer, 12-layer, and 18-layer models.\nWe observe that, in each model, early layers perform more\nchanges, and the change rate decreases gradually. This is\nExample 1\nSource: W¨urde ich Gleichgesinnte erst nach Einbruch der Dunkelheit treffen k¨onnen?\nReference: Would I be forced to meet like-minded people only after dark?\nGeneration:\nStep:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nLayer 1:\nWould\nI\nmeet\nmeet\nmeet\nminded\nminded\nminded\nafter\ndarkdarkdarkcollapsed\n?\nLayer 2:\nWould\nI\nonly\nmeet\nlike\nminded\nminded\nminded\nafter\ndarkness\ndarkcollapsed\n?\nLayer 3:\nWould\nI\nmeet\nmeet\nlike\n@-@\nminded\npeople\nafter\ndarkness\ndarkcollapsed\n?\nLayer 4:\nWould\nI\nmeet\nmeet\nlike\n@-@\nminded\npeople\nafter\ndarkdarkness\ncollapsed\n?\nLayer 5:\nWould\nI\nonly\nmeet\nlike\n@-@\nminded\npeople\nafter\ndarkness\nness\ncollapsed\n?\nLayer 6:\nWould\nI\nonly\nmeet\nlike\n@-@\nminded\npeople\nafter\ndarkdarkness\ncollapsed\n?\nExample 2\nSource: Konflikte sind unvermeidbar, sie m¨ussen aber im Streben nach einem gemeinsamen Weg ¨uberwunden werden.\nReference: Conflict is inevitable, but must be overcome by the desire to walk together.\nGeneration:\nStep:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\nLayer 1:\nConf- lic- ts\nare\ninevitable\nbut\nthey\nmust\nbe\novercome\nthe\npursuit\nfor\na\ncommon\npath\n.\nLayer 2:\nConf- lic- ts\ninevitable\n,\nbut\nthey\nmust\nbe\novercome\nthe\npursuit\nof\na\ncommon\npath\n.\nLayer 3:\nConf- lic- ts\ninevitable\ninevitable\nbut\nthey\nthey\nmust\novercome\nin\npursuit\nfor\na\ncommon\npath\n.\nLayer 4:\nConf- lic- ts\ninevitable\ninevitable\nbut\nbut\nthey\nmust\novercome\novercome\npursuit\nfor\na\ncommon\npath\n.\nLayer 5:\nConf- lic- ts\nare\ninevitable\n,\nbut\nthey\nmust\nbe\nthe\npursuit\nfor\na\ncommon\npath\n.\nLayer 6:\nConf- lic- ts\nare\ninevitable\n,\nbut\nthey\nmust\nbe\nthe\npursuit\nfor\na\ncommon\npath\n.\nFigure 5: Layer-wise predictions of our DSLP w/ vanilla NAT with examples in the test set of WMT’14 DE–EN translation.\nIn our model, we use BPE segmentation, where prefixes are marked by - for the convenience of reading. A hyphen token is\nrepresented by “@-@.”\nreasonable because the last few layers have produced highquality translations, requiring less calibration.\nWord Repetition. NAT models often generate repetitive tokens, because of the weakness in dealing with multimodality (Gu et al. 2018), as expressions with slight different wordings can be thought of as a mode of the target distribution. Following Ghazvininejad et al. (2019) and Chan\net al. (2020), we measure the fraction of repetitive tokens in\neach layer, shown in Figure 4b.\nAs seen, the repetition rate is high at early layers. However, it decreases drastically along the decoding layers. In\ngeneral, the repetition rate is less than 15% at the final layer\nfor all three models. We also observe that a deeper model\n(12- or 18-layer) has fewer repetitions than a shallow model\n(6-layer). This further confirms that our layer-wise prediction is able to perform meaningful calibration and address\nthe weakness of NAT models.\nMixed Training. We tune the mixing ratio λ and show the\nresults in Figure 6a. We find that a mixing ratio around 0.3\nperforms well, but the performance decreases significantly\nwhen the ratio is too high. This is because the intermediate predictions are important for the prediction awareness\nof follow-up layers; therefore the layer-wise predictions can\nnot be fully replaced with groundtruth tokens.\nTo better understand how the mixed training scheme\nworks, we let the ratio of mixed groundtruth tokens anneal to zero throughout the training. In our experiments,\nwe observe that with the annealed mixing ratio, the training\neventually degenerates to that of the standard DSLP model,\nwhich results in worse final performance (see Figure 6b).\nThis phenomenon suggests that the intermediate predictions\nare not perfect during the entire training; constantly mixing the groundtruth provides correction to layer-wise predictions, which results in a better training. Based on our analysis, we set the mixing ratio as 0.3 during the entire training.\nCase Study. In Figure 5, we demonstrate a case study\non the test set of WMT’14 DE–EN dataset with our DSLP.\nHere, we present German-to-English translation for accessibility concerns.\n0.1\n0.3\n0.5\n0.7\nMixing ratio\n23\n24\n25\nBLEU\n(a)\nAnnealed\nFixed\n23.6\n23.8\n24.0\n24.2\n24.4\nBLEU\n(b)\nFigure 6: (a) Performance with different mixing ratios. (b)\nImpact of annealing the mixing ratio to zero during training.\nAs seen, the prediction by lower layers is of low quality,\nand calibration is indeed performed through the decoding\nlayers. In Example 1, the words meet, minded, and minded\nare repeated several times in Layer 1. This is expected, as no\ncalibration has been performed in this layer. Then, our layerwise prediction-aware decoder gradually revises out most of\nthe repetitions. In Example 2, the phrase must be overcome\nthe pursuit is not fluent. The grammatical error is also corrected by the calibration process.\nIn general, our case study confirms that the proposed\nDSLP model indeed performs calibration by layer-wise\nprediction-awareness, and that the translation quality is generally improved layer-by-layer.",
        "conclusion": "In this paper, we propose a deeply supervised, layer-wise\nprediction-aware Transformer (DSLP) for NAT. Our approach is generic and can be combined with different NAT\nmodels. We conducted experiments on four machine translation tasks with four base models (vanilla NAT, CMLM,\nGLAT, and CTC). The results show that our method consistently improves the translation quality with an average\nof more than 1 BLEU score, which is a large improvement\nover strong baselines. Our best variant achieves better BLEU\nscores than its autoregressive teacher model on three of four\nbenchmark datasets, while being 14.8 times faster.",
        "summary_en": "How do we perform efficient inference while retaining high translation quality? Existing neural machine translation models, such as Transformer, achieve high performance, but they decode words one by one, which is inefficient. Recent non-autoregressive translation models speed up the inference, but their quality is still inferior.Therefore, this paper proposes DSLP, a highly efficient and high-performance model for machine translation. The key insight is to train a non-autoregressive Transformer with Deep Supervision and feed additional Layer-wise Predictions. The paper conducted extensive experiments on four translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO). Results show that the approach consistently improves the BLEU scores compared with respective base models. Specifically, the best variant outperforms the autoregressive model on three translation tasks, while being 14.8 times more efficient in inference.",
        "summary_zh": "这篇论文介绍了一种高效、高性能的机器翻译模型 DSLP，用于非自回归机器翻译，旨在在保持高翻译质量的同时实现高效的推理，解决现有神经机器翻译模型（如Transformer）逐词解码效率低下的问题以及最近的非自回归翻译模型质量不佳的情况。其关键在于使用深度监督训练非自回归Transformer，并提供额外的逐层预测。在四个翻译任务上进行了大量实验，结果显示，与基准模型相比，本文方法持续提高了BLEU分数。具体而言，最佳变量在三个翻译任务上优于自回归模型，同时推理效率是自回归模型的14.8倍。"
    },
    {
        "title": "Rephrasing the Reference for Non-autoregressive Machine Translation",
        "abstract": "Non-autoregressive neural machine translation (NAT) models suffer from the multi-modality problem that there may exist multiple possible translations of a source sentence, so the reference sentence may be inappropriate for the training when the NAT output is closer to other translations. In response to this problem, we introduce a rephraser to provide a better training target for NAT by rephrasing the reference sentence according to the NAT output. As we train NAT based on the rephraser output rather than the reference sentence, the rephraser output should ﬁt well with the NAT output and not deviate too far from the reference, which can be quantiﬁed as reward functions and optimized by reinforcement learning. Experiments on major WMT benchmarks and NAT baselines show that our approach consistently improves the translation quality of NAT. Speciﬁcally, our best variant achieves comparable performance to the autoregressive Transformer, while being 14.7 times more efﬁcient in inference.",
        "introduction": "Non-autoregressive neural machine translation (NAT, Gu\net al. 2018) models signiﬁcantly speed up the decoding but\nsuffer from performance degradation compared to autoregressive models. The performance degradation is mainly attributed to the multi-modality problem that there may exist multiple possible translations of a source sentence, so\nthe reference sentence will be inappropriate for the training when the NAT output is closer to other translations. The\nmulti-modality problem manifests itself as the large crossentropy loss during the training, which cannot evaluate the\nNAT output properly. To minimize the training loss, NAT\ntends to generate a mixture of multiple translations rather\nthan a consistent translation, which typically contains many\nrepetitive tokens in the generated results.\nA number of efforts have explored ways to help NAT\nhandle the multi-modality problem. A basic solution is\nsequence-level knowledge distillation (Kim and Rush 2016;\nZhou, Gu, and Neubig 2020), which replaces the reference\nby the output of an autoregressive teacher to reduce the\nFigure 1: An example of the reference and the desired\nrephraser output. The reference is inappropriate for the training, and the rephraser output should keep the same semantics and correctly evaluate the model output.\n“modes” (alternative translations for an input) in the training data. Besides, one thread of research explores the use\nof latent variables or word alignments to reduce the nondeterminism in the translation process (Gu, Cho, and Li\n2017; Ma et al. 2019; Shu et al. 2020; Song, Kim, and Yoon\n2021), and another thread of research focuses on designing\nrobust training objectives for NAT to mitigate the effect of\nmulti-modality (Libovick´y and Helcl 2018; Shao et al. 2020;\nGhazvininejad et al. 2020; Du, Tu, and Jiang 2021).\nOur work handles the multi-modality problem in an unexplored way. Since the reference sentence may be inappropriate for the training, we introduce a rephraser to provide\na better training target for NAT by rephrasing the reference\nsentence according to the NAT output. As Figure 1 shows,\nwhen the reference is “I ate pizza this morning” and the NAT\noutput is “this morning I ate apple”, although the only prediction error is ‘apple’, predictions of all positions will be\nhighly penalized by the inappropriate reference. If the reference can be rephrased to “this morning I ate pizza”, then all\npredictions will be correctly evaluated, while the target-side\nsemantics can still be preserved.\nHow to obtain a good rephraser is the central focus of\nour approach. We use a shallow Transformer decoder as\nthe rephraser. Since there is no direct supervision for the\nrephraser, we quantify our requirements for the rephraser\nas reward functions and optimize them with reinforcement\nlearning. First, the rephraser output should ﬁt well with the\nNAT output, otherwise an inappropriate reference will result\nin a large and inaccurate training loss. Therefore, the ﬁrst\nreward is related to the training loss on the rephraser output. Second, the rephraser output should still maintain the\noriginal target-side semantics, so the second reward is the\nsimilarity between the reference sentence and rephraser output. Finally, we use an annealing strategy to ﬁnd a balance\nbetween the two rewards.\nWe evaluate our approach on major WMT benchmarks\n(WMT14 En↔De, WMT16 En↔Ro) and NAT baseline\nmodels (vanilla NAT, CMLM, CTC). Experimental results\nshow that our approach consistently improves the translation quality of NAT. Speciﬁcally, our best variant achieves\ncomparable performance to the autoregressive Transformer,\nwhile being 14.7 times more efﬁcient in inference.",
        "background": "In this section, we brieﬂy introduce different probability\nmodels for neural machine translation (NMT). We use X\nto denote the source sentence and use Y = {y1, ..., yT } to\ndenote the reference sentence.\nAutoregressive NMT Autoregressive (AT) sequence models have achieved great success on machine translation, with\ndifferent choices of architectures such as RNN (Bahdanau,\nCho, and Bengio 2015), CNN (Gehring et al. 2017), and\nTransformer (Vaswani et al. 2017). Autoregressive models\nfactorize the translation probability as follows and maximize\nit with the cross-entropy loss:\nLAT (θ) = −\nT\nX\nt=1\nlog(p(yt|X, y<t, θ)).\n(1)\nDuring the inference, the preceding predicted tokens have\nto be fed into the decoder to generate the next token, which\nleads to the high translation latency of autoregressive NMT.\nVanilla NAT Non-autoregressive translation with the parallelizable Transformer architecture has been proposed to reduce the translation latency (Gu et al. 2018). NAT is able\nto generate all target tokens simultaneously since it breaks\nthe sequential dependency in probability factorization. The\nvanilla NAT is also trained with the cross-entropy loss:\nLNAT (θ) = −\nT\nX\nt=1\nlog(p(yt|X, θ)).\n(2)\nThe vanilla NAT is equipped with a length predictor that predicts the target length during the inference, and the translation of the given length is obtained by argmax decoding.\nCMLM CMLM is an iterative decoding approach based\non the conditional masked language model (Ghazvininejad\net al. 2019). CMLM predicts the masked target tokens based\non the source sentence and observed target tokens:\nLCMLM(θ) = − log(P(Ymask|X, Yobs, θ)),\n(3)\nwhere Ymask and Yobs represent the masked and observed\ntarget tokens respectively. During the training, Ymask is randomly selected among the target tokens. During the inference, the entire target sentence is masked in the ﬁrst iteration, and then predictions with low conﬁdence are masked\nand predicted again in the following iterations.\nCTC Recently, CTC (Graves et al. 2006) is receiving increasing attention in NAT for its superior performance and\nthe ﬂexibility of variable length prediction (Libovick´y and\nHelcl 2018; Saharia et al. 2020). CTC-based NAT does\nnot require a length predictor but generates an overlong\nsequence containing repetitions and blank tokens, which\nwill be removed by a collapse function Γ−1 in the postprocessing to recover a normal sentence. CTC considers all\nsequences A which the target sentence Y can be recovered\nfrom, and marginalize the log-likelihood with dynamic programming:\nLCT C(θ) = − log\nX\nA∈Γ(Y )\np(A|X, θ),\n(4)\nwhere the probability p(A|X, θ) is modeled by a nonautoregressive Transformer.",
        "approach": "In this section, we describe our approach in detail. First,\nwe present the overview of our model, which incorporates\nthe rephraser module into the encoder-decoder architecture. Then, we introduce the training methods for NAT with\nrephraser. Finally, we discuss the extensions of our approach\non other major NAT baselines (i.e., CMLM and CTC).\nModel Overview\nWe present the overview of our model in Figure 2. In addition to the traditional encoder-decoder architecture, we introduce a rephraser module to rephrase the reference sentence according to the NAT output, which aims to provide\na better training target for NAT. The rephraser only affect\nthe training stage, so the inference cost remains the same.\nThe total training cost of our approach is around 1.3 times\ncompared to the NAT baseline.\nAs Figure 1 illustrates, we hope that the rephraser\ncan rephrase the reference sentence into a form suitable\nfor model training, which depends on both the reference\nsentence and NAT output. Therefore, we use the nonautoregressive Transformer decoder as the architecture of\nrephraser, which takes the reference Y as the input and\ncross-attends to the output of NAT decoder. We use Yr =\n{yr\n1, ..., yr\nT } to denote the rephraser output and use Pr to denote the probability distribution of rephraser, which can be\ndecomposed into the following form:\nPr(Yr|X, Y, θ) =\nT\nY\nt=1\npr(yr\nt |X, Y, θ).\n(5)\nNotice that here the rephraser does not change the target\nlength, otherwise the cross-entropy loss will be inapplicable.\nWe let the rephraser be a shallow model with the number of\nlayers Nr = 2 to reduce the additional training cost.\nAs Figure 2 shows, we train NAT with the rephraser output Yr rather than the original reference Y . Speciﬁcally, we\napply argmax decoding to obtain the rephraser output Yr and\ncalculate the cross-entropy loss based on Yr:\nLr(θ) = −\nT\nX\nt=1\nlog(p(yr\nt |X, θ)).\n(6)\nFigure 2: The architecture of the vanilla NAT with rephraser. The rephraser is stacked behind the decoder to provide a better\ntraining target for NAT. The cross-entropy loss, which is used to train the NAT model, is calculated based on the rephraser\noutput rather than the reference sentence. The rephraser is trained to optimize two rewards, corresponding to the similarity with\nthe reference sentence and the loss of NAT model respectively.\nSince there is no direct supervision for the rephraser, we\nquantify our requirements for the rephraser as two reward\nfunctions and optimize them with reinforcement learning.\nThe details will be discussed in the next section.\nTraining\nIn this section, we will describe in detail how to train the\nNAT model with a rephraser. We apply reinforcement learning to train the rephraser to produce better training targets for\nNAT. In order to make the learning of rephraser easier and\navoid falling into local optima, we use a pre-training strategy\nto establish a good initial state for the rephraser. Since our\nmethod is sensitive to a hyperparameter that interpolates between two reward functions, we utilize a annealing strategy\nto ﬁnd an optimal interpolation.\nReinforcement Learning\nWe apply reinforcement learning to train the rephraser since there is no direct supervision\nfor it. The objective of the rephraser is to provide an appropriate training target for NAT, where the appropriateness can\nbe quantiﬁed as two reward functions. As illustrated in Figure 1, the rephraser output should ﬁt well with the NAT output, otherwise the training loss for NAT will be dramatically\noverestimated. Therefore, we use a reward Rloss to encourage the reduction of training loss. Besides, the rephraser output should not change the target-side semantics, so we use\na reward Rsim to measure the semantic similarity between\nthe reference sentence and rephraser output.\nGiven the rephraser output Yr = {yr\n1, ..., yr\nT }, we deﬁne\nthe reward Rloss as the negative training loss of NAT, which\nis the log-likelihood of Yr:\nRloss(Yr) =\nPT\nt=1 log(p(yr\nt |X, θ))\nT\n,\n(7)\nwhere the length normalization keeps the scale of reward\nstable. We use S(Y1, Y2) to denote the similarity function\nthat measures the semantic similarity between Y1 and Y2,\nand deﬁne the reward Rsim as:\nRsim(Yr) = S(Y, Yr).\n(8)\nThe similarity function can be chosen from the evaluation\nmetrics in machine translation (e.g., BLEU (Papineni et al.\n2002), METEOR (Banerjee and Lavie 2005), BERTScore\n(Zhang* et al. 2020)), and we use BLEU in our experiments.\nWe use a hyperparameter α ∈ [0, 1] to interpolate between\nthe two rewards for the rephraser:\nR(Yr) = αRsim(Yr) + (1 − α)Rloss(Yr).\n(9)\nWe apply the REINFORCE algorithm (Williams 1992) to\noptimize the expected reward:\n∇θJ (θ) = ∇θ\nX\nYr\nPr(Yr|X, Y, θ)R(Yr)\n=\nE\nYr∼Pr\n[∇θ log Pr(Yr|X, Y, θ)R(Yr)].\n(10)\nSpeciﬁcally, we apply monte carlo sampling that samples\na sentence Yr with its probability Pr(Yr|X, Y, θ). In nonautoregressive models, it is equivalent to sampling a word\nyt in each position t, which is very efﬁcient since the whole\nprobability distribution is available. Then we calculate the\nreward R(Yr) and update the rephraser with the estimated\ngradient ∇θ log Pr(Yr|X, Y, θ)R(Yr). To reduce the variance of gradient estimation, we subtract a baseline reward\nfrom R(Yr) (Weaver and Tao 2001). In our setting, the baseline reward is obtained by sampling additional K = 2 sentences and calculating their average reward.\nPre-training\nIf we directly train the rephraser with reinforcement learning, the rephraser will be trapped in a\nnon-optimal state that it only outputs meaningless sentences\ncomposed of some most frequent words. This is a common\nproblem of reinforcement learning since it is based on exploration and therefore sensitive to the initial state.\nIn order to make the learning of rephraser easier and avoid\nfalling into local optima, we use a pre-training strategy to establish a good initial state for the rephraser. As the optimal\nrephraser output is usually a combination of the reference\nsentence and NAT output, we let the rephraser learn from\nboth of them in the pre-training stage. Speciﬁcally, we use\nthe average of two cross-entropy losses to train the rephraser,\nwhere the ﬁrst is calculated based on the reference sentence\nand the second is calculated based on the argmax decoding\nresult of NAT. As the rephraser is not ready for use, we train\nthe NAT model with the original cross-entropy loss (Equation 2) in the pre-training stage.\nAnnealing\nIn reinforcement learning, we use a hyperparameter α to interpolate between the two rewards Rloss and\nRsim. The hyperparameter α is critical since it determines\nthe behavior of the rephraser, but ﬁnding the optimal α with\ngrid-search will dramatically increase the training cost. To\nsolve this problem, we replace the expensive grid-search\nwith an annealing strategy, which helps us efﬁciently ﬁnd\nthe optimal interpolation.\nThe behavior of the rephraser is determined by the value\nof α, where the rephraser output is closer to the reference\nunder a high α and closer to the NAT output under a low α.\nIt will be harmful to the NAT model if α is much lower than\noptimal, where the NAT model may become over-conﬁdent\nand easily collapse. In comparison, the impact of a high α\nis more moderate since the training target will be close to\nthe reference sentence. Therefore, if we gradually decrease\nthe value of α after the pre-training, it will act like a gradual\nﬁne-tuning process that the model gradually performs better\nuntil the optimal α is reached. With this strategy, we only\nneed to select checkpoints on the validation set to ﬁnd the\noptimal interpolation.\nSpeciﬁcally, let the number of ﬁne-tuning steps be T and\nthe current step be t. We use a linear annealing schedule to\ngradually decrease α from αmax to αmin in the ﬁne-tuning:\nα = t\nT αmin + (1 − t\nT )αmax.\n(11)\nThough we replace α with two hyperparameters αmax and\nαmin, the model performance is not very sensitive to them.\nTherefore, we do not need to tune them carefully and can use\nthe same set of hyperparameters across different datasets.\nExtensions\nCurrently, we have discussed how to incorporate the\nrephraser module into the vanilla NAT. Actually, our approach is a general framework that does not assume a speciﬁc NAT model. In this section, we extend our approach\nto CMLM and CTC, which are major NAT baselines introduced in section .\nCMLM The main difference between CMLM and vanilla\nNAT is that CMLM has observed some target tokens and\nonly predicts the masked tokens in the training. Therefore,\nwe let the observed tokens remain unchanged and only\nrephrase the reference of the masked tokens.\nCTC The major feature of CTC-based NAT is the ﬂexibility\nof variable length prediction, which allows the rephraser to\nchange the target length. Therefore, we use a CTC-based\nrephraser structure, which takes the decoder output as input\nand cross-attends to the embedding of the reference sentence\nY . Similarly, we remove the repetitions and blank tokens to\nobtain the rephraser output Yr and calculate the probability\nPr(Yr) with dynamic programming.",
        "experiments": "Experimental Setup\nData\nWe\nconducted\nexperiments\non\nmajor\nbenchmarking\ndatasets\nin\nprevious\nNAT\nstudies:\nWMT14\nEnglish↔German (En↔De, 4.5M sentence pairs) and\nWMT16 English↔Romanian (En↔Ro, 0.6M sentence\npairs). For WMT14 En↔De, the validation set is newstest2013 and the test set is newstest2014. For WMT16\nEn↔Ro, the validation set is newsdev-2016 and the test set\nis newstest-2016. We learn a joint BPE model (Sennrich,\nHaddow, and Birch 2016) with 32K merge operations to\nprocess the data and share the vocabulary for source and\ntarget languages. We use BLEU (Papineni et al. 2002) to\nevaluate the translation quality.\nKnowledge Distillation We follow previous works on NAT\nto apply sequence-level knowledge distillation (Kim and\nRush 2016) to reduce the complexity of training data.\nWe employ the base version of autoregressive Transformer\n(Vaswani et al. 2017) as the teacher model and train NAT\nmodels on the translations generated by teachers.\nNAT Baselines We conduct experiments on three major\nNAT baselines: vanilla NAT (Gu et al. 2018), CMLM\n(Ghazvininejad et al. 2019), and CTC (Libovick´y and Helcl\n2018). We adopt Transformer-base as the model architecture. For vanilla NAT and CTC, we apply the uniform copy\n(Gu et al. 2018) to construct decoder inputs. For CTC,\nwe also evaluate the rephraser performance when the nonmonotonic latent alignments (NMLA) objective (Shao and\nFeng 2022) is applied for ﬁne-tuning.\nImplementation Details We set αmax to 0.75, αmin to 0.5,\nthe sampling times K to 2, and the depth of rephraser Nr to\n2 across all datasets. For CTC, the length for decoder inputs\nis 3× as long as the source length. All models are optimized\nwith Adam (Kingma and Ba 2014) with β = (0.9, 0.98)\nand ϵ = 10−8. For vanilla NAT and CTC, each batch contains approximately 64K source words. For CMLM, to keep\nconsistency with previous works (Ghazvininejad et al. 2019,\n2020; Du, Tu, and Jiang 2021), we use the batch size 128K\nand use 5 length candidates for inference. All models are\npre-trained for 300K steps and ﬁne-tuned for 30K steps.\nDuring the ﬁne-tuning, we measure validation BLEU for every 500 steps and average the 5 best checkpoints to obtain\nthe ﬁnal model. We use the GeForce RTX 3090 GPU to train\nmodels and measure the translation latency.\nModels\nIter\nSpeed\nWMT14\nWMT16\nEN-DE\nDE-EN\nEN-RO\nRO-EN\nAT\nTransformer (teacher)\nN\n1.0×\n27.54\n31.57\n34.26\n33.87\nTransformer (12-1)\nN\n2.6×\n26.09\n30.30\n32.76\n32.39\n+ KD\nN\n2.7×\n27.61\n31.48\n33.43\n33.50\nVanilla NAT\nNAT-FT (Gu et al. 2018)\n1\n15.6×\n17.69\n21.47\n27.29\n29.06\nBag-of-ngrams (Shao et al. 2020)\n1\n10.7×\n20.90\n24.61\n28.31\n29.29\nEM (Sun and Yang 2020)\n1\n16.4×\n24.54\n27.93\n–\n–\nGLAT (Qian et al. 2021)\n1\n15.3×\n25.21\n29.84\n31.19\n32.04\nVanilla NAT (ours)\n1\n15.6×\n20.42\n24.88\n29.21\n29.37\nVanilla NAT w/ rephraser\n1\n15.6×\n25.33\n29.57\n31.63\n31.72\nCMLM\nCMLM (Ghazvininejad et al. 2019)\n1\n–\n18.05\n21.83\n27.32\n28.20\nCMLM + DSLP (Huang et al. 2022a)\n1\n15.0×\n21.76\n25.30\n30.29\n30.89\nCMLM + AXE (Ghazvininejad et al. 2020)\n1\n–\n23.53\n27.90\n30.75\n31.54\nCMLM + OAXE (Du, Tu, and Jiang 2021)\n1\n–\n26.10\n30.20\n32.40\n33.30\nCMLM (ours)\n1\n15.0×\n18.21\n22.86\n28.15\n28.97\nCMLM w/ rephraser\n1\n15.0×\n26.65\n30.70\n32.72\n33.03\nCTC\nCTC (Libovick´y and Helcl 2018)\n1\n–\n16.56\n18.64\n19.54\n24.67\nImputer (Saharia et al. 2020)\n1\n–\n25.80\n28.40\n32.30\n31.70\nREDER (Zheng et al. 2021)\n1\n15.5×\n26.70\n30.68\n33.10\n33.23\nFully-NAT (Gu and Kong 2021)\n1\n16.8×\n27.20\n31.39\n33.71\n34.16\nNMLA (Shao and Feng 2022)\n1\n14.7×\n27.57\n31.28\n33.86\n33.94\nDDRS (Shao, Wu, and Feng 2022)\n1\n14.7×\n27.60\n31.48\n34.60\n34.65\nCTC (ours)\n1\n14.7×\n26.34\n29.58\n33.45\n33.32\nCTC w/ rephraser\n1\n14.7×\n27.32\n30.97\n33.80\n33.84\nNMLA w/ rephraser\n1\n14.7×\n27.81\n31.53\n34.08\n34.03\nTable 1: Performance comparison between our models and existing methods. AT means autoregressive. ‘12-1’ means the Transformer with 12 encoder layers and 1 decoder layer. ‘Iter’ means the number of decoding iterations.\nModels\nIter\nWMT14\nWMT16\nEN-DE\nDE-EN\nEN-RO\nRO-EN\nCMLM + AXE (Ghazvininejad et al. 2020)\n1\n20.40\n24.90\n30.47\n31.42\nCMLM + OAXE (Du, Tu, and Jiang 2021)\n1\n22.40\n26.80\n–\n–\nCMLM (ours)\n1\n10.82\n14.64\n23.51\n24.25\nCMLM w/ rephraser\n1\n23.12\n27.44\n32.30\n32.07\nTable 2: The performance of CMLM with rephraser and other existing methods on raw data. ‘–’ means not reported.\nMain Results\nWe report the performance of our models and existing oneiteration NAT approaches in Table 1. By rephrasing the reference sentence, our approach improves all baseline models\nby a large margin and achieves comparable performance to\nthe state-of-the-art method on each baseline model. Notably,\nour approach achieves an average improvement of 6.2 BLEU\non CMLM and 3.6 BLEU on vanilla NAT. The rephraser\nalso works well on the strong baseline of CTC and NMLA\nand achieves the comparable performance to autoregressive\nTransformer while being much faster in inference.\nWe also evaluate our approach on iterative NAT. In Figure\n3, we report the performance of CMLM models under different iterations. On WMT14 En-De, CMLM with rephraser\nconsistently outperforms the CMLM baseline, showing that\nthe rephraser is also helpful for iterative NAT.\nThe above models are trained with the help of sequencelevel knowledge distillation, which replaces the reference\nwith the output of a teacher model. As our rephraser is also\ncapable of modifying the reference, we conduct experiments\non raw data to see whether our approach can be used without knowledge distillation. In Table 2, we report the performance of rephraser and other existing methods on the\nCMLM baseline. CMLM with rephraser signiﬁcantly improves the baseline by more than 10 BLEU on average and\nalso outperforms other existing methods. Compared with the\nknowledge distillation variant, the performance degradation\nis about 3.5 BLEU on WMT14 En↔De and less than 1\nBLEU on WMT16 En↔Ro. Though the rephraser cannot\nreplace knowledge distillation yet, the relatively small performance degradation shows the potential in this direction.\nAblation Study\nIn this section, we conduct ablation studies on WMT14 EnDe validation set to justify the settings in the main experiment, including the model size, rephraser architecture, reFigure 3: Performance comparison between CMLM and\nCMLM with rephraser under different iterations.\nCTC\nParams\nSpeed\nBLEU\n+2 layers\n+30K steps\n62.10M\n14.7×\n24.77\n✓\n70.51M\n12.9×\n24.89\n✓\n62.10M\n14.7×\n24.80\n✓\n✓\n70.51M\n12.9×\n24.87\nCTC w/ rephraser\n70.51M\n14.7×\n25.54\nTable 3: The effect of 2 extra decoder layers and 30K extra\ntraining steps on the CTC model. ‘Params’ means the number of parameters. ‘Speed’ means the speedup to autoregressive Transformer under batch size 1.\nward function, and annealing strategy.\nModel Size & Training Steps Our approach requires 2 extra decoder layers for the rephraser module and 30K extra\ntraining steps for the rephraser ﬁne-tuning. Considering that\nthese factors may also strengthen the NAT baseline, we conduct experiments on CTC to study their effects. As Table\n3 shows, the extra decoder layers and training steps only\nhave marginal effects on the model performance. In comparison, CTC with rephraser outperforms these variants, and\nthe rephraser does not affect the decoding speed.\nRephraser Architecture As the rephraser is the core of\nour approach, its architecture will have a certain impact on\nmodel performance. Our rephraser is a Transformer decoder\nthat is fed the reference Y and cross-attends to the output of\nNAT decoder. There are other choices of the rephraser architecture that has the NAT output and reference Y as inputs.\nFor example, we can swap the two inputs of the rephraser\nand we call this architecture ‘swap’, which is fed the NAT\noutput and cross-attends to the reference Y . We can also\ncompress the two inputs into one with a feed-forward layer\nand then feed it into a Transformer encoder, and we call this\narchitecture ‘enc’. Besides, we can change the depth of the\nrephraser, and we try two choices Nr = 1 and Nr = 4. We\ntry these variants on vanilla NAT and report the results in\nTable 4.\nModels\nbase\nswap\nenc\nNr = 1\nNr = 4\nBLEU\n24.06\n24.01\n23.61\n23.53\n24.09\nTable 4: The performance of vanilla NAT with different\nrephraser architectures.\nMetrics\nBLEU\nMETEOR\nBERTScore\nBLEU\n24.06\n52.88\n84.24\nMETEOR\n23.71\n52.65\n84.11\nBERTScore\n23.07\n52.02\n83.80\nTable 5: The performance of vanilla NAT with different reward functions for the rephraser.\nWe can see that the Transformer decoder architecture is\nbetter than the encoder, and swapping the two inputs of the\ndecoder does not have an obvious effect on the model performance. Regarding the depth of rephraser, increasing Nr\nto 4 does not make a big difference, but reducing Nr to 1\nwill certainly degrade the model performance.\nReward Function In the default setting, we use BLEU to\nmeasure the similarity between the reference and rephraser\noutput. As our approach is capable of incorporating different\nevaluation metrics as reward functions, we investigate the effect of two other reward functions METEOR (Banerjee and\nLavie 2005) and BERTScore (Zhang* et al. 2020) on vanilla\nNAT. As shown in Table 5, the BLEU reward performs better\nthan METEOR and BERTScore under all these three evaluation metrics, which is a little surprising since BERTScore is\na better evaluation metric based on pre-trained models. We\nspeculate that the strength of BLEU lies in its robustness,\nwhich is solely based on the statistical n-gram matching.\nAnnealing We replace the static α with a annealing strategy\nfrom αmax to αmin, which is less sensitive to hyperparameters. To justify this claim, we deviate the hyperparameters\n{α, αmax, αmin} from the optimal value by ∆ and report\nhow the model performance is affected in Table 6. Though\nthe model is sensitive to the static α, the performance of\nannealing is almost unaffected by a small deviation of hyperparameters, which well supports our claim.\nAnalysis\nIn this section, we present a qualitative analysis on generated outputs of the WMT14 En-De test set to better understand where the improvements come from and whether the\nrephraser can alleviate the multi-modality problem.\nPrediction Conﬁdence Due to the multi-modality problem,\nNAT may consider many possible translations at the same\ntime, which makes NAT less conﬁdent in generating outputs.\nTherefore, we compare the prediction conﬁdence between\nbaseline models and our models to investigate whether the\nrephraser can alleviate the multi-modality problem. We use\nthe information entropy H(X) = −P\nx p(x) log p(x) to\nmeasure the prediction conﬁdence, where lower entropy indicates higher conﬁdence. Table 7 shows that NAT models\n∆\n0\n0.05\n-0.05\nAnneal\n24.06\n24.03\n23.88\nStatic\n23.97\n23.55\n23.63\nTable 6: The performance of vanilla NAT with deviation ∆\nfrom the optimal setting. ‘Anneal’ represents annealing and\n‘Static’ represents the static α.\nModels\nVanilla NAT\nCMLM\nCTC\nw/o rephraser\n2.07\n2.64\n0.26\nw rephraser\n1.35\n1.52\n0.06\nTable 7: The average entropy of NAT models. Lower entropy\nindicates higher prediction conﬁdence.\nwith rephraser have much lower entropy than baseline models, illustrating the effectiveness of rephraser in alleviating\nthe multi-modality problem.\nToken Repetitions With low prediction conﬁdence, NAT\nmay generate a mixture of many possible translations, which\nmakes the translation inconsistent and typically contains\nmany repetitive tokens. Therefore, we report the percentage of token repetitions in Table 8, which shows that our\napproach can signiﬁcantly reduce token repetitions. Notice\nthat we do not report the results of CTC since it naturally\ncontains no token repetitions.\nCase Study\nWe present a case of the rephraser output in Table 9 to better understand the working mechanism of rephraser. Before\nrephrasing, the reference is not aligned with NAT output\nsince there exists the multi-modality problem (“but our loyalty is clear.” and “but it is not about our loyalty.”). After\nrephrasing, the reference basically keeps the original semantics and is well aligned with NAT output.",
        "related work": "Gu et al. (2018) ﬁrst proposed non-autoregressive Transformer to reduce the latency of machine translation by generating outputs in parallel. However, there is a large performance gap between the AT and the earliest NAT model,\nwhich is mainly attributed to the multi-modality problem.\nA number of efforts have explored ways to help NAT handle the multi-modality problem. One thread of research reduces the modality in the target space by leaking part of the\ntarget information in the training. Kaiser et al. (2018); Ma\net al. (2019); Shu et al. (2020); Bao et al. (2021) augmented\nNAT with latent variables based on vector quantization or\nvariational inference. Besides, Gu et al. (2018); Ran et al.\n(2021); Bao et al. (2019); Song, Kim, and Yoon (2021) explored the use of word alignments to help the generation\nof NAT, and Akoury, Krishna, and Iyyer (2019); Liu et al.\n(2021) enriched NAT with syntactic structures.\nAnother thread of research focuses on designing robust\ntraining objectives for NAT to mitigate the effect of multimodality. Wang et al. (2019) introduced regularization terms\nModels\nVanilla NAT\nCMLM\nw/o rephraser\n10.2%\n15.4%\nw rephraser\n2.6%\n2.3%\nTable 8: The percentage of token repetitions in the generated\noutputs of Vanilla NAT and CMLM.\nReference\nOur identity might sometimes be fu@@\nzz@@ y , but our loy@@ alty is clear .\nNAT\nOur identity may sometimes be urred ,\nbut it is not about our loy@@ alty .\nRephraser\nOur identity might sometimes be vague ,\nbut that is not about our loy@@ alty .\nTable 9: A case of the rephraser output from the validation\nset of WMT14 De-En.\nto reduce errors of repeated and incomplete translations. (Tu\net al. 2020) viewed NAT as an inference network trained\nto minimize the autoregressive teacher energy. Shao et al.\n(2019, 2020, 2021) introduced sequence-level training objectives for NAT. Ghazvininejad et al. (2020); Du, Tu, and\nJiang (2021) improved the cross-entropy loss with better\nalignments. Recently, the CTC loss (Graves et al. 2006) is\nreceiving increasing attention in NAT (Libovick´y and Helcl\n2018; Saharia et al. 2020; Gu and Kong 2021; Shao and Feng\n2022), which is further enhanced with a directed acyclic\ngraph to explicitly model the probability of transistion paths\n(Huang et al. 2022b; Shao, Ma, and Feng 2022).\nOur work is closest to diverse distillation (Shao, Wu,\nand Feng 2022), which provides multiple references in the\ndataset and dynamically select one reference to train the\nmodel. Diverse distillation has a high distillation cost, and\nthe provided references cannot ensure that all possible outputs are covered. Our work overcomes these limitations by\ndirectly rephrasing the reference, which has a lower cost and\ncan provide a more ﬂexible reference.\nWe train the rephraser with the reinforcement learning\ntechnique (Williams 1992), which is widely used in neural\nmachine translation to optimize sequence-level objectives\nlike BLEU (Ranzato et al. 2016; Bahdanau et al. 2017; Wu\net al. 2018; Shao et al. 2021). There are also works on paraphrasing the reference to provide a better training target for\nNMT (Sekizawa, Kajiwara, and Komachi 2017; Zhou, Sperber, and Waibel 2019; Freitag et al. 2020), where the paraphrasing is conducted by human or a paraphrase dictionary.",
        "conclusion": "Since the reference may be inappropriate for the training of\nNAT, we propose rephrasing the reference to provide a better training target for NAT. We apply reinforcement learning to obtain a good rephraser and train NAT based on the\nrephraser output. Experiments on major benchmarks and\nNAT baselines demonstrate the effectiveness of our method.",
        "summary_en": "Non-autoregressive neural machine translation (NAT) models suffer from the multi-modality problem that there may exist multiple possible translations of a source sentence, so the reference sentence may be inappropriate for the training when the NAT output is closer to other translations. In response to this problem, this paper introduces a rephraser to provide a better training target for NAT by rephrasing the reference sentence according to the NAT output. As the paper trains NAT based on the rephraser output rather than the reference sentence, the rephraser output should fit well with the NAT output and not deviate too far from the reference, which can be quantified as reward functions and optimized by reinforcement learning. Experiments on major WMT benchmarks and NAT baselines show that the approach consistently improves the translation quality of NAT. Specifically, the best variant achieves comparable performance to the autoregressive Transformer, while being 14.7 times more efficient in inference.",
        "summary_zh": "这篇论文介绍了一种重述器，旨在解决NAT存在的多模态问题，即源句可能有多种可能的翻译，导致参考句与NAT的输出不匹配的情况。这个重述器根据NAT输出重述参考句，为NAT提供更好的训练目标。通过使用重述器的输出而不是参考句来训练NAT，确保了重述器的输出与NAT输出非常匹配，并且不会偏离参考句太远。这个过程可以量化为奖励函数，并通过强化学习进行优化。在主要的WMT基准和NAT基准上的实验结果表明，该方法持续提高了NAT的翻译质量。特别是，该方法最佳变量在推理效率上比自回归Transformer高出14.7倍。"
    },
    {
        "title": "Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation",
        "abstract": "Benefiting from the sequence-level knowledge distillation, the Non-Autoregressive Transformer (NAT) achieves great success in neural machine translation tasks. However, existing knowledge distillation has side effects, such as propagating errors from the teacher to NAT students, which may limit further improvements of NAT models and are rarely discussed in existing research. In this paper, we introduce selective knowledge distillation by introducing an NAT evaluator to select NAT-friendly targets that are of high quality and easy to learn. In addition, we introduce a simple yet effective progressive distillation method to boost NAT performance. Experiment results on multiple WMT language directions and several representative models show that our approach can realize a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances. Further analysis shows that distilling only 5% of the raw translations can help an NAT outperform its counterpart trained on raw data by about 2.4 BLEU.",
        "introduction": "Non-autoregressive Transformer (NAT, Gu et al. 2018) introduces a promising paradigm of parallel decoding. Unlike sequentially predicting the words in an autoregressive\nmodel, NAT models can generate a sentence in parallel\nbased on a conditional independence assumption, improving the inference speed by over 10 times. Besides, such a\nparallel decoding paradigm also has the potential to avoid\nthe exposure bias that has a long-term discussion in sequential decoding models (Vaswani et al. 2017). As a result, we\nsee NAT models achieve great success in machine translation tasks (Qian et al. 2021b), surpassing many autoregressive models in WMT211.\nDespite the great potential of NAT models, they rely on\nsequence-level knowledge distillation (KD, Kim and Rush\n2016) to achieve success. The introduced conditional independence assumption prevents NAT models from leveraging the inherent structures to overcome the multi-modality\nproblem, where each input may correspond to several valid\noutputs in the training data. In such background, Gu et al.\nFigure 1: An illustration of our selective knowledge distillation. Standard knowledge distillation reduces the complexity\nof raw data at the cost of translation quality. In contrast, we\npropose combining the merits of raw and KD data, balancing the complexity and quality of training data.\n(2018) introduce sequence-level knowledge distillation to\nbypass the multi-modality problem of NAT models. They\nfirst train an autoregressive Transformer (AT, Vaswani et al.\n2017) as a teacher model, and then train the NAT models using the teacher’s output as targets. The deterministic outputs\ngenerated by the teacher can directly avoid the one-to-many\nsituation in raw training data and improve the performance\nof an NAT model by over 5.0 BLEU (Papineni et al. 2002)\nin machine translation.\nHowever, there are still several problems in standard\nknowledge distillation, which may limit the performance\nof NAT models. First, NAT models learning only from AT\nteachers may miss some important knowledge in the original\ndata, such as prediction on low-frequency words (Ding et al.\n2021b). Second, the outputs generated by the AT teacher are\nnot necessarily suitable for the training of NAT models, as\nthese architectures have quite different modeling paradigms.\nIt should be noted that existing NAT research (Gu et al.\n2018; Ghazvininejad et al. 2019; Guo, Xu, and Chen 2020;\nBao et al. 2021) only regards knowledge distillation as a necessary data processing technique but lacks a deeper discussion. Therefore, designing knowledge distillation strategies\nto help NAT models learn better is still an open question.\n#\nraw/distilled\noutputs\nTranslation #1\nraw\nI entirely agree with the PPE Group that paragraph 9 is central .\ndistilled\nI fully agree with the PPE Group that paragraph 9 is of key importance .\nTranslation #2\nraw\nThat is up to the Heads of Governments to do this week .\ndistilled\nThat is up to the Heads of Government to do this this week .\nTranslation #3\nraw\nOnce you have started reading , you can not put it down .\ndistilled\nAnyone who starts reading it keeps his breath until the last word .\nTranslation #4\nraw\nNice and clean hotel in great location , great value for money .\ndistilled\nGravino Cinco is fresher and newer than Tryp Ciudad Hotel !\nTable 1: Examples of different situations. Our method mainly improves performance on sentences like Translation 2, where\nminor mistakes are introduced by the AT teacher. In our experiments, the NAT evaluator generated a translation exactly the\nsame as the distilled one, while the NAT student trained on selected data corrected the mistake by removing the repeated token.\nIn this paper, we propose a selective knowledge distillation technique for training NAT models to tackle the two\nissues in standard knowledge distillation. More specifically,\nwe introduce an NAT model trained on distilled data as an\nevaluator to construct the training data, replacing the original distilled data with raw data dynamically in the learning progress. There are two intuitions behind our selective knowledge distillation: First, our approach can access\nraw data and avoid repeating the mistakes made by the AT\nteacher. Second, due to its similar modeling paradigm, the\nNAT evaluator can effectively assess whether the data is suitable for the training of NAT students. The NAT evaluator\njudges each sentence in the original training set by scoring the predicted tokens. We select sentences with higher\nscores as the targets which generally contain minor modality change from the distilled data but show better translation\nquality as raw data. In tuition, these sentences can be safely\nexposed to NAT students during training. Besides, we introduce a hard-to-easy curriculum learning strategy while training, which has been demonstrated effective for automatic\nspeech recognition systems (Braun, Neil, and Liu 2017).\nWe conduct experiments on two widely-used machine\ntranslation benchmarks, WMT14 En-De and WMT16 EnRo and over an inference-efficient AT structure (Kasai et al.\n2020) and two representative NAT architectures (Qian et al.\n2021a; Ghazvininejad et al. 2019). Experiment results show\nthat our selective knowledge distillation consistently improves models’ performance on each dataset. Further analyses show that a small ratio (5%) of distilled data is sufficient to improve NAT significantly, demonstrating that our\nmethod can effectively select the NAT-friendly raw translations. As an early attempt to introduce raw data for training\nNAT models, we hope this work will raise more attention to\nselecting beneficial examples from authentic data to recover\nthe missing information while keeping the merits of knowledge distillation.",
        "background": "Neural Machine Translation can be defined as a sequenceto-sequence generation problem: given source sentence\nX = {x1, x2, · · · , xN}, to generate target sentence Y =\n{y1, y2, · · · , yL} according to P(Y |X, θ), where θ denotes\nthe parameters of a network.\n2.1\nNon-Autoregressive Neural Machine\nTranslation\nNon-Autoregressive Transformer (NAT, Gu et al. 2018) imposes the conditional independence assumption among target words while factorizing the probability P(Y |X; θ):\nP(Y |X, θ) = P(L|X, θ)\nL\nY\ni=1\nP(yi|X, θ)\nwhere L is the length of the target sequence.\nThe conditional independence assumption allows NAT\nto significantly outperform autoregressive Transformer (AT)\nin inference speed, but it also leads to an inferior translation quality compared to AT. A well-recognized explanation\nis that NAT models suffer from the multi-modality problem (Gu et al. 2018), where the model fails to capture the\nhighly multimodal distribution of target translations adequately. For example, a source sentence might have several\nground-truth translations that differ in wording and structure, and NAT models are likely to get confused since they\nhave to select from multiple choices only through the source\nsentence. In contrast, an AT model can easily learn these different translations by predicting tokens based on the source\nsentence and previous tokens.\n2.2\nKnowledge Distillation\nTo alleviate the multi-modality problem, sequence-level\nknowledge distillation (KD, Kim and Rush 2016) is adopted\nas a preliminary step for training an NAT model, where\nthe original translations are replaced with those generated\nby a pretrained autoregressive teacher. The distilled data\neases the training by introducing more deterministic knowledge and significantly improves the performance of an NAT\nstudent. Some previous works propose generating several\ndistilled translations and select the most suitable candidate (Zhou, Gu, and Neubig 2019; Shao, Wu, and Feng\n2022) to gain more benefits from knowledge distillation.\nHowever, knowledge distillation has some side effects\nlike leading to more errors on low-frequency words (Ding\net al. 2021b). Due to the differences in architectures between\nAT and NAT, the translations generated by an AT teacher are\nnot always suitable for the learning of NAT. Another obvious limitation of KD is that it propagates mistakes made by\nthe AT teacher to NAT students. Table 1 shows that the translation generated by the AT teacher contains mistakes which\nmight harm the performance of the student. Therefore, how\nto break such limitations of AT-based KD and utilize authentic data to improve the translation quality remains a question.",
        "method": "The intuition behind our method is that introducing raw\ntranslations which do not significantly increase the complexity will not make training much more challenging but\nfree NAT from some mistakes made by the AT teacher. Section §3.1 introduces how to select NAT-friendly raw sentences, combining the high translation quality of raw data\nand reduced complexity of distilled data. Besides, we also\nintroduce a hard-to-easy learning strategy for dynamically\nconfiguring the raw data ratio in the training process, as presented in Section §3.2.\n3.1\nSelecting NAT-friendly Raw Translations\nWhile raw data are of high quality in most cases, the multimodality problem prevents an NAT model from capturing\nthe distribution of target translations properly. Contrarily,\ndistilled data eases the training of NAT by reducing the complexity of targets, but mistakes made by the AT teacher will\nbe easily propagated to the student if only distilled data is\nexposed. It is natural to think that NAT models should learn\nsome missing information in the distilled translations from\nthe original ones to improve translation quality, and a simple\nsolution is to expose part of the raw data to NAT. The question remaining is how to evaluate whether a raw translation\nshould be exposed.\nWe propose to evaluate each translation in the raw data\nthrough an NAT evaluator trained on distilled data, replacing a raw translation with its distilled version when the NAT\nevaluator fails to generate outputs similar to the reference.\nSpecifically, given source sentence X, we first get a decoded output ˆY = fteacher(X) using the NAT evaluator.\nThen we evaluate the raw translation Y through a metric\nscore(X, Y ) = 1 − d(Y, ˆY )/|Y |, which measures the difference between the ground truth translation and the predicted output. The translations with high scores are considered NAT-friendly.\nIn the following part we explain why an NAT evaluator can decide whether a raw sentence can be safely exposed. Since we want to keep both the high translation quality of raw sentences and the simplified modes of distilled\nsentences, a naive answer is that raw sentences with fewer\nmodes can be set as targets for NAT. If the NAT evaluator\ntrained on distilled data can get a prediction close to the raw\ntarget, then the raw and distilled translations are probably\nquite similar in their modes. To illustrate the details, here\nAlgorithm 1: Data Selection for the k-th Update\nRequire: Dk{(X, Y, Y KD)}, NAT evaluator fteacher\n1: Tk ← T0 + k/K · (T1 − T0)\n2: D′\nk ← {}\n3: for all (X, Y, Y KD) ∈ Dk do\n4:\nscore(X, Y ) ← 1 − d(Y, fteacher(X))/|Y |\n5:\nif score(X, Y ) ≥ Tk then\n6:\nD′\nk ← D′\nk ∪ {(X, Y )}\n7:\nelse\n8:\nD′\nk ← D′\nk ∪ {(X, Y KD)}\n9:\nend if\n10: end for\n11: return D′\nk\nwe list four typical situations where the distilled translations\nare different from the original ones:\n• Minor Modality Change: A few words are substituted\nby their synonyms without introducing great changes to\nthe structure and semantics of the raw sentence. Therefore, both the raw and distilled translation can be used as\nthe target.\n• Minor Mistakes: While the structure and semantics of\nthe original sentence is preserved, a few mistakes like\nfalsely predicted low-frequency words or word repetition\nare introduced. Learning from the raw translation can be\nhelpful to correct these mistakes.\n• Dramatic Modality Change: Despite sharing the same\nsemantics, the raw and distilled translation are expressed\nin quite different ways. The raw translation contains\nmodes too challenging for an NAT model.\n• Dramatic Mistakes: The distilled sentence is not welltranslated, but we are not sure whether the raw translation\nis a better target to learn from since even the AT.\nTable 1 provides the examples corresponding to each situation. Minor modality changes (Translation #1) can be tolerated since they do not greatly increase the modes of training\ndata, and correcting minor mistakes (Translation #2) is the\nmain goal of our method. The NAT evaluator is not likely to\nget a close prediction when there exists dramatic differences\nbetween raw and distilled data (Translation #3), so when it\ngives a raw sentence a high score, it is highly likely that\nthe sentence satisfies our requirement of simple and clean\ntranslation. Besides, an NAT evaluator can avoid the cases\nwhere a distilled sentence is close to the original one but still\ntoo challenging for an NAT (Translation #4). Therefore, we\ncan choose to distill only the raw sentences with low scores\nunder the NAT evaluator and keep the rest unchanged. In\nthis way, the dataset displays higher translation quality while\nkeeping the general complexity suitable for NAT.\n3.2\nHard-to-Easy Data Selection\nMotivated by the success of curriculum learning (Qian et al.\n2021a; Guo et al. 2020; Liu et al. 2020), we further introduce a hard-to-easy learning strategy to improve the performance. Ding et al. (2021b) show that pretraining with raw\ndata can improve the performance of NAT by rejuvenating\nlow-frequency words. To keep the merits of low-mode, they\nfurther trained the pretrained model on distilled data. We\ncombine this idea with our data selection method by decreasing the ratio of raw data in the training process. Specifically,\nthe training data for each update can be formulated as:\n{(X, Y )|score(X, Y ) ≥ Tk ∧ (X, Y, Y KD) ∈ Dk}∩\n{(X, Y KD)|score(X, Y ) < Tk ∧ (X, Y, Y KD) ∈ Dk}\nwhere Tk and Dk denote the threshold and the set of tuples (X, Y, ˆY ) for the kth update respectively. Tk can be determined by a preset function or feedbacks from the NAT\nstudent. In our experiments, we adopt a linear function for\nTk which is computed as Tk = T0 + k\nK (T1 − T0), where\nK is the total number of updates, the constants T0 and T1\ncan be determined according to the distribution of score\nP(score(X, Y )) given a specific NAT evaluator and the raw\ntraining data. The whole data selection process can be found\nin Algorithm 1. This process is an additional stage following\nstandard training procedures for NAT, thus being generic to\nvarious data and architectures.",
        "experiments": "4.1\nExperimental Settings\nDatasets\nWe conduct experiments on two widely-used\nmachine translation datasets: WMT14 English-German (EnDe) and WMT16 English-Romanian (En-Ro), which consist\nof 3.96M and 0.6M sentence pairs, respectively. Following\nthe common practices, we process the datasets with Moses\nscript (Koehn et al. 2007) and segment the words into subword units using byte-pair encoding (BPE, Sennrich, Haddow, and Birch 2016). The subword embeddings are shared\nbetween the source and target language. For the sequencelevel knowledge distillation, we employ the Transformer\nwith base settings in Vaswani et al. (2017) as the teacher.\nModel\nWe evaluate our selective knowledge distillation on\nDeepShallow (Kasai et al. 2020), CMLM (Ghazvininejad\net al. 2019), and GLAT+CTC (Qian et al. 2021a). DeepShallow is an inference-efficient AT structure with a deep encoder and a single-layer autoregressive decoder, which also\nbenefits from knowledge distillation. We adopt a 6-layer encoder in the experiments. CMLM iteratively generates the\ntarget sequence from the masked input. For the previous\ntwo models, we compute d(Y, ˆY ) using the Hamming distance PL\ni=1[Yi ̸= ˆYi]. GLAT builds the word interdependencies to improve the performance of single-pass parallel generation. During training, the decoder is fed with randomly masked target sequence, and the number of masked\ntokens depends on the prediction accuracy. The performance\nof GLAT can be further improved by connectionist temporal classification (CTC, Graves et al. 2006), which utilizes\nan alignment-based objective. Another advantage of CTC\nis that it can align the targets according to decoder outputs\nso that ground-truth tokens are not required to be predicted\non a fixed position, thus making the NAT evaluator more\ntolerant to minor mistakes when evaluating a raw translation. To compute score(X, Y ) for applying our approach\non GLAT+CTC, we use dynamic programming to get the\naligned path Y align with the largest align score (Graves et al.\n2006) and adopt the Hamming distance as the metric, which\nis computed as d(Y align, ˆY ) = PL′\ni=1[Y align\ni\n̸= ˆYi].\nTraining Settings\nWe follow the hyperparameters of\nmodels in their original papers. We set the dropout rate to\n0.1 for WMT14 En-De/De-En and 0.3 for WMT16 En-Ro.\nFor the optimizer, we use Adam with β = (0.9, 0.999) to\ntrain our model. The learning rate warms up to 5e−4 within\n4k steps and then decays with the inverse square-root schedule. For the sampling ratio λ in GLAT+CTC, we adopt linear\nannealing from 0.5 to 0.3. As to the hard-to-easy learning\nstrategy, we set T0 = 0.4, T1 = 1.0 under En-De/De-En and\nT0 = 0.6, T1 = 1.0 under En-Ro for GLAT+CTC. We set\nT0 = 0, T1 = 1.0 for other models. All the NAT evaluators\nand students are trained with batches of 64k tokens, lasting\n300k updates and 100k updates for En-De/De-En and En-Ro\nrespectively. To better utilize the NAT evaluators, the students are initialized with parameters of the teachers trained\nafter 25k updates for En-De/De-En and 10k updates for EnRo, when the general knowledge has been acquired. We average the top 5 checkpoints chosen by the validation BLEU\nscores to create the final model.\nBaselines\nWe compare our method with standard KD\nwhich distills the whole training set. Another baseline is\nLow Frequency Rejuvenation (LFR, Ding et al. 2021a),\nwhich also exposes raw data to the NAT. They trained NAT\nmodels with raw, bidirectional KD and standard KD data\nin three different stages. We also apply their method to\nGLAT+CTC with the training updates split to approximately\n2 : 2 : 3 in ratio for each stage. Their method is trained\nfor 325k updates on En-De/De-En and 110k updates on EnRo for fair comparison. Note that their method augments\nthe training data by introducing (distilled source, raw target) sentence pairs, while ours only utilizes raw and standard KD data. We evaluate all the models using the tokenized and cased BLEU scores (Papineni et al. 2002), and\na learned metric COMET (Rei et al. 2020) with the recommended model wmt20-comet-da.\n4.2\nMain Results\nTable 2 and Table 3 present the main results on the\nbenchmarks. Our method outperforms baselines consistently\nacross different language pairs. We enable the model to learn\ndirectly from authentic data without greatly increasing the\nmodes by selecting NAT-friendly raw translations using an\nNAT evaluator. Compared with the previous work (Ding\net al. 2021a) which also exposes raw data directly to NAT,\nwe can determine the period of exposure for each sentence\nby setting the threshold dynamically in the training process.\nWe highlight the empirical advantages of our method:\n• Simple, effective and generic. Our method adds a simple\ndata selection procedure to the standard training pipeline,\nwhile it can effectively improve the performance of\nNAT across different datasets. Since the method is\narchitecture-irrelevant, it can be applied to a wide range\nof architectures while maintaining their advantages, even\nincluding inference-efficient AT structures.\nMethods\nIter\nWMT14\nWMT16\nSpeed Up\nEn-De\nDe-En\nEn-Ro\nAT Models\nTransformer (Vaswani et al. 2017)\nT\n27.30\n/\n/\n1.0×\nTransformer *\nT\n27.34 (0.309)\n31.73 (0.388)\n34.68 (0.515)\n1.0×\nDeepShallow (Kasai et al. 2020) *\nT\n26.00 (0.152)\n30.62 (0.308)\n32.25 (0.401)\n2.4×\nIterative NAT Models\nCMLM (Ghazvininejad et al. 2019)\n10\n27.03\n30.53\n33.08\n1.7×\nJM-NAT (Guo, Xu, and Chen 2020)\n10\n27.31\n31.02\n/\n5.7×\nNon-iterative NAT Models\nNAT-FT (Gu et al. 2018)\n1\n17.69\n21.47\n27.29\n15.6×\nGLAT (Qian et al. 2021a)\n1\n25.21\n29.84\n31.19\n15.3×\nGLAT + CTC (Qian et al. 2021a)\n1\n26.39\n29.54\n32.79\n14.6×\nDA-Transformer (Huang et al. 2022)\n1\n27.91\n31.95\n/\n7.0×\nOur Models\nDeepShallow w/ Standard KD *\nT\n27.05 (0.246)\n31.36 (0.326)\n32.99 (0.416)\n2.4×\nDeepShallow w/ Selective KD (ours)\nT\n27.23 (0.252)\n31.70 (0.352)\n33.28 (0.438)\n2.4×\nCMLM w/ Standard KD *\n10\n26.64 (0.137)\n30.24 (0.215)\n32.85 (0.357)\n2.1×\nCMLM w/ Selective KD (ours)\n10\n27.06 (0.170)\n30.65 (0.226)\n33.38 (0.374)\n2.1×\nGLAT + CTC w/ Standard KD *\n1\n26.19 (0.119)\n30.74 (0.274)\n32.73 (0.362)\n14.2×\nGLAT + CTC w/ Selective KD (ours)\n1\n26.82 (0.144)\n31.30 (0.302)\n33.34 (0.381)\n14.2×\nTable 2: BLEU and COMET scores of NAT models on WMT14 En-De/De-En and WMT16 En-Ro benchmarks. COMET\nscores are listed in parentheses if available. * indicates the results are obtained based on our implementation. To highlight the\nadvantage in efficiency, we did not apply strategies like reranking which improve the performance at the cost of inference speed.\nMethods\nWMT14\nWMT16\nEn-De\nDe-En\nEn-Ro\nLFR\n26.56\n31.13\n33.27\nSelective KD (ours)\n26.82\n31.30\n33.34\nTable 3: BLEU scores of GLAT+CTC using our method and\nLFR (Ding et al. 2021a) based on our implementation.\n• Well balance the translation quality and complexity of\ndata. Our method can configure the translation quality and complexity of training data by setting different\nthresholds for data selection. As the ratio of raw data increases, the translation quality improves, and the complexity of training data increases only slightly since we\ndeliberately select the simple raw translations.\n4.3\nAnalysis\nProperties of Selected Raw Data.\nOur method aims at\nselecting more NAT-friendly raw translations, which contain few modes and show high quality. To validate that our\ndata selection process indeed find a set of training data that\nhas the desired properties, we measure the complexity of our\ntraining data using two metrics:\n• Translation Uncertainty: Zhou, Gu, and Neubig (2019)\nproposed to measure the translation uncertainty of parallel data based on conditional entropy. They simplified\nconditional entropy to the sum of entropy of target words\nconditioned on the aligned source words:\nC(d) =\n1\n|Vx|\nX\nx∈Vx\nH(y|x)\nwhere d is a given dataset and Vx is the set of source\nvocabularies.\n• Alignment Shift: We measure the change of sentence\nstructure according to the relative distance between\naligned words. Specifically, given source sentence X and\nits translation Y , we get\nτ(X, Y ) =\n1\n|Y |\nX\ni,j\n[Xi = align(Yj)] · | i\n|X| − j\n|Y ||.\nS(d) is computed as the average of τ(X, Y ) over all\npairs: S(d) =\n1\n|d|\nP\n(X,Y )∈d τ(X, Y ).\nWe adopt an alignment model (Dyer, Chahuneau, and\nSmith 2013) for the metrics above. The metrics are computed over 1M randomly sampled sentence pairs from our\nprocessed WMT14 En-De. To display the effects of our\nmethod, we compute the metrics for distilled data, selected\nraw data (using GLAT+CTC), raw data replaced by KD data\nand the overall training data under different threshold T.\nAs shown in Figure 2, the translation uncertainty and\nalignment shifts of replaced raw data (red) exceed those of\nselected raw data (green) by a large margin, indicating that\nour method can effectively separate raw data into classes of\ndifferent complexity. When the threshold T is high enough,\nthe selected raw data even displays lower complexity than\nthe average level of distilled data. This further proves that\nFigure 2: C(d) and S(d) on 1M pairs randomly sampled from WMT14 En-De. We set T = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.01] for\nthe experiments, and the ratio of raw data=[0.98, 0.91, 0.75, 0.51, 0.28, 0.10, 0.00] respectively with a GLAT+CTC evaluator.\nSelected Raw is the set of raw sentences selected under T, while Replaced Raw is the set of raw sentences to be distilled.\nWe concatenate Replaced Raw after distillation and Selected Raw to get Training Data, which is the data exposed to the NAT\nstudent during training. We neglect C(d) and S(d) when there is not enough data for analysis.\nFigure 3: BLEU scores of GLAT+CTC for examples of different lengths on WMT14 En-De.\nModel\nEn-De\nDe-En\nEn-Ro\nStandard KD\n1.06‰\n0.56‰\n0.80‰\nSelective KD\n0.82‰\n0.38‰\n0.64‰\nTable 4: Word repetition ratio of GLAT+CTC on WMT14\nEn-De/De-En and WMT16 En-Ro.\nthe selected raw data contains fewer modes. Observing the\nresults on training data (blue), we find that the metrics grow\nsmoothly as the ratio of raw data increases, which means that\na flexible trade-off between translation quality and complexity of data can be realized.\nOur Method Reduces Repetition.\nWe also measure the\npercentage of repeated tokens to analyze whether our\nmethod can reduce the occurrence of repetition which is a\ntypical mistake caused by the multi-modality problem. We\nsee in Table 4 that exposing raw data during training can\nfurther reduce token repetition ratio. Although our data conLength\nScore\nExposure Period\n< 10\n0.826\n71.0%\n[10, 20)\n0.740\n56.6%\n[20, 30)\n0.696\n49.3%\n[30, 40)\n0.680\n46.6%\n[40, 50)\n0.670\n45.1%\n[50, 60)\n0.658\n43.0%\n≥ 60\n0.644\n40.6%\nTable 5: Average score and exposure period for raw translations of different lengths on WMT14 En-De with a GLAT\n+CTC evaluator. Exposure period is given by the percentage\nof updates where the raw translation can be directly learned.\ntains more modes than fully distilled data, it still achieves a\nbetter result. We think the improvement comes from learning directly from authentic distribution, which exhibits better word interdependencies and fewer mistakes.\nLong Sentences Benefits More.\nFigure 3 presents the\nBLEU score on sentences of different lengths. As seen,\nlonger sentences benefit more from our selective knowledge\ndistillation. Intuitively, the long sentences may contain more\nmistakes during distillation; thus, learning from authentic\ndata can help the NAT student avoid or correct these mistakes and strengthen its ability to model long sentences. We\nalso find that the performance drops slightly on sentences\nwith fewer than ten tokens. As shown in Table 5, shorter\nsentences have higher average scores, thus exposed to the\nstudent NAT for a longer period. In such a case, long-term\nexposure to raw data may confuse the model’s training, as it\nsuffers from the multi-modality of the raw data.\n4.4\nAblation Study\nEffects of Threshold T.\nWe further analyze the effects\nof threshold T in Figure 4. We fix the threshold T so\nFigure 4: Performance of GLAT+CTC on WMT14 En-De\nwith fixed threshold and dynamical threshold (0.4→1.0).\nthat the training data remains unchanged during the training process. The model can achieve significant improvement (+2.4 BLEU) by distilling only 5% of the training\ndata. We attribute this phenomenon to the effectiveness of\nour data selection process, which can filter translations that\ngreatly complicate the training data. The growth in performance becomes much slower as the ratio of distilled translations increases. Another finding is that the model trained\non 80%-distilled data slightly outperforms the one trained\non fully distilled data. According to Zhou, Gu, and Neubig (2019), a potential explanation is that the complexity\nof the 80%-distilled data is more suitable for the capacity\nof GLAT+CTC architecture. The dynamic threshold outperforms all the fixed threshold settings, embodying the advantage of our hard-to-easy strategy.\nModel Initialization.\nTo study how model initialization\ninfluences our method, we initialize the GLAT+CTC student\nwith parameters of the teacher trained after t updates, where\nt ranges from 25k to 300k with step 25k. We find that initialization with teacher trained after only 25k updates when the\nimprovement on validation set begins to slow down achieves\nthe best performance (26.82 BLEU), but the performance\ngap between these differently initialized models is negligible. This suggests that the improvement of our method\ndoes not come from a longer training process (initialization\n+ training). However, removing teacher initialization brings\nabout a degeneration of 0.47 BLEU. We believe that transferring some basic knowledge from the teacher can free the\nstudent from learning everything from scratch on the more\nchallenging raw data, enabling the student to focus on the\nmissing knowledge in distilled data.",
        "related work": "Non-autoregressive Machine Translation\nGu et al.\n(2018) first proposed Non-Autoregressive Transformer\n(NAT) for machine translation, which significantly boost\nthe inference speed by generating the outputs in parallel.\nDespite the efficiency, NAT still lags behind AT in performance. Various methods have been proposed to bridge the\nperformance gap. A line of work proposes to enhance the\ndecoder inputs of NAT (Lee, Mansimov, and Cho 2018;\nWei et al. 2019; Wang et al. 2019). Another branch of\nwork proposes to model the interdependencies between target outputs, which is explicitly missing in vanilla NAT\n(Ghazvininejad et al. 2019; Qian et al. 2021a). In addition,\na series of work takes the latent variable as inputs to modeling the target-side information (Kaiser et al. 2018; Ma et al.\n2019; Akoury, Krishna, and Iyyer 2019; Bao et al. 2021,\n2022). These work lines focus on model architecture and\ntraining method, so they can be easily combined with our\nmodel-agnostic method.\nTraining Data Manipulation\nMore close to our work\nis the thread of studies on manipulating training data for\nNAT. Zhou, Gu, and Neubig (2019) show that sequencelevel knowledge distillation (Kim and Rush 2016) reduces\nthe complexity of training data and propose several methods to adjust the complexity of distilled data in order to\nmatch the model’s capacity. Sun and Yang (2020) jointly optimizes AT and NAT models to remove the multi-modality in\ntarget sentences. Shao, Wu, and Feng (2022) generate several high-quality reference translations and select the most\nsuitable candidates by comparing them with the NAT outputs. Some recent studies show that distilled data has some\nside effects like leading to more errors on predicting lowfrequency words (Ding et al. 2021b). In order to solve this\nproblem, Ding et al. (2021a) proposed to pretrain NAT models on raw data, which is closely related to our work. Our\nmethod follows the idea of exposing raw data to NAT, but\nis different from theirs by introducing an NAT evaluator to\nevaluate each raw translation. By changing the ratio of raw\nsentences in the training data, we can configure the complexity of data in the training process and benefit more from raw\ndata by exposing some raw translations for a longer period.\nCurriculum Learning\nOur work adopts a hard-to-easy\nstrategy in training NAT models by decreasing the ratio of\nraw data in the training process, which is contrary to curriculum learning (Bengio et al. 2009) in spirits. Curriculum\nlearning methods train machine learning models from easy\nto hard data, but (Braun, Neil, and Liu 2017) showed that\nlearning from hard to easy can be effective. They conducted\nexperiments on automatic speech recognition systems and\nuse signal-to-noise ratio (SNR) to create hard-to-easy curriculum. Compared with the opposite ranking of the examples from easy to hard, the hard-to-easy strategy provides\nbetter results.",
        "conclusion": "In this paper, we propose selective knowledge distillation\nto tackle error propagation from an autoregressive teacher\nin standard knowledge distillation for NAT models. Specifically, we employ an NAT evaluator to progressively replace\nthe targets from distilled data with raw data for training NAT\nstudents, enabling them to benefit from both the high-quality\nraw data and easy-to-learn distilled data. Experiment results\nvalidate that our approach can effectively improve performance on machine translation tasks. Extensive analyses also\nreveal that an effective data selection strategy has a great potential to improve the performance.",
        "summary_en": "Benefiting from the sequence-level knowledge distillation, the Non-Autoregressive Transformer (NAT) achieves great success in neural machine translation tasks. However, existing knowledge distillation has side effects, such as propagating errors from the teacher to NAT students, which may limit further improvements of NAT models and are rarely discussed in existing research. Therefore,this paper introduces selective knowledge distillation by introducing an NAT evaluator to select NAT-friendly targets that are of high quality and easy to learn. In addition, the paper introduces a simple yet effective progressive distillation method to boost NAT performance. Experiment results on multiple WMT language directions and several representative NAT models show that the approach can realize a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances. Further analysis shows that distilling only 5% of the raw translations can help an NAT outperform its counterpart trained on raw data by about 2.4 BLEU.",
        "summary_zh": "这篇论文介绍了一种名为选择性知识蒸馏的方法，旨在解决非自回归神经机器翻译中已有知识蒸馏存在副作用的问题。该方法引入了一个NAT评估器来选择高质量且易于学习的 NAT 友好目标。同时，还提出了一种简单而有效的渐进式蒸馏方法来提升NAT性能。实验结果表明，该方法在多个WMT语言方向和几种代表性NAT模型上均取得了强大的性能，实现了训练数据质量和复杂度之间的灵活权衡。最后的结果显示，只蒸馏原始翻译的5%就能使NAT的性能比在原始数据上训练的同行高出约2.4 BLEU。"
    },
    {
        "title": "Rethinking Interpretation: Input-Agnostic Saliency Mapping of Deep Visual Classifiers",
        "abstract": "Saliency methods provide post-hoc model interpretation by attributing input features to the model outputs. Current methods mainly achieve this using a single input sample, thereby failing to answer input-independent inquiries about the model. We also show that input-specific saliency mapping is intrinsically susceptible to misleading feature attribution. Current attempts to use ‘general’ input features for model interpretation assume access to a dataset containing those features, which biases the interpretation. Addressing the gap, we introduce a new perspective of input-agnostic saliency mapping that computationally estimates the high-level features attributed by the model to its outputs. These features are geometrically correlated, and are computed by accumulating model’s gradient information with respect to an unrestricted data distribution. To compute these features, we nudge independent data points over the model loss surface towards the local minima associated by a human-understandable concept, e.g., class label for classifiers. With a systematic projection, scaling and refinement process, this information is transformed into an interpretable visualization without compromising its model-fidelity. The visualization serves as a standalone qualitative interpretation. With an extensive evaluation, we not only demonstrate successful visualizations for a variety of concepts for large-scale models, but also showcase an interesting utility of this new form of saliency mapping by identifying backdoor signatures in compromised classifiers.",
        "introduction": "Deep perceptual models are at the heart of many recent scientific developments (LeCun, Bengio, and Hinton 2015),\n(Nature 2021). Their applications now reach beyond the\nrudimentary decision making, to the automation of highstake domains, e.g., self-driving vehicles (Deng et al. 2021),\nsmart security (DARPA 2020), health-care (Tang et al.\n2021). This is a direct consequence of their established\nhuman-level ability to discriminate between intricate visual patterns (LeCun, Bengio, and Hinton 2015). However,\nlike any deep learning approach, they are black-box techniques (Vinuesa and Sirmacek 2021). It is normally very\nhard to interpret what information has been learned by these\nmodels and how it influences their predictions.\nFigure 1: Difference between the proposed and conventional\nsaliency mapping. (Top) We propose to compute maps that\nare input-agnostic and can attribute outputs to generic data\nconcepts. (Bottom) Conventional approaches allow interpretations that are specific to a given input.\nOwing to the significance of the perceptual models, their\ninterpretability is currently considered a mainstream problem in computer vision (Fong, Patrick, and Vedaldi 2019),\n(Fong and Vedaldi 2017), (Petsiuk, Das, and Saenko 2018),\n(Zeiler and Fergus 2014), (Jalwana et al. 2021), (Selvaraju\net al. 2017). In the literature, it has been translated to the\ntask of identifying the input features deemed salient by the\nmodel to make its predictions. Mainly two broad strategies\nare popular to accomplish this task. The first searches for\nsalient features in an input image by selectively perturbing its different regions and analyzing the resulting effects\non the model predictions (Fong, Patrick, and Vedaldi 2019),\n(Fong and Vedaldi 2017), (Petsiuk, Das, and Saenko 2018),\n(Zeiler and Fergus 2014). Though effective, these methods\nmay require heuristics to control their search space, potentially compromising the model-fidelity of the map due to the\nexternal influence (Jalwana et al. 2021).\nThe second strategy is commonly known as backpropagation saliency mapping (Selvaraju et al. 2017), (Rebuffi et al.\n2020), (Simonyan, Vedaldi, and Zisserman 2013), (Springenberg et al. 2014), (Zeiler and Fergus 2014), (Zhang et al.\n2018). It estimates feature saliency by analysing the graFigure 2: Central idea: We iteratively integrate and refine the gradient information of the model’s loss surface to estimate an\ninput-agnostic map that captures geometric input features considered salient by the model. An iteration draws i.i.d. samples\nfrom a distribution and nudges them to the nearby local minima that belong to a human-understandable concept (e.g., a class\nlabel). By integrating (\nR\n) the gradient information from these nudges, and projecting (Ψ) it onto a norm-bounded surface, we\namplify the salient geometric patterns w.r.t. the model. A refinement is further employed to improve the visualization without\ncompromising the model-fidelity of the computed map.\ndients and activations of the internal layers of the model\nfor a given input. Low resolution map estimates, and failing the basic sanity checks, are known challenges faced by\nthis strategy (Adebayo et al. 2018), (Jalwana et al. 2021).\nLeaving aside the peculiar issues of both strategies, the eventual saliency map computed by the both are always inputspecific. Implying, the resulting interpretation is only valid\nfor a single sample, see Fig. 1. This is too restrictive, especially when we consider that the ultimate objective of this\nresearch direction is model interpretation.\nIn this work, we approach saliency mapping from a different perspective. Our objective is to map generic (as opposed to input-specific) features of the modelled data which\nare deemed salient by the model. Under this input-agnostic\nperspective, the target map is a human-understandable visualization of the salient features attributed by the model to\nits outputs. The central idea of our technique is illustrated\nin Fig. 2. To compute the map, we iteratively explore the\nloss surface of the model by nudging a set of independent\ndata points in the directions of nearby local minima associated with the output. The direction estimated by the nudges\nare integrated to characterize the landscape of the model’s\nloss surface. By a gradual accumulation of this information\nand its projection onto a norm-restricted surface, we amplify\nthe geometric correlation in the map which is subsequently\nrefined computationally. When expanded to an image-grid,\nthis map visualizes salient geometric features associated by\nthe model to its output, i.e., a concept.\nA unique property of our saliency map is that it enables a\nholistic visualization of the model’s understanding of its own\noutputs. This can lead to many interesting applications. To\nillustrate one, this work also presents a case study to detect\nTrojan trigger patterns in compromised classifiers that contain backdoors (Wang, Hassan, and Akhtar 2022). We leverage our input-agnostic visualization to map the geometric\npatterns to which the classifier’s output nodes are more sensitive. A model that has a backdoor, leads to visualisations\nthat contain traces of the patterns used to trigger the backdoor, thereby allowing Trojan detection. The contributions\nof this paper are as follows.\n• Systematically highlighting the limitations of inputspecific saliency estimation, it introduces a new perspective of mapping generic data features attributed by a\nmodel to its outputs - input-agnostic saliency mapping.\n• It devises a first-of-its-kind method for the proposed\ninput-agnostic saliency mapping using model gradients.\n• It showcases a utility of the new saliency mapping with\nbackdoor trigger identification in classifiers.\n• It introduces a quantitative metric for the newly proposed\nmapping framework and performs evaluation on largescale ImageNet models to support the claims.",
        "related work": "For perceptual model interpretation through saliency mapping, there are two popular streams of methods. The first\nperturbs different regions of the input and records the effects\nof these perturbations to estimate the contribution of regions\nto the model prediction (Fong and Vedaldi 2017), (Fong,\nPatrick, and Vedaldi 2019), (Petsiuk, Das, and Saenko\n2018), (Sundararajan, Taly, and Yan 2017), (Erion et al.\n2021). The second uses activations of the deeper layers of\nthe underlying neural network and back-propagated model\ngradients to estimate a saliency map (Selvaraju et al. 2017),\n(Jalwana et al. 2021), (Simonyan, Vedaldi, and Zisserman\n2013). Due to the rising importance of model interpretation\nin numerous applications, there is a wide interest of the community in developing methods along both the directions. Influential contributions for both are discussed below.\nAmong\nthe\nperturbation-based\nsaliency\nmethods,\nRISE (Petsiuk, Das, and Saenko 2018) and Occlusion (Zeiler and Fergus 2014) estimate the maps by\nweighting the perturbation masks with respect to the\nchanges in the model confidence score. Other methods, for\ninstance, Extremal perturbations (Fong, Patrick, and Vedaldi\n2019), Meaningful perturbations (Fong and Vedaldi 2017),\nReal-time saliency (Dabkowski and Gal 2017) and (Ribeiro,\nSingh, and Guestrin 2016), cast the underlying problem\ninto an optimization objective. Although the perturbation\nmethods are generally effective, they do not specifically\nshield the computed maps from external influence. A major\nchallenge behind this problem is that the search nature of\nthese methods requires exploring a vast solution space.\nHence, the techniques rely on heuristics, priors or external\nconstraints for tractability. This can affect the model-fidelity\nof the resulting interpretations (Jalwana et al. 2021).\nWithin the perturbation-based methods, there is also a\nbranch of techniques that takes axiomatic approach towards\nsaliency map creation (Sundararajan, Taly, and Yan 2017),\n(Erion et al. 2021), (Pan, Li, and Zhu 2021), (Srinivas and\nFleuret 2019). Defining a path from a baseline image to the\ninput, this paradigm integrates the effects of perturbation to\nimages along this path to compute a saliency map. Although\ntechniques vary in defining the paths and signal integration\nschemes, all of them compute saliency map for a single image. These methods do provide certain desirable theoretical\nproperties, however they also entail high computational cost\nfor image-specific interpretations.\nThe back-propagation saliency methods (Simonyan,\nVedaldi, and Zisserman 2013), (Selvaraju et al. 2017),\n(Zeiler and Fergus 2014), (Jalwana et al. 2021) are highly\npopular as perceptual model interpretation tools. Normally,\nthey construct a saliency map for the regions of an input\nimage. These methods are also relatively computationally\nefficient (Zintgraf et al. 2017), (Kapishnikov et al. 2019).\nSimonyan, Vedaldi, and Zisserman (2013) were among the\nfirst to use model gradients for interpretation. Numerous improvements to this idea have been subsequently proposed to\nhandle the noise sensitivity of the model gradients. Springenberg et al. (2014) proposed Guided back-prop, while\nZeiler and Fergus (2014) altered the back-propagation rules\nfor the ReLU layers of the model for that purpose. Similarly, SmoothGrad (Smilkov et al. 2017) computes the average gradients over the samples in the close vicinity of the\noriginal input to mitigate the gradient noise sensitivity.\nAlong a similar line of thought, DeepLIFT (Shrikumar,\nGreenside, and Kundaje 2017), Excitation Backprop (Zhang\net al. 2018) and LRP (Bach et al. 2015), recast the backpropagation rules for saliency mapping to restrict the sum\nof attribution signal to unity. In another effort to control the\nsignal noise, Sundararajan, Taly, and Yan (2017) combined\nmultiple attribution maps. There are a number of methods\nthat estimate the saliency map by merging layer activations\nof a model and its gradient information. Popular examples\ninclude CAM (Zhou et al. 2016), linear approximation (Kindermans et al. 2016), GradCAM (Selvaraju et al. 2017), NormGrad (Rebuffi et al. 2020) and CAMERAS (Jalwana et al.\n2021). Though appearing late in the literature, these methods are currently dominating the backpropagation-based\nsaliency mapping corpus of the literature.\nBoth of the above streams have an obvious limitation.\nThey both explain the model behavior using only the features present in the given input sample. Realizing this restriction, works such as (Bau et al. 2017) and (Ghorbani\net al. 2019) use ‘general’ object features to explain the\nmodel. However, these general features are extracted from\na dataset. Thus, the eventual explanation is intrinsically biased to that data. Moreover, similar to the above-discussed\nstreams, these methods must still use individual inputs to visualize the explanations. In this work, we devise a saliency\nmapping technique that truly liberates model explanation\nfrom the input, resulting in a first-of-its-kind input-agnostic\nsaliency mapping mechanism.",
        "proposed saliency mapping": "To motivate the idea and relate it to the existing practice, we\nstart our discussion with the conventional saliency mapping.\nConventional Saliency Mapping\nLet K : K(I) → y be a visual classifier that maps an image I ∈ Rh×w×c with ‘c’ channels to a prediction vector\ny ∈ RL. The ℓth coefficient yℓ of y is the largest if I belongs\nto class ‘ℓ’. Assume a set PI = {p1\nI, p2\nI, ..., pw×h\nI\n} ⊂ Rc,\nwhich contains the pixels of I. The broad common objective\nof the saliency-based interpretation methods for visual classifiers is to compute an ordered array WI ⊂ R, s.t. |WI| =\n|PI| and the ith element of this array, i.e. wi\nI ∈ WI, encodes\na weight for the corresponding element in PI. The WI can\nbe an array containing only binary values, which represents\na mask for I that suppresses the irrelevant pi\nI ∈ PI for the\ntransform K(I) → y. Or, it can contain real values encoding\nthe importance of all pi\nI ∈ PI for the performed transform.\nThe above formalization presents a unified view of the\nobjective of the prevailing input-specific saliency methods.\nTo this end, the existing techniques seek the function S :\nS(K, I) → WI for saliency mapping. We refer to S as the\nsaliency function in the text to follow.\nProblem with the Input-Specific View:\nThough useful\nfor certain objectives, we identify that the sought saliency\nfunction only weakly depends on the classifier itself. This\nmakes it susceptible to providing misleading model interpretations. We make a formal proposition about it.\nProposition 1: Due to the weak dependence of the saliency\nfunction S on the classifier K, S is susceptible to compute\nWI for a canonical classifier K∗ instead of K.\nTo establish Prop. (1), we need to first define canonical classifier and weak dependence, as understood in this work.\nDefinition 1: (Canonical classifier) Provided that ‘ℓ’ is the\nknown label of an input I, a canonical classifier K∗ behaves\nas K∗(I) → y∗, where yℓ → 1 for y∗.\nDefinition 2: (Weak dependence) For a model M\n:\nM(I) → y whose prediction behavior can be expressed\nas a piece-wise function\nM(I) =\n\n\n\n\n\nM1 : M1(I) → y1\nI ∈ U1\n...\n...\nMn : Mn(I) → yn\nI ∈ Un,\nwhere Ui is the ith open disconnected set of the nearby samples of I, a saliency function S only weakly depends on M\nwhen Wp\nI ≈ Wq\nI for S(Mp, I) → Wp\nI and S(Mq, I) →\nWq\nI for the sets Up and Uq, despite Up ∩ Uq = ∅.\nThe above-defined notion of weak dependence is partially\ninspired by the work of Srinivas and Fleuret (2019). However, the context and application of our definition is different. To understand the weak dependence property as stipulated by Def. (2), imagine an image of a ‘panda’ on a chair,\nand an image of a ‘queen’ on a throne. Due to their significant content dissimilarity, these images exist in different sets\nUp and Uq. Let M be a classifier that correctly predicts the\nlabels of both the images. This will naturally result in largely\ndifferent corresponding prediction vectors yp and yq. Implying, the underlying classifier behavior for these images\nis modelled by considerably different components Mp and\nMq of the piece-wise function M. A strong dependence\nof S on M asserts that S computes proportionally different Wp\nI and Wq\nI . However, an input-specific saliency function S may commit to very similar maps for the two images,\ni.e., Wp\nI ≈ Wq\nI , when the silhouettes of the panda and the\nqueen in the images are very similar, despite Up ∩ Uq = ∅.\nThe above example illustrates the phenomenon of weak\ndependence of the saliency function on the classifier under the traditional input-specific saliency mapping. Now, we\nturn to establishing Prop. (1). We focus on the pursuit of estimating a perfect map W∗\nI using S. To exemplify, if the\nsought W∗\nI is a binary mask, it suppresses all the pixels\nin I that are irrelevant to the object of category ℓ. Notice\nthat, existing methods perform saliency mapping using apriori knowledge of ℓ. In effect, they pose the query, “provided\nthat I’s label is ℓ, compute WI”. Irrespective of the used\nsaliency function, the ideal solution, i.e., W∗\nI , to this query\nis a map that identifies the pixels of I that maximize the\nvalue of yℓ. Following Def. (2), since S is only weakly dependent on the model itself, the quest of computing W∗\nI can\neasily force S to select a piece-wise component of M that\nfavors yℓ → 1, irrespective of the actual prediction vector\nof I. Such a solution would actually be performing saliency\nmapping for the canonical classifier, as defined in Def. (1),\nnot the observed model behavior.\nOur analysis above indicates a pitfall in the pursuit of\nprecise input-specific saliency mapping. Its obvious implication is that highly refined image saliency maps can actually misinterpret the actual model behavior. The literature already identifies multiple saliency methods failing basic sanity checks (Adebayo et al. 2018). Interestingly, sanity checks\nfailure is much more common among the methods aiming at\na higher map precision. This phenomenon is naturally explainable through our above-provided analysis.\nRemark: Under the completeness axiom (Sundararajan,\nTaly, and Yan 2017), yℓ is bounded to δ ≤ 1 in Def. (1). To\nthat end, our analysis still holds for yℓ → δ. Implying, the\ninput-specific methods satisfying the completeness property\ncan still suffer from misleading maps.\nThe need for input-agnostic view: Not only that inputspecific maps are susceptible to misleading interpretations,\nan inaccurate map for a given sample becomes a fatal error\nfor these methods because they can only offer interpretation\nwith respect to a single sample. Addressing the problem at\nthe grass-root level requires the interpretation to be agnostic to the input samples. To represent the model well, such\nan interpretation must strongly depend on the model itself.\nThis view of model explanation can not only allow us to ensure model-fidelity of the interpretations, but also enable us\nto answer more general queries about the model. Hence, we\ndevelop a saliency mapping method under this view, considering it as a complementary interpretation tool.\nInput-Agnostic Saliency Mapping\nWe first provide a concise definition of the desired inputagnostic saliency mapping function SK - the subscript indicates that the function depends on K.\nDefinition 3: (Input-agnostic saliency mapper) For a given\nclassifier K, SK : SK(ci\nℑ) → νi\ncℑ is an input-agnostic\nsaliency mapper, where νi\ncℑ\n∈\nRh×w×c is a humanunderstandable visualization of a semantic concept ci\nℑ ∈\nC = {c1\nℑ, c2\nℑ, ..., cL\nℑ} and ℑ denotes the input data distribution over which K is induced.\nMultiple aspects in Def. (3) need emphasis. First, notice\nthe absence of I in favor of the use of distribution ℑ - inline with the key idea of input-agnostic saliency. Second, the\nsaliency map is now with respect to a semantic concept ci\nℑ,\nnot an input sample. Here, the semantic concept (or simply\nthe ‘concept’) is a high-level human-understandable notion\nthat is also discernible to the model. In the context of visual classifiers, this work considers the concept to be a class\nlabel, hence |C| = L. It is noteworthy though, we do not\nenforce any other constraint over ci\nℑ. It is implicit that the\nconcept emerges from ℑ, and it is understood by K because\nthe classifier is learned using ℑ. Lastly, the output of the\nsaliency function is now an image νi\ncℑ ∈ Rh×w×c instead\nof a set of weights. This image visualizes a human-defined\nconcept, as understood by the classifier. Since a concept can\nbe a broad and complex idea, it is likely to exhibit plentiful visual manifestations. Hence, νi\ncℑ is not expected to be\nunique. This makes SK a one-to-many mapping function.\nOptimization objective: To compute the desired visualization, we need to optimize for\nmax\nν\nP\n\u0000\nK(νcℑ) → yℓc\n\f\f ℑ\n\u0001\ns.t. νcℑ = F(K),\n(1)\nwhere P(.|.) denotes a conditional probability, ℓc is the class\nlabel for the concept cℑ, and F(.) is a non-trivial function\nensuring that the visualization strictly depends on the classifier K. The input-agnostic saliency mapper SK in Def. (3)\ninstantiates F(K), where we let ci\nℑ ∈ C as an input parameter of the function to allow visualizations for the multi-class\nclassifiers. In Eq. (1) and text below, we ignore the superscript ‘i’ to avoid clutter. We also ignore ℑ for clarity when\nemphasis on the data distribution is not required.\nAlgorithm: Following Eq. (1), the implementation objective\nof SK is to maximize the probability P(K(SK(cℑ)) → ℓc) in\nan input-agnostic manner. We achieve this with the saliency\nmapper given as Alg. (1). The algorithm leverages insights\nfrom Lemma (1) below.\nLemma 1: For K with cross-entropy loss JCE, P(K(I) →\nyℓc|I ∼ ℑ) increases along − E\nI∼ℑ[∇IJCE(ℑ, θ, ℓc)].\nProof: Denote “K(I) → yℓc|I ∼ ℑ” by ζI. The P(ζI) increases along ∇I(log P(ζI)), where ∇I is the derivative\nw.r.t. I. For K with the loss JCE and model parameters θ,\nthis is the same direction as −∇IJCE(ℑ, θ, ℓc). Thus, P(ζI)\nwill increase along − E\nI∼ℑ[∇IJCE(ℑ, θ, ℓc)].\nIn Alg. (1), we perform a guided stochastic gradient descent over the loss surface of K with the help of data distribution ℑ. The distribution is approximated with a set of\nits samples in I. In an iteration, the algorithm computes the\nmodel’s loss gradients for the concept label ℓc w.r.t. a minibatch of the samples from ℑ. Conceptually, these gradients\npoint to the directions of the local minima associated with\nℓc. In the light of Lemma (1), we compute Expected value of\nthe gradients and further guide the descent with the first and\nsecond moments - lines 5-7. The use of moments is inspired\nby the Adam optimizer (Kingma and Ba 2014). Hence, following Adam, we also fix the values of the hyper-parameters\nβ1 and β2. We additionally guide the descent with a binary\nselection between the original and the flipped direction if\nthe latter identifies a better local solution - lines 8 -131. We\nempirically found it beneficial in our experiments. Collectively, the process from line 3 to 14 in Alg. (1) implements\nthe integration of gradient information in Fig. 2.\nIn an iteration, νk is able to encode patterns related to ℓc\nbecause it is computed under the objective of nudging random samples to the local minima related to the concept lines 8 -13. The model must associate the features encoded\nin νk to ℓc for the nudging to work. However, an unbounded\nconstruction of νk can lead to uninteresting solutions where\nthe algorithm maximizes only the influential component(s)\nof νk to achieve its objective. This does not help the cause\nof human-meaningful visualization with νk. To encourage\ncorrelation among the components of νk, we project it onto\na bounded ℓ2-ball in each iteration - line 15, which implements the projection operation of Fig. 1. The underlying\nconstraint resulting from this projection, i.e., ||νk||2 ≤ η,\nleads to a collaborative behavior among the coefficients of\nνk that emerges into meaningful geometric patterns when\nthe vector is visualized as an image.\nIn Alg. (1), we allow multiple hyper-parameters as inputs\nalong the classifier K and class label ℓc. Considering the\nabove discussion and the fact that Alg. (1) solves a stochastic\ngradient descent problem, the significance of these parameters is self-explanatory, except for the seed ν. As shown in\nFig. 2, we further refine the visualization after K iterations\nof Alg. (1). Subsequently, Alg. (1) is again applied to improve the visualization. The seed ν is the output of the refinement process. In the first round, ν ∈ Rh×w×c is a black\nimage.\nMap refinement: The output of Alg. (1) is a visualization\nof the salient geometric features associated by K to a concept, i.e., label ℓc. However, this map is constructed with the\nmodel’s gradient information, which can be noisy. Hence,\nwe need to further refine the map. To that end, we solve for\nthe following optimization problem\nmin\nνc E\nh\nJCE(ℑ − νc, θ, ℓc) + λ\n\u0000\nνc ⊙ (1 − Ξ)\n\u0001i\n,\n(2)\n1Clip is the standard clipping function that clips out any value\nexceeding the image dynamic range.\nAlgorithm 1: Input-agnostic saliency mapper SK\nInput: Classifier K, concept label ℓc, sample set I, minibatch size b, total iterations K, ball norm η, seed ν.\nOutput: Visualisation νc ∈ Rh×w×c.\n1: Initialize ν0 = ν, µ0, σ0 to 0 and k = 0.\nSet β1 = 0.9, β2 = 0.999.\n2: for k = 0 to K do\n3:\nI ∼ I s.t. |I| = b and apply ∀Ii ∈ I, Clip (Ii − νk)\n4:\nxk ←\nE\nIi∈I\n\u0002\n∇IiJ (θ, ℓc)]\n5:\nµk ← β1µk−1 + (1 − β1)xk\n6:\nσk ← β2σk−1 + (1 − β2)(xk ⊙ xk)\n7:\nv ←\n\u0010\nµk\np\n1 − βk\n2\n\u0011\n⊙\n\u0000√σk(1 − βk\n1)\n\u0001−1\n8:\nI+\nv ←\n\b\nIi : Ii =Clip\n\u0010\nIi − (vk−1+\nv\n||v||2 )\n\u0011 \t\n∀Ii ∈I\n9:\nI−\nv ←\n\b\nIi : Ii =Clip\n\u0010\nIi − (vk−1−\nv\n||v||2 )\n\u0011 \t\n∀Ii ∈I\n10:\nif E\n\u0002\nK(I+\nv )→ℓc\n\u0003\n≥ E\n\u0002\nK(I−\nv )→ℓc\n\u0003\nthen\n11:\nνk ← νk−1 + v\n12:\nelse\n13:\nνk ← νk−1 − v\n14:\nend if\n15:\nνk ← Ψ(νk), s.t. Ψ(νk) = νk ⊙ min\n\u0010\n1,\nη\n||νk||2\n\u0011\n16: end for\n17: νc = νK\n18: return\nwhere λ is a regularizer and Ξ is a weighting matrix - discussed shortly, the other symbols follow from above. In\nEq. (2), ℑ − νc signifies (another) nudge of the inputs towards the local minima associated with ℓc. This follows\nfrom the central idea of Alg. (1), and is possible because\nνc is already available to us. However, now we would like\nit to be mainly influenced by the regular geometric features\nof ℓc, disregarding the noisy component of νc. The second\nterm in Eq. (2) imposes that. Here, Ξ ∈ Rh×w×c is a matrix\ncomputed by forward passing νc through K and projecting\nthe activations of the convolutional base of K onto Rh×w×c\nwith interpolation. The process resembles the use of activations in computing GradCAM maps (Selvaraju et al. 2017).\nHowever, instead of computing the map, we use it to clean\nthe map. Hence, we normalize the coefficients of Ξ in the\nrange [0,1] and perform an element-wise weighting of νc\nwith 1 − Ξ, denoted by ⊙ in Eq. (2). This suppresses the\ncomponent of νc that does not contain perceptually meaningful information, as deemed by the model itself.\nWe achieve the optimization objective of Eq. (2) using\nthe Adam optimizer. The resulting cleaned up map is further\nclipped and projected onto an ℓ2-ball for a seamless subsequent processing with Alg. (1). Our method cycles between\nAlg. (1) and the refinement stage to compute the final visualization. It is noteworthy that the complete signal in our\neventual map originates in the model itself, and no part of\nit is generated by an external operator to maintain model\nfidelity (Jalwana et al. 2021). The only external operator\nwe use is the interpolation function in the refinement stage.\nHowever, that is used to ‘remove’ the unwanted signal from\nFigure 3: Representative input-agnostic visualizations of human-defined concepts, as understood by VGG-16. Natural image\nfor each concept is provided for reference only.\nthe map. Jalwana et al. (2021) noted such model-fidelity to\nbe highly desirable for reliable model interpretation.",
        "experiments": "Visualizations\nTo verify our approach, we apply our technique to visualize ImageNet concepts (Deng et al. 2009), as understood\nby VGG-16 and ResNet-50 models. We use ImageNet pretrained Pytorch models, and randomly pick 10 labels to visualize. Recall, in the context of this work, a concept is a highlevel human-understandable notion that is also discernible\nfor the model. Hence, the experiments consider ImageNet\nclass labels as the concepts. In Fig. 3, we show example\nvisualizations of representative concepts resulting from the\nproposed technique for VGG-16. .\nWe observe in the computed images that the patterns\nare visually relatable to the concepts mentioned along side.\nMoreover, we are able to generate multiple visualizations of\na given concept that are slightly different from each other,\nbut represent the concept well. Multiple images for a given\nconcept are generated by simply executing the method multiple times. The complementary visualizations for different\nruns indicate generalizable concept understanding by the\ndeep visual classifier. Notice that, the shown results are useful in answering queries such as, “what concept the model\nassociates to its specified output neuron?”, or “what generic\ngeometric patterns the model attributes to a learned concept/class label?”. These kind of queries are not answerable\nwith image-specific view of saliency mapping.\nQuantitative results: Whereas our method is a qualitative\ninterpretation technique, we also allow its quantitative evaluation. To that end, we must introduce a new metric to\nevaluate our first-of-its-kind input-agnostic saliency mapping technique. Termed ‘model-score’ - Mscore, the proposed\nmetric is inspired by Inception-score (Salimans et al. 2016),\nwhich is commonly used to benchmark generative model\nperformance. Since our method eventually results in a ‘visualisation’, it can also be seen as an image generation technique. Inception-score measures the meaningfulness of generated images by selecting an Inception model as a proxy for\nthe human visual system. Let us call the classifier used to\ngenerate our visualization, a ‘source’ classifier Ks. For evaluation, we seek to quantify meaningfulness of our visualizations w.r.t. any ‘target’ classifier Kt that is induced over the\nsame data distribution used to train Ks. Hence, in essence,\nwe seek a generalized version of the Inception-score that\nalso accounts for the fact that the image is generated with\nrespect to a given source classifier.\nWe compute the proposed Mscore by measuring the probabilities Pt|s\ncond and Pt|s\nmarg, where\nPt|s\ncond = p(Kt(νc) → yℓc|νc = SKs(c)),\n(3)\nPt|s\nmarg =\nX\ni\np(Kt(νi\nc) → yℓc|νi\nc = SKs(c)).\n(4)\nIn the above expressions, Pt|s\ncond and Pt|s\nmarg are respectively the\nconditional and marginal probabilities that the target classifier correctly predicts the class label of the concept visualized by νc, where the label is determined by applying the\nsaliency function SKs to Ks. The Mscore is then computed as\nMscore(t|s) = exp\n\u0010\nE\n\u0002\nKL\n\u0000\nPt|s\ncond || Pt|s\nmarg)\n\u0003\u0011\n/L,\n(5)\nSource(s)\nTarget(t) VGG-16 ResNet-50 DenseNet-121 Avg.\nVGG-16\n0.99\n0.71\n0.81\n0.84\nResNet-50\n0.69\n0.99\n0.94\n0.89\nTable 1: M score(t|s) of the visualized concepts for VGG16 and ResNet-50. Maximum possible value for any targetsource pair is 1. Larger values are more desirable.\nwhere KL denotes the Kullback–Leibler divergence, and L\nis the total number of visualized concepts.\nThe proposed Mscore is a comprehensive metric for a given\nsource-target classifier pair trained on the same data distribution ℑ. By definition it values range in [ 1\nL, 1], where larger\nvalues as more desirable. To keep experiments computationally manageable, we assume that the source and target classifiers in our experiments only understand the chosen L = 10\nconcepts. We generate 10 visualizations per concept for both\nVGG-16 and ResNet-50 and compute Mscore using different\ntarget models. The results are summarized in Table 1.\nIn Table 1, the visualizations generated for a given\n(source) model have Mscore ≈ 1 when the same model is\nused as the target. This verifies that our visualizations are\ncorrectly mapping the model’s understanding of the underlying concepts with high fidelity. When we change the target\nmodel to other ImageNet models (same ℑ), the score slightly\ndrops. However, it still remains considerably high. This signifies that the visualizations are indeed generically understandable by the different models of ℑ. Our results align\nperfectly with the intuition that Mscore(t|s) should be larger\nwhen t ̸= s is a more accurate classifier for the concepts in\nℑ. The large Mscore values across different well-trained target classifiers conclusively establish successful visual mapping of the generic concepts by our method.\nBackdoor Detection\nBackdoor (a.k.a. Trojan) attacks manipulate visual models\nby forcing them to misbehave when exposed to a ‘trigger’ in\nthe input (Wang, Hassan, and Akhtar 2022). These attacks\nare stealthy because the model behaves normally for clean\ninputs, and the model user is unaware of the trigger pattern.\nSince the presence of a trigger in the input is not known,\nit is not possible to use image-specific saliency to identify\nbackdoor in the model. The ability to visualize the patterns\nassociated with a model’s outputs in an input-agnostic manner can resolve the issue. We conduct a study in which three\ncompromised VGG-16 models are trained using ImageNet.\nThese models behave normally for a clean input, but always\npredict a given (random) target label when the input contains\na trigger pattern.\nIn Fig. 4, we show the trigger patterns used in our experiments, which are chosen at random based on the literature (Wang, Hassan, and Akhtar 2022). We apply the proposed input-agnostic saliency mapping to the compromised\nmodels. It was found that for the (false) target class, the constructed maps contained a discernible visual footprint of the\ntrigger pattern. The figure shows example maps for the Castle class of a compromised model, when the trigger (Nike\nFigure 4: Backdoor identification. Left: Used trigger patterns. Center: Example images of the Target class (Castle)\nand a triggered input. The model correctly predicts the label\nof castle images, and also predicts any triggered image as\nCastle. Right: Representative examples of computed maps\nfor the concept Castle, capturing discernible trigger patterns.\nsign) caused the model to predict Castle as the label of any\nimage containing the Nike sign.\nWe note that modern backdoor attacks are not limited\nto only using ‘visible’ trigger patterns (Wang, Hassan, and\nAkhtar 2022). Detection of non-visible triggers is not covered by our study. Our intention here is to showcase a utility\nof our novel view of input-agnostic saliency mapping. We\nbelieve, further exploration along this view will allow multiple interesting applications, and open new avenues for the\nmodel interpretation methods.\nHyper-Parameter Settings\nIt may appear that Alg. (1) and map refinement use multiple\nhyper-parameters. However, those parameters are mostly related to help the underlying gradient descent schemes, and\nare widely understood, which helps in to easily selecting\ntheir reasonable values. The finally selected parameter values are b = 128, K = 650, η = 30 for Alg. (1). For the\nrefinement, we use 150 iterations while keep the other related parameter values the same, and let λ = 50. We cycle\nbetween Alg. (1) and refinement 2 times. This requires on\naverage ∼19 and ∼14 minutes respectively to generate an\nimage for VGG-16 and ResNet-50 on NVIDIA RTX 3090\nwith 24GB RAM using Pytorch implementation.",
        "conclusion": "We provide a novel perspective on saliency mapping of visual classifiers that maps generic geometric features associated by the model with its outputs. Our input-agnostic\nmap construction gradually accumulates the gradient information of the model’s loss surface with respect to its training distribution. We also motivate the need of such a map\ntheoretically, highlighting a critical limitation of the inputspecific saliency mapping paradigm. We demonstrate a utility of the newly found saliency mapping in backdoor detection of compromised models. Our novel perspective is likely\nto instigate interesting methods and their uncharted utilities\nin model interpretation domain.",
        "summary_en": "Saliency methods provide post-hoc model interpretation by attributing input features to the model outputs. Current methods mainly achieve this using a single input sample, thereby failing to answer input-independent inquiries about the model. And input-specific saliency mapping is intrinsically susceptible to misleading feature attribution. Current attempts to use `general' input features for model interpretation assume access to a dataset containing those features, which biases the interpretation. Addressing the gap, this paper introduces a new perspective of input-agnostic saliency mapping that computationally estimates the high-level features attributed by the model to its outputs. These features are geometrically correlated, and are computed by accumulating model's gradient information with respect to an unrestricted data distribution. To compute these features, the paper nudges independent data points over the model loss surface towards the local minima associated by a human-understandable concept, e.g., class label for classifiers. With a systematic projection, scaling and refinement process, this information is transformed into an interpretable visualization without compromising its model-fidelity. The visualization serves as a stand-alone qualitative interpretation. With an extensive evaluation, the paper not only demonstrates successful visualizations for a variety of concepts for large-scale models, but also showcases an interesting utility of this new form of saliency mapping by identifying backdoor signatures in compromised classifiers.",
        "summary_zh": "这篇论文介绍了一种名为“输入不可知显著性映射”的新方法，用于解释深度视觉分类器的输出。它解决了当前方法仅适用于单个输入样本且容易产生误导的问题。该方法通过计算模型对输出的高级特征的几何相关性，从而实现了对模型输出的解释。经过系统的投影、缩放和精化过程，这些信息被转化为可解释的可视化结果，并且不损害模型的准确性。研究结果表明，该方法成功地实现了对大规模模型中各种概念的可视化解释，并且通过识别受损分类器中的后门特征，展示了这种新形式显著性映射的有趣应用价值。"
    },
    {
        "title": "A Semi-parametric Model for Decision Making in High-Dimensional Sensory Discrimination Tasks",
        "abstract": "Psychometric functions typically characterize binary sensory decisions along a single stimulus dimension. However, reallife sensory tasks vary along a greater variety of dimensions (e.g. color, contrast and luminance for visual stimuli). Approaches to characterizing high-dimensional sensory spaces either require strong parametric assumptions about these additional contextual dimensions, or fail to leverage known properties of classical psychometric curves. We overcome both limitations by introducing a semi-parametric model of sensory discrimination that applies traditional psychophysical models along a stimulus intensity dimension, but puts Gaussian process (GP) priors on the parameters of these models with respect to the remaining dimensions. By combining the flexiblity of the GP with the deep literature on parametric psychophysics, our semi-parametric models achieve good performance with much less data than baselines on both synthetic and real-world high-dimensional psychophysics datasets. We additionally show strong performance in a Bayesian active learning setting, and present a novel active learning paradigm for the semi-parametric model.",
        "introduction": "Understanding the mappings from physical stimuli to mental percepts is an important goal of perceptual neuroscience\nand psychophysics. A popular experimental technique is to\nmeasure behavioral responses to varying levels of a single\nstimulus feature, such as contrast of an image or volume of\na sound, and generate a behavioral response curve. The average response probabilities are then often fit to a functional\nform that is sigmoidal in shape, for example using the probit,\nlogit, or Weibull functions (Strasburger 2001). These parametric models tend to have specific interpretable parameters\nthat are of use to practitioners, such as thresholds and slopes\n(Brand and Kollmeier 2002).\nImproving the accuracy and sample efficiency of parameter estimation for these psychometric functions remains an\narea of active research (Sch¨utt et al. 2016; Shen and Richards\n2012). However, much of the existing work focuses on a single stimulus feature, ignoring the fact that stimuli continuously vary in important dimensions other than intensity, such\nas color or pitch. To understand sensory sensitivity in these\nsettings, univariate parameterized models require densely\nsampling the entire stimulus feature domain; i.e. creating\na single psychometric function for each value of any nonintensity stimulus feature.\nRecent work overcomes these limitations and extends the\nclassical densely-sampled univariate psychophysical curve\nin terms of both modeling and experimental design. On the\nmodeling front, recent work captures correlations in psychometric functions across stimulus dimensions, either using prespecified multidimensional parametric models (Watson 2017), or using nonparametric Gaussian processes (GPs)\n(Gardner et al. 2015a,b; Owen et al. 2021). For experimental design, dense sampling of the entire multidimensional\ninput space has been replaced with efficient active learning schemes (Houlsby et al. 2011; Settles 2009; Gardner\net al. 2015a,b; Watson 2017). Together, flexible psychometric models and multidimensional adaptive sampling methods\nhave led to improvements in estimating psychometric tuning in multivariate stimulus settings (Gardner et al. 2015a;\nOwen et al. 2021; Letham et al. 2022).\nHowever, existing high-dimensional psychometric models suffer from some limitations. The fully parametric approach, while interpretable, is constrained in practice, requiring an a priori model for all dimensions, with parameters\nlearned using an inefficient grid-search (Watson 2017). On\nthe other hand, the more flexible, nonparametric models that\nleverage the power of GPs are not regularized to have sensible tuning properties, such as positive monotonicity in stimulus intensity. Moreover, they do not afford an experimenter\nwith interpretable parameters like those given in the classical\nunivariate context (Gardner et al. 2015b; Owen et al. 2021).\nTo address these limitations, we propose a semiparametric model of the psychometric field well-suited for\nhigh-dimensional stimuli. Over the intensity dimension, our\npsychometric model is governed by a characteristic function\nthat has a sigmoidal shape with identifiable slope and offset parameters. Each of these parameters, however, is governed by a GP across context (non-intensity) stimulus dimensions, admitting a flexible characterization of the psychometric field in a high-dimensional continuous stimulus\nspace. While the posterior of our model is intractable, we develop two approximations, each with different benefits. The\nfirst, the full semi-parametric model, uses a semi-parametric\nvariational posterior that factorizes over the slope and offset parameters of the sigmoid to learn the model. For the\nsecond, the MVN approximate model, we derive a new approximation for the elementwise product of the multivariate normal (MVN) distributions of the slope and offset GPs\nin our model. This approximation implicitly defines a new,\nsingle GP kernel specific to the psychophysics setting, and\nlets us perform inference with fewer variational parameters,\nmaintaining a form that is adaptable to standard GP inference methods and active learning machinery. We evaluate\nour model on simulated and real data in up to 8 dimensions,\nand find that this semi-parametric approach not only provides interpretable results in high-dimensional stimulus settings, but also offers a faster and more accurate estimation\nprocedure for the psychometric field.\nGiven that Bayesian active learning also plays an important role in high dimensional psychophysical tuning estimation (Gardner et al. 2015a; Owen et al. 2021; Letham et al.\n2022), we conclude our model evaluation showing semiparametric performance under a variety of existing active\nlearning objectives. We find that our full semi-parametric\nmodel shows strong performance under a variety of these\nobjectives, and our MVN approximate model allows for\nanalytic acquisition functions, including recently-developed\nlook-ahead approaches (Letham et al. 2022). Finally, we introduce a novel threshold-based acquisition function for use\nwith our full semi-parametric model that shows strong active\nlearning performance in a high-dimensional experiment.\nThe Semi-parametric Psychophysical Model\nWe consider data of the form D = {xn, yn}N\nn=1 where yn ∈\n{0, 1} are participant responses and xn = (xin, xsn) describe stimulus configurations. We separate stimulus context\n(s) dimensions from the intensity (i) dimension. Throughout\nthe manuscript we only consider 1-dimensional intensity, so\nthat xin is a scalar. We assume that intensity predicts responses according to a standard psychophysical parametric\nmodel of the form p = σ(k(xi +c)) for some slope k, offset\nc, and sigmoid link σ : R → [0, 1] and probability of detection p. Our framework can be flexibly adapted to incorporate any sigmoid from the literature, including the probit (as\nstandard in GP classification), logistic (common elsewhere\nin machine learning), Weibull or Gumbell CDF (common\nin psychophysics, (Strasburger 2001; May and Solomon\n2013)), or other common probability mappings. We can additionally modify these sigmoids, for example shifting or\nscaling them to manage psychophysics experiments where\nparticipants are asked to discriminate between stimuli, and\nthe response probability is lower-bounded above 0. Common experimental paradigms can produce a lower bound response probability at 0.5 (for two-alternative decisions) or\n0.25 (for four-alternative ‘odd one out’ trials).\nWe place independent GP priors on the slope and offset governed by the context dimensions, which implies\nthat each of fk = [k(xs1), k(xs2), . . . , k(xsN )] and fc =\n[c(xs1), c(xs2), . . . , c(xsN )] is Gaussian distributed:\nfk ∼ N (m, Σk) , fc ∼ N (0, Σc) ,\n(1)\nwhere Σk, Σc are N × N covariance matrices whose\n(n, n′)th entry is given by a kernel function κ(xsn, xsn′ )\nand N is the total number of stimuli sampled. We use the\nstandard radial basis function (RBF) kernel with independent hyperparameters for the slope and offset GP kernels.\nEach kernel is governed by its own hyperparameters θc and\nθk for the offset and slope GPs, respectively. Finally, m is\na positive constant to center the prior distribution of slope\nvalues at some positive number (we use m = 2 for all experiments; see appendix for additional detail and evaluation).\nIn this formulation, we can write the joint distribution of\nlatent stimulus values as:\nz = fk ◦ (fc + xi) ,\n(2)\nwhere ◦ denotes the Hadamard (elementwise) product and\nxi = (xi1, xi2, . . . xiN ). The likelihood of a set of observations is given by a set of independent Bernoulli distributions\nwith probabilities equal to z taken through the sigmoid link.\nTo make sense of this mapping, consider a joint draw of N\nslopes fk ∈ RN and intercepts fc ∈ RN. Suppressing the\ndependence of k and c on xs, the resultant transformed collection of variables has each zn = kncn + knxin, producing z = [k1c1 + k1xi1, k2c2 + k2xi2, . . . , kNcN + kNxiN ].\nNote that this is just a multivariate extension of the standard\nk(x + c) input into the link, written jointly to accommodate\nthe GP prior on k and c. A graphical depiction of the semiparametric model is given in Fig. 1A.\nFig. 1B shows prior samples of the probability of detection, p, plotted along the intensity dimension, for three\ndifferent models. The first is an unconstrained GP model\n(GP-RBF) which treats all dimensions equally (Owen et al.\n2021), and the second has an additive GP kernel that is linear in the intensity dimension and RBF in context dimensions (GP-linear) (Gardner et al. 2015b). The third (Fig. 1B,\nRight) is our proposed semi-parametric model. These probability samples for each model are drawn from the prior and\nillustrate the hypothesis space of the considered psychometric functions. The GP-RBF model allows for tuning that is\nnot sigmoidal (or even monotonic), and the GP-linear model\nrestricts the shape of the sigmoid while still permitting negative slopes. In contrast, our semi-parametric model shows\nclassical psychometric curves along the intensity dimension\nand is free to vary independently in its offset c and slope k.\nOur semi-parametric model is easily adaptable to use any\nlink function with interpretable slope and intercept values, as\nwell as scaled and/or shifted variants thereof. In the present\ncontribution we consider the logistic and Probit mapping, as\nwell as the so-called Weibull function which is the cdf of\na left Gumbel distribution in log-space (Strasburger 2001;\nMay and Solomon 2013). We also consider a ‘floor link’\nwhose minimum is set to the known lower bound of the response probability in the given task. For our results on simulated functions, we choose the link that performed best with\nquasi-random sampling for each model1. We show results\nwith all links on human psychophysical data and include results from all links and floors in the appendix.\n1This is the Gumbel-link with floor for the semi-parametric\nmodel for all except for GlobalMI acquisition, which requires a\nprobit link with a floor of 0. For all other models this is the Probit\nlink with a floor of 0.\nlink\nA\nB\nFigure 1: A. Graphical model of our semi-parametric approach. B. Samples from the prior along the intensity dimension for\nthree psychophysical models. The GP-RBF model’s prior contains unrealistic psychometric functions that are not monotonic.\nThe previously published GP-linear model overly restricts the shape of the sigmoid due to the linear kernel in intensity, while\nadmitting functions with negative slopes. Our new model’s prior, here with a Gumbel link, contains a more diverse set of\nrealistic psychometric functions than either baseline variant.",
        "inference for the semi-parametric psychophysical model": "The marginal likelihood for our model is:\nP(y|Xs, xi, θk, θc) =\nZ\np(y|xi, fk, fc)p(fk|Xs, θk)p(fc|Xs, θc)dfkdfc,\n(3)\nwhere Xs is a concatenation of (xs,1, xs,2 . . . xs,N) and y is\na concatenation of the (y1, y2 . . . yn) observations. This likelihood is intractable, but we provide two distinct strategies\nfor approximating it: one by factorized variational inference\n(VI), and the other by approximating the model itself, which\nlets us apply standard VI methods for GPs.\nFactorized Variational Inference for the\nSemi-parametric Model\nWe define MVN variational distributions qk and qc for the\nslope and offset to perform VI. We can write an evidence\nlower bound (ELBO) as follows:\nlog p(y |Xs, xi) ≥\nL := Eqc(fc),qk(fk)[log p(y | fk, fc, xi)]\n− KL[qk(fk)∥p(fk | Xs)]\n− KL[qc(fc)∥p(fc | Xs)],\nwhere the two KL terms between MVNs are available in\nclosed form. For the remaining term, since the slope and\noffset GPs are combined into a scalar latent term per observation, we can compute the expectation by one-dimensional\nGauss-Hermite quadrature. Gradients of all terms are available by automatic differentiation, which lets us optimize this\nobjective by standard methods (Hensman, Matthews, and\nGhahramani 2015; Balandat et al. 2020).\nApproximation to the Semi-parametric Model\nWhile the inference approach above is tractable, its key disadvantage is that the prior on the latent z is no longer a\nGaussian Process prior. This limits our ability to use standard black box GP variational inference for this model, as\nwell as other tooling that relies on an MVN prior. Furthermore, since we have MVN posteriors on both slope and offset, the variational approximation has twice as many parameters, which potentially slows down inference (of interest\nfor human-in-the-loop applications). To address these disadvantages, we derive an approximation to our model that\navoids the need to use a variational approximation for both\nthe slope and offset GP function values. Specifically, we directly approximate the latent function z with an MVN using\nmoment-matching.\nWe are interested in a MVN approximation to the latent\nfunction:\nZ = Fk ◦ (Fc + xi) .\nHere, Fk, Fc are random variables distributed according to\nthe Gaussians in (1), and Z is now also a random variable\n(in contrast to fk,fc, z, indicating realizations of the random\nvariables). For convenience of derivation, let ˜Fc = Fc +\nxi. The GP prior on Fc implies that ˜Fc ∼ N (xi, Σc). We\ncan compute the mean of Z in terms of the slope and offset\ndistributions:\nE[Z] = E[Fk ◦ ˜Fc] = mxi.\nWe can similarly compute the covariance of the latent Z in\nterms of the slope and offset. For convenience, denote Yk =\nFk − m and Yc = ˜Fc − xi. We have that\nZ = (Yk + m) ◦ (Yc + xi)\n= mxi + mYc + xi ◦ Yk + Yk ◦ Yc,\n(4)\nwith Yc ∼ N(0, Σc) and Yk ∼ N(0, Σk). The covariance of\nthe cross-term can be computed as\nCov[Yk ◦ Yc]i,j = E[(Yk ◦ Yc)i(Yk ◦ Yc)j]\n(5)\n= E[(Yk,iYc,i)(Yk,jYc,j)]\n= E[Yk,iYk,j]E[Yc,iYc,j]\n(6)\n= (Σk)i,j(Σc)i,j,\nwhere (5) uses that Yk and Yc have 0 mean, and (6) uses their\nindependence. Thus, Cov[Yk ◦ Yc] = Σk ◦ Σc.\nApplying this result to (4) we can compute the covariance\nfor the latent function:\nCov[Z] = m2Σc + xixT\ni ◦ Σk + Σc ◦ Σk\n= m2Σc + (Σc + xixT\ni ) ◦ Σk.\nThe primary benefit of this approximation is that rather\nthan learn the full semi-parametric latent function Z, which\nincludes parameterizing slopes, fk, and offsets, fc, we simply\ninfer values for the moment-matched approximate latent\n˜Z ∼ N(mxi, m2Σc + (Σc + xixT\ni ) ◦ Σk).\n(7)\nIn this formulation, the latent function is effectively a GP\nwith a novel kernel function specific to the psychophysics\nproblem, and we can thus apply standard methods for model\nfitting, as well as for using the model in active learning as\nwe will see below.\nApproximate Normality of the Latent Function\nThe accuracy of the moment-matched MVN in (7) will depend on\nhow close the true posterior for the latent function Z is to being normally distributed. We show here the conditions under\nwhich Z is Gaussian (Pinelis 2018).\nLet Σ = m2Σc + xixT\ni ◦ Σk. Then,\nΣ−1/2Z = W + Σ−1/2(Yk ◦ Yc),\nwhere W ∼ N(0, 1). Note that\nE∥Σ−1/2(Yk ◦ Yc)∥2 ≤\n\r\r\rΣ−1/2\r\r\r\n2\nE ∥Yk ◦ Yc∥2\n=\n\r\rΣ−1\r\r tr (Σk ◦ Σc) .\nThus, as\n\r\rΣ−1\r\r tr (Σk ◦ Σc) → 0, Z is Gaussian with mean\n0 and covariance Σ. This means that when the variances of\nthe slope and offset values, k and c, are low, the Hadamard\nproduct of the GPs is approximately Gaussian, and there will\nbe little loss to using the Hadamard approximation model.\nEmpirically, we later show that the Hadamard model incurs a\nmodest performance loss relative to the full semi-parametric\nmodel, but (as shown in the appendix) the Hadamard model\nis much less able to benefit from psychophysics-specific\nmodifications to the link function.\nere about cholesky and specifics of inference....",
        "active learning with semi-parametric gps": "Our semi-parametric model grants additional benefits for\nBayesian active learning. Active learning methods define an\nacquisition function that prescribes the value of sampling a\nparticular candidate point given the previously observed data\nand the current estimate of the model posterior. By optimizing this acquisition function, the next input (x) is chosen for\nsampling. A key goal in psychophysics is the estimation of\npsychometric detection thresholds, more formally known as\na level set estimation (LSE) problem, i.e. finding the regions\nwhere the psychometric function is above or below some\npre-determined threshold value r. In the general case of active learning for LSE, the location of the threshold is defined\nimplicitly, and sampling strategies operate on an estimate of\nan arbitrary latent function.\nWith our semi-parametric model, the threshold is\nuniquely determined as a function of the context dimensions,\nand can be computed directly from the model posterior for\nk and c as:\nxr\ni = σ−1(r)\nk(xs) − c(xs).\nWe can use samples from this posterior to compute several\nquantities of interest, for example the threshold posterior\nvariance. In addition to being of interest to practitioners, reducing the posterior variance of the threshold is a natural,\nsimple objective for active learning. We term it ThresholdBALV as it applies Bayesian Active Learning by Variance\n(Settles 2009) to the threshold posterior.",
        "results": "We showcase the benefits of the semi-parametric model in\na few ways. First, we demonstrate performance on two synthetic psychometric test functions, where we show that our\nmodel can achieve good performance with less data than\npreviously proposed baselines. Second, we evaluate performance on multiple real-world datasets, and show that our\nmodels outperform baselines in terms of predictive performance on unseen data. Finally, we demonstrate the compatibility of our models with active learning methods, again\nshowing good performance with far less data than baselines,\nas well as competitive behavior of our novel ThresholdBALV acquisition function. We consider as baselines previous models used for flexible modeling and active learning for psychophysics, namely an otherwise-unconstrained\nGP-RBF model (Owen et al. 2021), and a GP-linear model\nwith a linear kernel in the intensity dimension and an RBF\nkernel in the remaining dimensions (Schlittenlacher, Turner,\nand Moore 2018, 2020; Song, Garnett, and Barbour 2017;\nGardner et al. 2015b).\nTwo Dimensional Task\nBefore evaluating the semi-parametric model in a high dimensional setting, we first demonstrate performance in a\nsimple 2-d psychometric test function, previously proposed\nin (Owen et al. 2021), and detailed in the appendix. This\nfunction has a monotonic (probit) probability of detection\nalong an intensity dimension, and smoothly varies as a linear combination of sines and cosines in a second dimension.\nWe used a quasi-random Sobol sequence (Sobol’ 1967) to\nselect stimulus locations x for our 2d test function.\nFig. 2A shows the prediction performance of the semiparametric model on this function and Fig. 2C shows better\nestimation of the psychometric curve after 500 samples. For\nour evaluation metric we use the Brier score (Brier 1950),\ncomputed in expectation over the model’s posterior. We use\nthe Brier score because it assesses the calibration of the approximate posterior, and we use the expectation to account\nfor the quality of posterior uncertainty estimation (we consider other metrics in the appendix). Our proposed model,\nwith or without the MVN approximation, achieves low Brier\nscores much faster than baselines, though all models eventually achieve very good performance in this relatively simple\ntest function. We will see next that the benefits are magnified\nin higher dimensions.\nEight Dimensional Task\nTo simulate more a realistic, multidimensional sensory context we test the semi-parametric model using an 8-d psychometric test function previously reported by (Letham et al.\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n200\n300\nNumber of observations\nExpected Brier Score\nGP−Linear\nGP−RBF\nSemiP−Exact (ours)\nSemiP−MVN (ours)\n2d Discrimination Test Function\nA\nB\n0.05\n0.10\n0.15\n0.20\n100\n200\n300\nNumber of observations\nExpected Brier Score\n8d Discrimination Test Function\nC\nD\nFigure 2: A. Performance on 2-d example as a function of number of quasi-random samples drawn, for all models. Curves are\naverages from 100 replications, with standard errors shaded. B. Same for 8-d test function. C. 2-d function estimation after 500\nsamples for all models. All models recover the test function with sufficient data. D. Inferred response probabilities in 8-d test\nfunction after 500 observations. Plots in each row have a randomly chosen dimension plotted along the y axis, and the intensity\ndimension along the x axis.\n2022) and described in the supplement. This function retains\nmonotonicity in detection probability along the intensity dimension in the same way as the 2d function, but detection\nprobability is a smooth function of stimulus feature inputs\nin the other seven dimensions.\nFor this high dimensional test function, the semiparametric model better estimates detection probability p in\nfewer samples than competing models, and the approximate\nMVN model achieves essentially identical performance as\nthe full semi-parametric model. Fig 2 B shows the Brier\nscore for each model for the first 300 samples. To get a sense\nof how well the models are actually able to estimate this high\ndimensional psychometric test function, we include 4 random 2d slices through the 8 dimensional space after training\non 500 random samples (Fig 2 D).\nResults on Human Behavioral Data\nTo emphasize the generality and utility of our model, we\nevaluate performance on five participants from two realworld 6-dimensional datasets. All data are from visual psychophysical tasks. One dataset is a participant in a twoalternative forced choice (2AFC) task with 3000 trials from\nLetham et al. (2022), and the remaining are four participants performing a 4 alternative forced choice (4AFC) from\nWuerger et al. (2020) whose trial counts range from ∼200500 depending on the subject2. In the 2AFC task, the participant is presented with an animated circular Gabor patch,\none half of which has been scrambled to resemble white\nnoise. The scrambled side is selected at random. The stimulus varied along eight dimensions, six of which (contrast,\nbackground luminance, temporal and spatial frequency, size,\nand eccentricity) have data published. In the 4AFC task, a\nGabor stimulus was presented in one of four quadrants of a\nscreen, and participants were asked to select which quadrant\ncontained the stimulus. This stimulus varied with size, orientation, frequency, and color. We used contrast as the intensity\ndimension for both tasks. For additional subject information\nand example stimuli, see supplementary materials. We run\n15-fold cross-validation, train our model on 80% of the data\nand test on 20%, and report cross-validated log-likelihood.\nWe see across all five subjects that some variant of the semiparametric model consistently has superior cross-validated\nlog-likelihood on held-out trials, regardless of whether we\nuse the MVN or exact semi-parametric variant. We also note\nthat no one link function consistently performs best—this\ninconsistency across choice of link is one reason the specific\n2The\nformer\ndataset\nis\navailable\nat\nhttps://github.com/\nfacebookresearch/bernoulli lse/tree/main/data, and the latter at\nhttps://www.repository.cam.ac.uk/handle/1810/304228.\nFigure 3: Mean cross-validated log-likelihood for 15-fold cross validation on an example subject from a 2AFC psychometric\ntask, and four example subjects in a 4AFC task from (Watson 2017). Gray bars indicate standard error across folds.\nparametric form of choice is still an area of active research.\nNonetheless, we highlight that the semi-parametric model\nand MVN approximation are consistently strong performers.\nActive Learning\nLastly, we evaluate the semi-parametric model’s estimation\nof the 8-d test function using a variety of common Bayesian\nactive learning schemes, including the BALD acquisition\nfunction based on mutual information (Houlsby et al. 2011;\nGardner et al. 2015a; Owen et al. 2021), the BALV active learning scheme based on posterior variance (Settles\n2009), and GlobalMI, a global lookahead acquisition function based on threshold estimation (Letham et al. 2022).\nThe computation of the GlobalMI aquisition function requires an MVN posterior on the latent function z and a probit\nlink with a floor of 0, which means it can be applied directly\nwith GP-RBF and GP-Linear, and to our MVN approximate model. To use GlobalMI with the full semi-parametric\nmodel, we switch the link to a probit with floor of 0 (from\nGumbel with a floor set to chance), and apply the MVN approximation we derived above to the variational posteriors\nqc(fc) and qk(fk). In this setting we can still use the full\nsemi-parametric model for evaluation. In addition to these\nbaselines, we include the ThresholdBALV acquisition function as described earlier for use with the semi-parametric\nmodel.\nFig. 4 shows performance of all models under the considered active learning schemes. Here, we are not using\nthe standard Brier score as we did previously, which was\nover the full model posterior. Instead are using a Brier\nscore on the sublevel-set (threshold) posterior, i.e. the Brier\nscore on estimating the probability of p above and below\na threshold of 0.75. We choose this performance metric as\nthe ThresholdBALV and GlobalMI acquition functions are\nbased specifically on threshold estimation. Performance of\nthese acquisition methods using other metrics are shown in\nthe supplement. We see that the BALD and BALV baseline\nacquisition functions perform comparatively poorly, and this\nis true irrespective of the model.\nHowever, we see using threshold-based schemes (GlobalMI and ThresholdBALV) that the semi-parametric models perform well, especially during early acquisition. At\nthe end of acquisition, GlobalMI acquisition in conjunction\nwith the GP-RBF model performs marginally better than\nall other acquisition-based models. It is additionally important to note that quasi-random Sobol sampling for this 8d function performs remarkably well against these active\nlearning schemes, often as good or better than all acquisition functions tested. The unusual effectiveness of quasirandom sampling in this setting has been previously reported (Letham et al. 2022) and we see it with our models as\nwell as with the GP-RBF and GP-linear baselines. Exploring performance of these acquisition functions compared to\nquasi-random sampling for high dimensional psychophysics\nis an interesting avenue for future work. Here, we simply\nwish to emphasize that the semi-parametric model and its\nMVN approximation are compatible and competitive under a variety of existing active learning schemes, and our\nproposed ThresholdBALV acquisition shows strong performance early on in sampling for an 8-d test function. For\nfurther evaluation of active learning for the semi-parametric\nmodels, see the supplement. oundaries, allowing for more\ninformative samples.\nConclusion\nWe have demonstrated that a semi-parametric model for psychometric field estimation based on a parameterized sigmoid\nfunction can be adapted to high-dimensional psychophysical contexts using GPs as a non-parametric constraint on\nthe sigmoid parameters. This semi-parametric approach not\nonly offers parameters with scientific interpretation in the\ncontext of discrimination behavior, but offers accuracy improvements for estimating high dimensional tuning curves\ncompared to competing methods. It does the latter both by\nproviding a better prior, and by enabling a new active learning objective based on the semi-parametric functional form.\nWe further introduce a moment-matching approximation to\nGP−Linear\nGP−RBF\nSemiP−Exact (ours)\nSemiP−MVN (ours)\n100\n200\n100\n200\n100\n200\n100\n200\n0.2\n0.3\n0.4\n0.5\n0.6\nNumber of observations\nThreshold Brier Score\nSobol\nBALV\nBALD\nGlobalMI\nThresholdBALV (ours)\nThreshold active learning\nFigure 4: Performance metrics for threshold estimation under a variety of common active learning paradigms.\nour model that can be used as a psychophysics-specific GP\nprior, or to produce approximate MVN posteriors compatible with analytic acquisition functions. We evaluate our contributions relative to baselines on both synthetic and real\ndata and show a number of performance gains, especially\nfor smaller sample sizes that are important in real human-inthe-loop experiments.\nLimitations\nFirst, the evaluations in this paper either use\nsynthetic test functions or human visual psychophysical data\non a per-subject basis, so we cannot say at this stage how\nthe model will perform with data from other sensory modalities or for cross-participant prediction. Second, we focus\nour evaluation on the Brier score in expectation over the\nposterior. While taking other metrics in expectation over the\nposterior shows similar behavior (as we demonstrate in the\nappendix), focusing on the posterior mean only (as done\nin prior work) changes the story somewhat. In particular,\nif we only consider the posterior mean of the RBF model,\nits key deficiency of having an overly flexible hypothesis\nspace is mitigated and its performance looks stronger. Third,\nwhile we demonstrate that our work is compatible with\nactive learning methods (even ones that require an MVN\nposterior), we do not offer an exhaustive evaluation of active learning methods, and the benefits of our contributions\nfor active learning appear to be focused on small sample\nsizes. In line with this, we focus our evaluation and narrative around performance with relatively small data (consistent with the goal of sample-efficient psychophysics) but it\nis likely that with much larger datasets, the GP-RBF model’s\nuniversality will let it match or outperform models with\nmore restricted hypothesis spaces such as ours.\nEthics Statement\nOur work is primarily concerned with\nunderstanding low-level human perception, and as such carries relatively low risk of societal and ethical harm. Some\nrisks include the misuse or de-anonymization of data, and\noverly broad or incorrect conclusions made based on data\nthat is too limited, collected in a biased way, or based on\nmisunderstanding or misusing the model. With respect to\ndata misuse, we use only de-identified data that has been\npreviously published, where informed consent was obtained,\nand is of low sensitivity (it is behavioral responses to simple visual stimuli). With respect to overly broad conclusions,\nwe keep our claims narrowly focused on the quality of the\nmodel, and do not provide new interpretations or conclusions related to the datasets we use for evaluation. Furthermore, we think the specificity of our model for the psychophysics problem domain makes it less likely to be applied (and misused) in other settings than more generic models. On a more positive note, increasing sample efficiency for\npsychophysics studies may improve the experience of human research participants, who can sometimes be required\nto participate in dozens of hours of data collection when traditional grid or staircase methods are used.\nComputational Load\nWith respect to computational load\nand environmental impact, the benchmarks were all carried\nout over the course of a few days (≈70-100 hours) on a single EC2 c6i.metal node, and the cross-validation folds\nwere performed over a smaller node over a similar period of\ntime. These hours are largely taken up by replications across\nseeds (for benchmarks) and folds (for cross-validation)—for\npractical usage, the models we use take seconds to estimate\non a typical laptop, which makes them accessible for use by\nmost practitioners and researchers.\nCode Release\nThe code needed to use our contribution in new experiments and benchmarks (models, likelihoods, acquisition function, etc) is available as part of\nAEPsych (https://aepsych.org/). The code needed to generate the specific benchmarks and figures in the paper\nis available at https://github.com/facebookresearch/semiparametric-psychophysics.",
        "conclusion": "We have demonstrated that a semi-parametric model for psychometric field estimation based on a parameterized sigmoid function can be adapted to high-dimensional psychophysical contexts using GPs as a non-parametric constraint on the sigmoid parameters. This semi-parametric approach not only offers parameters with scientific interpretation in the context of discrimination behavior, but offers accuracy improvements for estimating high dimensional tuning curves compared to competing methods. It does the latter both by providing a better prior, and by enabling a new active learning objective based on the semi-parametric functional form. We further introduce a moment-matching approximation to our model that can be used as a psychophysics-specific GP prior, or to produce approximate MVN posteriors compatible with analytic acquisition functions. We evaluate our contributions relative to baselines on both synthetic and real data and show a number of performance gains, especially for smaller sample sizes that are important in real human-inthe-loop experiments.",
        "summary_en": "Psychometric functions typically characterize binary sensory decisions along a single stimulus dimension. However, real-life sensory tasks vary along a greater variety of dimensions (e.g. color, contrast and luminance for visual stimuli). Approaches to characterizing high-dimensional sensory spaces either require strong parametric assumptions about these additional contextual dimensions, or fail to leverage known properties of classical psychometric curves. This paper overcomes both limitations by introducing a semi-parametric model of sensory discrimination that applies traditional psychophysical models along a stimulus intensity dimension, but puts Gaussian process (GP) priors on the parameters of these models with respect to the remaining dimensions. By combining the flexibility of the GP with the deep literature on parametric psychophysics, the semi-parametric models achieve good performance with much less data than baselines on both synthetic and real-world, high-dimensional psychophysics datasets. The paper additionally shows strong performance in a Bayesian active learning setting, and present a novel active learning paradigm for the semi-parametric model.",
        "summary_zh": "这篇论文介绍了一种半参数化模型来用于高维感知辨别任务，旨在解决传统的心理模型只能描述单一刺激维度上的二元感知决策，无法解决实际生活中维度更多的感知任务。具体而言，该模型在刺激强烈的维度上应用传统的心理物理模型，在其他维度上使用高斯过程先验对这些模型的参数进行建模。该半参数化模型在合成世界和真实世界的高维心理物理数据集上，比基准模型表现出更好的性能，并且在贝叶斯主动学习环境中也表现出良好的性能。此外，作者还提出了一种针对半参数化模型的新型主动学习范式。"
    },
    {
        "title": "Generalizing Downsampling from Regular Data to Graphs",
        "abstract": "Downsampling produces coarsened, multi-resolution representations of data and it is used, for example, to produce lossy compression and visualization of large images, reduce computational costs, and boost deep neural representation learning. Unfortunately, due to their lack of a regular structure, there is still no consensus on how downsampling should apply to graphs and linked data. Indeed reductions in graph data are still needed for the goals described above, but reduction mechanisms do not have the same focus on preserving topological structures and properties, while allowing for resolution-tuning, as is the case in regular data downsampling. In this paper, we take a step in this direction, introducing a unifying interpretation of downsampling in regular and graph data. In particular, we define a graph coarsening mechanism which is a graph-structured counterpart of controllable equispaced coarsening mechanisms in regular data. We prove theoretical guarantees for distortion bounds on path lengths, as well as the ability to preserve key topological properties in the coarsened graphs. We leverage these concepts to define a graph pooling mechanism that we empirically assess in graph classification tasks, providing a greedy algorithm that allows efficient parallel implementation on GPUs, and showing that it compares favorably against pooling methods in literature.",
        "introduction": "The concept of information coarsening is fundamental in\nthe adaptive processing of data, as it provides a simple, yet\neffective, means to obtain multi-resolution representations of\ninformation at different levels of abstraction. In large scale\nproblems coarsening also serves to provide computational\nspeed-ups by solving tasks on the reduced representation,\nideally with a contained loss in precision with respect to\nsolving the original problem.\nCoarsening\nis\nkey\nin\nConvolutional\nNeural\nNetworks (CNNs, Fukushima 1980; LeCun et al. 1989), where\npooling is often used to repeatedly subsample an image to\nextract visual feature detectors at increasing levels of abstraction (e.g., blobs, edges, parts, objects, etc). Downsampling is\nalso popular in the adaptive processing of timeseries where,\nfor instance, it is used in clockwork-type Recurrent Neural\nNetworks (Koutn´ık et al. 2014; Carta, Sperduti, and Bacciu\n2021) to store information extracted at different frequencies\nand timescales. More recently, the Graph Convolutional Networks (GCNs, Micheli 2009; Gori, Monfardini, and Scarselli\n2005; Bacciu et al. 2020) community popularized graph reduction mechanisms as a structured counterpart of the image\npooling mechanism in classical CNNs.\nThe definition of a reduction mechanism that downsamples\ninformation at regular intervals between data points (e.g., a\nsample, a pixel, a timestamped observation, etc) is straightforward when working with images and time series. It can\nbe achieved simply by picking up a data point every k ones,\nwhere k is a given reduction factor defining the distance between the sampled points in the original data, possibly aggregating the properties of non-selected point with appropriate\nfunctions. The same approach cannot be straightforwardly\napplied to graphs, which lack regularity and a consistent ordering among their constituent data points, i.e., the nodes.\nTherefore, defining a well-formed notion of downsampling\nfor graphs becomes non-trivial. The research community\nhas been tackling this issue by a number of approaches, including differentiable clustering of node embeddings (Ying\net al. 2018; Bianchi, Grattarola, and Alippi 2020), graph reductions (Shuman, Faraji, and Vandergheynst 2016; Loukas\n2019), and node ranking (Cangea et al. 2018; Gao and Ji\n2019). Notably, approaches like the latter select important\nnodes in a graph and simply discard the rest without protecting the linked structure of the network, while reduction\nmethods typically focus on preserving structure without accounting for the role or relevance of nodes involved.\nWhat is yet an open problem is how to define a controllable\ngraph coarsening method, which reduces the size while preserving the overall structure by sampling representative yet\nevenly spaced elements, similarly to the approaches discussed\nabove for image and time series reduction.\nThis paper provides a first approach introducing such a\ntopology-preserving graph coarsening and its use in graph\npooling. We provide mechanisms which are the graph equivalent of pooling and striding operators on regular data, accompanying our intuition with formal proofs (in the Supplementary Material) of the equivalence of such operators on graphs\nwhich model regular data.\nCentral to our contribution is the definition of a mechanism\nto find a set of nodes that are approximately equally spaced\n(at distance no less than k) in the original graph. We build\non the graph-theoretic concept of Maximal k-Independent\nSets (k-MIS), that also comes with the ability to pin-point\nimportant nodes in each area of the graph. The selected nodes\nare then used as vertices of the reduced graph whose topology\nis defined in such a way that key structural properties of the\noriginal graph are well preserved. To this end, we provide\ntheoretical guarantees regarding distance distortions between\na graph and its reduction. Additionally, we prove the reduced\ngraph has the same number of connected components as\nthe original. The latter point is particularly relevant for a\ngraph pooling mechanism as it guarantees that the structure\nis not broken in disconnected fragments, which can hinder the\nperformance of neural message passing in the GCN layers.\nSuch properties are fundamental to ensure that the original\ngraph is downsampled evenly throughout its structure, preserving distances and sparseness of the key focal points in\nthe graph. By this means, the reduced graph can be used as\nan accurate fast estimator of the distances between nodes in\nthe original graph, where the amount of compression can be\neasily regulated through the choice of the k reduction factor.\nConcurrently, we borrow from node-ranking methods (Gao\nand Ji 2019) to produce k-MISs that maximize the total\nweights associated to the selected nodes, in order to preserve\nrelevant nodes without compromising structure.\nIn summary, our contributions are the following:\n• We introduce a graph coarsening method leveraging k-MIS\nthat is the graph-structured counterpart of equispaced sampling in flat data. We provide a greedy parallel algorithm\nto efficiently compute the k-MIS reduction, which is well\nsuited to use in GPU accelerators (Section 3).\n• We give formal proof of equivalence of our approach to\nregular downsampling in convolutional neural networks,\nwhen applied to diagonal grid graphs (Section 4 and Supplementary Material).\n• We prove theoretical guarantees on the distance distortions\nbetween a graph and its reduction. We provide also a formal\ncomplexity analysis of the introduced algorithms, proving,\nboth theoretically and experimentally, their scalability on\nlarge real-world graphs (Section 4 and Supplementary Material).\n• We integrate k-MIS reduction both as a pooling layer and as\na downsampling operator for GCNs, providing an empirical\nconfirmation of its advantages over literature approaches\non graph classification benchmarks (Section 6).",
        "notation and definitions": "We represent a graph G as a pair of disjoint sets (V, E),\nwhere V = {1, . . . , n} is its node set and E ⊂ V × V its\nedge set, with |E| = m. A graph can also be represented as\na symmetric matrix A ∈ Rn×n\n+\n, such that Auv = Avu is\nequal to a weight associated to the edge uv ∈ E or zero if\nuv ̸∈ E. The neighborhood N(v) of v is the set of nodes\nadjacent to it (denoted N[v] if includes v itself), and the\ndegree deg(v) of v is defined as the number of its neighbors,\ni.e., deg(v) = |N(v)|. The unweighted distance between two\nnodes u, v ∈ V , denoted as d(u, v), is defined as the length\nof the shortest path between the two nodes. If there is no\npath between the two nodes, then d(u, v) = ∞. The k-hop\nneighborhood Nk(v) of v (Nk[v] if inclusive) is the set of\nnodes that can be reached by a path in G of length at most k.\nThe k-th power of a graph Gk is the graph where each node\nof G is connected to its k-hop neighbors. To avoid confusion,\nany function may be denoted with a subscript to specify the\ngraph on which is defined (e.g., dG). An independent set, is a\nset of nodes S ⊆ V such that no two of which are adjacent in\nG. An independent set is maximal if is not a subset of another\none in G. A (maximal) k-independent set is a (maximal)\nindependent set of Gk.",
        "graph coarsening with k-mwis": "When dealing with signals, images, or other kinds of Euclidean data, downsampling often amounts to keeping every\nk-th data point, where k is a given reduction factor. This\nmeans, for a generic discrete n-dimensional Euclidean datum, keeping a subset of its points such that every two of\nthem are exactly k points far from each other on every of its\ndimensions. On graph-structured data, we lose this regularity\nalong with the concept of dimensionality, and hence defining\na new notion of downsampling that applies to graph becomes\nnon-trivial.\nHere we define a graph coarsening method that, similarly\nto classical downsampling, reduces the size of a graph G by\na given “factor”, by finding a set of almost evenly spaced\nnodes within G. These nodes will form the node set of the\nreduced graph, while its topology will be constructed starting\nfrom G in a way in which some of its key properties will\nbe preserved, such as connectivity, or approximated, such as\npairwise node distances.\nCoarsening algorithm.\nGiven a graph G = (V, E) and a\ndistance k, we want to obtain a coarsen representation of G\nby first selecting a set of nodes S ⊆ V , that we refer to as\ncentroids, such that every two centroids are more than k hops\ndistant from each other, and such that no area of the graph\nremains unsampled; in other words, a maximal k-independent\nsets (k-MIS) of G: this way, each centroid will be more than\nk hops from every other, while the maximality ensures every\nnode of G is within k hops from a centroid.\nAny MIS of a graph Gk is a k-MIS of G (Agnarsson,\nDamaschke, and Halld´orsson 2003), thus a k-MIS could be\nna¨ıvely computed by known MIS algorithms, such as Luby\n(1985) or Blelloch, Fineman, and Shun (2012), on the k-th\npower of the adjacency matrix of G. Using this approach\nwill require O(n2) space since the density of Gk increases\nrapidly with k, becoming rapidly impractical for real world\ngraphs with millions or billions of nodes. To overcome this\nproblem, we introduce Algorithm 1 that efficiently computes\na k-MIS of G without explicitly computing its k-th power.\nOnce the k-MIS S ⊆ V is computed with Algorithm 1, we\nconstruct the coarsened graph H = (S, E′) as follows:\n1. using Algorithm 2, we compute a partition P of V of size\n|S|, such that\n(a) every P ∈ P contains exactly one centroid and (a\nsubset of) its k-hop neighbors, and\nAlgorithm 1 Parallel Greedy k-MIS algorithm, adapted from Blelloch, Fineman, and Shun (2012). Given a graph G, a subset of\nits nodes U ⊆ V , and a node ranking π, returns a maximal kindependent set in G, with k ∈ N.\n1: function k-MIS(G, U, π)\n2:\nif |U| = 0 then return ∅\n3:\nπ0 ← π\n4:\nfor i = 1, . . . , k do\n5:\nfor v ∈ U do in parallel\n6:\nπi(v) ← minu∈N[v]∩U πi−1(u)\n7:\nS0 ← {v ∈ U | π(v) = πk(v)}\n8:\nfor i = 1, . . . , k do\n9:\nSi ← S\nv∈Si−1 N[v]\n10:\nR ← U \\ Sk\n11:\nreturn S0 ∪ k-MIS(G, R, π)\nAlgorithm 2 Parallel k-MIS partitioning algorithm. Given a graph\nG, k ∈ N, and a node ranking π, returns a partition of G.\n1: function CLUSTER(G = (V, E), k, π)\n2:\nS ← k-MIS(G, V , π)\n3:\nπ0 ← π\n4:\nfor v ∈ V \\ S do in parallel\n5:\nπ0(v) ← +∞\n6:\nfor i = 1, . . . , k do\n7:\nfor v ∈ V do in parallel\n8:\nπi(v) ← minu∈N[v] πi−1(u)\n9:\nreturn {{u ∈ V | πk(u) = π(v)}}v∈S\n(b) for every node in P there is a centroid in P at distance\nat most k-hops;\n2. for every edge in E we add an edge in E′ joining the two\nnearest centroids in the partitions containing the source\nand destination nodes. If this generates multiple edges, we\ncoalesce them into a single one, and we aggregate their\nweights according to a predefined aggregation function\n(e.g., sum);\n3. (pooling, optional) in case of weights/labels associated to\nthe nodes, these can also be aggregated according to the\npartitioning P.\nA detailed discussion of Algorithms 1 and 2 will be provided\nlater in Section 4.\nNode ordering.\nA key property of our k-MIS algorithm\n(similarly to the one of Blelloch, Fineman, and Shun (2012))\nis that it is deterministic: given a graph G and a ranking of its\nnodes π : V → {1, . . . , n}, that defines the position of the\nnodes in a given ordering, Algorithm 1 will always produce\nthe same k-MIS, for any k ≥ 0. This property has some\ninteresting consequences:\n• The ranking π can be used to lead Algorithm 1 to greedily\ninclude nodes having a higher rank under a given order of\nimportance, such as a centrality measure, a task-dependent\nrelevance, or a (possibly learned) scoring value. (Note that\nthe computation of the ranking can impact the complexity\nof the algorithm.)\n• If the ranking can be uniquely determined by the nodes\nthemselves (e.g., in function of their attributes or their\nneighbors), Algorithms 1 and 2 become injective and hence,\nPOOLING, MEAN\n(p = k + 1)\nOURS, MEAN\n(lexicographic)\nOURS, MEAN\n(intensity)\nOURS, STRIDED\n(intensity)\nk = 0\nk = 1\nk = 2\nk = 3\nk = 4\nFigure 1: (first column) Average pooling and (second to fourth\ncolumns) our method using different ranking and aggregation\nfunctions, for varying values of k.\npermutation invariant.1 This can be obtained by ranking\nthe nodes with respect to a score computed by means of a\n(sufficiently expressive) GCN, as learning injective functions over the nodes in a graph is a problem strictly related\nto the one of graph isomorphism, a topic that is gaining\na lot of traction in the graph learning community (Morris\net al. 2019; Xu et al. 2019; Maron et al. 2019; Loukas 2020;\nGeerts and Reutter 2021; Papp and Wattenhofer 2022).\n• A properly chosen ranking can produce a marginally\ngreater total score of the selected nodes with respect to\nthe one that we would get by greedily selecting the top\nscoring ones. This aspect will be discussed more in detail\nin Section 4.\nWe now provide two examples on how we can change the\nranking of the nodes to prioritize salient aspects according to\na specific preference. Examples are conducted on the graph\ndefined by the first sample of the MNIST dataset (LeCun,\nCortes, and Burges 2010), a 28 × 28 monochromatic image\n(first row of Fig. 1) where every pixel is connected to the\nones in the same pixel row, column or diagonal.\nFirst, we simulate the typical downsampling on images\n(also known as average pooling (Fukushima 1980)), where\nsquared partitions of p × p pixels are averaged together (first\n1Notice that, in our setting, if π : V → {1, . . . , n} is injective,\nthen it is also bijective and, hence, a permutation.\ncolumn of Fig. 1). To do this, we set the ranking π of Algorithm 2 as the lexicographic ordering: given (i, j) the coordinate of a pixel, we rank the nodes in decreasing order of\n28i + j. The resulting reduction is in the second column of\nFig. 1: averaging intensities of pixels in the same partition\nproduces a coarsened graph which is identical to classical\ndownsampling. Note that this result is partly due to the fact\nthat Algorithm 2 also makes use of π to define the clustering,\nsuch that the nodes in a partition have always a lower rank\nwith respect to the centroid in the same partition.\nSecondly, we rank nodes in decreasing order of intensity,\nthus prioritizing the pixels (i.e., the nodes) belonging to the\ndrawn digit. Here we show two different results: the first,\nwhere we average the lightness and coordinates of the nodes\nin the same clusters (third column of Fig. 1), and a second\none, where we just keep the ones belonging to the nodes in\nthe k-MIS (fourth column). We see that the reduced graphs\nindeed prioritized the digit against other pixels, producing a\ncoarsened representation where the digit is also remarkably\nrecognizable.",
        "theoretical analysis and results": "Regular downsampling.\nDownsampling plays a key role\nin Convolutional Neural Networks (CNNs, Goodfellow, Bengio, and Courville 2016), where it is adopted, for instance,\nin strided convolutions and pooling layers. In strided convolutions, an input tensor (e.g., a time series, an image, or a\nvoxel grid) is reduced by applying the convolved filter every\ns-th of its entries, along every dimension, while skipping\nthe others. In pooling layers, instead, tensors are reduced\nby summarizing every p-sided sub-tensors, taken at regular\nintervals. (More specific reductions are also possible, where\ndistinct intervals are used for every dimension.)\nWe can show that, on n-dimensional diagonal grid graphs\n(i.e., grids where nodes are also diagonally adjacent), Algorithms 1 and 2 behave exactly as the aforementioned downsampling strategies, if we rank their nodes by their position\nin lexicographic order. This is of particular interest as the adjacencies in these graphs can represent the receptive fields of\na single convolutional layer when applied to a some regular\ndata of the same shape, like images (2-dimensional) or voxel\ngrids (3-dimensional). Specifically, if G = (V, E) is a diagonal grid constructed using the entries of a given tensor as\nnodes, and π is the ranking of these entries in lexicographic\norder of their position indices, we have that\n1. k-MIS(G, V, π) selects the same entries of a strided convolution with s = k + 1,\n2. CLUSTER(G, k, π) partitions the tensor as a pooling layer\nwith p = k + 1, and\n3. the reduced graph obtained by contracting the resulting\npartition is again a diagonal grid of the same dimensions\nof their output tensor.\nA formal restatement and proof of these properties are\nprovided in the Supplementary Material, while in Fig. 1 we\nshow an example of the equivalence between pooling (first\ncolumn) and our reduction method (second column).\nConnectivity of the reduced graph.\nFor the sake of conciseness, hereafter we denote with (H, ρ) = R(G, k) the\nfunction reducing a graph G by contracting the clusters obtained with Algorithm 2, as described in Section 3. The term\nH = (S, E′) denotes the reduced graph, where S is the kMIS of G, while ρ : V → S is the function mapping every\nnode to the (exactly one) centroid in its cluster. The following\nresults are invariant with respect to the ranking parameter\nand the aggregation function used to reduce the edges or the\nnodes.\nWe follow a simple observation: for every edge in uv ∈ E′\nwith u ̸= v, the nodes u and v are within 2k + 1 hops in G,\nsince two nodes in S are connected in H only if an edge in\nG crosses their two clusters. This property, combined with\nthe lower bound implicitly defined by the k-MIS, yields the\nfollowing bounds.\nRemark 1. For any uv ∈ E(H) such that u ̸= v, we have\nthat k + 1 ≤ dG(u, v) ≤ 2k + 1.\nAn example of this property is shown in Fig. 2, where\nbounds in Remark 1 apply for the Minnesota road network (Davis and Hu 2011) reduced with different values\nof k. From the above observation, we can obtain the two\nfollowing properties.\nProposition 1. Let G be a connected graph and (H, ρ) =\nR\n\u0000\nG, k\n\u0001\n, with k ≥ 0. Then, ∀u, v ∈ V (G),\ndH(ρ(u), ρ(v)) ≤ dG(u, v) ≤ (2k+1) dH(ρ(u), ρ(v))+2k.\nCorollary 1. For any k ≥ 0, G and H = R\n\u0000\nG, k\n\u0001\nhave the\nsame number of connected components.\nThe full proofs are provided in the Supplementary Material. Both Proposition 1 and Corollary 1 are fundamental in\nour proposal of using k-MIS reduction as a pooling method\nin Graph Neural Networks. In particular: (i) differently from\nseveral other pooling techniques (Cangea et al. 2018; Gao\nand Ji 2019; Knyazev, Taylor, and Amer 2019; Lee, Lee, and\nKang 2019; Zhang et al. 2020; Ranjan, Sanyal, and Talukdar\n2020; Ma et al. 2020), we can guarantee that the input graph\nis not divided in multiple components, and that, if applied\nrepeatedly, our method will eventually produce a single representation node for the whole graph; (ii) when training with\nbatches of graphs at a time, our method guarantees also that\ndifferent graphs are not joined together.\nAlgorithm discussion and complexity.\nIn order to avoid\ncomputing the k-th graph power of a possibly large-scale\ngraph, Algorithm 1 modifies the one by Blelloch, Fineman,\nand Shun (2012, Algorithm 2, also restated in the Supplementary Material) to compute the k-MIS without explicitly generating every k-hop neighborhood. Given a graph G = (V, E),\na subset of its nodes U ⊆ V , and a (injective) node mapping\nπ : V → {1, . . . , n} (that we can consider as a ranking of\nthe nodes under a given permutation), Algorithm 1 works as\nfollows:\n1. if U ⊆ V is not empty, in Lines 3 to 7 we find the set of\nnodes S0 with minimum rank among their k-hop neighbors (i.e., their neighbors in Gk). This is done with k steps\nof label propagation such that, at each step, every node\nk = 0\nk = 1\nk = 2\nk = 4\nk = 8\nk + 1\n2k + 1\nFigure 2: Minnesota road network (Davis and Hu 2011) reduced with different values of k. For k = 0, the two bounds coincide,\nas the graph is not reduced at all. For k = 1, the real distance covered by an edge is polarized (is either 2 or 3). For greater values\nof k, the edges’ real distance span over all the range [k + 1, 2k + 1] ∩ N.\ntakes the minimum label found within their (1-hop) neighbors. We only propagate labels belonging to nodes still in\nU;\n2. in Lines 8 to 10 we remove from U all the nodes that are\nat most k-hops from a node in S0 (i.e., all their neighbors\nin Gk). This is also done with k steps of label propagation starting from the nodes in S0, where this time the\npropagated label is a flag signaling that the node shall be\nremoved;\n3. finally, the algorithm makes a recursive call in Line 11 using only the remaining nodes. The resulting set is merged\nwith S0 and returned.\nIt is easy to see that, if k = 1, steps 1 to 3 become exactly\nBlelloch’s algorithm, whereas by taking a general k every\nstep is extended to consider k-hop neighbors of G, thus efficiently emulating Blelloch’s algorithm on Gk.\nAs for complexity, Blelloch, Fineman, and Shun (2012)\npropose several trade-offs between work and depth on a\nconcurrent-read/concurrent-write PRAM model (CRCW,\nwith minimum priority concurrent write). Here, we consider\none version (Algorithm 2 from Blelloch, Fineman, and Shun\n(2012)) which allows an efficient parallel implementation\nwith O(m) work and O(log3 n) depth with high probability (see Blelloch, Fineman, and Shun 2012, Lemma 4.2), and\nmost closely resembles the structure of Algorithm 1. Our\nalgorithm introduces a factor k (compared to the one of Blelloch, Fineman, and Shun (2012)) on the operations performed\non lines Lines 3 to 7 and Lines 8 to 10 to compute the k-hop\nneighborhood. It follows that the work and depth of Algorithm 1 are bounded by k times that of Blelloch’s algorithm,\ni.e., O(k(n + m)) work and O(k log3 n) depth w.h.p., where\nan extra O(n) work is needed to generate the additional vector of labels, which is modified every k iterations. Regarding\nAlgorithm 2, after computing the k-MIS, the algorithm performs k steps of label propagation, which add O(k(n + m))\nwork and O(k log n) depth to the total computation. Total\nspace consumption is O(n + m), comprising input and O(1)\nlabel vectors of size O(n).\nProposition 2. Given a graph G, an integer k ∈ N,\nand a random ranking of the nodes π, both Algorithms 1\nand 2 can be implemented to run on a CRCW PRAM using O(k(n + m)) work, O(k log3 n) depth, and O(n + m)\nspace. The depth bound holds w.h.p.\nBounds on the total weight.\nIn any greedy MIS algorithm,\nwhenever we add a node to the independent set we have to\nremove all of its neighbors from the graph. Having observed\nthis, a typical heuristic to compute larger-weight independent\nsets is to select nodes with high weight and low degree (Caro\n1979; Wei 1981). Following this intuition, Sakai, Togasaki,\nand Yamazaki (2003) proposed the following rules: given\nx ∈ Rn\n+ a vector of positive weights associated to each\nnode, add to the independent set the node v maximizing either (i) xv/(deg(v) + 1), or (ii) xv/(P\nu∈N[v] xu). Both\nrules can be trivially extended to k-hop neighborhoods by\ncomputing Gk, which would however require O(n2) space,\nunless done sequentially. Parallel computation of the neighborhood function degk(v) = |Nk(v)| in limited space can\nbe achieved only by resorting to approximations, e.g. using\nMonte Carlo methods (Cohen 1997) or approximate sets representations (Palmer, Gibbons, and Faloutsos 2002; Boldi,\nRosa, and Vigna 2011), and still this would not extend to\napproximate rule (ii).\nTo overcome these limitations, we overestimate the sum\nof the weights in the k-hop neighborhood of each node,\nby computing instead ck = (A + I)kx ∈ Rn\n+, where\nA, I ∈ {0, 1}n×n are, respectively, the adjacency and the\nidentity matrices. The matrix (A + I)k ∈ Nn×n\n0\nrepresents\nthe number of k-walks (i.e., sequences of adjacent nodes\nof length k) from every pair of nodes in the graph. Clearly,\n[(A + I)k]uv ≥ 1 if v ∈ Nk[u], while the equality holds for\nevery pair of nodes for k = 1. When x = 1, ck is equal to\nthe k-path centrality (Sade 1989; Borgatti and Everett 2006).\nNotice that we do not need to compute (A+I)k explicitly, as\nck can be obtained with a sequence of k matrix-vector products, that can be computed in O(n + m) space, O(k(n + m))\nwork and O(k log n) depth.\nIn the following, we provide a generalization of the bounds\nof Sakai, Togasaki, and Yamazaki (2003) when a k-MIS is\ncomputed by Algorithm 1 with the ranking defined by rules\n(i)-(ii) approximated by the k-walk matrix ck. We remark that,\nfor k = 1, the following theorems are providing the same\nbounds as the one given by Sakai, Togasaki, and Yamazaki\n(2003). The full proofs can be found in the Supplementary\nMaterial.\nTheorem 1. Let G = (V, E) be a graph, with (unweighted)\nadjacency matrix A ∈ {0, 1}n×n and with x ∈ Rn\n+ representing a vector of positive node weights. Let k ∈ N be an\ninteger, then define w : V → R+ as\nw(v) =\nxv\n[(A + I)k1]v\n,\n(1)\nand πw as the ranking of the nodes in decreasing order of\nw. Then, k-MIS(G, V, πw) outputs a maximal k-independent\nset S such that P\nu∈S xu ≥ P\nv∈V w(v).\nTheorem 2. Let G = (V, E) be a graph, with (unweighted)\nadjacency matrix A ∈ {0, 1}n×n and with x ∈ Rn\n+ representing a vector of positive node weights. Let k ∈ N be an\ninteger, then define w : V → R+ as\nw(v) =\nxv\n[(A + I)kx]v\n,\n(2)\nand πw as the ranking of the nodes in decreasing order of\nw. Then, k-MIS(G, V, πw) outputs a maximal k-independent\nset S such that P\nu∈S xu ≥ P\nv∈V w(v) · xv.\nTheorem 3. Let G = (V, E) be a non-empty graph with\npositive node weights x ∈ Rn\n+, and let πw be a ranking\ndefined as in Theorem 1 or 2 for any given k ∈ N. Then\nP\nv∈S xv ≥ α(Gk)/∆k, where S = k-MIS(G, V, πw) and\n∆k = maxv∈V [(A + I)k1]v.\nRecalling that α(Gk) is the optimal solution, Theorem 3\nshows that our heuristics guarantee a ∆−1\nk\napproximation.\nThis bound degrades very quickly as the value of k increases,\nsince the number of k-walks may exceed the total number of\nnodes in the graph. In the Supplementary Material we show\nthat, in practice, the total weight produced by Algorithm 1\nis on par with respect to the one obtained using the exact\nneighborhood function for low values of k. This aspect is\nof practical value as in general the k values used for graph\npooling are on the low-end.",
        "related works": "Maximal k-Independent Sets.\nComputing a k-MIS can be\ntrivially done in (superlinear) polynomial time and space using matrix powers (Agnarsson, Damaschke, and Halld´orsson\n2003) and any greedy MIS algorithm (e.g., Luby 1985; Blelloch, Fineman, and Shun 2012). Koerts (2021) proposed a\nformulation of the problem both as an integer linear program and as a semi-definite program, but still relying on\nthe k-th power of the input graph. Several papers propose\nefficient algorithms to solve the maximum (weighted or unweighted) k-IS problem on specific classes of graphs (Agnarsson, Greenlaw, and Halld´orsson 2000; Agnarsson, Damaschke, and Halld´orsson 2003; Eto, Guo, and Miyano 2014;\nBhattacharya and Houle 1999; Duckworth and Zito 2003;\nPal and Bhattacharjee 1996; Hsiao, Tang, and Chang 1992;\nHota, Pal, and Pal 2001; Saha and Pal 2003), which fall beyond the scope of this article. To the best of our knowledge,\nthe only other parallel algorithm for computing a maximal\nk-independent set was proposed by Bell, Dalton, and Olson\n(2012) as a generalization of the one of Luby (1985) for\nk ≥ 1. This algorithm is essentially the same as Algorithms 1\nand 2, but without the ranking argument, making the algorithm non-deterministic, as the nodes are always extracted in\na random order.\nGraph Coarsening and Reduction.\nMISs (i.e., with k =\n1) were adopted as a first sampling step in Barnard and Simon\n(1994), although their final reduction step may not preserve\nthe connectivity of the graph. Using MIS was also suggested\nby Shuman, Faraji, and Vandergheynst (2016) as an alternative sampling step for their graph reduction method. The\nspectral reduction proposed by Loukas (2019, neighborhood\nvariant) does not use sampling as a first reduction step, but\nsequentially contracts node neighborhoods until a halting\ncondition is reached, performing similar steps to the classical\ngreedy algorithm for maximum-weight independent sets.\nGraph Pooling.\nIn a contemporary and independent work,\nStanovic, Ga¨uz`ere, and Brun (2022) introduced a pooling\nmechanism based on maximal independent (vertex) sets,\nnamed MIVSPool. Their method is analogous to ours, but\nrestricted to the case of 1-MIS, that they compute using the\nparallel algorithm of Meer (1989). Another related model is\nEDGEPOOL (Diehl et al. 2019), which computes instead a\nmaximal matching, i.e., a maximal independent set of edges,\nselecting the edges depending on a learned scoring function.\nNouranizadeh et al. (2021) also proposed a pooling method\nthat constructs an independent set maximizing the mutual\ninformation between the original and the reduced graph. To\ndo so, the authors leverage on a sequential algorithm with\ncubic time complexity and also no guarantees that the resulting set is maximal. Apart from a few other cases (Dhillon,\nGuan, and Kulis 2007; Luzhnica, Day, and Lio’ 2019; Ma\net al. 2019; Wang et al. 2020; Bacciu and Di Sotto 2019;\nBacciu et al. 2021; Bianchi et al. 2020), pooling in Graph\nNeural Networks (GNNs) usually entails an adaptive approach, typically realized by means of another neural network. These pooling methods can be divided in two types:\ndense and sparse. Dense methods, such as DIFFPOOL (Ying\net al. 2018), MINCUTPOOL (Bianchi, Grattarola, and Alippi\n2020; Bianchi et al. 2020), MEMPOOL (Khasahmadi et al.\n2019), STRUCTPOOL (Yuan and Ji 2019), and DMON (Tsitsulin et al. 2022), compute for each node a soft-assignment\nto a fixed number of clusters defined by a reduction factor\nr ∈ (0, 1), thus generating a matrix requiring O(rn2) space.\nSparse methods, such as GPOOL/TOPKPOOL (Gao and Ji\n2019; Cangea et al. 2018), SAGPOOL (Lee, Lee, and Kang\n2019; Knyazev, Taylor, and Amer 2019), GSAPOOL (Zhang\net al. 2020), ASAPOOL (Ranjan, Sanyal, and Talukdar 2020),\nPANPOOL (Ma et al. 2020), IPOOL (Gao et al. 2021), and\nTAGPOOL (Gao, Liu, and Ji 2021), instead, compute a score\nfor each node (requiring O(n) space), and reduce the graph\nby keeping only the top ⌈rn⌉ scoring ones and dropping the\nrest. Although scalable, these methods provide no theoretical guarantees regarding the preservation of connectivity\nof the reduced graph, as the n − ⌈rn⌉ dropped nodes may\ndisconnect the graph.",
        "experimental analysis": "Table 1 summarizes the average classification accuracy\nobtained on selected classification benchmarks using the\nsame underlying Graph Neural Networks (GNNs) and different kinds of pooling mechanisms. For classification\ntasks, we chose those datasets having the highest numModel\nDD\nREDDIT-B\nREDDIT-5K\nREDDIT-12K\nGITHUB\nBASELINE\n75.51 ± 1.07\n78.40 ± 8.68\n48.32 ± 2.38\n45.04 ± 6.63\n69.89 ± 0.28\nBDO\n76.69 ± 1.79\n85.63 ± 1.43\n45.95 ± 5.49\n41.89 ± 7.14\n65.64 ± 0.90\nGRACLUS\n75.17 ± 2.11\n84.05 ± 5.81\n43.22 ± 12.24\n43.08 ± 9.32\n67.64 ± 0.57\nEDGEPOOL\n74.70 ± 1.57\n85.98 ± 1.57\n52.44 ± 1.11\n47.58 ± 0.78\n68.72 ± 0.52\nTOPKPOOL\n74.92 ± 2.03\n81.10 ± 3.82\n45.28 ± 3.88\n38.55 ± 2.35\n65.93 ± 0.45\nSAGPOOL\n73.26 ± 2.26\n84.90 ± 3.94\n46.29 ± 5.61\n42.30 ± 3.70\n64.29 ± 5.70\nASAPOOL\n73.73 ± 2.18\n78.37 ± 5.22\n39.53 ± 7.76\n39.14 ± 3.58\n66.98 ± 0.96\nPANPOOL\n73.26 ± 1.94\n77.44 ± 4.95\n46.04 ± 3.78\n40.97 ± 3.02\n62.48 ± 2.84\nk-MIS (strided)\n76.44 ± 1.50\n86.32 ± 1.90\n54.30 ± 0.53\n46.06 ± 0.58\n67.87 ± 0.48\nk-MIS (max-pool)\n76.91 ± 1.06\n87.57 ± 1.96\n53.44 ± 1.52\n47.51 ± 0.99\n68.24 ± 0.94\nk-MIS (mean-pool)\n73.56 ± 1.19\n86.98 ± 1.13\n54.00 ± 0.94\n46.73 ± 0.85\n68.60 ± 0.67\nTable 1: Classification accuracy on selected benchmark datasets (mean ± std)\nber of nodes from in the TUDataset (Morris et al. 2020)\ncollection (i.e., DD (Dobson and Doig 2003), GITHUBSTARGAZERS (Rozemberczki, Kiss, and Sarkar 2020),\nREDDIT-BINARY and -MULTI-5K/12K (Yanardag and\nVishwanathan 2015)), where pooling layers may prove more\nuseful. All datasets were divided in training (70%), validation (10%), and test (20%) sets using a randomized stratified\nsplit with fixed seed. All models have the same general architecture: 3 GNNs (optionally) interleaved by 2 layers of\npooling, a global pooling method (sum and max), and a final\nMLP with dropout (Srivastava et al. 2014) as classifier. All\nmodels were trained using Adam optimizer (Kingma and Ba\n2017). We performed a model selection using the training\nand validation split, and then we computed the average test\nset classification accuracy obtained by the best configuration,\non 10 inference runs using different seed values. The hyperparameter concerning the reduction factor (k in our case, or r\nfor other methods) has been chosen among the other parameters during the model selection phase. All models have been\nimplemented and trained using PyTorch (Paszke et al. 2019)\nand PyTorch Geometric (Fey and Lenssen 2019). A detailed\ndescription of the datasets, models, and experimental settings\nare provided in the Supplementary Material, together with\nadditional experiments regarding the controlled scaling and\nefficiency of our method, showing that it can reduce graphs\nwith over 100 million edges in less than 10 seconds.\nWe compared our reduction method against different kinds\nof pooling layers readily available on the PyG library (we\navoided DIFFPOOL-like dense methods as they do not scale\nwell on the selected datasets), and also against our own\nmethod using random rankings of the nodes, as done in the\naggregation scheme of Bell, Dalton, and Olson (BDO, 2012).\nThis method, along with the baseline (with no pooling) and\nGRACLUS, are the only compared architectures that require\nno additional parameters. For our method, the node scoring\nfunction is computed by means of a sigmoid-activated linear\nlayer having as input the features of the nodes. As in the\nother parametric methods, the feature vectors of the nodes\nare multiplied by the final score beforehand, to make the\nscoring function end-to-end trainable. The computed scores\nconstitute the node weights and, as described in Section 4,\nthe resulting ranking is obtained according to Eq. (2). The\nreduction is performed in two settings: strided, in which we\nperform no aggregation, and pool, in which we aggregate the\nfeature vectors in each partition using, respectively, max and\nmean aggregations.\nLooking at Table 1 (where the top two accuracy scores for\neach dataset are in boldface) it is immediately evident how\nthe proposed k-MIS-based approaches obtain consistently\nhigh accuracy, suggesting that the evenly-spaced centroid\nselection is indeed able to capture essential properties of each\ngraph. On the other hand, the random permutation variant\nof Bell, Dalton, and Olson (2012), seem to overall perform\nworse than the other k-MIS-based strategies, while still obtaining a considerable result on DD and REDDIT-B. This\nsuggests that exploiting the ranking function of Algorithm 1\nto select relevant nodes is indeed able to improve the representativeness of the downsampled graph. It is also particularly\nnoteworthy how one of the best performing model, EDGEPOOL, is also the only other parametric pooling method to\npreserve the connectivity of the graph, as its reduction step\nconsists of contracting a maximal matching. This highlights\nthe importance of preserving the connectivity of the network\nwhen performing pooling in GNNs, while also suggesting\nthat evenly-spaced reductions can benefit graph representation learning tasks. Finally, we observe a remarkable performance of the baseline algorithm (no pooling) on the GITHUB\ndataset: we may speculate that the graphs are simple enough\nto not require pooling, yet at the same time k-MIS approaches\nobtains competitive accuracy, suggesting it is a reliable and\nversatile choice.",
        "conclusion": "We introduced a new general graph coarsening approach that\naims to preserve fundamental topological properties of the\noriginal graph, acting like a structured counterpart of downsampling methods for regular data. The coarsening reduction\ncan be regulated by the parameter k, going from the original\ngraph, when k = 0, to up to a single node as k approaches\nthe graph’s diameter, shrinking graphs uniformly as pairwise\ndistances maintain a stretch controlled by k. Furthermore, we\nshowed how this parameter generalizes to the pooling and\nstride intervals when applied to diagonal grid graphs.\nThe algorithm is designed to provide such guarantees while\nat the same time allowing a scalable parallel implementation,\nwhich processes graphs with up to 100 million edges in just\na few seconds on a single GPU.\nThe empirical analysis provided evidence of effectiveness\nof our k-MIS pooling in several graph classification benchmarks, showing superior performance with respect to related\nparametric and non-parametric methods from the literature.\nThis approach fills a methodological gap between reduction techniques for structured data and their rigorous counterparts on regular data. Given its generality and scalability, it\nhas potential of positively impacting a plethora of computationally-intense applications for large scale networks, such\nas graph visualization, 3D mesh simplification, and classification.",
        "summary_en": "Downsampling produces coarsened, multi-resolution representations of data and it is used, for example, to produce lossy compression and visualization of large images, reduce computational costs, and boost deep neural representation learning. Unfortunately, due to their lack of a regular structure, there is still no consensus on how downsampling should apply to graphs and linked data. Indeed reductions in graph data are still needed for the goals described above, but reduction mechanisms do not have the same focus on preserving topological structures and properties, while allowing for resolution-tuning, as is the case in regular data downsampling. Therefor,this paper takes a step in this direction, introducing a unifying interpretation of downsampling in regular and graph data. In particular,the paper defines a graph coarsening mechanism which is a graph-structured counterpart of controllable equispaced coarsening mechanisms in regular data. The paper proves theoretical guarantees for distortion bounds on path lengths, as well as the ability to preserve key topological properties in the coarsened graphs. The paper leverages these concepts to define a graph pooling mechanism that the paper empirically assesses in graph classification tasks, providing a greedy algorithm that allows efficient parallel implementation on GPUs, and showing that it compares favorably against pooling methods in literature.",
        "summary_zh": "这篇论文研究了如何在图数据中应用下采样的问题。由于图和链接数据缺乏规则结构，人们对如何对图和链接数据进行下采样仍未达成共识。因此，本文引入了对常规数据和图数据下采样的统一解释。作者定义了一种图粗化机制，并证明了其路径长度失真界限的理论保证，以及在粗化图中保留关键拓扑特性的能力。最后，作者定义了一个图池化机制，在图分类任务中对其进行了实证评估，并提供了一种允许在GPU上高效并行执行的贪婪算法。结果表明，该算法优于文献中的池化方法。"
    },
    {
        "title": "Tight Performance Guarantees of Imitator Policies with Continuous Actions",
        "abstract": "Behavioral Cloning (BC) aims at learning a policy that mimics the behavior demonstrated by an expert. The current theoretical understanding of BC is limited to the case of finite actions. In this paper, we study BC with the goal of providing theoretical guarantees on the performance of the imitator policy in the case of continuous actions. We start by deriving a novel bound on the performance gap based on Wasserstein distance, applicable for continuous-action experts, holding under the assumption that the value function is Lipschitz continuous. Since this latter condition is hardy fulfilled in practice, even for Lipschitz Markov Decision Processes and policies, we propose a relaxed setting, proving that value function is always H¨older continuous. This result is of independent interest and allows obtaining in BC a general bound for the performance of the imitator policy. Finally, we analyze noise injection, a common practice in which the expert’s action is executed in the environment after the application of a noise kernel. We show that this practice allows deriving stronger performance guarantees, at the price of a bias due to the noise addition.",
        "introduction": "The degree of interaction of the human in the ecosystem\nof artificial intelligence is progressively becoming more and\nmore prominent (Zanzotto 2019). In this setting, the human\nplays the role of an expert that, with different tools, interacts\nwith the artificial agents and allows the agent to leverage\ntheir knowledge to improve, quicken, and make the learning\nprocess more effective (Jeon, Milli, and Dragan 2020).\nImitation Learning (IL, Osa et al. 2018) can be considered one of the simplest forms of interaction between a human and an artificial agent. This kind of interaction is unidirectional since the human expert provides the agent with\na set of demonstrations of behavior that is optimal w.r.t. an\nunknown objective. The agent, on its part, aims to learn a\nbehavior as close as possible to the demonstrated one. Classically, we distinguish between two realizations of IL: Behavioral Cloning (BC, Bain and Sammut 1995) and Inverse\nReinforcement Learning (IRL, Arora and Doshi 2021). BC\naims at mimicking the behavior of the agent by recovering a policy that matches as much as possible the expert’s\ndemonstrated behavior. Instead, IRL has the more ambitious\ngoal of reconstructing a reward function that justifies the expert’s behavior. Thus, it aims at representing the expert’s intent rather than their behavior. In this sense, IRL is more\nchallenging than BC, as its output, the reward function, is a\nmore powerful tool that succeeds in being deployed even in\nthe presence of a modification of the environment.\nAlthough IL techniques have been successfully applied to\na large variety of real-world applications (e.g., Asfour et al.\n2008; Geng, Lee, and H¨ulse 2011; Rozo, Jim´enez, and Torras 2013; Likmeta et al. 2021), their theoretical understanding in terms of performance of the imitation policy is currently limited. Recently, in (Xu, Li, and Yu 2020), a first\nanalysis of the error bounds has been provided for BC and\nGenerative Adversarial Imitation Learning (Ho and Ermon\n2016). However, these results involve the presence of an fdivergence (R´enyi et al. 1961), usually total variation (TV)\nor KL-divergence, between the expert’s policy and the imitator one. Consequently, they are significant only when the action space is finite, while becoming vacuous for experts with\ncontinuous actions. To further argue on the limitation of this\nanalysis, consider the case in which BC is reduced to minimize the mean squared error (MSE) between the expert’s\naction and the imitator one. Even in this simple scenario, as\nwe shall see, the current analysis based on TV cannot relate\nMSE with the performance of the imitator policy. This represents a relevant limitation since many of the applications\nof IL are naturally defined with continuous-actions context.\nOriginal Contributions\nIn this paper, we aim to take a\nstep forward to a more comprehensive theoretical understanding of BC. Specifically, we devise error bounds that\nrelate the performance difference JπE ´JπI between the expert’s policy πE and the imitator one πI to their divergence.\nOur bounds are based on the Wasserstein distance (Villani\n2009) and, for this reason, are meaningful even in the presence of continuous-action spaces (Section 3). Our work contains the following contributions:\n1. We will prove a performance bound for standard BC\nin case of Lipschitz reward-transition for the MDP (see\n(Rachelson and Lagoudakis 2010)) and Lipschitz continuity of the value function.\n2. Since the latter assumption is often violated in practice,1,\n1It is well-known that the value function is Lipschitz continuous\nwe extend the result by only requiring Lipschitzness of\nthe MDP, even if it requires a weaker performance bound.\nWe also show that, the less regularity on the value function, the slower the convergence of BC (Section 4).\n3. Finally, we focus on a popular practice employed in imitation learning, i.e., noise injection (Laskey et al. 2017a).\nIn this setting, the expert’s action, before being executed\nin the environment, is corrupted with noise to make the\nimitation process more robust. We show that noise injection allows achieving stronger theoretical guarantees\nat the price of competing against a noisy expert, which\ncould have a lower performance (Section 5).\nIn particular, in the second point, we show that the value\nfunction of a Lipschitz MDP is always H¨older continuous,\nwith a suitable choice of the exponent depending on the\nproperties of the MDP and policy. This represents a result of\nindependent interest that overcomes a well-known limitation\nof the Lipschitz continuity of the value function (Rachelson\nand Lagoudakis 2010; Pirotta, Restelli, and Bascetta 2015),\nwith possible applications outside BC.",
        "preliminaries": "In this section, we provide the background (Section 2.1) and\nthe foundations of Markov Decision Processes (Section 2.2).\n2.1\nMathematical Background\nNotation Let X be a set and F be a σ-algebra over X, we\ndenote with PpXq the set of probability measures over the\nmeasurable space pX, Fq. Let x P X, we denote the Dirac\ndelta measure centered in x as δx. Let f : X Ñ R be a\nfunction, we denote the L8-norm as }f}8 “ supxPX fpxq\nand with }f}i the Li-norm for i P t1, 2u.\nLipschitz Continuity Let pX, dX q and pY, dYq be two\nmetric spaces and L ą 0. A function f : X Ñ Y is said to\nbe L-Lipschitz continuous (L-LC) if:\ndYpfpxq, fpx1qq ď LdX px, x1q,\n@x, x1 P X.\nWe denote the Lipschitz semi-norm of function f as }f}L “\nsupx,x1PX,x‰x1 dYpfpxq, fpx1qq{dX px, x1q. In the real space\n(X Ď Rn), we use the Euclidean distance, i.e., dX px, x1q “\n}x ´ x1}2. For probability measures (X “ PpΩq), the most\nintuitive distance is the total variation (TV), defined as:\nTVpµ, νq “\nsup\n}f}8ď1\nˇˇˇˇ\nż\nΩ\nfpωq pµ ´ νq pdωq\nˇˇˇˇ @µ, ν P PpΩq\nHowever, with continuous deterministic distributions, the\nTV takes its maximum value 1 (Figure 1). Thus, we introduce L1-Wasserstein distance (Villani 2009), defined as:\nWpµ, νq “\nsup\n}f}Lď1\nˇˇˇˇ\nż\nΩ\nfpωqpµ ´ νqpdωq\nˇˇˇˇ\n@µ, ν P PpΩq\nIt is worth noting that, for deterministic distributions, we\nhave Wpδx, δx1q “ dX px, x1q.\nunder the demanding assumption that γLpp1 ` Lπq ă 1 (Rachelson and Lagoudakis 2010) requiring the Lipschitz constants of the\ntransition model Lp and of the policy Lπ to be very small.\nH¨older Continuity The notion of Lipschitz continuity is\ngeneralized by H¨older continuity. Let pX, dX q and pY, dYq\nbe two metric spaces and L, α ą 0. A function f : X Ñ Y\nis said to be pα, Lq-H¨older continuous (pα, Lq-HC) if:\ndYpfpxq, fpx1qq ď LdX px, x1qα,\n@x, x1 P X.\nIt is worth noting that: (i) Lipschitz continuity is obtained by\nH¨older continuity for α “ 1; (ii) only constant functions are\npα, Lq´HC for α ą 1; (iii) in bounded domains, the higher\nthe value of α the more restrictive the condition.\nConvolution Let f, g : Rn Ñ R be two functions, their\nconvolution is defined for all x P Rn as:\npf ˚ gqpxq :“\nż\nRn fpx ´ yqgpyqdy “\nż\nRn fpyqgpx ´ yqdy.\nWe introduce the following regularity assumption regarding\nthe probability measures.\nDefinition 1. A probability measure L P PpRnq is L-TVLipschitz continuous (L-TV-LC) if:\nTVpLp¨ ` hq, Lp¨qq ď L}h}2,\n@h P Rn.\nUnder this assumption, we can prove that the convolution\nregularizes bounded and possibly irregular functions.\nProposition 1. Let f : Rn Ñ R be a function such that\n}f}8 ď M, and let L P PpRnq be an L-TV-LC probability\nmeasure that admits density function ℓ : Rn Ñ Rě0. Then,\nthe convolution f ˚ ℓ is 2LM-LC continuous.\n2.2\nMarkov Decision Processes\nA discrete-time discounted Markov Decision Process (MDP,\nPuterman 2014) is a 6-tuple M “ pS, A, p, r, γ, µq where\nS and A are the measurable sets of states and actions,\np : S ˆ A Ñ PpSq is the transition model that defines the\nprobability measure pp¨|s, aq of the next state when playing\naction a P A in state s P S, r : S ˆ A Ñ R is the reward function defining the reward rps, aq upon playing action a P A in state s P S, γ P r0, 1q is the discount factor,\nand µ P PpSq is the initial-state distribution. The agent’s\nbehavior is modeled by a policy π : S Ñ PpAq, which assigns a probability measure πp¨|sq of the action to be taken\nin state s P S. When the policy is deterministic, we denote\nwith πpsq the action played in state s P S. A policy determines a γ-discounted visitation distribution, defined as:\ndπpsq :“ p1 ´ γq ř`8\nValue Functionst“0 γt Ppst “ s|π, µq for every s P S.\nThe state-action value function (or Qfunction) which quantifies the expected discounted sum of\nthe rewards obtained under a policy π, starting from a state\ns P S and fixing the first action a P A:\nQπps, aq :“ Eπ\n«`8\nÿ\nt“0\nγtrpst, atq\nˇˇˇˇs0 “ s, a0 “ a\nff\n,\n(1)\nwhere Eπ denotes the expectation w.r.t. to the stochastic process at „ πp¨|stq and st`1 „ pp¨|st, atq for all t P N. The\nstate value function (or V-function) is defined as V πpsq :“\nEa„πp¨|sqrQπps, aqs, for all s P S. Given an initial state distribution µ, the expected return is defined as:\nJπ :“ E\ns„µrV πpsqs “\n1\n1 ´ γ\nE\ns„dπ,a„πp¨|sqrrps, aqs.\n−4\n−2\n0\n2\n4\n0\n0.2\n0.4\nx\npdf(x)\n−4\n−2\n0\n2\n4\n0\n2\n4\nx\n−4\n−2\n0\n2\n4\n0\n2\n4\nx\nFigure 1: Comparison between TV and Wasserstein distances for two Gaussian distributions µ and ν. Left:\nTVpµ, νq « 0.38, Wpµ, νq “ 1, Center: TVpµ, νq « 1,\nWpµ, νq “ 1, Right: TVpµ, νq « 1, Wpµ, νq “ 0.4\nLipschitz MDPs\nWe now introduce notions that will allow\nus to characterize the smoothness of an MDP (Rachelson\nand Lagoudakis 2010). To this end, we assume that the state\nspace S and action space A are metric spaces endowed with\nthe corresponding distance functions dS and dA.\nAssumption 1 (Lipschitz MDP). An MDP M is pLp, LrqLC if, for all ps, aq, ps1, a1q P S ˆ A it holds that:\nWppp¨|s, aq, pp¨|s1, a1qq ď Lp\n`\ndSps, s1q ` dApa, a1q\n˘\n,\nˇˇrps, aq ´ rps1, a1q\nˇˇ ď Lr\n`\ndSps, s1q ` dApa, a1q\n˘\n.\nAssumption 2 (Lipschitz Policy). A (Markovian stationary)\npolicy π is Lπ-LC if, for all s, s1 P S it holds that:\nWpπp¨|sq, πp¨|s1qq ď LπdSps, s1q.\nNote, if instead of the Wasserstein metric, we had used the\nTV, these assumptions would be way more restrictive, not\nholding for deterministic environment/policies with continuous state-action spaces (Munos and Szepesv´ari 2008). Under\nAssumptions 1 and 2, provided that γLpp1 ` Lπq ă 1, the\nQ-function Qπ is LQ-LC with LQ ď\nLr\n1´γLpp1`Lπq (Rachelson and Lagoudakis 2010, Theorem 1).",
        "bound for imitating policies based on wasserstein distance": "The high-level goal of this work is to find a theoretical\nguarantee for the imitator policies learned with BC. Specifically, we want to bound the difference in expected return\nJπE ´ JπI between the imitator policy πI learned with BC\nand the expert policy πE in terms of a distributional divergence between the corresponding action distributions.\nThe best-known results for this kind of analysis, in the\ncase of discrete action spaces, are proved in (Xu, Li, and Yu\n2020) and we report it below for completeness.2\nTheorem 2 (Xu, Li, and Yu (2020), Theorem 1). Let πE be\nthe expert policy and πI be the imitator policy. If |rps, aq| ď\nRmax for all ps, aq P S ˆ A, it holds that:\nJπE ´ JπI ď 2Rmax\np1 ´ γq2\nE\ns„dπErTVpπEp¨|sq, πIp¨|sqqs.\n2The result reported in (Xu, Li, and Yu 2020) involves the KLdivergence and is obtained, via Pinsker’s inequality, from the one\nwe report that is tighter (Appendix A.2 of Xu, Li, and Yu (2020)).\nAs anticipated, this result is not suitable for continuous\naction spaces, since the TV between different policies would\ntake its maximum value 1 whenever one of the two policies\nis deterministic. The following example clarifies the issue.\nExample 1. Suppose that the action space is a real space\nA Ď Rn and that both expert πE and the imitator πI policies are deterministic. A common way to perform BC is to\nminimize the mean squared error (MSE) between the expert’s\naction and the imitator one. Suppose we are able to provide\nthe following guarantee on the MSE, for some ε ą 0:\nE\ns„dE\n”\n}πEpsq ´ πIpsq}2\n2\nı\nď ε2.\n(2)\nHowever, this condition provides no guarantee in TV. Indeed, by taking πIpsq “ πEpsq `\nε\n?n1n, being 1n the\nvector of all 1s, Equation (2) is fulfilled, but we obtain:\nEs„dπE rTVpπEp¨|sq, πIp¨|sqqs\n“\nEs„dπE r1tπEpsq\n‰\nπIpsqus “ 1, where 1 is the indicator function.\n3.1\nA Bound Based on Wasserstein Distance\nEven if the existing analysis of Xu, Li, and Yu (2020) cannot\nbe applied in continuous action spaces, as shown in Example 1, it is not hard to leverage the regularity of the MDP to\neffectively bound the performance difference JπE ´ JπI.\nTheorem 3. Let πE be the expert policy and πI be the imitator policy. If that state-action value function QπI of the\nimitator policy πI is LQπI -LC, then it holds that:\nJπE ´ JπI ď LQπI\n1 ´ γ\nE\ns„dπErWpπIp¨|sq, πEp¨|sqqs.\nProof. Using the performance difference lemma (Kakade\nand Langford 2002), we have:\nJπE ´ JπI “\n1\n1 ´ γ\nE\ns„dπE\n„\nE\na„πEp¨|sqrAπIps, aqs\nȷ\n,\nwhere AπIps, aq “ QπIps, aq ´ V πIpsq is the advantage\nfunction. The inner expectation can be written as:\nE\na„πEp¨|sqrAπIps, aqs\n“\nż\nA\nQπIps, aqpπEpda|sq ´ πIpda|sqq\nď sup\nsPS\n}QπIps, ¨q}L WpπEp¨|sq, πIp¨|sqq,\nwhere the inequality follows from the definition of Wasserstein metric. The result is obtained by observing that\nsupsPS }QπIps, ¨q}L ď }QπI}L ď LQπI .\nA similar bound was previously derived by (Pirotta,\nRestelli, and Bascetta 2015, Theorem 1) and (Asadi, Misra,\nand Littman 2018, Theorem 2). However, (Pirotta, Restelli,\nand Bascetta 2015) assume that the policy is LC w.r.t. a\npolicy parametrization. Instead, the result of (Asadi, Misra,\nand Littman 2018) involves the transition model instead of\nthe policy and requires a bound uniform over S ˆ A on\nthe Wasserstein distance between the true and the estimated\nmodels. Let us now revisit Example 1 in light of Theorem 3.\n−1\n−0.5\n0\n0.5\n1\n−10\n0\n10\nState s\nValue function V π\nValue function V π\nLipschitz upper bound\n−1\n−0.5\n0\n0.5\n1\n−10\n0\n10\nState s\nValue function V π\nH¨older upper bound α = 0.01\nH¨older upper bound α = 0.715\nFigure 2: State value functions of Example 2. Left: the\nbound of (Rachelson and Lagoudakis 2010) hold and it is\ntight. Right: the bound of (Rachelson and Lagoudakis 2010)\ndoes not hold, but our bound based on H¨older continuity\nholds, for different values of α P p0, 1q.\nExample 1 (continued). Under Equation (2) , we can provide an effective guarantee on the Wasserstein distance:\nE\ns„dπErWpπEp¨|sq, πIp¨|sqqs “\nE\ns„dπEr}πEpsq ´ πIpsq}1s\nď Es„dπE r}πEpsq ´ πIpsq}2\n2s\n1\n2 ď ε,\nwhere in the first inequality, we used Jensen’s inequality.\nComparing Theorem 3 with Theorem 2, we no longer require the uniform bound Rmax on the reward function, but\nwe introduce an additional assumption on the regularity of\nthe imitator Q-function QπI. Clearly, we should find suitable assumptions under which LQπI is finite. As we anticipated in Section 2.2, the only known result that provides\nsuch an estimate under the assumption of Lipschitz MDP\nand Lipschitz policy with Wasserstein metric is (Rachelson and Lagoudakis 2010), where the authors proved that,\nif γLpp1 ` Lπq ă 1 is satisfied, LQπ can be chosen as:\nLQπ :“\nLr\n1 ´ γLpp1 ` Lπq.\n(3)\nHowever, we argue that condition γLpp1 ` Lπq ă 1 is very\ndemanding and often unrealistic. Indeed, to fulfill it we need\nat least one of these conditions to be satisfied:\n(i) γ ! 1: in practice, it is almost always false, since the\ndiscount factor is often chosen to be close to 1;\n(ii) Lp ă 1: this is a very unrealistic assumption, since it\nwould make all the states shrink exponentially when the\nsame actions are performed;\n(iii) Lπ « 0: the action depends very little on the state so\nthat there is a very limited possibility of controlling the\nenvironment (this condition alone is not even sufficient).\n3.2\nThe Tightness of the Value Function Lipschitz\nConstant\nIt is legitimate to question whether the value LQπ of Equation (3), widely employed in the literature (e.g., Rachelson\nand Lagoudakis 2010; Pirotta, Restelli, and Bascetta 2015;\nAsadi, Misra, and Littman 2018), is a tight approximation\nof the Lipschitz semi-norm }Qπ}L. In this section, we prove\nthat the result cannot be improved, at least when requiring\nthe Lipschitz continuity of the value function. Example 2\nshows that the value function Qπ can be made non-LC even\nwhen the MDP and the policy are LC, while Theorem 4\nproves that a bound like that of Theorem 3 cannot be obtained for a generic Lipschitz MDPs and policies.\nExample 2. Let M be an MDP and π be a policy defined\nas follows, given the constants Lp, Lr ą 0:\n• S “ r´1, 1s;\n• A “ t0u;\n• The dynamic is deterministic. From every state s P S,\nperforming action 0, the only possible, the environment\nmoves to the state s1 “ clippLps, ´1, 1q.3 This means that\nppds1|s, aq “ δclippLps,´1,1qpds1q;\n• rps, aq “ Lrs;\n• The initial state distribution is µ “ Unipr0, 1sq (not influential for the derivation that follows).\nThis MDP is pLp, Lrq-LC and the policy has Lipschitz constant equal to Lπ “ 0, since there is one action only. Equation (3) ensures that the state value function V π (that is\nequal to the state-action value function Qπ since there is\none action only) is LC with constant:\nLV π “\nLr\n1 ´ γLp\n.\nSince the state space is one dimensional, we can compute\nthe state value function V π exactly:\nV πpsq “ Lr\n`8\nÿ\nk“0\nγk clip\n`\nLk\nps, ´1, 1\n˘\n,\n@s P S.\nAs shown in Figure 2 left, the point of maximal slope s “ 0.\nEven if we have employed the specific values Lp “ 1.15,\nLr “ 1, and γ “ 0.75, it is simple to see that this property\nis valid in general. Moreover, we have plotted in orange the\nline which passes through the origin, having slope equal to:\nLV π “\nLr\n1 ´ γLp\n“\n1\n1 ´ 0.75 ¨ 1.15 « 7.27,\nwhich is the tangent line to the state value function in s “ 0,\nas it also can be found analytically:\nBV π\nBs p0q “ Lr\n`8\nÿ\nk“0\nγkLk\np “\nLr\n1 ´ γLp\n.\nThis means that, in this case, the choice of the Lipschitz constant provided by the theory (Equation 3) is actually tight.\nWhat happens if we reach the hard edge of γLpp1`Lπq “\nγLp ą 1, where Equation (3) does not guarantee any property? For instance, by taking Lp “ 1.15, Lr “ 1, and\nγ “ 0.9, we lose any Lipschitz property, finding a derivative which is unbounded, as shown in Figure 2 right.\nNote that, in this example, we are able to find a non-LC\nstate value function even in the apparently simple case of\nA “ t0u, where Lπ “ 0. Therefore, this example also\nshows that the dynamics of the system alone is enough to\nmake the state value function irregular. Furthermore, the\nsame example can be adapted to prove that, for a generic\nLipschitz MDP and a pair of Lipschitz policies, a bound like\nthe one of Theorem 3 cannot be obtained in general.\n3clippx, a, bq is the clipping function, i.e., maxtmintx, bu, au.\nTheorem 4. There exist an pLp, Lrq-LC MDP and an LπLC policy π such that for every finite constant C ą 0, (even\ndepending on Lp, Lπ, and Lr), there exists an Lπ-LC policy\nπ1 such that:\nJπ ´ Jπ1 ě C\nE\ns„dπrWpπp¨|sq, π1p¨|sqqs.\nThe proof is reported in Appendix. If we set π “ πE\nas the expert policy and π1 “ πI as an imitator policy,\nTheorem 4 shows that, even if the MDP and the policies\nare LC, we cannot, in general, upper bound the performance difference JπE ´ JπI with the expected Wasserstein distance Es„dπrWpπEp¨|sq, πIp¨|sqqs. This is in line\nwith the fact that, without additional assumptions, e.g., when\nγLppLπ ` 1q ă 1 does not hold, Theorem 4 is vacuous.\nTherefore, these bounds cannot be improved in the framework of Lipschitz continuity, however, a weaker notion of\nregularity can be used to generalize the previous theorems.",
        "h¨older continuity is all we need": "In this section, we propose an approach for overcoming the\nlimitations of the Lipschitz continuity, discussed in the previous section. In Section 4.1, we show that the state-action\nvalue function Qπ is always H¨older continuous, provided\nthat the MDP and the policy are LC. Then, in Section 4.2,\nwe apply these findings to BC, deriving a bound on the performance difference JπE ´ JπI in terms of the Wasserstein\ndistance that holds for every LC MDP and policy.\n4.1\nThe H¨older Continuity of the Value Function\nThe first step to improve the result of (Rachelson and\nLagoudakis 2010) is to observe that, like in Example 2, even\nwhen the value function is not Lipschitz continuous, it keeps\nbeing continuous. This observation is not, in principle, accounted for by the previous analysis, which provides no result when γLpp1 ` Lπq ą 1. This suggests that employing a notion of regularity that is stronger than continuity\nbut weaker than Lipschitz continuity, as H¨older continuity,\nmight lead to an improvement of the analysis. Indeed, we\nare able to prove the following generalization.\nTheorem 5 (H¨older-continuity of the Q-function). Let M\nbe an pLp, Lrq-LC MDP, let π be an Lπ-LC policy, and let\n0 ă α ă α :“ min\n\"\n1,\n´ log γ\nlogpLpp1 ` Lπqq\n*\n.\nIf the state space S and the action space A admit finite diameter4 diampSq and diampAq, respectively, then the stateaction value function Qπ is pα, LQπ,αq´HC with a H¨older\nconstant bounded by:\nLQπ,α :“ Lr pdiampSq ` diampAqq1´α\n1 ´ γpLpp1 ` Lπqqα\n.\nThe proof is reported in Appendix.Furthermore, we can\neasily obtain the H¨older constant of the state value function\nV π.\n4The diameter of a metric space pX, dX q is defined as:\ndiampXq “ supx,x1PX dX px, x1q.\nProposition 6 (H¨older-continuity of the V-function). Let π\nbe an Lπ-LC policy. If the state-action value function Qπ is\npα, LQπ,αq-HC, then the corresponding state value function\nV π is pα, LV π,αq-HC with:\nLV π,α :“ LQπ,αpLπ ` 1qα.\nThese result represent a generalization of those of\n(Rachelson and Lagoudakis 2010), which are obtained by\nsetting α “ 1.\nMoreover, this Theorem 5 implies that the value functions\nof an LC MDP and policy is always continuous, since any\nHC function is also continuous, regardless of its constants,\nas it seemed from the previous example. Coming back to\nExample 2, we can perform further analyses.\nExample 2 (continued). We can use Theorem 5 to provide\nan upper bound on the value function even if the Lipschitz\ncontinuity does not hold. The critical exponent is given by:\nα “ ´\nlog γ\nlogpLpp1 ` Lπqq « 0.72.\nFor every value of α ă α, the state value function V π is\npα, LV π,αq´HC. As we can see in Figure 2 right, for small\nα, the bound provided by LV π,α|s|α is tight for s Ñ 1.\n4.2\nA More General Bound Based on Wasserstein\nDistance\nSimilarly to what we have done in Section 3, to a result of\nregularity, we are able to associate a result about the loss of\nBC, bounding the difference in performance between two\npolicies with their Wasserstein distance. Indeed, thank to\nTheorem 5, we can prove the following bound.\nTheorem 7 (Optimal Error Rate for BC). Let πE be the expert policy and πI be the imitator policy. If that state-action\nvalue function QπI of the imitator policy πI is pα, LQπI ,αqHC, then it holds that:\nJπE ´ JπI ď LQπI ,α\n1 ´ γ\nE\ns„dπE rW pπEp¨|sq, πIp¨|sqqαs .\nFurthermore, if the MDP M is pLp, Lrq-LC and the imitator policy πI is LπI-LC, the bound is tight for what concerns the exponent α that cannot be improved above the critical value α of Theorem 5.\nThe proof is reported in Appendix.As expected, a low\nvalue of α leads to a looser bound. Unfortunately, this\nbound, despite being tight in the exponent, is difficult to\nmanage in practice. Indeed, in order to minimize the righthand side, Es„dπE rWpπEp¨|sq, πIp¨|sqqαs, one should know\nthe value α in advance. However, α ď α depends on the Lipschitz constants of the environment and of the policy, which\nare usually unknown. Therefore, no imitation learning algorithm can be trained to minimize this error explicitly. Fortunately, we can see that, weakening this result, we can obtain\na more practical guarantee. Since 0 ă α ă 1, we can apply\nJensen’s inequality to obtain:\nJπE ´ JπI ď LQπI ,α\n1 ´ γ\nE\ns„dπE rW pπEp¨|sq, πIp¨|sqqsα . (4)\nIn this formulation, we minimize the expected Wasserstein distance only, and the knowledge of α is not needed,\nbut its value impacts the kind of guarantee we can provide.\nRemark 1. If we perform BC in a pLp, Lrq-LC MDP and\nwith a LπI-LC imitator policy πI, the best possible performance guarantee (from Equation 4) is given by:\nJπE ´ JπI ď Opεαq,\nwhere ε is the square root of the imitation MSE, i.e., ε2 “\nEs„dπE r}πEpsq ´ πIpsq}2\n2s as defined in Example 1, and\nα ă α “ ´\nlog γ\nlogpLpp1`LπI qq, the critical exponent.\nTherefore, a very low value of α, corresponding to lack\nof regularity, can badly influence the possibility of learning\na good imitator policy.",
        "noise injection": "BC may struggle when the regularity assumptions are lacking. However, in practice, using a noisy expert policy may\nsignificantly help the learning process (Laskey et al. 2017b).\nThis empirical benefit is justified by the intuition that noise\nhelps in exploring the neighborhood of the expert trajectories. In this section, we formulate this empirical evidence\nin a mathematically rigorous way. Indeed, we show how to\nbreak the barrier enforced by Theorem 7, whose result is obtained by a deterministic expert. Clearly, these advantages\ncome with the price that a noisy expert might experience a\nloss in expected return compared to the deterministic one.\n5.1\nNoise Injection: A Mathematical Formulation\nThe simplest form of noise injection is realized by adding to\nthe expert’s action aE,t a noise component ηt. In particular,\nassuming that the action space is real, i.e., A Ď Rn, we have:\n@t P N :\n$\n’\n&\n’\n%\nat,E „ πEp¨|stq\nηt\niid„ L\nat “ at,E ` ηt\n,\n(5)\nwhere tηtutPN is a noise sequence whose components are\nindependent between each other and from the sequences\nof states and actions, and identically distributed by law\nL P PpAq. If L admits a density function, we can express\nthe density function of the played action at as the convolution of the expert policy density function πE and the density\nfunction ℓ of the noise law L. Note that the formalization\nin Equation (5) encompasses distributions that do not correspond to the intuitive idea of noise (e.g., when L is a discrete\nlaw). To obtain a meaningful result, we enforce the following assumption.\nAssumption 3. The law of the noise L admits a density\nfunction w.r.t. a reference measure ℓ : Rn Ñ Rě0 and is\nTV-LC (see Definition 1) with constant Lℓ.\nUnder this assumption, denoting with πE,ℓ the policy with\nnoise injection, i.e., at „ πE,ℓp¨|stq, we have that:\nπE,ℓpa|sq “\nż\nRn πEpa1|sqℓpa ´ a1qda1, @ps, aq P S ˆ A.\nThis represents the convolution of the policy density function πE and the noise density function ℓ. In other words, this\nshows that the action taken by the expert policy aE,t is averaged over the noise probability distribution.\nAssumption 3 covers the most common types of noise,\nlike the Gaussian or the uniform ones. In fact, we can prove\nthat every univariate unimodal distribution satisfies Definition 1.Considering multivariate Gaussian noise, we directly\nderive the Lℓ constant.\nExample 3. Suppose the noise is sampled from a zero-mean\nGaussian distribution Np0, Σq with covariance matrix Σ,\nthe previous integral writes, for all ps, aq P S ˆ A:\nπE,ℓpa|sq “\nż\nRn πEpa1|sq e´ 1\n2 pa´a1qT Σ´1pa´a1q\np2πqn{2detpΣq1{2\nloooooooooooomoooooooooooon\nℓpa´a1q\nda1,\nwhere we recognise the Gaussian n-variate density ℓ. Assumption 3 is verified since, for h P Rn:\nTVpNph, Σq, Np0, Σqq ď\nc\n1\n2KLpNph, Σq, Np0, Σqq\n“ 1\n2 }h}Σ´1 ď\n1\n2\na\nsminpΣq\n}h}2,\nwhere we used Pinsker’s inequality and sminp¨q denoted the\nminimum singular value of a matrix. In particular, if Σ is\ndiagonal as σ2I, we have that Lℓ “ 1{p2σq.\nIt is worth noting that, in the diagonal covariance case,\nLℓ is proportional to σ´1. This suggests that, the smaller the\nimpact of the noise L, i.e., the smaller the standard deviation\nσ, the higher the constant Lℓ. Indeed, as σ decreases, the\nregularization effect of the noise becomes less relevant (in\nthe limit σ Ñ 0, noise injection vanishes).\n5.2\nA Bound Based on Wasserstein Distance for\nNoise Injection\nWe are now able to prove a performance guarantee for BC\nwith noise injection. The idea is based on a simple yet interesting fact. We can use the noise to smooth a bounded\nfunction, as in Proposition 1. Applying this approach to the\nstate-action value function, leads to the following result.\nTheorem 8. Let πE be the expert policy and πI be the imitator policy. Let us suppose that we have injected a noise of\ndensity function ℓ, satisfying Assumption 3 to obtain a noisy\nexpert πE,ℓ and a noisy imitator πI,ℓ. If |QπIps, aq| ď Qmax\nfor all ps, aq P S ˆ A, it holds that:\nJπE,ℓ ´ JπI,ℓ ď 2LℓQmax\n1 ´ γ\nE\ns„dπE,ℓrWpπEp¨|sq, πIp¨|sqqs.\nThe proof is reported in Appendix.Some observations\nare in order. First, note the similarity with Theorem 3,\nwith the only difference being the substitution of LQπ with\n2LℓQmax. Second, we require no smoothness assumption\n(e.g., Lipschitz continuity) on the environment or on the\npolicy. Yet, if in the previous result of Theorem 3 the constant LQπ could easily become infinite, now, the constant\n2LℓQmax can be easily bounded by 2LℓRmax\np1´γq2 , since Qmax ď\nRmax\n1´γ . From an intuitive perspective, the need for smoothness in the environment is replaced with an assumption on\nthe density function of the noise. Lastly, we note that on\n0\n0.1\n0.2\n0.3\n−200\n−180\n−160\n−140\n−120\nNoise standard deviation (σ)\nExpected return Jπ\nPendulum\n0\n0.1\n0.2\n0.3\n150\n200\n250\nNoise standard deviation (σ)\nLunar Lander\n0\n0.1\n0.2\n0.3\n100\n200\n300\nNoise standard deviation (σ)\nBipodal Walker\nDDPG\nTD3\nPPO\nFigure 3: The performance of the expert Jπ as a function of the standard deviation of the noise σ. The performance is measured\non 40 episodes int environment repeated for 20 different random seeds (nuance represents the 95% non-parametric c.i.).\nthe right-hand side of the formula, the error is measured by\nthe Wasserstein distance of the non-noisy policies. This is\nadvisable since it implies that the intrinsic error due to the\nnoise does not affect the bound besides the γ-discounted visitation. We show in Appendix that this quantity is always\nsmaller than its counterpart involving the noisy policies.\nRemark 2. If we perform BC injecting a noise ηt of density\nfunction ℓ and satisfying Assumption 3, we have the following performance guarantee:\nJπE,ℓ ´ JπI,ℓ ď Opεq,\nwhere ε is the MSE of the imitation policy as in Remark 1.\nIn comparison with Remark 1 for standard BC, we can\nappreciate that, here, the exponent α disappeared. Indeed,\nwe have a performance bound that decreases linearly in the\nMSE. In many cases, when the environment is not intrinsically very smooth, or the expert policy is irregular, the α\nparameter can be very small, slowing down the convergence\nsignificantly. Instead, a liner decay is a relevant improvement of the v speed. Furthermore, as already noted, no assumption of regularity is required in Theorem 8, so that the\nlast result has a much wider range of applications.",
        "practical considerations": "In the previous sections, we have seen that the use of noise\ninjection allows having a much better performance guarantee than standard BC (see Remarks 1 and 2). Still, in practice, what matters is to have an imitator policy that is good\nitself rather than an imitator that is simply good in mimicking a given policy. Therefore, if with the noise injection\nwe negatively affect the performance of the expert, i.e., if\nJπE,ℓ ! JπE, the results given about noise injection could\nbecome useless. On the contrary, we argue that adding noise\nto the expert’s action to a certain extent, does not particularly\naffect performance. In Figure 3, we show the results of testing this statement on some of the most common continuousactions environments of the OpenAI gym (Brockman et al.\n2016) library. In this simulation,5 we first train an expert policy with DDPG (Lillicrap et al. 2015), TD3 (Fujimoto, Hoof,\nand Meger 2018) and PPO (Schulman et al. 2017) in the following OpenAI gym environments:\n5Details can be found in Appendix.\n• Pendulum-v0: this environment has a continuous action space r´2, 2s. The objective is to apply torque on a\npendulum to swing it into an upright position. The whole\nsystem is very regular, as it is governed by simple differential equations, and is also deterministic, except for the\ninitial position of the pendulum, which is random.\n• LunarLanderContinuous-v2: this environment\nhas a continuous action space r´1, 1s2. Here, we have\nto make a rocket land safely in a landing pad. The dynamics is quite complex, and stochasticity is present to\nsimulate the effect of the wind.\n• BipedalWalker-v3: this environment has a continuous action space r´1, 1s4. Here we have to make a\nbipedal robot walk. The dynamics is even more complex,\nbut the whole system is deterministic.\nThen, we evaluated the performance of these experts with\nnoise injection with Gaussian noise with different standard\ndeviations. As we can see in figure 3, even when the noise\nincreases until it is close to the radius of the action space,\nat least in seven cases out of nine, the performance does not\nsuffer significant drops. Intuitively, this can be explained by\nthe fact that we applied an i.i.d. zero-mean noise sequence\nthat is independent of the state and the action. Thus, its effect\ndoes not accumulate over the horizon.",
        "conclusion": "In this paper, we have addressed BC for continuous-action\nenvironments from a theoretical perspective. We have shown\nthat the existing theoretical guarantees on BC are not suitable when dealing with continuous actions. Thus, we have\nderived a first bound for the performance guarantees, under\nthe assumption that the imitator value function is Lipschitz\ncontinuous. Since this latter assumption is demanding (i.e.,\nit is not guaranteed even when the underlying MDP and policy are LC), we have relaxed it by studying the continuity\nproperties of the value function. As a result of independent\ninterest, we have proved that the value function is always\nH¨older continuous, under the milder assumption that the underlying MDP and policy are LC. Then, we have applied\nthese findings to obtain a general bound for the performance\ngap of BC, which we have proved to be tight. Finally, we\nhave formalized noise injection and we have shown the advantages of this practice when applied to BC.",
        "summary_en": "Behavioral Cloning (BC) aims at learning a policy that mimics the behavior demonstrated by an expert. The current theoretical understanding of BC is limited to the case of finite actions. Therefore,this paper studies BC with the goal of providing theoretical guarantees on the performance of the imitator policy in the case of continuous actions.The paper starts by deriving a novel bound on the performance gap based on Wasserstein distance, applicable for continuous-action experts, holding under the assumption that the value function is Lipschitz continuous. Since this latter condition is hardy fulfilled in practice, even for Lipschitz Markov Decision Processes and policies, the paper proposes a relaxed setting, proving that value function is always H’older continuous. This result is of independent interest and allows obtaining in BC a general bound for the performance of the imitator policy. Finally, the paper analyzes noise injection, a common practice in which the expert's action is executed in the environment after the application of a noise kernel. This paper shows that this practice allows deriving stronger performance guarantees, at the price of a bias due to the noise addition.",
        "summary_zh": "这篇论文研究了行为克隆（BC），旨在为连续行动情况下模仿者策略的性能提供理论保证。针对连续动作，该论文提出了基于Wasserstein距离的性能差距上界，以及在值函数为Lipschitz连续的假设下成立的条件。然而，由于这个条件在实践中难以满足，论文又提出了一个放松条件，证明了值函数总是H'older连续的。最后，论文分析了常见的噪声注入技术，发现这种做法可以提供更强的性能保证，但会引入噪声偏差。"
    },
    {
        "title": "Planning for Learning Object Properties",
        "abstract": "Autonomous agents embedded in a physical environment need the ability to recognize objects and their properties from sensory data. Such a perceptual ability is often implemented by supervised machine learning models, which are pre-trained using a set of labelled data. In real-world, open-ended deployments, however, it is unrealistic to assume to have a pre-trained model for all possible environments. Therefore, agents need to dynamically learn/adapt/extend their perceptual abilities online, in an autonomous way, by exploring and interacting with the environment where they operate. This paper describes a way to do so, by exploiting symbolic planning. Specifically, we formalize the problem of automatically training a neural network to recognize object properties as a symbolic planning problem (using PDDL). We use planning techniques to produce a strategy for automating the training dataset creation and the learning process. Finally, we provide an experimental evaluation in both a simulated and a real environment, which shows that the proposed approach is able to successfully learn how to recognize new object properties.",
        "introduction": "Agents embedded in a physical environment, like autonomous robots, need the ability to perceive objects in the\nenvironment and recognize their properties. For instance, a\nrobot operating in an indoor environment should be able to\nrecognize whether a certain box found in the environment is\nopen or closed. From these perceptions the agent can build\nand use abstract representations of the states of the environment to reach its goals by automatic planning techniques.\nThe common approach to provide an agent with such perceptual capabilities consists in pre-training offline a (set of)\nperception models from hundreds of thousands of annotated\ndata (e.g., images or other sensory data). See for instance\n(Asai 2019; Dengler et al. 2021; Lamanna et al. 2022).\nIn offline training approaches, the perception capabilities\nare fixed once and for all. This is in stark contrast with a\nmain requirement in many robotics applications: agents embedded in real-world, open-ended environments should be\nable to dynamically and autonomously improve their perceptual abilities by actively exploring their environments.\nThis is also in agreement with the emerging popular research\narea of interactive perception (Bohg et al. 2017). When perception functions are modelled by (deep) neural networks,\nan open and interesting challenge is whether agents can autonomously decide when and how to improve their perception models, by collecting the needed training data and using\nthem to train the neural network.\nIn this paper, we explore a way to address this challenge\nwith an automated planning approach. In particular, we design a PDDL planning domain (McDermott et al. 1998) for\nplanning to learn (or improve) the perceptual capabilities of\nthe agent. We focus on the problem of automatically training\nneural networks able to recognize properties of objects, e.g.,\nopen/closed, by relying on a pre-trained object detector. We\nextend a PDDL planning domain, called base domain, with\nnew actions and predicates for learning properties of object\ntypes. Such an extension, called learning domain, is specified in a meta-language of the base language. It contains\nthe reification of properties and types of the base language.\nFor instance, if Is Open is a property of the base domain,\nthe learning domain contains the object ‘is open’ of type\nProperty. Furthermore, the learning domain contains actions for collecting training examples for properties, and actions for training the network with them.\nThe online learning of object properties is obtained by\nplanning in the union of the base and learning domains, and\nby executing the generated plan. The base domain allows\nthe agent to plan to reach a state where the agent can observe an object with a certain property, e.g., a state where\nIs Open(box0) is true. The learning domain allows the\nagent to plan for actions that collect observations of objects\nwith the property being true, e.g., take pictures of box0,\nwhich is known to be open, and add them to the positive\ntraining examples for the property Is Open. In this way, the\nagent automatically maps low level perceptions (e.g., images\nof an open box) into the symbolic property of objects at the\nabstract planning level (e.g., “the box is open” in PDDL).\nWe provide an experimental evaluation both in a photorealistic simulated environment (Kolve et al. 2017) and in a\nreal world setting. In the simulated environment, we evaluate\nthe ability to learn certain object properties under two different conditions of object detection: noisy and ground truth.\nThe results indicate that the approach is able to successfully\nlearn and recognize the object properties with high precision/recall for most object classes. In the real world environment, actions for changing object properties (e.g., opening a book) are performed cooperatively with humans, e.g.,\nby asking a human to physically perform the required action.",
        "related work": "As far as we know, the proposed approach is new. Several\napproaches are based on the idea of exploiting automated\nlearning for different planning tasks, like for learning action\nmodels, heuristics, plans, or policies. Most of the research\non learning action models, see, e.g., (Aineto, Jim´enez, and\nOnaindia 2018, 2019; Bonet and Geffner 2020; McCluskey\net al. 2009; Cresswell, McCluskey, and West 2013; Gregory and Cresswell 2015; Lamanna et al. 2021; Juba, Le,\nand Stern 2021), does not deal with the problem of learning from real value perceptions. Other works have addressed\nthe problem of learning planning domains from perceptions in the form of high dimensional raw data (such as images), see, e.g., (Asai and Fukunaga 2018; Asai 2019; Janner\net al. 2018; Dengler et al. 2021; Konidaris, Kaelbling, and\nLozano-P´erez 2018; Liberman, Bonet, and Geffner 2022).\nIn these works, the abstract planning domain is obtained\nby offline pre-training, and the mapping between perceptions to the abstract model is fixed, while we learn/adapt\nthis mapping online. Our approach shares some similarities\nwith the work on planning by reinforcement learning (RL)\n(Sutton and Barto 2018), since we learn by acting in a (simulated) environment, and especially with the work on deep RL\n(DRL) (Mnih et al. 2015, 2016), which dynamically trains a\ndeep neural network by interacting with the environment.\nHowever, DRL focuses on learning policies and perceptions are mapped into state embeddings that cannot be easily mapped into human comprehensible symbolic (PDDL)\nstates. In general, while all the aforementioned works address the problem of using learning techniques for planning,\nwe address a different problem, i.e., using planning techniques for learning automatically and online to recognize\nobject properties from low level high dimensional data.\nWe share a similar motivation of the research on interactive perception, see (Bohg et al. 2017) for a comprehensive\nsurvey, and especially the work in this area for learning properties of objects, see, e.g., (Natale, Metta, and Sandini 2004).\nHowever, most of this work has the objective to integrate\nacting and learning, and to study the relation between action and sensory response. We instead address the challenge\nof building autonomous systems with planning capabilities\nthat can automate the training and learning process.\nA wide variety of approaches and applications have been\nproposed for enhancing robotic agents with active learning techniques (Kulick et al. 2013; Cakmak and Thomaz\n2012; Cakmak, Chao, and Thomaz 2010; Chao, Cakmak,\nand Thomaz 2010; Ribes et al. 2015; Hayes and Scassellati\n2014; Huang, Jin, and Zhou 2010; Ashari and Ghasemzadeh\n2019). In these works, the robotic agents improve their skills\nor learn new concepts by collecting and labeling data in an\nonline way. However, all these methods label data by means\nof either human supervisions or a confidence criteria applied\non the prediction of a pre-trained model. In contrast, our approach does not require human supervision, and collected\ndata are labelled by applying actions.\nIn (Ugur and Piater 2015), similarly to our approach, the\nauthors propose a method for learning the predicates corresponding to action effects after their executions. They learn\naction effects by clustering hand-crafted visual features of\nthe manipulated objects, extracted from the continuous observations obtained after executing the actions. However,\nthe learned predicates lack of interpretation, which must be\ngiven by a human. On the contrary, our approach learns explainable predicates. Moreover, they focus on learning the\neffects of a single action (i.e., stacking two blocks) in a fully\nobservable environment using ground truth object detection.\nOur approach learns predicates corresponding to the effects\nof several actions in partially observable environments, and\nusing a noisy object detector.\nThe approach by (Migimatsu and Bohg 2022) learns to\nmap images into the truth values of predicates of planning\nstates. Differently from us, their approach is offline and requires the sequence of images labeled with actions, while\nour approach plans for generating this sequence online. We\nshare the idea of learning state representations through interaction with (Pinto et al. 2016), where they learn visual\nrepresentations of an environment by manipulating objects\non a table. Notably, they learn the visual representation in\nan unsupervised way, through a CNN trained on a dataset\ngenerated by interacting with objects. However, the learned\nrepresentations lack of interpretation. Furthermore, in (Pinto\net al. 2016), the learned representations are not suitable for\napplying symbolic planning.\nFinally, our extension of the planning domain with a metalanguage shares some commonalities with the work on planning at the knowledge level (see, e.g., (Petrick and Bacchus\n2002)), which addresses the different problem of planning\nwith incomplete information and sensing.",
        "preliminaries": "Symbolic planning\nA planning domain D is a tuple\n⟨P, O, H⟩ where P is a set of first order predicates with\nassociated arity, O is a set of operators with associated arity, and H associates to every operator op ∈ O an action schema. The action schema is composed of a triple\n\n\npar(op), pre(op), eff+(op), eff−(op)\n\u000b\nin which par(op) is a\nn-tuple of distinct variables x1, . . . , xn, where n is the arity\nof op, and pre(op) is a set of first order formulas with predicates in P and arguments in x1, . . . , xn, and eff+(op) and\neff−(op) are set of atomic first order formula with predicates\nin P and arguments in x1, . . . , xn. Among the unary predicates of P, we distinguish between object types and object\nproperties, which are denoted with t and p, respectively.\nGiven a planning domain D and a set of constants C, a\ngrounded planning domain is obtained by grounding all the\nactions schema of D with the constants in C. The set of\nstates of a grounded planning domain D(C) is the set of\nall possible subsets of atoms that can be built by instantiating every predicate in P in all possible ways with the constants in C. A ground action model defines a transition function among states where (s, op(c1, . . . , cn), s′) if and only if\ns |= pre(op(c1, . . . , cn)) and s′ = s∪eff+(op(c1, . . . , cn))\\\neff−(op(c1, . . . , cn)).\nA planning problem Π on a grounded planning domain\nD(C) is a triple Π = ⟨D(C), s0, g⟩, where s0 is an initial state and g is a first order formula that identifies a\nset of states of D(C) in which the goal is satisfied, i.e.,\n{s ∈ 2P(C) | s |= g}. A plan π for problem Π is a sequence\nof ground actions ⟨a1, . . . , ak⟩ such that (si−1, ai, si) for\ni = 1, . . . , k is a transition of D(C) and sk |= g.\nPerception functions\nThe agent perceives the environment by sensors that return real-value measurements on\nsome portion of the environment. For example, the perception of an agent with a on-board camera and a system for\nestimating its position consists in a vector (x, y, z) of coordinates and an RGB-D image taken by the agent’s on-board\ncamera. Observations are partial (e.g., the camera provides\nonly the front view) and could be incorrect (e.g., the estimation of the position could be noisy). We suppose that, at the\ntime when a perception occurs, the agent’s knowledge about\nthe environment is represented by a grounded planning domain D(C), where C represents the set of objects already\ndiscovered in the environment. Each c ∈ C is associated with\nan anchor (Coradeschi and Saffiotti 2003) that describes the\nperceptual features of c that have been collected by the agent\nso far (e.g., the pictures of c from different angles, the estimated position and size of c, etc.). At the beginning, the set\nC of constants is empty. The agent performs and processes\neach perception in order to extract some knowledge about\nthe objects in the environment, and about their properties in\nthe current state. This is achieved by combining an object\ndetector and a set of property classifiers.\nThe object detector identifies a set of objects in the current perception (e.g., RGB-D image) and predicts their types\n(i.e., it selects one type among the object types of the planning domain). Every detected object is associated with numeric features (e.g., the bounding box, the estimation of the\nposition, etc.), which are used to build the anchors of the detected object. The features of each detected object are compared with the features of the objects already known by the\nagent, i.e., those present in the current set of constants C.\nIf the features of the detected object matches (to a certain\ndegree) the features of a c ∈ C, then the features of c are updated with the new discovered features. Otherwise C is extended with a new constant c anchored to the features of the\ndetected object, and the type t(c) is asserted in the planning\ndomain, where t is the type returned by the object detector.\nFor every object c of type t returned by the object detector and for every property p that applies to t, a classifier ρt,p\npredicts if c has/has not the property p. Notice that not all\nproperties apply to a type, e.g., a laptop cannot be filled or\nempty. Furthermore, for the same property we use different\nclassifiers for different types, since predicting that a bottle is\nopen or that a book is open from visual features are two very\ndifferent tasks. ρt,p can be specified either explicitly by a set\nof predefined rules, or it can be a machine learning model\ntrainable by supervised examples. For instance, the classifier that checks if an object is Close To the agent is defined\nby a threshold on the distance between the agent and the object position. Other properties (e.g., Is Open) are predicted\nusing a neural network, which takes as input object images\nand returns the probability of the property being true.\nPlan execution\nTo achieve its goal (expressed in a formula\nof the language of the planning domain), the agent generates\na plan using a classical planner (e.g., we used Fast-Forward\n(Hoffmann 2001)), and then it executes the plan. However,\nthe symbolic actions of the plan need to be translated into\nsequences of operations executable by the agent’s actuators\n(e.g., rotate of 30◦, grasp the object in position x, y, z, move\nforward of 30cm). Designing effective and robust methods\nfor producing this mapping is a research area which goes out\nof the scope of this paper, see for instance (Eppe, Nguyen,\nand Wermter 2019). In our experiments, we adopt state-ofthe-art path planning algorithms (based on a map learned online by the agent) and ad-hoc compilations of actions. However, it is worth noting that we do not assume the execution of the actions leads to the symbolic state predicted by\nthe planning domain. For instance, the execution of the action Go Close To(c) might end up in a situation where the\nagent is not close enough to the object c and the predicate\nClose To(c) is false, despite being a positive effect of the\naction Go Close To(c). Moreover, the execution of a symbolic action can have effects that are not predicted by the action schema. For instance some properties of an object might\nbecome true even if they are not in the positive effects of the\nsymbolic actions. For these reasons, after action executions,\nthe agent must check if the plan is still valid, and if not, it\nshould react to the unexpected situation, e.g., by replanning.",
        "problem definition": "We place an agent in a random position of an unknown environment; we initialize it with the following components:\n(i) a set of sensors on the environment; (ii) a trained object\ndetector ρo; (iii) a planning domain D = (P, O, H); (iv)\na method for executing its ground actions; (v) an untrained\nneural network ρt,p for predicting the property p of the objects of type t, for a subset of pairs (t, p) of interest.\nWe focus on the online training of the ρt,p’s; our aim is to\ndesign a general method to autonomously generate symbolic\nplans for producing a training set Tt,p, for every pair (t, p)\nof interest, and use Tt,p to train the perception function ρt,p.\nTt,p contains pairs (c, v), where c is (the name of) an object of type t with the associated anchor (e.g., the visual\nfeatures of the object) and v ∈ {p, ¬p} is the value of the\nproperty p. Since Tt,p is automatically created by acting in\nthe environment, it may contain wrong labels. We evaluate\nthe effectiveness of our method on the performance (precision and recall) of each ρt,p against a ground truth data set\ncollected independently by the agent.",
        "method": "We explain the proposed method with a simple example.\nSuppose an agent aims to learn to recognize the property\nIs Turned On for objects of type Tv, it can proceed as follows: (i) look for an object (say tv0) of type Tv; (ii) turn tv0\non to make sure that Is Turned On(tv0) is true, (iii) take\npictures of tv0 from several perspectives, and label them as\npositive examples for Is Turned On. To produce negative\nObserve(o, t, p):\npre: ¬Viewed(o, t, p)\nClosed To(o)\nKnown(o, t, p)\neff+:Sufficient Obs(t, p)\nViewed(o, t, p)\nExplore for(t, p)\npre: ∀x(Known(x, t, p) → Viewed(x, t, p))\n¬ Sufficient Obs(t, p)\neff+:Explored for(t)\nTrain(t, p, q):\npre\nSufficient Obs(t, p)\nSufficient Obs(t, q)\neff+:Learned(t, p, q)\nTable 1: Schemas for Observe, Explore for and Train.\nexamples for the same property, the agent can proceed in the\nsame fashion, applying the action Turn Off(tv0).\nThe behaviour explained above should be automatically\nproduced and executed by the agent for every learnable pair\n(t, p), where t denotes an object type and p a learnable property. Therefore, in the following, we explain a procedure that\nextends automatically the planning domain of the agent to\nexpress the goal of learning p for t, and such that the procedure for collecting training data for ρt,p is generated by\na symbolic planner, and can be executed by the agent. This\nmethod requires that, for every learnable pair (t, p), the planning domain contains at least an operator applicable to object of type t that makes p true, and one that makes p false.\nThis means that we have to extend the planning domain\nwith the capability of expressing facts about its properties\nand types, i.e., we have to extend it with meta predicates and\nnames for the elements of the planning domain D.\nExtended Planning Domain for Learning\nTable 1 summarizes how we extend the planning domain for\nobserving, exploring and learning.\nNames for types and properties\nFor each object type t ∈\nP (e.g., Box), we add a new constant ‘t’ (e.g., ‘box’)1. For\neach object property p ∈ P (e.g., Is Open), we add two\nnew constants, namely ‘p’ and ‘not p’ (e.g., ‘is open’ and\n‘not is open’).\nEpistemic predicates\nWe extend P with predicates for\nstating that an agent knows/believes that an object has\na certain property in a given state. The binary predicate\nKnown(o,‘p’) (resp. Known(o, ‘not p’)) indicates that the\nagent knows that the object o has (resp. does not have)\nthe property p. The atom Known(x,‘p’) is automatically\nadded to the positive (resp. negative) effects of all the actions that have p(x) in their positive (resp. negative) effects; similarly, the atom Known(x,‘not p’) is automatically\nadded to the positive (resp. negative) effects of all the actions that have p(x) in their negative (resp. positive) effects. For example, the atoms Known(x, ‘is turned on’)\n1Quotes are used to indicate names for elements of P.\nand Known(x, ‘not is turned on’) are added to the positive and negative effects of Turn On(x), respectively. Similarly, the atoms Known(x, ‘is turned on’) and Known(x,\n‘not is turned on’) are respectively added to the negative\nand positive effects of Turn Off(x).\nPredicates and operators for observations\nWe extend\nthe planning domain with the operator Observe(o, t, p),\nwhich takes as input an object o, a type t, and a property p. The low level execution of Observe(o, t, p) consists in extending the training dataset Tt,p with observations\n(i.e., images) of object o taken from different perspectives.\nThe positive effects of Observe(o, t, p) contain the atom\nViewed(o, p), and the preconditions of Observe(o,t,p) contain the atom ¬Viewed(o, p), which prevents the agent from\nagain observing o for the property p in the future.\nThe atom Sufficient Obs(t, p) is added to the positive\neffects of the action Observe(o, t, p). Whether the agent,\nafter executing Observe(o, t, p), has not collected enough\nobservations of objects of type t with property p, the atom\nSufficient Obs(t, p) is actually false, in contrast with\nwhat is predicted by the planning domain, and the agent has\nto plan for observing other objects of type t.\nPredicates and operators for exploration\nThe planning\ndomain\nis\nextended\nwith\nthe\nbinary\noperator\nExplore for(t, p) that explores the environment looking for new objects of type t. The precondition of\nExplore for(t, p) is that all the known objects of\ntype\nt\nhave\nbeen\nviewed\nfor\nthe\nproperty\np,\ni.e.,\n∀x(Knows(x, t, p) → Viewed(x, t, p)). Indeed, finding a\nnew object creates a new object o in the planning domain, and makes aware the agent that properties of o\ncan be observed. Explored for(t) is a positive effect of\nExplore for(t, p) in the planning domain. Such an effect\nindicates that the environment has been (even partially) explored for finding new objects of type t. However, the actual\nexecution of Explore for(t, p) will not make it true until\nthe environment has been completely explored, or a maximum number of iterations has been reached.\nPredicates and operators for learning\nWe extend the\nplanning domain with the predicate Learned(t, p, not p),\nindicating if the agent has collected enough observations,\nand trained ρt,p. We add to the planning domain the operator Train(t, p, q). When the agent executes the action\nTrain(t, p, q), the network ρt,p is trained using Tt,p as\npositive examples and Tt,q as negative examples. The preconditions of this action include Sufficient Obs(t, p)\nand Sufficient Obs(t, q) that guarantee to have sufficient\npositive and negative examples for training ρt,p. This action\nhas only one positive effect, which is Learned(t, p, q).\nSpecifying the goal formula\nIn the extended planning domain, the goal formula g for learning a property p for an\nobject type t is defined as:\ng = Learned(t, p, not p) ∨ Explored for(t).\n(1)\nFor\nexample,\nsuppose\nthat\nan\nagent\naims\nto\nlearn\nthe property Turned On for objects of type Tv, then\ng = Learned(‘tv’, ‘turned on’, ‘not turned on’) ∨\nAlgorithm 1: PLAN AND ACT TO LEARN OBJECT PROPS\nInput: D = (P, O, H) a planning domain\nInput: g = V\n(t,p)∈T P (Learned(t, p) ∨ Explored For(t))\n1: extend D with actions and predicates for learning\n2: C ← names for types and properties in P\n3: s ← ∅\n4: TT P ← {Tt,p = ∅ | (t, p) ∈ TP}\n5: ρT P ← {ρt,p = random init. | (t, p) ∈ TP}\n6: π ← PLAN(D(C), s, g)\n7: while π ̸= ⟨⟩ do\n8:\nop ← POP(π)\n9:\ns ← s ∪ eff+(op) \\ eff−(op)\n10:\nC, TT P , ρT P ← EXECUTE(op)\n11:\ns ← OBSERVE()\n12:\nπ ← PLAN(D(C), s, g)\n13: end while\nExplored for(‘tv’).\nIf\nthe\ncurrent\nset\nof\nconstants\ncontains\nan\nobject,\nsay\ntv0,\nof\ntype\nTv\nsuch\nthat\nViewed(tv0,‘tv’,‘is turned on’)\nand\nViewed(tv0,‘tv’,‘not is turned on’) are both false,\nthen the goal is reachable by the plan:\nGo Close To(tv0)\nTurn On(tv0)\nObserve(tv0, ‘tv’,‘turned on’)\nTurn Off(tv0)\nObserve(tv0,‘tv’, ‘not turned on’)\nTrain(‘tv’, ‘turned on’,‘not turned on’).\nAfter the execution of all the actions but the last one\nof the above plan, if the agent has not collected enough\ntraining data for\n‘turned on’ and\n‘not turned on’,\nthe\natoms\nSufficient Obs(‘tv’,‘turned on’)\nand\nSufficient Obs(‘tv’,‘not turned on’) will be false,\nand the last action of the plan cannot be executed. In such\na case, the agent has to replan in order to find another tv\nwhich has not been observed yet.\nFinally, notice that whether all the TVs known by the\nagent have been observed for the property Turned On,\nthen the formula ∀x(Known(x,‘tv’,‘turned on’)\n→\nViewed(x, ‘tv’, ‘turned on’)\nis\ntrue,\nand\nthe\ngoal\ncan be achieved by generating a plan that satisfies\nExplored for(‘tv’),\ni.e.,\nby\nexecuting\nthe\naction\nExplore For(‘tv’,‘turned on’), which explores the\nenvironment for new TVs.\nMain Control Cycle\nThe main control cycle of the agent is described in Algorithm 1, which takes as input a planning domain D and the\ngoal g for learning a set TP of type-property pairs. At the\nbeginning, the set of constants C contains only the names for\ntypes and properties, and the state s is empty (lines 2–3). For\nevery pair (t, p) ∈ TP, the algorithm initializes the training\nset Tt,p to the empty set, and the neural networks ρt,p (lines\n4–5). Then, a plan π is generated (line 6). In the while loop\n(lines 7–13), the state s is updated according to the action\nschema (line 9). Next, the first action of the plan is executed\nand the set of known constants C, the datasets Tt,p, and the\nneural networks ρt,p are updated (line 10). Notice that, since\nthe perceived effects of actions might not be consistent with\nthose contained in the action schema, a sensing using the not\ntrainable perception functions is necessary, and the state is\nupdated accordingly (line 11). Moreover, since π might be\nno more valid in the updated state, a new plan must be generated (line 12). The algorithm terminates if either the whole\nenvironment has been explored or a maximum number of\niterations has been reached, since, in such cases, the atom\nExplored for(t) is set to true, and plan π for g is empty.",
        "experimental evaluation": "We evaluate our approach on the task of collecting a dataset\nand training a set of neural networks to predict the four properties Is Open, Dirty, Toggled, and Filled on 32 object\ntypes, resulting in 38 pairs (t, p), since not all properties are\napplicable to all object types.\nSimulated environment\nWe experiment our approach in\nthe ITHOR (Kolve et al. 2017) photo-realistic simulator of\nfour types of indoor environments: kitchens, living-rooms,\nbedrooms, and bathrooms. ITHOR simulates a robotic agent\nthat navigates the environment and interacts with the objects\nby changing their properties (e.g., opening a box, turning on\na tv). The agent has two sensors: a position sensor and an\non-board RGB-D camera. For our experiment we split the\n120 different environments, provided by ITHOR, into 80 for\ntraining, 20 for validation, and 20 for testing. Testing environments are evenly distributed among the 4 room types.\nObject detector\nFor the object detector ρo, we used the\nYoloV5 model (Jocher et al. 2021), which takes as input an\nRGB image and returns the object types and bounding boxes\ndetected in the input image. For training ρo, we have generated the training (and validation) sets by randomly navigating in the training (and validation) environments, and using\nthe ground truth object types and bounding boxes provided\nby ITHOR. The training and validation sets contain 115 object types and are composed by 259859 and 56190 examples, respectively. For validating the object detector, we performed 300 runs (with 10 epochs for each run) of the genetic\nalgorithm proposed in (Jocher et al. 2021).\nProperty predictors\nFor the perception functions ρt,p\npredicting properties, we adopted a ResNet-18 model (He\net al. 2016) with an additional fully connected linear layer,\nwhich takes as input the RGB image of the object and returns\nthe probability of p being true for the object. We consider\nthat the input object has the property p if the probability is\nhigher than a given threshold (set to 0.5 in our experiments).\nEvaluation metrics and ground truth\nWe evaluate each\ntrained ρt,p using precision and recall against a test set Gt,p,\nobtained by randomly navigating the 20 testing environments and using the ground truth information provided by\nITHOR. In particular, for the Is Open property we generated a test set with 8751 examples, 2512 for the Toggled\nproperty, 1310 for the Filled property, and 3304 for the\nDirty one. It is worth noting that the size of the test set\nfor the Is Open property is higher than other ones since the\nsize of Gt,p size of Tt,p Precision\nRecall\nObject type\nND\nGTD\nND\nGTD ND\nGTD ND\nGTD\nDirty\nbed\n564\n564\n1502 671\n0.95 0.57\n0.43 0.61\nbowl\n280\n280\n383\n1027\n0.67 0.98\n0.81 0.73\ncloth\n96\n210\n61\n503\n0.93 0.95\n0.78 0.70\ncup\n96\n262\n146\n986\n0.63 0.99\n0.95 0.54\nmirror\n654\n678\n2490 3100\n0.91 0.90\n0.68 0.80\nmug\n230\n432\n225\n1367\n0.88 0.94\n0.42 0.74\npan\n140\n200\n20\n476\n0.76 0.99\n0.87 0.79\nplate\n166\n406\n47\n1304\n0.61 0.97\n0.97 0.77\npot\n210\n272\n51\n929\n0.76 0.99\n0.91 0.98\nWeighted avg\n0.84 0.89\n0.68 0.74\nFilled\nbottle\n22\n22\n78\n150\n0.65 0\n1.00 0\nbowl\n328\n256\n390\n1091\n0.64 1.00\n0.73 0.77\ncup\n116\n286\n200\n1028\n0.92 0.90\n0.56 0.68\nhouseplant\n34\n34\n18\n72\n0.50 0.50\n0.65 0.82\nkettle\n84\n337\n0.25\n0.40\nmug\n126\n354\n250\n1136\n0.80 0.86\n0.51 0.56\npot\n226\n274\n93\n809\n0.67 1.00\n0.89 0.79\nWeighted avg\n0.70 0.86\n0.72 0.66\nIs Open\nbook\n148\n268\n367\n1471\n1.00 0.94\n0.76 0.81\nbox\n204\n204\n959\n1044\n0.92 0.88\n0.37 0.54\ncabinet\n2892 2892\n1545 1669\n0.81 0.80\n0.74 0.79\ndrawer\n3343 3747\n1237 2624\n0.79 0.75\n0.77 0.71\nfridge\n400\n400\n803\n1109\n0.78 0.81\n0.72 0.75\nlaptop\n360\n360\n1124 1531\n0.93 0.97\n0.85 0.82\nmicrowave\n250\n250\n742\n843\n0.68 0.82\n0.50 0.68\nshowercurtain 144\n134\n271\n567\n0.47 0.96\n0.41 0.76\nshowerdoor\n74\n140\n56\n346\n0.88 0.71\n0.19 0.98\ntoilet\n356\n356\n1024 1148\n0.89 0.90\n0.63 0.74\nWeighted avg\n0.81 0.80\n0.72 0.75\nToggled\ncandle\n54\n124\n3\n118\n0.59 0.33\n0.63 0.60\ncellphone\n216\n682\n0.84\n0.94\ncoffeemachine 320\n320\n999\n996\n0.95 0.97\n0.72 0.61\ndesklamp\n12\n56\n254\n255\n1.00 0.91\n1.00 0.97\ndesktop\n56\n184\n1.00\n0.93\nfaucet\n602\n480\n921\n1663\n0.84 0.85\n0.89 0.92\nfloorlamp\n44\n12\n88\n68\n0.83 0.75\n0.50 1.00\nlaptop\n432\n432\n1545 1777\n0.91 0.83\n0.61 0.74\nmicrowave\n252\n252\n1131 1124\n1.00 1.00\n0.76 0.72\nshowerhead\n46\n12\n1.00\n1.00\ntelevision\n222\n238\n269\n510\n0.99 0.94\n0.85 0.95\ntoaster\n280\n280\n713\n1072\n0.86 0.98\n0.59 0.70\nWeighted avg\n0.90 0.88\n0.74 0.80\nTable 2: Size of the ground truth test set Gt,p, the generated\ntraining set Tt,p, and performance in terms of precision and\nrecall on the 38 type-property pairs.\nnumber of object types that can be open is higher than the\nones with other properties.\nExperiments with the Simulator\nWe run our approach in each testing environment for training\nthe neural network model associated to ρt,p with the training set Tt,p collected online. At each run, the agent starts in\na random position of the environment and executes 2000 iterations, where at each iteration a low-level operation (e.g.,\nmove forward of 30cm) is executed.\nTo understand how the errors of the objects detector affect\nthe performance, we propose two variants of our approach,\nnamely ND (Noisy Detections) and GTD (Ground Truth Detections). In both variants, the agent trains ρt,p on the training set Tt,p collected in a single environment, and is evaluated on the test set Gt,p previously generated in the same\nFigure 1: Pepper taking images of a laptop and asking a human to manipulate it for learning the property Folded.\nenvironment. In the ND variant, the agent is provided with\na pre-trained object detector ρo; while in the GTD variant\nthe agent is provided with a perfect ρo, i.e., the ground truth\nobject detections provided by ITHOR. In both variants, the\nneural networks ρt,p’s are trained for 10 epochs with 1e−4\nlearning rate; the other hyperparameters are set to the default\nvalues provided by PyTorch1.9 (Paszke et al. 2019).\nExperimental results\nWe compare the versions ND and\nGTD for each learned property; the results are shown in Table 2. In particular, the columns of Table 2 contain the object type, the number of examples collected in the training\nand test sets, respectively Gt,p and Tt,p, the metrics precision and recall averaged over all 20 environments. It is worth\nnoting that the size of the test set can vary among ND and\nGTD, since we remove from the test set the object types\nthat are missing in the training set, i.e., the object types that\nhave not been observed by the agent. This is because we are\ninterested in evaluating the learning performance on the object types that the agent actually manipulates and observes.\nMoreover, there are particular object types (e.g., desktop and\nshowerhead in Table 2) that are never recognized by the object detector, hence they are missing in the training set, and\nthey are assigned the ‘-’ value in Table 2.\nTable 2 shows the results obtained for learning properties\nDirty, Filled, Is Open, and Toggled. Not surprisingly,\nboth the weighted average precision and recall of the GTD\nversion are almost always higher than the ND ones, i.e., the\noverall learning performance are better when the agent is\nprovided with ground truth object detections. The recall is\ngenerally lower than the precision, this is because for almost all object types, the number of negative examples is\nhigher than the positive one, i.e., the training datasets are\nnot balanced. Therefore, the agent is more likely to predict\nthat a property is false, which causes more false negatives\nand a decrease of the recall. In our experiments, we tried\nto balance the observations in the collected training sets by\nrandomly removing positive or negative examples, but we\nobtained worse performance. More sophisticated strategies\nmight measure the information of each observation and remove the less informative ones; however, this problem is out\nof the scope of this paper.\nIn the Dirty property results, for all object types but bed,\nObject type\nProperty\nPrecision\nRecall\nbowl\nEmpty\n0.63\n0.98\nlaptop\nFolded\n0.97\n1.00\nbook\nIs Open\n1.00\n0.99\ncup\nFilled\n0.93\n0.83\nWeighted avg\n0.88\n0.95\nTable 3: Precision and recall obtained by the neural networks\npredicting object properties in a real environment.\nthe number of examples in the training set is higher for GTD,\nas expected. The examples of beds in GTD are lower because in all bedrooms the agent focused on observing other\nobject types. Indeed, for all other object types contained in\nbedrooms (i.e., cloth, mug and mirror), the examples collected by GTD are more than the ND ones.\nMoreover, for the Dirty property, the precision obtained\nby GTD is significantly higher than the ND one, for almost\nall object types (i.e., 7 out of 9). For the mirror object type,\nthe precision achieved by both ND and GTD is almost equal.\nRemarkably, for the bed object type, the precision of the\nGTD version is much lower than the ND one. This is because, for large objects such as beds, the GTD version is\nmore likely to collect examples not representative for the\nproperties to be learned. For instance, the agent provided\nwith ground truth object detections recognizes the bed even\nwhen it sees just a corner of the bed, whose image is not significant for predicting whether the bed is dirty or not. Moreover, the examples of objects of type bed in the training set\ncollected by ND is much higher than the GTD one.\nThe recall of the GTD version is not always higher than\nthe ND one. In our experiments, we noticed that, for both\nND and GTD, an high precision typically entails a low recall, and viceversa. This is because typically the agent collects more positive or negative examples of a single object\ntype. For instance, the precision achieved by ND on object\ntypes bed, cloth, mirror and mug is high and the recall is\nlow. Similarly, the recall achieved by ND on object types\nbowl, cup, pan, plate and pot is high and the precision is\nlow. The recall obtained by GTD is lower than the precision\nfor all object types but bed, where there is no significant difference. Overall, the weighted average metric values show\ngood performance, i.e., our approach is effective for learning\nto recognize properties without any dataset given a priori as\ninput. Similar considerations given for the Dirty property\napply to results obtained for properties Is Open, Toggled\nand Filled, reported in Table 2. However, it is worth noting that for the Filled property, the metric values obtained\nby both ND and GTD versions are particularly low for the\nobject types houseplant and kettle. This is because, for the\nmentioned object types, the Filled property is hard to recognize from the object images. For instance, the fact that\nan object of type kettle is filled with water cannot be recognized from its image, since the water in the kettle is not\nvisible from an external view such as the agent one. Furthermore, GTD with the object type bottle achieves 0 value of\nboth precision and recall, this is a particular situation where\nthe neural network associated to the Filled property never\npredicts false positives when evaluated on examples of objects of type bottle, hence precision and recall equals 0.\nFinally, we compared the performance of the property\npredictors learned by ND (Table 2) with a baseline where\nproperty predictors are trained on data manually collected as\nGt,p. The baseline achieves an overall precision and recall of\n0.81 and 0.84, respectively; while our approach achieves an\noverall precision and recall of 0.81 and 0.71, respectively.\nThese results show that the online learning can achieve a\nprecision comparable to the offline setting where data are\nmanually collected, while its recall gets worse as the prediction gives false negative results more often.\nReal World Demonstrator\nTo test our method in a real-world setting, we used a Softbank’s Pepper humanoid robot in PEIS home ecology (Saffiotti et al. 2008), shown in Figure 1. As an object detector, we adopted a publicly available model of YoloV5 pretrained on the MS-COCO dataset (Lin et al. 2014). For manipulation actions, Pepper asks a human to do the manipulations, due to its limited capabilities of manipulating objects.\nWe used Pepper’s speech-to-text engine for simple verbal interaction with the human. Given an object type and a property, Pepper first looks for the object and then asks the human about the property’s state. Next, it collects samples and\nasks the human to change the state of the property, and after\nhuman confirmation, it further collects samples.\nWe run experiments for learning pairs (type, property)\nreported in Table 3. For each pair, we run the experiment\n7 times with different objects of the same type. At each\nrun, Pepper collects 100 examples of the observed property,\ngrouped into 50 positive and 50 negative examples. For each\nproperty, we took 4 runs for training (i.e., 400 examples),\nand 3 runs for testing (i.e., 300 examples). Table 3 shows\nthe precision and recall obtained on the test sets. Both the\naverage precision and recall are high. For the simpler properties (i.e., Is Open and Filled), Pepper almost perfectly\nlearned to recognize them. These results demonstrate that\nour approach can be effective also in real world environments.",
        "conclusion": "We address the challenge of using symbolic planning to automate the learning of perception capabilities. We focus on\nlearning object properties, assuming that an object detector\nis pretrained. We experimentally show that our approach is\nfeasible and effective. Still a lot of work must be done to\naddress the general problem of planning and acting to learn\nin a physical environment. For example, planning for online\ntraining the object detector or learning relations among different objects. We assume that actions that change an object\nproperty can be executed without being able to fully recognize the property itself. This is feasible in a simulated environment before deploying a robot in the real world. In a real\nworld environment, where this assumption is more critical,\nwe can use our method to improve agents’ perception capabilities rather than learning them from scratch. Moreover,\nsome actions can be executed without knowing the object\nproperties, e.g., pushing a button to turn on a TV.",
        "summary_en": "Autonomous agents embedded in a physical environment need the ability to recognize objects and their properties from sensory data. Such a perceptual ability is often implemented by supervised machine learning models, which are pre-trained using a set of labelled data. In real-world, open-ended deployments, however, it is unrealistic to assume to have a pre-trained model for all possible environments. Therefore, agents need to dynamically learn/adapt/extend their perceptual abilities online, in an autonomous way, by exploring and interacting with the environment where they operate. This paper describes a way to do so, by exploiting symbolic planning. Specifically, the paper formalizes the problem of automatically training a neural network to recognize object properties as a symbolic planning problem (using PDDL). The paper uses planning techniques to produce a strategy for automating the training dataset creation and the learning process. Finally, the paper provides an experimental evaluation in both a simulated and a real environment, which shows that the proposed approach is able to successfully learn how to recognize new object properties.",
        "summary_zh": "这篇论文介绍了一种利用符号规划来解决自主代理在物理环境中学习对象属性的方法。通常，这种能力是通过监督式机器学习模型实现的，然而，在真实世界、开放式部署中，假设存在一个预先训练的模型适用于所有可能的环境是不实际的。因此，代理需要通过在环境中进行探索和交互，以自主的方式在线动态学习、适应和扩展其感知能力。本文将自动训练神经网络识别对象属性的问题形式化为符号规划问题，并利用规划技术制定了自动化训练数据集创建和学习过程的策略。最后，在模拟环境和真实环境中的实验结果表明，所提出的方法能够成功学习如何识别新的对象属性。"
    },
    {
        "title": "Diversity Maximization in the Presence of Outliers",
        "abstract": "Given a set X of n points in a metric space, the problem of diversity maximization is to extract a set S of k points from X so that the diversity of S is maximized. This problem is essential in AI-related fields, such as web search, databases, recommender systems, and data mining. Although there have been extensive studies of this problem, these studies assume that X is clean. This usually does not hold, because realworld datasets usually contain outliers. The state-of-the-art algorithm for the diversity maximization problem is based on furthest point retrieval, which is too sensitive to outliers. We therefore address the problem of diversity maximization with outliers and propose two algorithms with performance guarantee. The first algorithm runs in O((k + z)n) time, guarantees 1 2-approximation, and returns no outliers, where z is the number of outliers. The second algorithm runs in O(kz) time (which is independent of n), guarantees 1 6(1+ϵ)approximation, and returns no outliers with constant probability. We conduct experiments on real datasets to demonstrate the effectiveness and efficiency of our algorithms.",
        "introduction": "Given a set X of n points in a metric space, the problem of\ndiversity maximization is to extract a set S of k points from\nX so that the diversity of S (or dissimilarity between the\nk points in S) is maximized. This is an important problem\nin AI-related fields, such as web search (Ceccarello, Pietracaprina, and Pucci 2018), databases (Agarwal, Sintos, and\nSteiger 2020), recommender systems (Hirata et al. 2022),\nand data mining (Bauckhage, Sifa, and Wrobel 2020). In the\nabove applications, the sizes of datasets are growing, as we\nhave many sources that generate data. Because of this, analysis of these large datasets and/or building machine-learning\nmodels on them often face a challenge of efficiency. Extracting a summary, i.e., a set of representative points, from a\ngiven dataset is a promising approach to overcoming this\nchallenge, and the diversity maximization problem can output such a summary (Ceccarello, Pietracaprina, and Pucci\n2020; Moumoulidou, McGregor, and Meliou 2021; Zadeh\net al. 2017). This is because it can control the summary size,\ni.e., k, and the summary preserves the diversity of (or the\ninformation on) a given dataset as much as possible.\n(a) Max-Min diversification (b) Max-Sum diversification\nFigure 1: Difference between Max-Min and Max-Sum diversification (k = 10). Triangles are selected as solutions.\nGiven a set S of k points in X, the diversity of S is usually evaluated by an objective function. The most frequently\nused objective functions are Max-Min and Max-Sum. Given\nX and k, the Max-Min diversification problem is to select k\npoints in X so that the minimum distance between any two\ndistinct points in a solution set S is maximized. The MaxSum diversification problem is to select k points in X so that\nthe sum of the distances between all two distinct points in S\nis maximized. Figure 1 compares the k = 10 points (colored\ntriangles) selected by the Max-Min and Max-Sum diversification problems. Figure 1(b) illustrates that the Max-Sum\ndiversification problem returns S having similar points. On\nthe other hand, Figure 1(a) illustrates that the result set obtained by the Max-Min diversification problem is distributed\nuniformly in the data space. This result is better as a summary of a given dataset, so this paper considers Max-Min as\nobjective function.\nDue to the effectiveness of the Max-Min diversification\nproblem, there exist extensive works on this problem (and\nits variants) (Addanki et al. 2022; Aghamolaei, Farhadi,\nand Zarrabi-Zadeh 2015; Borassi et al. 2019; Drosou and\nPitoura 2014; Erkut, ¨Ulk¨usal, and Yenicerio˘glu 1994; Indyk et al. 2014; Moumoulidou, McGregor, and Meliou\n2021; Ravi, Rosenkrantz, and Tayi 1994; Wang, Fabbri,\nand Mathioudakis 2022). Because this problem is NP-hard,\nthese works devised error-bounded approximation algorithms. The state-of-the-art algorithm for the Max-Min diversification problem is GMM. (Section 2.2 introduces this\n(a) GMM\n(b) GREEDY\nFigure 2: Result sets obtained by GMM (existing algorithm)\nand GREEDY (one of our algorithms) when k = 10\nalgorithm in detail.) Given a set X of n points in a metric space, GMM returns a 1\n2-approximation result in O(kn)\ntime. It is known that, unless P = NP, this bound is tight, i.e.,\ncannot be improved in polynomial time.\nOne main issue of the above existing works is their assumption: a given dataset X is clean, i.e., X contains no outliers. This usually does not hold, because real-world datasets\nusually contain outliers that exist far from the other points\n(Bhaskara, Vadgama, and Xu 2019; Dahiya et al. 2021; Im\net al. 2020; Wang, Guo, and Ding 2021). Unfortunately, the\nexisting algorithms for the Max-Min diversification problem are sensitive to outliers. For example, GMM is based\non furthest point retrieval, and the furthest point is usually\nan outlier. Figure 2(a) illustrates a result set S (consisting\nof red triangles) obtained by GMM. The points located in\nthe center are the same as those in Figure 1, and the other\npoints are outliers. GMM is clearly sensitive to the presence of outliers, as the points in S are dominated by outliers. Another state-of-the-art algorithm (Borassi et al. 2019)\nalso has a similar case, which is presented in Section 5. This\ndemonstrates that simply running an existing algorithm on\nX having outliers does not function. In addition, for example, when training a machine learning model through S,\nwhich is obtained by one of the above algorithms and contains outliers, we face “garbage in, garbage out.”\nOur Contributions. Motivated by the above observations,\nwe address the problem of Max-Min diversification with\noutliers. If we can identify outliers, it is possible to remove\nthem from X and then GMM is done on a set of the remaining points in X. A straightforward approach to identifying\nthe outliers requires the evaluation of each point in X. This\napproach, however, does not scale to n, as it incurs O(n2)\ntime, as shown in Section 2.3. Therefore, to scale well to n,\nan algorithm, which runs in at most linear time to n with\napproximation guarantee, is required. Designing such an algorithm is a non-trivial task and is challenging.\nWe overcome this challenge and propose two novel algorithms. Although they are simple, analysing their performances is not trivial. The main contributions of this paper\nare as follows:\n• We tackle the problem of Max-Min diversification with\noutliers, for the first time.\n• We propose GREEDY (cf. Theorem 1), an O((k + z)n)\ntime algorithm that guarantees a 1\n2-approximation and returns no outliers (z is the number of outliers), as Figure\n2(b) illustrates.\n• We propose CORESET (cf. Theorem 2), an O(kz) time\nalgorithm that guarantees a\n1\n6(1+ϵ)-approximation and returns no outliers with a constant probability (under a reasonable assumption), where ϵ < 1 is a small constant.\n• We conduct experiments using real datasets and demonstrate that our algorithms are much faster than a baseline\none while preserving a competitive diversity. For example, CORESET is three to four orders of magnitude faster\nthan the baseline algorithm.",
        "preliminary": "2.1\nProblem Definition\nLet X be a set of n points in a metric space. We use\ndist(x, x′) to denote the distance between x and x′. We assume that dist(·, ·) satisfies the identity of indiscernibles,\nsymmetry, and triangle inequality. Furthermore, we assume\nthat dist(·, ·) can be evaluated in O(1) time. We define\ndist(x, X′), i.e., the distance between a point x and a set\nX′, as minx′∈X′ dist(x, x′).\nFor ease of presentation, let us first consider that X contains only inliers (non-outliers). The problem of Max-Min\ndiversification is to select k points in X so that the minimum\ndistance between the k points is maximized. Formally,\nDEFINITION 1 (MAX-MIN DIVERSIFICATION WITHOUT\nOUTLIERS PROBLEM). Given a set X of points and an integer k ≥ 2, this problem is to compute S∗ such that\nS∗ = arg max\nS⊆X,|S|=k\nmin\nx,x′∈S dist(x, x′).\n(1)\nWe use div(S) to denote minx,x′∈S dist(x, x′). It has been\nproven that (i) this problem is NP-hard and (ii) no polynomial time algorithms can return a solution with an approximation factor better than\n1\n2 unless P = NP (Ravi,\nRosenkrantz, and Tayi 1994).\nNow consider that X contains outliers, so X = Xin ∪\nXout, where Xin (Xout) is a set of inliers (outliers) in X.\nOur problem is to obtain S∗ on X\\Xout.\nDEFINITION 2 (MAX-MIN DIVERSIFICATION WITH OUTLIERS PROBLEM). Given a set X of points and an integer\nk ≥ 2, this problem is to compute S∗ such that\nS∗ =\narg max\nS⊆X\\Xout,|S|=k\nmin\nx,x′∈S dist(x, x′).\n(2)\nWe use l∗ to denote div(S∗). This problem is also NP-hard\ntrivially, so this paper considers approximation algorithms.\nTo solve this problem, this paper puts the following assumptions.\nASSUMPTION 1. We have |Xout| = z.\nASSUMPTION 2. For each x\n∈\nXout, we have (i)\ndist(x, X\\{x}) > dist(x′, X\\{x′}) for every x′ ∈ Xin\nand (ii) dist(x, X\\{x}) > αl∗, where α ≥ 1 is a sufficiently large constant.\nAssumption 1 is the same as that in works of k-clustering\nwith outliers (Bhaskara, Vadgama, and Xu 2019; Ceccarello, Pietracaprina, and Pucci 2019; Ding, Yu, and Wang\n2019; Im et al. 2020). The first condition in Assumption 2 is\nderived from the problem of distance- or nearest neighborbased outlier detection (Amagata, Onizuka, and Hara 2021,\n2022), which has empirically good performance (Campos\net al. 2016; Gu, Akoglu, and Rinaldo 2019). This assumption is also essentially similar to that held by the problem of\nk-clustering with outliers. The second condition in Assumption 2 is natural for the problem of Max-Min diversification\nwith outliers. This is because, if dist(x, X\\{x}) ≤ l∗, including an outlier x in S cannot be seen as unusual, and\neven the optimal solution S∗ can contain x, which contradicts Definition 2. This therefore justifies the validity of the\nsecond condition in Assumption 2. Our theoretical analyses\nuse the above assumptions.\n2.2\nGMM\nWe introduce GMM (Ravi, Rosenkrantz, and Tayi 1994), a\nstate-of-the-art algorithm for the problem of Max-Min diversification without outliers (Definition 1), because this is\na building block for our techniques. GMM initializes a solution set S by a random point in X. Then, it computes the\nfurthest point from S, denoted by x∗, i.e.,\nx∗ = arg max\nx∈X\\S\ndist(x, S),\n(3)\nand x∗ is added into S. This is repeated until |S| = k. Algorithm 1 summarizes GMM and has the following facts\n(Ravi, Rosenkrantz, and Tayi 1994).\nFACT 1. Algorithm 1 runs in O(kn) time and returns a 1\n2approximate result for the problem of Max-Min diversification without outliers, i.e., div(S) ≥ div(S∗)\n2\n.\nFACT 2 (ANTICOVER PROPERTY). S ← GMM(X, k) has\nthe following properties: (i) ∀x ∈ S, dist(x, S\\{x}) ≥\ndiv(S) and (ii) ∀x ∈ X, dist(x, S) ≤ div(S).\n2.3\nBaseline Algorithm\nSince this is the first work on the problem of Max-Min diversification with outliers, we first consider how to solve this\nproblem by employing existing techniques. Assumption 2\nsuggests that, if we run a nearest neighbor search for each\npoint in X, we can identify the z outliers. After removing\nthese outliers from X in this way, we have Xin. Therefore,\nby running GMM(Xin, k), we can obtain a 1\n2-approximate\nresult for the problem of Max-Min diversification with outliers. Algorithm 2 summarizes this baseline algorithm.\nAlthough this baseline has a theoretical approximation\nguarantee, its worst-case running time is O(n2), since it runs\na nearest neighbor search for every point in X. The practical\ntime of this algorithm can be alleviated by using some data\nstructure for nearest neighbor search in metric space, but this\nis still slow for large n.\n2.4\nRelated Work\nThe diversity maximization problem has been extensively\nstudied since the 1990s, as it outputs a succinct and effective subset of a given dataset. Particularly recently, diversity\nAlgorithm 1: GMM(X, k)\n1 S ← a random point in X\n2 while |S| < k do\n3\nx∗ = arg max\nx∈X\\S\ndist(x, S)\n4\nS ← S ∪ {x∗}\n5 return S\nAlgorithm 2: BASELINE(X, k, z)\n1 Xout ← z points with the largest distance to their\nnearest neighbor in X\n2 return S ← GMM(X\\Xout, k)\nmaximization under some constraint has been considered to\nsatisfy observations or requirements in the real world (Addanki et al. 2022; Ceccarello, Pietracaprina, and Pucci 2018,\n2020; Moumoulidou, McGregor, and Meliou 2021; Wang,\nFabbri, and Mathioudakis 2022). The presence of outliers,\nhowever, has not been considered for the diversity maximization problem.\nMax-Sum Diversification. This problem is also NP-hard,\nand efficient algorithms with bounded error guarantee were\ndeveloped. A 1\n2-approximation algorithm was presented in\n(Borodin et al. 2017). A MapReduce algorithm was developed in (Ceccarello et al. 2017); it needs to assume a\nbounded doubling dimension. There are some works that\nconsider constraints. For example, matroid constraint was\nconsidered in (Ceccarello, Pietracaprina, and Pucci 2018,\n2020), whereas the work in (Zhang and Gionis 2020) considers clustered data.\nMax-Min Diversification. Some other works, e.g., (Amagata and Hara 2019), also employ this objective function,\nand GMM provides a good solution for them. There are\nworks (Amagata and Hara 2016; Drosou and Pitoura 2014)\nthat consider how to deal with dynamic X. Fairness constraint has recently been considered in (Addanki et al. 2022;\nMoumoulidou, McGregor, and Meliou 2021; Wang, Fabbri,\nand Mathioudakis 2022). The algorithms proposed in these\nworks were extended from the algorithms for Max-Min diversification with no constraint (Ravi, Rosenkrantz, and Tayi\n1994; Borassi et al. 2019).",
        "outlier-aware greedy algorithm": "The main drawback of Algorithm 2 is its quadratic time to\nn, which is not scalable for a large n. Recall that this time\nis derived from running a nearest neighbor search for every\npoint in X. To improve the efficiency, we need to theoretically reduce the number of candidates for outliers, but this\nis not a trivial challenge. We overcome this challenge and\nprove that we can identify the z outliers without running a\nnearest neighbor search for every point in X. Our idea here\nis to leverage the sensitivity of GMM to outliers.\nLEMMA 1. Let S′ be the output of GMM(X, k + z), and S′\ncontains the z outliers.\nAlgorithm 3: GREEDY(X, k, z)\n1 S′ ← GMM(X, k + z)\n2 Z ← z points with the largest distance to their\nnearest neighbor in S′\n3 return S ← GMM(X\\Z, k)\nPROOF. Recall that GMM iteratively computes x∗, see\nEquation (3). Let x∗\ni be x∗ at the i-th iteration. Also, let Si\nbe S before x∗\ni is inserted. It is important to notice that\ndist(x∗\ni+1, Si+1) ≤ dist(x∗\ni , Si)\n(4)\nThis means that, as the size of S grows, div(S) decreases.\nNow assume that S′ contains only z′ ≤ z − 1 outliers.\nWe have l∗ = div(S∗) ≥ div(S′\\Xout), because we have\nk + 1 ≤ |S′\\Xout| ≤ k + z and Equation (4). Notice that\nX\\S′ has z − z′ outliers, and each of these outliers, say x,\nhas dist(x, X\\{x}) > αl∗. This contradicts Fact 2, so S′\nmust have z outliers.\n□\nFrom this lemma, we can reduce the number of candidates\nfor the outliers from n points to only k + z points. This enables to design a linear time algorithm for the problem of\nMax-Min diversification with outliers.\nAlgorithm 3 describes our first algorithm GREEDY. It first\nruns GMM(X, k + z) to obtain a set S′ of k + z candidate\npoints for the outliers. Then, it computes the nearest neighbor for each x ∈ S′ to identify the z outliers. After that, it\nruns GMM(X\\Z, k), where Z is a set of the z points. We\nintroduce the main result of this section below.\nTHEOREM 1. Algorithm 3 runs in O((k + z)n) time and\nreturns a 1\n2-approximate result, which has no outliers, for\nthe problem of Max-Min diversification with outliers.\nPROOF. From Fact 1, S′\n← GMM(X, k + z) runs in\nO((k + z)n) time. Identifying the z outliers from S′ needs\nO((k+z)n) time, as |S′| = k+z. It is straightforward to see\nthat GMM(X\\Z, k) runs in O(k(n − z)) time. Therefore,\nAlgorithm 3 runs in O((k + z)n) time.\nLemma 1 shows that the z outliers are included in S′, so\nX\\Z contains no outliers. From this observation and Fact 1,\nwe have div(S) ≥ div(S∗)\n2\n.\n□",
        "coreset-based algorithm with probable success guarantee": "This section proves that there exists an algorithm which does\nnot have a factor of n as its time complexity with sacrifice\nin a success probability (the probability that S contains no\noutliers) a bit. This algorithm is based on a coreset, a good\nsummary of X informally (its definition is introduced later,\nsee Definition 3). Note that the coreset is constructed offline.\nFor ease of presentation, we first devise an outlier-robust\nonline algorithm in Section 4.1. Then, Section 4.2 explains\nhow to construct a coreset. After that, Section 4.3 introduces\nour main algorithm in this section.\n4.1\nOnline Algorithm\nFor now, this section assumes that X′ ⊆ X is given for an\nonline algorithm. We prove that our online algorithm in this\nsection is linear only to k and |X′|. The main idea of making\nthis algorithm robust to outliers is to select a result point that\nis not near S but not too far from S, which is different from\nthe idea of GMM. To implement this idea, we use a guess\nof l∗ = div(S∗), denoted by ˆl.\nGuessing l∗. Let S′ be the set of k + z points obtained by\nGMM(X, k + z). Below, we show that div(S′) ≥ div(S∗)\n2\n,\nwhere S∗, such that |S∗| = k, is the optimal solution for the\nproblem of Max-Min diversification with outliers. This is\nnot a trivial result, because (i) S′ contains the z outliers (see\nTheorem 1), (ii) |S′| = k + z ̸= |S∗|, and S′\\Xout is not\nguaranteed to be the same as the output of GMM(Xin, k).\nCOROLLARY 1. Given S′ ← GMM(X, k + z), we have\ndiv(S′) ≥ div(S∗)\n2\n.\nPROOF. To prove this corollary, it is sufficient to demonstrate that x∗ in Equation (3) has dist(x∗, S′) ≥ l∗\n2 — (⋆).\n(Recall that S does not contain x∗ at the corresponding iteration.) When x∗ is an outlier, dist(x∗, S′) > l∗, so (⋆)\nholds. When x∗ is an inlier, we show that (⋆) holds by extending the proof of Theorem 2 in (Ravi, Rosenkrantz, and\nTayi 1994).\nAssume that S∗\n=\n{s∗\n1, ..., s∗\nk}. Let B∗\ni\n=\n{x\n∈\nX | dist(s∗\ni , x)\n<\nl∗\n2 }, and notice that B∗\ni contains at\nleast s∗\ni . In addition, the proof of Theorem 2 in (Ravi,\nRosenkrantz, and Tayi 1994) demonstrates that B∗\ni ∩ B∗\nj =\n∅ for i ̸= j. It is also important to notice that all outliers in X do not belong to S\nk B∗\ni . Now consider the jth iteration of GMM(X, k + z). In this iteration, S′ contains at most k − 1 inliers. Hence, for some i ∈ [1, k],\nwe have S′ ∩ B∗\ni = ∅. The definition of B∗\ni derives that\ndist(s∗\ni , S′) ≥\nl∗\n2 . That is, there exists at least one inlier\nx ∈ X\\S′ such that dist(x, S′) ≥ l∗\n2 . From this, when x∗\nis an inlier, (⋆) still holds.\n□\nConsequently, we have l∗ ∈ [div(S′), 2div(S′)]. By setting ˆl = (1 + ϵ)idiv(S′) for i ∈ [0, log 2], where ϵ < 1 is a\nsmall constant, we obtain ˆl =\nl∗\n1+ϵ. Recall that this guessing\nis done offline1, and we later show that div(S′) is obtained\nas a side product of coreset construction, see Remark 2.\nAlgorithm Description. Algorithm 4 shows the online algorithm. As with GMM, it first adds a random point in the\ninput set X′ ⊆ X into a temporary solution set Stemp. Then,\ngiven ˆl (a guess of l∗), it scans the input set X′. During this,\nif a given point x ∈ X′ has\nˆl\n2 ≤ dist(x, Stemp) ≤ ˆl, x\nis added into Stemp. This algorithm stops the scan when\n|Stemp| = k. This is repeated for each ˆl, and this algorithm\nfinally returns the solution set with the best diversity.\nLEMMA 2. Algorithm 4 runs in O(k|X′|) time. In addition,\nit returns no outliers and guarantees\n1\n2(1+ϵ)-approximation\nwith probability at least 1−\nz\n|X′| for the problem of Max-Min\ndiversification with outliers, if α ≥ 2.\n1This is common in (Addanki et al. 2022; Bhaskara, Vadgama,\nand Xu 2019; Ceccarello et al. 2017; Ding, Yu, and Wang 2019; Im\net al. 2020; Moumoulidou, McGregor, and Meliou 2021).\nAlgorithm 4: STREAMING(X′, k)\n1 S ← ∅\n2 for each ˆl (a guess of l∗) ∈ L do\n3\nStemp ← a random point in X′\n4\nfor each x ∈ X′ s.t.\nˆl\n2 ≤ dist(x, Stemp) ≤ ˆl do\n5\nStemp ← Stemp ∪ {x}\n6\nif |Stemp| = k then\n7\nbreak\n8\nif (|Stemp| = k) ∧ (div(S) < div(Stemp)) then\n9\nS ← Stemp\n10 return S\nPROOF. Given ˆl, we have |Stemp| ≤ k and the number of\naccessed points in X′ is at most |X′|, as Algorithm 4 scans\nX′ once. Since the number of guesses is at most log 2 =\nO(1), the time complexity of Algorithm 4 is O(k|X′|).\nRecall that Algorithm 4 selects a point x ∈ X′ such that\nˆl\n2 ≤ dist(x, Stemp). For ˆl =\nl∗\n1+ϵ, it is straightforward to\nsee that Algorithm 4 returns S such that div(S) ≥\nl∗\n2(1+ϵ).\nAssume that x1 ∈ X′ is firstly added into S, and x1 is\nan inlier with probability at least |X′|−z\n|X′| . Next, let B(x, ˆl)\nbe a ball centered at x ∈ S with radius ˆl. If B(x, ˆl) contains no outliers, only inliers can be added into S. When\nlgreedy = l∗, ˆl is at most 2l∗. Since each outlier x′ ∈ Xout\nhas dist(x′, X\\{x′}) > αl∗, B(x, 2l∗) contains no outliers\nif α ≥ 2. To summarize, as long as the first point in S is an\ninlier, S certainly contains only inliers if α ≥ 2. Now we\ncomplete the proof of Lemma 2.\n□\nREMARK 1. Recall that α is sufficiently large (see Section\n2.1): outliers are significantly different to the others usually.\nAssuming α ≥ 2 is therefore still reasonable.\n4.2\nCoreset Construction: Offline Processing\nTo start with, we formally define coreset below.\nDEFINITION 3 (CORESET). A set C ⊆ X is a β-coreset, if\nwe have S ⊆ C such that div(S) ≥ div(S∗)\nβ\n, where |S| =\n|S∗| = k.\nIn (Indyk et al. 2014), the following fact is demonstrated\n(see its Lemma 1).\nFACT 3. When X contains no outliers, GMM yields a 3coreset for the problem of Max-Min diversification.\nNote that this bound is shown to be tight in (Aghamolaei,\nFarhadi, and Zarrabi-Zadeh 2015).\nImportantly, the existing work (Indyk et al. 2014) proves\nthat, if Y ⊆ X satisfies the anticover property (see Fact 2),\nY is a 3-coreset for the problem of Max-Min diversification.\nWe use this observation to prove the following.\nLEMMA 3. GMM(X, k+z) returns a 3-coreset for the problem of Max-Min diversification with outliers.\nPROOF. From the proof of Lemma 1, C ← GMM(X, k+z)\nAlgorithm 5: CORESET(X, k, z)\n1 /* Offline processing */\n2 C ← GMM(c) where c = O(z) and c ≥ k + z\n3 /* Online processing */\n4 return S ← STREAMING(C, k)\ncontains the z outliers. To prove Lemma 3, we show that\nC\\Xout has the anticover property for any inlier in X.\nFor any inlier x ∈ C, we trivially have dist(x, C\\{Xout∪\n{x}}) ≥ div(C\\Xout). Also, for any inlier x ∈ C,\nwe trivially have dist(x, C\\Xout) = 0 ≤ div(C\\Xout).\nWe therefore focus on each inlier x′ ∈ X\\C and consider whether x′ has dist(x′, C\\Xout) ≤ div(C\\Xout).\nAssume that arg minx∈C dist(x, x′) is an outlier. Let this\noutlier be xout, and dist(x′, xout) > αl∗. This contradicts Corollary 1, so arg minx∈C dist(x, x′) must be an\ninlier. This means that dist(x′, C\\Xout) = dist(x′, C).\nFrom Fact 2, dist(x′, C)\n≤\ndiv(C). Now notice that\ndiv(C) = div(C\\Xout). These observations derive the\nfact that dist(x′, C\\Xout) ≤ div(C\\Xout) for any inlier\nx′ ∈ X\\C.\n□\nREMARK 2. From (Aghamolaei, Farhadi, and ZarrabiZadeh 2015), this bound is also tight. In addition, as\nGMM(X, k + z) is used to construct a coreset C, we have\nl∗\n2 ≤ div(C) ≤ l∗ from Corollary 1. Recall that Algorithm\n4 requires a guess of l∗, and div(C) is used for guessing.\n4.3\nPutting It All Together\nNow we are ready to introduce our final algorithm CORESET, which is described in Algorithm 5. It constructs a coreset C offline. When computing a solution set S, it runs\nSTREAMING(C, k).\nWe below introduce the main result of this section: the\ntime complexity of STREAMING(C, k) is independent of n\nwhile guaranteeing an error bound and success probability.\nTHEOREM 2. Given a coreset C built by GMM(X, c) where\nc = O(z) and c ≥ k + z, STREAMING(C, k) runs in\nO(kz) time. In addition, it returns no outliers and guarantees\n1\n6(1+ϵ)-approximation for the problem of Max-Min diversification with outliers, with at least a constant probability, if α ≥ 2.\nPROOF. From Lemma 2, it is trivial to see that STREAMING(C, k) runs in O(kz) time for |C| = O(z). Also, C ←\nGMM(X, k + z) derives the\n1\n6(1+ϵ)-approximation bound,\nwhich is seen from Lemmas 2 and 3. Given a fixed (constant)\nsuccess probability p, we have\n1 − z\n|C| = p ⇔ |C| =\nz\n1 − p = O(z).\n(5)\nThe above discussions complete the proof.\n□\nREMARK 3. The above theorem assumes that\nz\n1−p > k + z.\nThis holds when p is sufficiently large (e.g., p ≥ 0.9) and z is\nnot too small (i.e., a standard setting). If we do not have this\ncase, |C| = O(k+z) and CORESET needs O(k(k+z)) time\n(while the probable approximation guarantee still holds).\nREMARK 4. A coreset C is available for any k such that\n|C| ≥ k + z. Therefore, the offline processing can be done\nonce for such k, i.e., this offline processing is not unique for\na specific value of k.",
        "experiment": "All experiments were conducted on a Ubuntu 20.04 LTS machine equipped with Xeon Platinum 8268 CPU@2.90GHz\nand 768GB RAM.\nDataset. We used the following real datasets2.\n• FCT: a set of 10-dimensional cartographic variables for\nforest cover type, and n = 580, 812.\n• Household: a set of 7-dimensional sensor readings, and\nn = 2, 049, 280.\n• KDD99: a set of 16-dimensional packet records, and n =\n311, 029.\n• Mirai: a set of 115-dimensional Mirai malware infected\nnetwork capture data, and n = 764, 137.\nWe normalized each dataset so that its domain of each dimension was [0, 100] to have the same scale. After this, we\ninjected z outliers into a given dataset, as with (Bhaskara,\nVadgama, and Xu 2019; Ceccarello, Pietracaprina, and\nPucci 2019; Ding, Yu, and Wang 2019; Im et al. 2020). We\nused Euclidean distance for these datasets.\nAlgorithm. We evaluated the following algorithms.\n• GMM (Ravi, Rosenkrantz, and Tayi 1994): a\n1\n2approximation algorithm for Max-Min diversification\nwithout outliers.\n• PODS19 (Borassi et al. 2019): a ( 1\n5 − ϵ)-approximation\nalgorithm for Max-Min diversification without outliers.\n• BASELINE: the 1\n2-approximation algorithm for Max-Min\ndiversification with outliers (Algorithm 2).\n• GREEDY: our 1\n2-approximation algorithm for Max-Min\ndiversification with outliers (Algorithm 3).\n• STREAMING: our\n1\n2(1+ϵ)-approximation algorithm for\nMax-Min diversification with outliers (Algorithm 4 with\nX as its input).\n• CORESET: our\n1\n6(1+ϵ)-approximation algorithm for MaxMin diversification with outliers (Algorithm 5).\nWe set ϵ = 0.01. For BASELINE and GREEDY, we employed\na VP-tree (Yianilos 1993) to retrieve the nearest neighbor\npoint, because it is one of the most efficient data structure for\nmetric spaces (Chen et al. 2017). For CORESET, we set the\ncoreset size so that the success probability was 0.95. All algorithms were implemented in C++, compiled by g++ 9.4.0\nwith -O3 flag, and single threaded. Source codes of our algorithms are available3.\nParameter Setting. We set k = 100 and z = 200 by default. This setting of z is similar to those in the evaluation\npaper (Campos et al. 2016) and in the experiments using\nlarge datasets (Ceccarello, Pietracaprina, and Pucci 2019;\n2https://archive.ics.uci.edu/ml/datasets.php\n3https://github.com/amgt-d1/Max-Min-w-Outliers\nAlgorithm\nFCT\nHousehold\ndiv(S)\nTime\ndiv(S)\nTime\nBASELINE\n51.514\n312.489\n38.999\n391.429\nGREEDY\n51.514\n2.348\n38.999\n6.962\nSTREAMING\n49.614\n1.874\n37.374\n5.165\nCORESET\n50.158\n0.005\n38.369\n0.006\nTable 1: Average div(S) and running time [sec] (k = 100\nand z = 200) on FCT and Household\nAlgorithm\nKDD99\nMirai\ndiv(S)\nTime\ndiv(S)\nTime\nBASELINE\n80.281\n360.946\n113.460\n485.57\nGREEDY\n80.281\n2.135\n113.460\n31.046\nSTREAMING\n79.996\n1.574\n95.439\n20.955\nCORESET\n77.064\n0.009\n106.352\n0.098\nTable 2: Average div(S) and running time [sec] (k = 100\nand z = 200) on KDD99 and Mirai\nAlgorithm\nFCT\nHousehold\nKDD99\nMirai\nBASELINE\n311.955\n390.577\n360.585\n477.996\nGREEDY\n1.814\n5.256\n1.774\n23.474\nTable 3: Average time to identify z outliers [sec]\nGupta et al. 2017). When studying the impact of k (resp.\nz), the value of z (resp. k) was fixed. We ran each algorithm\n20 times and report the average result.\nGMM and PODS19 are not appropriate. When S contains outliers, div(S) tends to be large, which is trivial from\nAssumption 2. However, such S is meaningless, as demonstrated in Figure 2(a). We hence investigated how many outliers were included in S.\nWe found that 99% (at least 84%) points in S returned\nby GMM (PODS19) are outliers, suggesting that they do\nnot yield a meaningful result. We therefore did not consider\nGMM and PODS19 in the subsequent experiments. Note\nthat the other algorithms did not include any outliers in S.\nComparison with BASELINE. We compare our algorithms\nwith BASELINE by using the default parameter setting. Tables 1 and 2 show their div(S) and running time.\nAs BASELINE and GREEDY run GMM on Xin, they return the same S, so their div(S) is the same. However, their\nrunning times are totally different, and GREEDY is at least\none order of magnitude faster than BASELINE. Table 3 clarifies why we have this result and the efficacy of the outlier\nidentification approach of GREEDY.\nSTREAMING and CORESET yield a diverse set competitive with that of GREEDY. In addition, CORESET is significantly faster than the other algorithms. For example, CORESET is up to 67,000 times faster than BASELINE, showing\nthe efficacy of coreset even in the presence of outliers.\nBASELINE\nSTREAMING\nCORESET\nGREEDY\n0\n10\n20\n30\n40\n50\n60\n70\n50\n100\n150\n200\ndiv(S)\nk (FCT)\n(a) FCT (div)\n0\n10\n20\n30\n40\n50\n60\n50\n100\n150\n200\ndiv(S)\nk (Household)\n(b) Household (div)\n0\n20\n40\n60\n80\n100\n120\n50\n100\n150\n200\ndiv(S)\nk (KDD99)\n(c) KDD99 (div)\n0\n20\n40\n60\n80\n100\n120\n140\n160\n50\n100\n150\n200\ndiv(S)\nk (Mirai)\n(d) Mirai (div)\n50\n100\n150\n200\n10−3\n10−2\n10−1\n100\n101\n102\n103\nk (FCT)\nRunning time [sec]\n(e) FCT (time)\n50\n100\n150\n200\n10−3\n10−2\n10−1\n100\n101\n102\n103\nk (Household)\nRunning time [sec]\n(f) Household (time)\n50\n100\n150\n200\n10−3\n10−2\n10−1\n100\n101\n102\n103\nk (KDD99)\nRunning time [sec]\n(g) KDD99 (time)\n50\n100\n150\n200\n10−2\n10−1\n100\n101\n102\n103\nk (Mirai)\nRunning time [sec]\n(h) Mirai (time)\nFigure 3: Impact of k (best viewed in color)\nImpact of k. From the problem definition, it is trivial that\ndiv(S) decreases as k increases. Figures 3(a)–3(d) illustrate\nthis result and show that the relationship between the algorithms does not change for different k w.r.t. div(S). Figures\n3(e)–3(h) show the running times of the four algorithms. As\nBASELINE needs O(n2) time, its running time is stable. On\nthe other hand, the running times of the other algorithms are\nlinear to k, so the times increase as k increases. Since CORESET outperforms the other algorithms with a large margin, it\nis easy to imagine that CORESET can compute a solution\nmuch faster than them even when k is a larger scale.\nImpact of z. Figures 4(a)–4(d) observe that div(S) is robust\nagainst z, as they do not include the z outliers in S. Figures 4(e)–4(h) show that the running times of BASELINE and\nSTREAMING are generally stable, whereas those of GREEDY\nand CORESET are linear to z. This result is consistent with\ntheir time complexities.\nBASELINE\nSTREAMING\nCORESET\nGREEDY\n0\n10\n20\n30\n40\n50\n60\n100\n200\n400\n800\n1600\ndiv(S)\nz (FCT)\n(a) FCT (div)\n0\n10\n20\n30\n40\n50\n100\n200\n400\n800\n1600\ndiv(S)\nz (Household)\n(b) Household (div)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n200\n400\n800\n1600\ndiv(S)\nz (KDD99)\n(c) KDD99 (div)\n0\n20\n40\n60\n80\n100\n120\n100\n200\n400\n800\n1600\ndiv(S)\nz (Mirai)\n(d) Mirai (div)\n0\n200 400 600 800 1000120014001600\n10−3\n10−2\n10−1\n100\n101\n102\n103\nz (FCT)\nRunning time [sec]\n(e) FCT (time)\n0\n200 400 600 800 1000120014001600\n10−3\n10−2\n10−1\n100\n101\n102\n103\nz (Household)\nRunning time [sec]\n(f) Household (time)\n0\n200 400 600 800 1000120014001600\n10−3\n10−2\n10−1\n100\n101\n102\n103\nz (KDD99)\nRunning time [sec]\n(g) KDD99 (time)\n0\n200 400 600 800 1000120014001600\n10−2\n10−1\n100\n101\n102\n103\nz (Mirai)\nRunning time [sec]\n(h) Mirai (time)\nFigure 4: Impact of z (best viewed in color)",
        "conclusion": "This paper addressed the problem of Max-Min diversification with outliers for the first time, motivated by (i) the usefulness of the Max-Min diversification problem in many applications and (ii) the fact that real-world datasets usually\ncontain outliers. Existing algorithms for Max-Min diversification without outliers cannot be effective when outliers exist. We hence proposed two effective and efficient algorithms\nwith theoretical performance guarantee for the problem of\nMax-Min diversification with outliers. Our experimental results demonstrate their effectiveness and efficiency.\nThis paper has an assumption for outliers, and how to extend our algorithms for different assumptions is one of future works. Moreover, when points in X have demographic\ngroups, fairness constraint is often considered, as introduced\nin Section 2.4. Addressing a fair case of our problem remains an open issue.",
        "summary_en": "Given a set X of n points in a metric space, the problem of diversity maximization is to extract a set S of k points from X so that the diversity of S is maximized. This problem is essential in AI-related fields, such as web search, databases, recommender systems, and data mining. Although there have been extensive studies of this problem, these studies assume that X is clean. This usually does not hold, because real-world datasets usually contain outliers. The state-of-the-art algorithm for the diversity maximization problem is based on furthest point retrieval, which is too sensitive to outliers. This paper therefore addresses the problem of diversity maximization with outliers and propose two algorithms with performance guarantee. The first algorithm runs in O((k+z)n) time, guarantees 1/2-approximation, and returns no outliers, where z is the number of outliers. The second algorithm runs in O(kz) time (which is independent of n), guarantees 1/6(1+epsilon)-approximation, and returns no outliers with constant probability. The paper conducts experiments on real datasets to demonstrate the effectiveness and efficiency of the algorithms.",
        "summary_zh": "这篇论文研究了如何在存在异常值的情况下最大化多样性。传统的算法都假设数据集是干净，然而这一假设往往不成立，并且目前最先进的算法对异常值过于敏感，针对这些情况，本文提出了两种具有性能保证的算法。第一种算法的时间复杂度为O((k+z)n)，保证了1/2的近似度且不会返回任何异常值；第二种算法的时间复杂度为O(kz)，保证了1/6(1+epsilon)的近似度并以常数概率不返回异常值。作者在真实数据集上进行了实验，展示了算法的有效性和效率。"
    }
]