In the realm of Word Sense Disambiguation (WSD), two innovative approaches have emerged to tackle the inherent challenges of data imbalance and the nuanced distinctions between word meanings. The first approach draws inspiration from quantum mechanics, proposing a representation method in Hilbert space to effectively identify Long-Tail Senses (LTSs) by leveraging the concept of superposition states. This method aims to reduce reliance on large training datasets by theoretically proving and empirically verifying its effectiveness in standard WSD evaluation frameworks, achieving state-of-the-art performance. On the other hand, the second approach, SensEmBERT, integrates the expressive power of neural language models with the vast knowledge contained in semantic networks to generate high-quality latent semantic representations of word meanings across multiple languages. By positioning these vectors in a space comparable to that of contextualized word embeddings, SensEmBERT facilitates the linking of word occurrences to their meanings through a simple nearest neighbour approach, achieving or surpassing state-of-the-art results in English WSD tasks and outperforming existing methods in multilingual datasets without relying on manual semantic annotations. Together, these studies underscore the potential of leveraging both quantum-inspired representation methods and knowledge-based approaches to enhance the accuracy and efficiency of WSD tasks across languages, highlighting a promising direction for future research in leveraging diverse methodologies to address the challenges of data imbalance and semantic precision in natural language processing.