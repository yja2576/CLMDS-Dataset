In the evolving landscape of neural machine translation (NMT), significant advancements have been made to bridge the performance gap between non-autoregressive (NAT) and autoregressive (AT) translation models, while also enhancing inference speed. The ReorderNAT framework and the LaNMT model are at the forefront of these innovations. ReorderNAT effectively tackles the multimodality problem in NAT by integrating reordering information into the decoding process, which has been shown to elevate translation quality to levels comparable with AT models across various datasets. Similarly, LaNMT leverages a latent-variable mechanism with a deterministic inference procedure that not only adapts the translation length during inference but also optimizes a lower bound to the log-probability of the target sequence, achieving up to 12.5x faster decoding speeds while closely matching the performance of AT models. Building on these advancements, this paper introduces DSLP, a model that further refines the efficiency and performance of machine translation. DSLP, a non-autoregressive Transformer model, incorporates Deep Supervision and additional Layer-wise Predictions, setting a new benchmark in the field. Extensive experiments conducted on four translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO) demonstrate that DSLP not only consistently improves BLEU scores compared to base models but also outperforms the autoregressive model on three translation tasks, achieving an impressive 14.8 times increase in inference efficiency. The introduction of DSLP, alongside ReorderNAT and LaNMT, underscores a promising direction for future research in NMT, highlighting the potential of combining reordering cues, latent variables, and deep supervision techniques to achieve high-quality, efficient machine translation.