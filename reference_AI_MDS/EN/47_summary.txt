The recent advancements in non-autoregressive neural machine translation (NAT) models have been marked by innovative solutions aimed at enhancing translation quality and significantly reducing inference time, as evidenced by multiple studies. One such study introduces LaNMT, a latent-variable NAT model that employs a deterministic inference procedure to optimize the translation process, effectively closing the performance gap with autoregressive models on specific datasets and achieving up to 12.5x speedup in decoding. This model adapts the translation length automatically and utilizes parallel decoding of initial latent variables, further improved by rescoring with a teacher model. Another study addresses the multi-modality problem in NAT by introducing a rephraser that adjusts the reference sentence to better align with the NAT output, optimizing this alignment through reinforcement learning, thereby mitigating the issue of inappropriate reference sentences and achieving translation quality comparable to autoregressive models with a 14.7x increase in inference efficiency. Adding to these innovations, another paper introduces selective knowledge distillation, employing an NAT evaluator to select NAT-friendly targets that are of high quality and easy to learn, alongside a progressive distillation method to boost NAT performance. This approach allows for a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances. Experiment results across multiple WMT language directions and several representative NAT models show that distilling only 5% of the raw translations can help an NAT outperform its counterpart trained on raw data by about 2.4 BLEU. Collectively, these studies underscore the potential of refining NAT models through mechanisms such as deterministic inference, adaptive training targets, and selective knowledge distillation, to bridge the gap with their autoregressive counterparts while significantly enhancing computational efficiency. The integration of selective knowledge distillation with existing innovations highlights a concerted effort to mitigate the limitations of NAT models, including error propagation from teacher models and the multi-modality problem, thereby paving the way for more efficient and high-quality machine translation solutions.