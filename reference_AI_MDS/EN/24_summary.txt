Federated Learning (FL) has emerged as a cutting-edge approach to privacy-preserving machine learning, enabling collaborative model training without sharing sensitive data among participants. Despite its potential, FL faces significant vulnerabilities to adversarial attacks, including sophisticated backdoor attacks where adversaries implant malicious functionalities to misclassify data under specific conditions, and adversarial examples that exploit model weaknesses. Recent advancements have focused on fortifying FL's defenses against these threats. Innovative defense mechanisms, such as adjusting the aggregation server's learning rate based on the sign information of agents' updates, have shown promise in reducing the effectiveness of backdoor attacks while maintaining model accuracy. Another notable approach, Decision Boundary based Federated Adversarial Training (DBFAT), leverages local re-weighting and global regularization to enhance both accuracy and robustness, particularly under non-IID settings. Furthermore, a novel federated backdoor attack framework, distinct from traditional methods, has been introduced. This framework involves subtly modifying a small fraction of local model weights through sign flips and optimizing the trigger pattern in tandem with the client model, rendering the attack more persistent and stealthy against current defenses. However, a recent study introduces Cerberus Poisoning (CerP), a distributed backdoor attack method that fine-tunes the collusion between malicious participants and jointly adjusts the backdoor trigger and the poisoned model changes. This method has proven capable of evading a wide array of state-of-the-art defense mechanisms in FL, highlighting a significant and severe threat to the integrity and security of federated learning systems. This paper's case study, along with extensive studies on large-scale benchmark datasets and mainstream defensive mechanisms, underscores the urgent need for developing more robust and secure FL systems. It provides a critical examination of the strengths and weaknesses of several recent federated backdoor defenses across three major categories, offering practical suggestions for practitioners in training federated models. Collectively, these studies emphasize the ongoing challenge of ensuring the security of FL systems against increasingly stealthy and sophisticated adversarial attacks, calling for continuous innovation in defense strategies to safeguard the promising future of federated learning in adversarial environments.