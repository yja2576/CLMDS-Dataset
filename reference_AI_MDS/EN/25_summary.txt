The exploration of Federated Learning (FL) systems' vulnerabilities to adversarial threats is a critical area of research, with recent studies presenting innovative defense mechanisms against such vulnerabilities. One study introduces a defense mechanism that adjusts the aggregation server's learning rate based on the sign information of agents' updates, aiming to mitigate the risk of backdoor attacks. This method has shown effectiveness in significantly reducing the accuracy of backdoor attacks without compromising the overall model accuracy. Another study proposes the Decision Boundary based Federated Adversarial Training (DBFAT), which enhances FL systems' accuracy and robustness through local re-weighting and global regularization, even in non-IID settings. These studies underscore the importance of robust defense mechanisms in countering specific adversarial threats and enhancing the understanding of FL's vulnerabilities. However, despite these advancements, a recent paper reveals a novel threat, Cerberus Poisoning (CerP), which demonstrates that by carefully tuning the collusion between malicious participants, it is possible to deliver stealthy backdoor attacks that circumvent a wide spectrum of state-of-the-art defense methods in FL. CerP achieves this by jointly tuning the backdoor trigger and controlling the poisoned model changes on each malicious participant, posing a significantly severe threat to the integrity and security of federated learning practices. This revelation highlights a critical challenge in the ongoing battle against adversarial threats in FL, emphasizing the need for continuous innovation in defense strategies to address the evolving nature of these attacks. Together, these studies paint a comprehensive picture of the current landscape of FL security, revealing both the progress made and the challenges that lie ahead in ensuring the robustness and resilience of FL systems against adversarial threats.