Both studies delve into the vulnerabilities of Federated Learning (FL), a distributed machine learning approach that allows for the collaborative training of models without sharing individual data. The first study focuses on the adversarial robustness of FL, highlighting its susceptibility to adversarial examples, akin to centrally trained models. It introduces a novel algorithm, Decision Boundary based Federated Adversarial Training (DBFAT), aimed at enhancing both the accuracy and robustness of FL systems by incorporating local re-weighting and global regularization. This approach is shown to outperform existing methods in various settings. The second study shifts the focus to backdoor attacks within FL, where the repetitive server-client communication could be exploited to mislead the global model through specific trigger patterns. It proposes a new federated backdoor attack framework that subtly modifies local model weights and optimizes the trigger pattern alongside the client model, making the attack more persistent and stealthy against current defenses. Through a case study, the paper evaluates the effectiveness of recent federated backdoor defenses, offering practical recommendations for safeguarding federated models. Together, these studies underscore the critical need for robust defense mechanisms in FL against both adversarial and backdoor attacks, proposing innovative solutions and highlighting areas for future research to ensure the security and integrity of federated learning systems.