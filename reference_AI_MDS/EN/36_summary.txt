Recent advancements in named entity recognition (NER) have led to the exploration of novel neural network architectures beyond the traditional recurrent neural network (RNN) models, such as long-short-term-memory (LSTM), which, despite their effectiveness, suffer from computational inefficiencies due to their sequential processing nature. One study introduces a convolutional neural network (CNN)-based approach, the gated relation network (GRN), which overcomes the limitations of RNNs by leveraging CNNs for local context feature extraction and employing a gating mechanism to integrate these features into a global context for label prediction, thereby enabling parallel processing and achieving state-of-the-art performance on benchmark datasets without the need for external knowledge. Another research effort addresses the challenge of unified NER, which aims to concurrently identify flat, overlapped, and discontinuous named entities with a single model. This study proposes a novel architecture, W^2NER, that conceptualizes unified NER as a word-word relation classification task, utilizing multi-granularity 2D convolutions and a co-predictor to refine word pair grid representations and accurately deduce word-word relations. This model outperforms existing methods on a variety of English and Chinese datasets, setting new benchmarks for unified NER. Both studies signify a shift towards more efficient and effective NER models by moving away from traditional sequential processing methods and towards architectures that can capture complex entity relationships and contexts, highlighting the ongoing innovation in the field of natural language processing.