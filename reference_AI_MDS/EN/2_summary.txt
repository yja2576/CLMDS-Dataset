Recent advancements in neural network methodologies have significantly enhanced the capability of machines to perform complex visual reasoning tasks, traditionally challenging for standard deep learning approaches. One innovative approach introduces the concept of Feature-wise Linear Modulation (FiLM), which employs a simple, feature-wise affine transformation to modulate neural network computation based on conditioning information, demonstrating remarkable effectiveness in visual reasoning tasks. This method notably reduces error rates on the CLEVR benchmark, showcasing its ability to modulate features coherently, maintain robustness against various modifications, and generalize effectively from limited examples. In parallel, another study explores the dynamics of reasoning by modeling visual relational reasoning as a path routing task within a structured visual graph. Utilizing reinforcement learning, this method navigates through sequences of nodes based on input sentences to infer reasoning outcomes, thereby clearly representing reasoning states and transitions. Adding to these methodologies, a recent paper proposes the integration of a split-transform-merge strategy with Visual Concept Reasoning Networks (VCRNet) to further enable reasoning between high-level visual concepts in convolutional neural networks. This approach, which associates each branch with a visual concept and updates concept states through graph-based interaction, aims to adaptively modulate the local descriptors for improved visual recognition tasks. The VCRNet model, described through split-transform-attend-interact-modulate-merge stages, demonstrates consistent performance improvements in tasks such as image classification, semantic segmentation, object detection, scene recognition, and action recognition with a minimal increase in parameters. These studies collectively underscore the importance of modeling reasoning processes and the adaptiveness in enhancing the interpretability and effectiveness of neural networks for complex visual reasoning. They present complementary methodologies that emphasize feature modulation, dynamic state transition modeling, and the integration of high-level concept reasoning, albeit through distinct approaches that cater to the nuanced demands of visual reasoning tasks.