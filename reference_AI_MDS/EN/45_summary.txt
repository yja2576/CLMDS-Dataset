Recent advancements in neural machine translation (NMT) have been pivotal in addressing the limitations of autoregressive models, which, despite offering high translation quality, are hindered by slow inference speeds due to their sequential word-by-word decoding process. Innovations such as ReorderNAT and LaNMT have made significant strides in this area. ReorderNAT improves translation quality by incorporating explicit reordering information into a non-autoregressive translation (NAT) framework, effectively narrowing the performance gap with autoregressive models while enhancing inference speed. LaNMT utilizes a latent-variable model with continuous variables and a deterministic inference procedure to dynamically adjust translation length and optimize target sequence log-probability, achieving notable quality improvements and speedups. Further building on these advancements, the DSLP model introduces deep supervision and layer-wise predictions to a non-autoregressive Transformer, consistently improving BLEU scores over base models and surpassing autoregressive models in terms of speed and efficiency. Additionally, addressing the multi-modality problem inherent in NAT models, a study introduces a rephraser that adjusts the reference sentence according to the NAT output, optimized through reinforcement learning. This method significantly improves translation quality by aligning the training target more closely with the NAT output, achieving performance comparable to autoregressive Transformers while being 14.7 times more efficient in inference. Complementing these approaches, another study introduces selective knowledge distillation, employing an NAT evaluator to select NAT-friendly targets of high quality and ease of learning, alongside a progressive distillation method to enhance NAT performance. This approach allows for a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances. Experiment results across multiple WMT language directions and several representative NAT models show that distilling only 5% of the raw translations can help an NAT outperform its counterpart trained on raw data by about 2.4 BLEU. Collectively, these approaches mark significant progress towards more practical and faster NMT systems, demonstrating the field's ongoing efforts to reconcile the challenges of maintaining translation quality while enhancing processing speed, and addressing the critical issue of error propagation from teacher models to NAT students, which has been rarely discussed in existing research.