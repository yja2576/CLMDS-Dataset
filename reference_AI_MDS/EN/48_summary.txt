Recent advancements in non-autoregressive neural machine translation (NAT) have shown promising results in accelerating inference while striving to close the quality gap with traditional autoregressive models. One innovative approach, ReorderNAT, tackles the multimodality problem inherent in NAT by incorporating reordering information into the decoding process. This method allows for both deterministic and non-deterministic decoding strategies, guiding the decoder towards more coherent translations by selecting words that align with a consistent translation path. This approach has demonstrated improved performance over existing NAT models and, in some cases, comparable quality to autoregressive models with significant speed enhancements. On another front, the application of sequence-level knowledge distillation has been refined to mitigate the propagation of errors from teacher models to NAT students, a challenge that has previously limited NAT's potential. By employing selective knowledge distillation through an NAT evaluator, this method identifies targets that are both high-quality and conducive to learning, further enhanced by a progressive distillation technique. This strategy enables a flexible balance between the quality and complexity of training data, leading to notable performance gains in NAT models across various language pairs, with evidence showing that distilling a mere 5% of raw translations can significantly outperform models trained on raw data. Together, these studies underscore the evolving landscape of NAT research, highlighting innovative solutions to longstanding challenges and paving the way for more efficient and accurate machine translation systems.