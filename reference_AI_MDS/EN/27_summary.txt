In the realm of federated learning (FL), a collaborative model training approach that prioritizes data privacy, the threat of backdoor attacks—where adversaries embed malicious functionalities to trigger misclassifications—presents a significant challenge. Research into this domain has yielded both innovative attack strategies and defense mechanisms. On one hand, a study introduces a lightweight defense mechanism that adjusts the aggregation server's learning rate based on the sign information of agents' updates, aiming to neutralize or mitigate the impact of backdoor attacks with minimal alterations to the FL protocol. This approach, supported by empirical evidence, demonstrates a notable improvement in defending against such attacks without compromising the accuracy of the trained models, alongside providing a convergence rate analysis. On the other hand, another investigation reveals the Cerberus Poisoning (CerP) method, a sophisticated distributed backdoor attack strategy that fine-tunes the collusion among malicious participants to minimize detectable discrepancies between poisoned and clean models, thereby evading a broad array of existing defense strategies. Through extensive testing across multiple large-scale datasets and against numerous defensive mechanisms, CerP exemplifies the persistent vulnerability of FL systems to well-crafted backdoor attacks. Together, these studies underscore the ongoing arms race between the development of stealthy backdoor attacks and the formulation of effective defenses within federated learning frameworks, highlighting the critical need for continuous innovation in both attack detection and mitigation techniques to safeguard the integrity and security of FL systems.