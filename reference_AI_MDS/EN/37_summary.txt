Recent advancements in natural language processing (NLP) have showcased significant progress in enhancing the efficiency and robustness of language models (LMs) through innovative training strategies and applications. On one hand, transformer-based autoregressive (AR) methods have been optimized for faster inference by introducing non-autoregressive (NAR) strategies, such as the conditional masked language model (CMLM), which, with an adaptive masking strategy, has achieved state-of-the-art results in tasks like neural machine translation, summarization, and code generation, marking a notable increase in speed. Concurrently, the refinement in the training of discriminative pre-trained language models (PrLMs) by addressing the issue of false negatives in sample construction has led to enhanced performance on benchmarks like GLUE and SQuAD, highlighting the importance of accurate sample evaluation for model robustness. Adding to these advancements, a novel investigation into the capacity of LMs to generate grounded, executable plans for embodied tasks introduces a new dimension to the field. This research, through the problem formulation named G-PlanET, explores whether LMs can possess commonsense knowledge of the physical world to input a high-level goal and environmental data, subsequently outputting a step-by-step actionable plan for a robotic agent. This approach significantly enhances LMs' ability in grounded planning by using tables to encode the environment and an iterative decoding strategy, as demonstrated by the dedicated metric, KAS, designed to assess plan quality. The integration of these studies underscores a shared objective in overcoming the inherent limitations of existing models, whether through improving inference speed, enhancing model robustness, or enabling LMs to interact with the physical world in a meaningful way, thereby pushing the boundaries of what is achievable in natural language processing and robotics.