In the realm of neural machine translation (NMT), two innovative approaches have been proposed to tackle the efficiency and quality challenges faced by traditional models such as the Transformer. The first approach introduces DSLP, a model that enhances non-autoregressive translation (NAT) by incorporating Deep Supervision and Layer-wise Predictions, aiming to retain high translation quality while significantly improving inference speed. This method has shown to outperform autoregressive models in terms of BLEU scores on multiple translation tasks, achieving up to 14.8 times faster inference. On the other hand, another study addresses the multi-modality problem inherent in NAT models—where multiple valid translations exist for a single source sentence—by introducing a rephraser mechanism. This mechanism adjusts the reference sentence to better match the NAT output, optimizing this fit using reinforcement learning to improve training effectiveness. This approach also demonstrates a substantial increase in inference efficiency, achieving speeds up to 14.7 times faster than traditional models, while maintaining comparable translation quality. Both studies underscore the potential of leveraging advanced training strategies and model architectures to enhance the speed and accuracy of machine translation, marking significant progress in the field of NMT.