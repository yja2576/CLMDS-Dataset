Both studies explore innovative strategies to enhance the performance and efficiency of language models through different approaches. The first research focuses on improving sequence-to-sequence generation tasks such as neural machine translation, summarization, and code generation by introducing an adaptive masking strategy to a conditional masked language model (CMLM), significantly boosting inference speed and performance across multiple datasets. This method not only achieves state-of-the-art results in neural machine translation but also outperforms autoregressive Transformer models in efficiency. On the other hand, the second study addresses the issue of false negatives in the training of discriminative pre-trained language models (PrLMs), which predict original texts from corrupted ones. By identifying and mitigating the impact of false negatives through enhanced pre-training methods, this research aims to improve the robustness and efficiency of PrLMs, demonstrating effectiveness on benchmarks like GLUE and SQuAD. Both pieces of research contribute to the field by optimizing language model training processes, albeit through tackling different challenges: one enhancing sequence generation capabilities and the other refining the pre-training phase for better model robustness and performance.