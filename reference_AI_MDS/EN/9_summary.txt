Recent advancements in self-supervised learning (SSL) for graph structured data have highlighted two distinct approaches: augmentation-based contrastive methods and predictive learning models. On one hand, the exploration into augmentation-free self-supervised learning frameworks, such as AFGRL, reveals the potential pitfalls of arbitrary augmentations that can significantly alter the underlying semantics of graphs, thereby affecting the performance of contrastive methods. AFGRL circumvents this issue by generating alternative graph views through nodes that share local structural information and global semantics, demonstrating superior performance in node-level tasks across various datasets. On the other hand, the development of the Wiener Graph Deconvolutional Network (WGDN) showcases the power of predictive models equipped with an augmentation-adaptive decoder, leveraging the graph wiener filter for enhanced information reconstruction. This approach not only challenges the dominance of contrastive learning in graph SSL but also proves its effectiveness through extensive experimentation. Both studies collectively underscore the importance of carefully designed learning frameworks in graph SSL, whether by avoiding detrimental augmentations or by employing powerful decoders, to achieve improved representation learning from unlabeled graph data.