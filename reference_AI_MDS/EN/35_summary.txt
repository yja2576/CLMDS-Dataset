In the realm of natural language processing (NLP), particularly within the task of named entity recognition (NER), recent studies have introduced innovative approaches to overcome the limitations of traditional methods, showcasing a trend towards more sophisticated and unified models. One such groundbreaking approach is the introduction of a novel alternative, W^2NER, which models unified NER as word-word relation classification, effectively addressing the challenges of flat, overlapped (nested), and discontinuous NER with a single model. This method, by modeling neighboring relations between entity words through Next-Neighboring-Word (NNW) and Tail-Head-Word-* (THW-*) relations and employing multi-granularity 2D convolutions, significantly outperforms current top-performing baselines across 14 widely-used benchmark datasets. This development parallels other recent advancements, such as a boundary enhanced neural span classification model that improves the detection of entities with nested structures by incorporating boundary detection to enhance span representation, and a convolutional neural network-based model, the gated relation network (GRN), which leverages CNNs for local context feature extraction and introduces a novel mechanism for modeling word relations. Both of these models aim to improve efficiency, accuracy, and the handling of complex entity structures, albeit through distinct methodological advancements. The W^2NER's unique approach to unified NER through word-word relation classification complements these innovations by offering a solution that not only tackles the inherent complexities of NER tasks but also sets a new benchmark for future research in the field. Together, these studies underscore a significant shift towards more integrated and efficient models in NER, highlighting the ongoing exploration and innovation aimed at overcoming the traditional challenges of entity recognition in NLP.