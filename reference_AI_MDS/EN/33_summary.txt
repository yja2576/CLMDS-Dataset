In the realm of natural language generation, recent studies have focused on enhancing the ability to generate sentences that not only exhibit fluency and naturalness but also adhere to specific lexical constraints. One approach introduces CGMH, a novel technique leveraging Metropolis-Hastings sampling to enable the generation of sentences that meet complex constraints, such as incorporating multiple keywords, without the need for parallel corpora for training. This method has shown promising results in tasks like keywords-to-sentence generation, unsupervised sentence paraphrasing, and error correction, outperforming previous supervised methods. Another study proposes a refined approach to lexically constrained sentence generation by employing a classifier to guide Markov Chain Monte Carlo (MCMC) sampling in editing candidate sentences. This "Predict and Revise" strategy, which fine-tunes a pre-trained model on synthetic data to obtain a reliable classifier, significantly improves sentence fluency and diversity by determining more effective editing positions and actions. Both studies underscore the potential of integrating advanced sampling techniques and machine learning models to navigate the challenges of generating constrained sentences, highlighting significant advancements in the field's ability to produce high-quality, constraint-abiding text across various applications.