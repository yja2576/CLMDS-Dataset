Recent advancements in addressing the challenges posed by long-tailed distributions in data-driven models have been notable, particularly in the fields of visual recognition and Word Sense Disambiguation (WSD). Both domains face the issue of data imbalance, where a significant number of classes or senses are underrepresented, making accurate recognition and disambiguation difficult. In visual recognition, progress has been achieved through both complex paradigms, such as meta-learning, and simpler, yet effective refinements to training procedures, including adjustments in data distribution and loss functions. A novel approach in this area also introduces a data augmentation technique based on class activation maps, specifically tailored for long-tailed recognition, which, when combined with re-sampling methods, yields superior results on benchmark datasets. On the other hand, the WSD task benefits from a novel representation method inspired by quantum mechanics' superposition states, designed to reduce reliance on large sample sizes and improve the representation of Long-Tail Senses (LTSs). This method, operating in Hilbert space, demonstrates its effectiveness by achieving state-of-the-art performance in standard WSD evaluations and showing promise in cross-lingual datasets. Both studies underscore the importance of innovative approaches to mitigate the effects of data imbalance, highlighting the potential of both complex and straightforward strategies to enhance model performance across different applications.