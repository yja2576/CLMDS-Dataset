近期在非自回归神经机器翻译（NAT）模型方面的进展，通过多项研究证明，已经通过创新解决方案标志性地提高了翻译质量并显著减少了推理时间。其中一项研究介绍了LaNMT，这是一个潜变量NAT模型，它采用确定性推理程序来优化翻译过程，有效地缩小了与自回归模型在特定数据集上的性能差距，并实现了高达12.5倍的解码加速。该模型自动调整翻译长度，并利用初始潜变量的并行解码，通过使用教师模型重新评分进一步改进。另一项研究通过引入一个重述器来解决NAT的多模态问题，该重述器调整参考句子以更好地与NAT输出对齐，通过强化学习优化这种对齐，从而缓解了不适当参考句子的问题，并实现了与自回归模型相当的翻译质量，同时推理效率提高了14.7倍。除了这些创新之外，另一篇论文引入了选择性知识蒸馏，采用NAT评估器选择高质量且易于学习的NAT友好目标，以及一种渐进式蒸馏方法来提升NAT性能。这种方法允许在NAT模型的训练数据质量和复杂性之间灵活权衡，实现了强大的性能。跨多个WMT语言方向和几个代表性NAT模型的实验结果显示，只蒸馏5%的原始翻译就可以使NAT模型的表现超过使用原始数据训练的对应模型约2.4 BLEU。这些研究共同强调了通过确定性推理、适应性训练目标和选择性知识蒸馏等机制精炼NAT模型的潜力，以弥补与自回归对应模型之间的差距，同时显著提高计算效率。将选择性知识蒸馏与现有创新整合，突出了一种共同努力，以减轻NAT模型的限制，包括来自教师模型的错误传播和多模态问题，从而为更高效、高质量的机器翻译解决方案铺平了道路。