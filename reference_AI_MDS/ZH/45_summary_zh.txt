近期在神经机器翻译（NMT）领域的进展对于解决自回归模型的局限性起到了关键作用。尽管自回归模型提供了高质量的翻译，但由于其逐字解码的顺序过程，推理速度缓慢。像ReorderNAT和LaNMT这样的创新在这一领域取得了重大进展。ReorderNAT通过将显式重排序信息纳入非自回归翻译（NAT）框架来提高翻译质量，有效缩小了与自回归模型的性能差距，同时提高了推理速度。LaNMT利用带有连续变量的潜变量模型和确定性推理过程来动态调整翻译长度和优化目标序列对数概率，实现了显著的质量改进和加速。在这些进展的基础上，DSLP模型将深度监督和逐层预测引入到非自回归Transformer中，一致地提高了基础模型的BLEU分数，并在速度和效率方面超越了自回归模型。此外，针对NAT模型固有的多模态问题，一项研究引入了一个重述器，根据NAT输出调整参考句子，通过强化学习进行优化。这种方法通过使训练目标与NAT输出更加紧密对齐，显著提高了翻译质量，达到了与自回归Transformer相当的性能，同时在推理上的效率提高了14.7倍。作为这些方法的补充，另一项研究引入了选择性知识蒸馏，采用NAT评估器选择高质量且易于学习的NAT友好目标，以及一种逐步蒸馏方法来提升NAT性能。这种方法允许在NAT模型的训练数据质量和复杂性之间进行灵活的权衡，取得了强大的性能。在多个WMT语言方向和几个代表性NAT模型上的实验结果显示，仅蒸馏5%的原始翻译就可以使NAT的表现超过其使用原始数据训练的对应模型约2.4 BLEU。这些方法的集体进展标志着向更实用、更快的NMT系统迈进了一大步，展示了该领域持续努力解决在提高处理速度的同时保持翻译质量的挑战，并且解决了从教师模型到NAT学生的错误传播这一在现有研究中很少讨论的