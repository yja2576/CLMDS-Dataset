这两项研究深入探讨了联邦学习（FL）的脆弱性，联邦学习是一种分布式机器学习方法，允许在不共享个人数据的情况下协作训练模型。第一项研究关注FL的对抗鲁棒性，强调其对抗性示例的敏感性，类似于中心训练模型。它引入了一种新颖的算法，即基于决策边界的联邦对抗训练（DBFAT），旨在通过结合本地重新加权和全局正则化来提高FL系统的准确性和鲁棒性。这种方法被证明在各种设置中优于现有方法。第二项研究将焦点转移到FL中的后门攻击，其中重复的服务器-客户端通信可能被利用来通过特定触发模式误导全局模型。它提出了一个新的联邦后门攻击框架，该框架微妙地修改本地模型权重，并优化触发模式与客户端模型，使攻击更持久和隐蔽，以对抗当前的防御措施。通过一个案例研究，该论文评估了最近联邦后门防御的有效性，为保护联邦模型提供了实用的建议。这些研究共同强调了FL中针对对抗性和后门攻击的强大防御机制的关键需求，提出了创新的解决方案，并突出了未来研究的领域，以确保联邦学习系统的安全性和完整性。