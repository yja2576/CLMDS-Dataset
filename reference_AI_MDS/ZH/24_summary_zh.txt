联邦学习（FL）已成为一种尖端的隐私保护机器学习方法，它使得参与者能够在不共享敏感数据的情况下协作训练模型。尽管FL具有巨大潜力，但它面临着对抗性攻击的重大漏洞，包括复杂的后门攻击，其中对手植入恶意功能以在特定条件下误分类数据，以及利用模型弱点的对抗性示例。最近的进展集中在加强FL对这些威胁的防御上。创新的防御机制，例如根据代理更新的符号信息调整聚合服务器的学习率，已显示出在减少后门攻击的有效性的同时保持模型准确性的前景。另一种值得注意的方法，基于决策边界的联邦对抗训练（DBFAT），利用局部重新加权和全局正则化来提高准确性和鲁棒性，特别是在非独立同分布（non-IID）设置下。此外，引入了一种与传统方法不同的新型联邦后门攻击框架。该框架通过微妙地修改一小部分本地模型权重的符号并与客户端模型一起优化触发模式，使得攻击对当前防御更持久和隐蔽。然而，最近的一项研究介绍了Cerberus Poisoning（CerP），一种分布式后门攻击方法，它对恶意参与者之间的勾结进行了微调，并共同调整了后门触发器和被污染模型的变化。这种方法已被证明能够规避FL中广泛的最先进防御机制，突显了对联邦学习系统的完整性和安全性构成的重大且严重威胁。本文的案例研究，连同对大规模基准数据集和主流防御机制的广泛研究，强调了开发更加鲁棒和安全的FL系统的迫切需要。它对几种最近的联邦后门防御的优势和劣势进行了关键性的审查，涵盖了三个主要类别，为从业者在训练联邦模型时提供了实用建议。这些研究共同强调了确保FL系统安全免受日益隐蔽和复杂的对抗性攻击的持续挑战，呼吁在防御策略上不断创新，以保护联邦学习在对抗性环境中的