在神经机器翻译（NMT）不断发展的领域中，为了缩小非自回归（NAT）与自回归（AT）翻译模型之间的性能差距，同时提高推理速度，已经取得了重大进展。ReorderNAT框架和LaNMT模型处于这些创新的前沿。ReorderNAT通过将重排序信息整合到解码过程中，有效地解决了NAT中的多模态问题，已被证明能将翻译质量提升至与AT模型在各种数据集上相当的水平。同样，LaNMT利用了一个带有确定性推理程序的潜变量机制，不仅能在推理时调整翻译长度，还优化了目标序列对数概率的下界，实现了高达12.5倍的解码速度提升，同时紧密匹配AT模型的性能。基于这些进展，本文介绍了DSLP模型，该模型进一步提炼了机器翻译的效率和性能。DSLP是一个非自回归Transformer模型，融入了深度监督和额外的逐层预测，为该领域设定了新的基准。在四个翻译任务（WMT'14 EN-DE和WMT'16 EN-RO的双向）上进行的广泛实验表明，DSLP不仅一致地提高了与基础模型相比的BLEU分数，而且在三个翻译任务上超越了自回归模型，实现了令人印象深刻的14.8倍推理效率提升。DSLP的引入，连同ReorderNAT和LaNMT，为NMT未来的研究指明了一个充满希望的方向，强调了结合重排序线索、潜变量和深度监督技术以实现高质量、高效机器翻译的潜力。