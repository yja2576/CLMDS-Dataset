这两项研究通过不同的方法探索了提高语言模型性能和效率的创新策略。第一项研究聚焦于通过向条件遮蔽语言模型（CMLM）引入一种自适应遮蔽策略，来改善序列到序列生成任务（如神经机器翻译、摘要生成和代码生成）的表现，显著提升了多个数据集上的推理速度和性能。这种方法不仅在神经机器翻译中达到了最先进的结果，而且在效率上也超过了自回归Transformer模型。另一方面，第二项研究解决了在训练判别式预训练语言模型（PrLMs）时出现的假阴性问题，这些模型通过从损坏的文本中预测原始文本。通过识别和减轻假阴性的影响，采用增强的预训练方法，这项研究旨在提高PrLMs的鲁棒性和效率，在GLUE和SQuAD等基准测试中展示了其有效性。这两项研究通过优化语言模型训练过程为该领域做出了贡献，尽管它们解决的是不同的挑战：一项是增强序列生成能力，另一项是改进预训练阶段，以获得更好的模型鲁棒性和性能。